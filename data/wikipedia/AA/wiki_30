{"id": "4071", "revid": "39569616", "url": "https://en.wikipedia.org/wiki?curid=4071", "title": "Bronski Beat", "text": "Bronski Beat were a British synthpop trio which achieved success in the mid-1980s, particularly with the 1984 chart hit \"Smalltown Boy\", from their debut album \"The Age of Consent\". \"Smalltown Boy\" was their only US \"Billboard\" Hot 100 single. All members of the band were openly gay and their songs reflected this, often containing political commentary on gay-related issues. The initial line-up, which recorded the majority of the band's hits, consisted of Jimmy Somerville (vocals), Steve Bronski (born Steven William Forrest, keyboards, percussion) and Larry Steinbachek (keyboards, percussion).\nSomerville left Bronski Beat in 1985, and went on to have success as lead singer of the Communards and as a solo artist. He was replaced by vocalist John Foster, with whom the band continued to have hits in the UK and Europe through 1986. Foster left Bronski Beat after their second album, and the band used a series of vocalists before dissolving in 1995.\nLarry Steinbachek died in 2016.\nSteve Bronski revived the band in 2016, recording new material with 1990s member Ian Donaldson.\nHistory.\n1983\u201385: Early years and \"The Age of Consent\".\nBronski Beat formed in 1983 when Jimmy Somerville, Steve Bronski (both from Glasgow) and Larry Steinbachek (from Southend) shared a three-bedroom flat at Lancaster House in Brixton, London. Steinbachek had heard Somerville singing during the making of \"\" and suggested they make some music. They first performed publicly at an arts festival, \"September in the Pink\". The trio were unhappy with the inoffensive nature of contemporary gay performers and sought to be more outspoken and political.\nBronski Beat signed a recording contract with London Records in 1984 after doing only nine live gigs. The band's debut single, \"Smalltown Boy\", about a gay teenager leaving his family and fleeing his home town, was a hit, peaking at No 3 in the UK Singles Chart, and topping charts in Belgium and the Netherlands. The single was accompanied by a promotional video directed by Bernard Rose, showing Somerville trying to befriend an attractive diver at a swimming pool, then being attacked by the diver's homophobic associates, being returned to his family by the police and having to leave home. (The police officer was played by Colin Bell, then the marketing manager of London Records). \"Smalltown Boy\" reached 48 in the U.S. chart and peaked at 8 in Australia.\nThe follow-up single, \"Why?\", adopted a Hi-NRG sound and was more lyrically focused on anti-gay prejudice. It also achieved Top 10 status in the UK, reaching 6, and was another Top 10 hit for the band in Australia, Switzerland, Germany, France and the Netherlands.\nAt the end of 1984, the trio released an album titled \"The Age of Consent\". The inner sleeve listed the varying ages of consent for consensual gay sex in different nations around the world. At the time, the age of consent for sexual acts between men in the UK was 21 compared with 16 for heterosexual acts, with several other countries having more liberal laws on gay sex. The album peaked at 4 in the UK Albums Chart, 36 in the U.S., and 12 in Australia.\nAround the same time, the band headlined \"Pits and Perverts\", a concert at the Electric Ballroom in London to raise funds for the Lesbians and Gays Support the Miners campaign. This event is featured in the film \"Pride\".\nThe third single, released before Christmas 1984, was a revival of \"It Ain't Necessarily So\", the George and Ira Gershwin classic (from \"Porgy and Bess\"). The song questions the accuracy of biblical tales. It also reached the UK Top 20.\nIn 1985, the trio joined up with Marc Almond to record a version of Donna Summer's \"I Feel Love\". The full version was actually a medley that also incorporated snippets of Summer's \"Love to Love You Baby\" and John Leyton's \"Johnny Remember Me\". It was a big success, reaching 3 in the UK and equalling the chart achievement of \"Smalltown Boy\". Although the original had been one of Marc Almond's all-time favourite songs, he had never read the lyrics and thus incorrectly sang \"What\u2019ll it be, what\u2019ll it be, you and me\" instead of \"Falling free, falling free, falling free\" on the finished record.\nThe band and their producer Mike Thorne had gone back into the studio in early 1985 to record a new single, \"Run From Love\", and PolyGram (London Records' parent company at that time) had pressed a number of promo singles and 12\" versions of the song and sent them to radio and record stores in the UK. However, the single was shelved as tensions in the band, both personal and political, resulted in Somerville leaving Bronski Beat in the summer of that year.\n\"Run From Love\" was subsequently released in a remix form on the Bronski Beat album \"Hundreds &amp; Thousands\", a collection of mostly remixes (LP) and B-sides (as bonus tracks on the CD version) as well as the hit \"I Feel Love\". Somerville went on to form The Communards with Richard Coles while the remaining members of Bronski Beat searched for a new vocalist.\n1985\u20131995: Post-Jimmy Somerville phase.\nBronski Beat recruited John Foster as Somerville's replacement (Foster is credited as \"Jon Jon\"). A single, \"Hit That Perfect Beat\", was released in November 1985, reaching 3 in the UK. It repeated this success on the Australian chart and was also featured in the film \"Letter to Brezhnev\". A second single, \"C'mon C'mon\", also charted in the UK Top 20 and an album, \"Truthdare Doubledare\", released in May 1986, peaked at 18. The film \"Parting Glances\" (1986) included Bronski Beat songs \"Love and Money\", \"Smalltown Boy\" and \"Why?\". During this period, the band teamed up with producer Mark Cunningham on the first-ever BBC Children In Need single, a cover of David Bowie's \"Heroes\", released in 1986 under the name of The County Line.\nFoster left the band in 1987. Following Foster's departure, Bronski Beat began work on their next album, \"Out and About\". The tracks were recorded at Berry Street studios in London with engineer Brian Pugsley. Some of the song titles were \"The Final Spin\" and \"Peace And Love\". The latter track featured Strawberry Switchblade vocalist Rose McDowall and appeared on several internet sites in 2006. One of the other songs from the project called \"European Boy\" was recorded in 1987 by disco group Splash. The lead singer of Splash was former Tight Fit singer Steve Grant. Steinbachek and Bronski toured extensively with the new material with positive reviews, however the project was abandoned as the group was dropped by London Records. Also in 1987, Bronski Beat and Somerville performed at a reunion concert for \"International AIDS Day\", supported by New Order, at the Brixton Academy, London.\nIn 1989, Jonathan Hellyer became lead singer, and the band extensively toured the U.S. and Europe with back-up vocalist Annie Conway. They achieved one minor hit with the song \"Cha Cha Heels\", a one-off collaboration sung by American actress and singer Eartha Kitt, which peaked at 32 in the UK. The song was originally written for movie and recording star Divine, who was unable to record the song before his death in 1988. 1990\u201391 saw Bronski Beat release three further singles on the Zomba record label, \"I'm Gonna Run Away\", \"One More Chance\" and \"What More Can I Say\". The singles were produced by Mike Thorne.\nFoster and Bronski Beat teamed up again in 1994, and released a techno \"Tell Me Why '94\" and an acoustic \"Smalltown Boy '94\" on the German record label, ZYX Music. The album \"Rainbow Nation\" was released the following year with Hellyer returning as lead vocalist, as Foster had dropped out of the project and Ian Donaldson was brought on board to do keyboards and programming. After a few years of touring, Bronski Beat then dissolved, with Steve Bronski going on to become a producer for other artists and Ian Donaldson becoming a successful DJ (Sordid Soundz). Larry Steinbachek became the musical director for Michael Laub's theatre company, 'Remote Control Productions'.\n2007\u2013Present: Steve Bronski solo activities and new version of Bronski Beat.\nIn 2007, Steve Bronski remixed the song \"Stranger to None\" by the UK alternative rock band, All Living Fear. Four different mixes were made, with one appearing on their retrospective album, \"Fifteen Years After\". Bronski also remixed the track \"Flowers in the Morning\" by Northern Irish electronic band Electrobronze in 2007, changing the style of the song from classical to Hi-NRG disco.\nIn 2015, Steve Bronski teamed up as a one-off with Jessica James (aka Barbara Bush) and said that she reminded him of Divine, because of her look and Eartha Kitt-like sound. The one-off project was to cover the track he made in 1989.\nIn 2016, Steve Bronski again teamed up with Ian Donaldson, with the aim of bringing Bronski Beat back, enlisting a new singer, Stephen Granville. In 2017, the new Bronski Beat released a reworked version of \"Age of Consent\" entitled \"Age of Reason\".\n\"Out &amp; About\", the unreleased Bronski Beat album from 1987, was released digitally via Steve Bronski's website. The album features the original tracks plus remixes by Bronski.\nOn 12 January 2017, Larry Steinbachek's sister Louise Jones told BBC News he had died the previous month after a short battle with cancer, with his family and friends at his bedside. He was 56.\nMembers.\nThe original member set of Bronski Beat was Jimmy Somerville, Steve Bronski and Larry Steinbachek. Following Somerville leaving to form pop group The Communards with Richard Coles, he was replaced by John Foster and later by Jonathan Hellyer. The band set-up has seen a number of changes.\nAwards and nominations.\n! Year !! Awards !! Work !! Category !! Result"}
{"id": "4074", "revid": "35358178", "url": "https://en.wikipedia.org/wiki?curid=4074", "title": "Barrel (disambiguation)", "text": "A barrel is a cylindrical container, traditionally made with wooden material.\nBarrel may also refer to:"}
{"id": "4077", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=4077", "title": "Binary prefix", "text": "A binary prefix is a unit prefix for multiples of units in data processing, data transmission, and digital information, notably the bit and the byte, to indicate multiplication by a power of 2.\nThe computer industry has historically used the units \"kilobyte\", \"megabyte\", and \"gigabyte\", and the corresponding symbols KB, MB, and GB, in at least two slightly different measurement systems. In citations of main memory (RAM) capacity, \"gigabyte\" customarily means bytes. As this is a power of 1024, and 1024 is a power of two (210), this usage is referred to as a binary measurement.\nIn most other contexts, the industry uses the multipliers \"kilo\", \"mega\", \"giga\", etc., in a manner consistent with their meaning in the International System of Units (SI), namely as powers of 1000. For example, a 500 gigabyte hard disk holds bytes, and a 1 Gbit/s (gigabit per second) Ethernet connection transfers data at nominal speed of bit/s. In contrast with the \"binary prefix\" usage, this use is described as a \"decimal prefix\", as 1000 is a power of 10 (103).\nThe use of the same unit prefixes with two different meanings has caused confusion. Starting around 1998, the International Electrotechnical Commission (IEC) and several other standards and trade organizations addressed the ambiguity by publishing standards and recommendations for a set of binary prefixes that refer exclusively to powers of 1024. Accordingly, the US National Institute of Standards and Technology (NIST) requires that SI prefixes only be used in the decimal sense: kilobyte and megabyte denote one thousand bytes and one million bytes respectively (consistent with SI), while new terms such as kibibyte, mebibyte and gibibyte, having the symbols KiB, MiB, and GiB, denote 1024 bytes, bytes, and bytes, respectively. In 2008, the IEC prefixes were incorporated into the international standard system of units used alongside the International System of Quantities (see ISO/IEC 80000).\nHistory.\nMain memory.\nEarly computers used one of two addressing methods to access the system memory; binary (base 2) or decimal (base 10).\nFor example, the IBM 701 (1952) used binary and could address 2048 words of 36 bits each, while the IBM 702 (1953) used decimal and could address ten thousand 7-bit words.\nBy the mid-1960s, binary addressing had become the standard architecture in most computer designs, and main memory sizes were most commonly powers of two. This is the most natural configuration for memory, as all combinations of their address lines map to a valid address, allowing easy aggregation into a larger block of memory with contiguous addresses.\nEarly computer system documentation would specify the memory size with an exact number such as 4096, 8192, or 16384 words of storage. These are all powers of two, and furthermore are small multiples of 210, or 1024. As storage capacities increased, several different methods were developed to abbreviate these quantities.\nThe method most commonly used today uses prefixes such as kilo, mega, giga, and corresponding symbols K, M, and G, which the computer industry originally adopted from the metric system. The prefixes \"kilo-\" and \"mega-\", meaning 1000 and respectively, were commonly used in the electronics industry before World War II.\nAlong with \"giga-\" or G-, meaning , they are now known as SI prefixes after the International System of Units (SI), introduced in 1960 to formalize aspects of the metric system.\nThe International System of Units does not define units for digital information but notes that the SI prefixes may be applied outside the contexts where base units or derived units would be used. But as computer main memory in a\nbinary-addressed system is manufactured in sizes that were easily expressed as multiples of 1024, \"kilobyte\", when applied to computer memory, came to be used to mean 1024 bytes instead of 1000. This usage is not consistent with the SI. Compliance with the SI requires that the prefixes take their 1000-based meaning, and that they are not to be used as placeholders for other numbers, like 1024.\nThe use of K in the binary sense as in a \"32K core\" meaning words, i.e., words, can be found as early as 1959.\nGene Amdahl's seminal 1964 article on IBM System/360 used \"1K\" to mean 1024.\nThis style was used by other computer vendors, the CDC 7600 \"System Description\" (1968) made extensive use of K as 1024.\nThus the first binary prefix was born.\nAnother style was to truncate the last three digits and append K, essentially using K as a decimal prefix similar to SI, but always truncating to the next lower whole number instead of rounding to the nearest. The exact values words, words and words would then be described as \"32K\", \"65K\" and \"131K\".\nThis style was used from about 1965 to 1975.\nThese two styles (K = 1024 and truncation) were used loosely around the same time, sometimes by the same company. In discussions of binary-addressed memories, the exact size was evident from context. (For memory sizes of \"41K\" and below, there is no difference between the two styles.) The HP 21MX real-time computer (1974) denoted (which is 192\u00d71024) as \"196K\" and as \"1M\",\nwhile the HP 3000 business computer (1973) could have \"64K\", \"96K\", or \"128K\" bytes of memory.\nThe \"truncation\" method gradually waned. Capitalization of the letter K became the \"de facto\" standard for binary notation, although this could not be extended to higher powers, and use of the lowercase k did persist. Nevertheless, the practice of using the SI-inspired \"kilo\" to indicate 1024 was later extended to \"megabyte\" meaning 10242 () bytes, and later \"gigabyte\" for 10243 () bytes. For example, a \"512 megabyte\" RAM module is 512\u00d710242 bytes (512 \u00d7 , or ), rather than .\nThe symbols Kbit, Kbyte, Mbit and Mbyte started to be used as \"binary units\"\u2014\"bit\" or \"byte\" with a multiplier that is a power of 1024\u2014in the early 1970s.\nFor a time, memory capacities were often expressed in K, even when M could have been used: The IBM System/370 Model 158 brochure (1972) had the following: \"Real storage capacity is available in 512K increments ranging from 512K to 2,048K bytes.\"\nMegabyte was used to describe the 22-bit addressing of DEC PDP-11/70 (1975)\nand gigabyte the 30-bit addressing DEC VAX-11/780 (1977).\nIn 1998, the International Electrotechnical Commission IEC introduced the binary prefixes kibi, mebi, gibi ... to mean 1024, 10242, 10243 etc., so that 1048576 bytes could be referred to unambiguously as 1 mebibyte. The IEC prefixes were defined for use alongside the International System of Quantities (ISQ) in 2009.\nDisk drives.\nThe disk drive industry has followed a different pattern. Disk drive capacity is generally specified with unit prefixes with decimal meaning, in accordance to SI practices. Unlike computer main memory, disk architecture or construction does not mandate or make it convenient to use binary multiples. Drives can have any practical number of platters or surfaces, and the count of tracks, as well as the count of sectors per track may vary greatly between designs.\nThe first commercially sold disk drive, the IBM 350, had fifty physical disk platters containing a total of 50,000 sectors of 100 characters each, for a total quoted capacity of 5 million characters. It was introduced in September 1956.\nIn the 1960s most disk drives used IBM's variable block length format, called Count Key Data (CKD).\nAny block size could be specified up to the maximum track length. Since the block headers occupied space, the usable capacity of the drive was dependent on the block size. Blocks (\"records\" in IBM's terminology) of 88, 96, 880 and 960 were often used because they related to the fixed block size of 80- and 96-character punch cards. The drive capacity was usually stated under conditions of full track record blocking. For example, the 100-megabyte 3336 disk pack only achieved that capacity with a full track block size of 13,030 bytes.\nFloppy disks for the IBM PC and compatibles quickly standardized on 512-byte sectors, so two sectors were easily referred to as \"1K\". The 3.5-inch \"360\u00a0KB\" and \"720\u00a0KB\" had 720 (single-sided) and 1440 sectors (double-sided) respectively. When the High Density \"1.44\u00a0MB\" floppies came along, with 2880 of these 512-byte sectors, that terminology represented a hybrid binary-decimal definition of \"1\u00a0MB\" = 210 \u00d7 103 = 1\u2009024\u2009000 bytes.\nIn contrast, hard disk drive manufacturers used \"megabytes\" or \"MB\", meaning 106 bytes, to characterize their products as early as 1974. By 1977, in its first edition, Disk/Trend, a leading hard disk drive industry marketing consultancy segmented the industry according to MBs (decimal sense) of capacity.\nOne of the earliest hard disk drives in personal computing history, the Seagate ST-412, was specified as \"Formatted: 10.0 Megabytes\". The drive contains four heads and active surfaces (tracks per cylinder), 306 cylinders. When formatted with a sector size of 256 bytes and 32 sectors/track it has a capacity of . This drive was one of several types installed in the IBM PC/XT and extensively advertised and reported as a \"10\u00a0MB\" (formatted) hard disk drive.\nThe cylinder count of 306 is not conveniently close to any power of 1024; operating systems and programs using the customary binary prefixes show this as 9.5625\u00a0MB. Many later drives in the personal computer market used 17 sectors per track; still later, zone bit recording was introduced, causing the number of sectors per track to vary from the outer track to the inner.\nThe hard drive industry continues to use decimal prefixes for drive capacity, as well as for transfer rate. For example, a \"300\u00a0GB\" hard drive offers slightly more than , or , bytes, not (which would be about ). Operating systems such as Microsoft Windows that display hard drive sizes using the customary binary prefix \"GB\" (as it is used for RAM) would display this as \"279.4\u00a0GB\" (meaning bytes, or ). On the other hand, macOS has since version 10.6 shown hard drive size using decimal prefixes (thus matching the drive makers' packaging). (Previous versions of Mac OS X used binary prefixes.)\nHowever, other usages still occur. Seagate has specified data transfer rates in select manuals of some hard drives in \"both\" IEC and decimal units.\n\"Advanced Format\" drives using 4096-byte sectors are described as having \"4K sectors.\"\nInformation transfer and clock rates.\nComputer clock frequencies are always quoted using SI prefixes in their decimal sense. For example, the internal clock frequency of the original IBM PC was 4.77\u00a0MHz, that is .\nSimilarly, digital information transfer rates are quoted using decimal prefixes:\nStandardization of dual definitions.\nBy the mid-1970s it was common to see K meaning 1024 and the occasional M meaning for words or bytes of main memory (RAM) while K and M were commonly used with their decimal meaning for disk storage. In the 1980s, as capacities of both types of devices increased, the SI prefix G, with SI meaning, was commonly applied to disk storage, while M in its binary meaning, became common for computer memory. In the 1990s, the prefix G, in its binary meaning, became commonly used for computer memory capacity. The first terabyte (SI prefix, bytes) hard disk drive was introduced in 2007.\nThe dual usage of the kilo (K), mega (M), and giga (G) prefixes as both powers of 1000 and powers of 1024 has been recorded in standards and dictionaries. For example, the 1986 ANSI/IEEE Std 1084-1986\ndefined dual uses for kilo and mega.\n The binary units Kbyte and Mbyte were formally defined in ANSI/IEEE Std 1212\u20131991.\nMany dictionaries have noted the practice of using traditional prefixes to indicate binary multiples.\nOxford online dictionary defines, for example, megabyte as: \"Computing: a unit of information equal to one million or (strictly) bytes.\"\nThe units Kbyte, Mbyte, and Gbyte are found in the trade press and in IEEE journals. Gigabyte was formally defined in IEEE Std 610.10-1994 as either or 230 bytes.\nKilobyte, Kbyte, and KB are equivalent units and all are defined in the obsolete standard, IEEE 100\u20132000.\nThe hardware industry measures system memory (RAM) using the binary meaning while magnetic disk storage uses the SI definition. However, many exceptions exist. Labeling of diskettes uses the megabyte to denote 1024\u00d71000 bytes. In the optical disks market, compact discs use \"MB\" to mean 10242 bytes while DVDs use \"GB\" to mean 10003 bytes.\nInconsistent use of units.\nDeviation between powers of 1024 and powers of 1000.\nComputer storage has become cheaper per unit and thereby larger, by many orders of magnitude since \"K\" was first used to mean 1024.\nBecause both the SI and \"binary\" meanings of kilo, mega, etc., are based on powers of 1000 or 1024 rather than simple multiples, the difference between 1M \"binary\" and 1M \"decimal\" is proportionally larger than that between 1K \"binary\" and 1k \"decimal,\" and so on up the scale.\nThe relative difference between the values in the binary and decimal interpretations increases, when using the SI prefixes as the base, from 2.4% for kilo to nearly 21% for the yotta prefix.\nConsumer confusion.\nIn the early days of computers (roughly, prior to the advent of personal computers) there was little or no consumer confusion because of the technical sophistication of the buyers and their familiarity with the products. In addition, it was common for computer manufacturers to specify their products with capacities in full precision.\nIn the personal computing era, one source of consumer confusion is the difference in the way many operating systems display hard drive sizes, compared to the way hard drive manufacturers describe them. Hard drives are specified and sold using \"GB\" and \"TB\" in their decimal meaning: one billion and one trillion bytes. Many operating systems and other software, however, display hard drive and file sizes using \"MB\", \"GB\" or other SI-looking prefixes in their binary sense, just as they do for displays of RAM capacity. For example, many such systems display a hard drive marketed as \"1\u00a0TB\" as \"931\u00a0GB\". The earliest known presentation of hard disk drive capacity by an operating system using \"KB\" or \"MB\" in a binary sense is 1984; earlier operating systems generally presented the hard disk drive capacity as an exact number of bytes, with no prefix of any sort, for example, in the output of the MS-DOS or PC\u00a0DOS CHKDSK command.\nLegal disputes.\nThe different interpretations of disk size prefixes has led to class action lawsuits against digital storage manufacturers.\nThese cases involved both flash memory and hard disk drives.\nRecent cases.\nThe most recent cases (2019+) did not settle and are currently on appeal. Notably, the defendant persuaded the district court of the Northern District of California to enter judgment in its favor by citing to a publication from the National Institute of Technology from 1998, which was published at a time that USB Drives did not exist and memory storage in gigabytes was not commercially feasible for the average consumer. However, the 1998 NIST publication was superseded in a 2008 NIST publication. The superseding publication does not maintain the same positions regarding the definition of gigabyte and megabyte as the 1998 publication. Additionally, NIST's 2008 Guide for the Use of the International System of Units (SI) makes clear that confusion of use of units is to be avoided, even if traditional units of must be used. Thus, the litigation has not ended in favor of the manufacturers and will not end until the appeals conclude, along with any other suits that may be filed.\nEarly cases.\nEarlier cases (2004-2007) were settled prior to any court ruling with the manufacturers admitting no wrongdoing but agreeing to clarify the storage capacity of their products on the consumer packaging.\nAccordingly, many flash memory and hard disk manufacturers have disclosures on their packaging and web sites clarifying the formatted capacity of the devices\nor defining MB as 1 million bytes and 1\u00a0GB as 1 billion bytes.\nWillem Vroegh v. Eastman Kodak Company.\nOn 20 February 2004, Willem Vroegh filed a lawsuit against Lexar Media, Dane\u2013Elec Memory, Fuji Photo Film USA, Eastman Kodak Company, Kingston Technology Company, Inc., Memorex Products, Inc.; PNY Technologies Inc., SanDisk Corporation, Verbatim Corporation, and Viking Interworks alleging that their descriptions of the capacity of their flash memory cards were false and misleading.\nVroegh claimed that a 256\u00a0MB Flash Memory Device had only 244\u00a0MB of accessible memory. \"Plaintiffs allege that Defendants marketed the memory capacity of their products by assuming that one megabyte equals one million bytes and one gigabyte equals one billion bytes.\"\nThe plaintiffs wanted the defendants to use the traditional values of 10242 for megabyte and 10243 for gigabyte.\nThe plaintiffs acknowledged that the IEC and IEEE standards define a MB as one million bytes but stated that the industry has largely ignored the IEC standards.\nThe parties agreed that manufacturers could continue to use the decimal definition so long as the definition was added to the packaging and web sites. The consumers could apply for \"a discount of ten percent off a future online purchase from Defendants' Online Stores Flash Memory Device\".\nOrin Safier v. Western Digital Corporation.\nOn 7 July 2005, an action entitled \"Orin Safier v. Western Digital Corporation, et al.\" was filed in the Superior Court for the City and County of San Francisco, Case No. CGC-05-442812.\nThe case was subsequently moved to the Northern District of California, Case No. 05-03353 BZ.\nAlthough Western Digital maintained that their usage of units is consistent with \"the indisputably correct industry standard for measuring and describing storage capacity\", and that they \"cannot be expected to reform the software industry\", they agreed to settle in March 2006 with 14 June 2006 as the Final Approval hearing date.\nWestern Digital offered to compensate customers with a free download of backup and recovery software valued at US$30. They also paid $500,000 in fees and expenses to San Francisco lawyers Adam Gutride and Seth Safier, who filed the suit.\nThe settlement called for Western Digital to add a disclaimer to their later packaging and advertising.\nCho v. Seagate Technology (US) Holdings, Inc..\nA lawsuit (Cho v. Seagate Technology (US) Holdings, Inc., San Francisco Superior Court, Case No. CGC-06-453195) was filed against Seagate Technology, alleging that Seagate overrepresented the amount of usable storage by 7% on hard drives sold between 22 March 2001 and 26 September 2007. The case was settled without Seagate admitting wrongdoing, but agreeing to supply those purchasers with free backup software or a 5% refund on the cost of the drives.\nUnique binary prefixes.\nEarly suggestions.\nWhile early computer scientists typically used k to mean 1000, some recognized the convenience that would result from working with multiples of 1024 and the confusion that resulted from using the same prefixes for two different meanings.\nSeveral proposals for unique binary prefixes were made in 1968. Donald Morrison proposed to use the Greek letter kappa (\u03ba) to denote 1024, \u03ba2 to denote 10242, and so on.\nWallace Givens responded with a proposal to use bK as an abbreviation for 1024 and bK2 or bK2 for 10242, though he noted that neither the Greek letter nor lowercase letter b would be easy to reproduce on computer printers of the day.\nBruce Alan Martin of Brookhaven National Laboratory further proposed that the prefixes be abandoned altogether, and the letter B be used for base-2 exponents, similar to E in decimal scientific notation, to create shorthands like 3B20 for 3\u00d7220, a convention still used on some calculators to present binary floating point-numbers today.\nNone of these gained much acceptance, and capitalization of the letter K became the \"de facto\" standard for indicating a factor of 1024 instead of 1000, although this could not be extended to higher powers.\nAs the discrepancy between the two systems increased in the higher-order powers, more proposals for unique prefixes were made.\nIn 1996, Markus Kuhn proposed a system with \"di\" prefixes, like the \"dikilobyte\" (K\u2082B or K2B). Donald Knuth, who uses decimal notation like 1\u00a0MB = 1000\u00a0kB, expressed \"astonishment\" that the IEC proposal was adopted, calling them \"funny-sounding\" and opining that proponents were assuming \"that standards are automatically adopted just because they are there.\" Knuth proposed that the powers of 1024 be designated as \"large kilobytes\" and \"large megabytes\" (abbreviated KKB and MMB, as \"doubling the letter connotes both binary-ness and large-ness\"). Double prefixes were already abolished from SI, however, having a multiplicative meaning (\"MMB\" would be equivalent to \"TB\"), and this proposed usage never gained any traction.\nIEC prefixes.\nThe set of binary prefixes that were eventually adopted, now referred to as the \"IEC prefixes\", were first proposed by the International Union of Pure and Applied Chemistry's (IUPAC) Interdivisional Committee on Nomenclature and Symbols (IDCNS) in 1995. At that time, it was proposed that the terms kilobyte and megabyte be used only for 103 bytes and 106 bytes, respectively. The new prefixes \"kibi\" (kilobinary), \"mebi\" (megabinary), \"gibi\" (gigabinary) and \"tebi\" (terabinary) were also proposed at the time, and the proposed symbols for the prefixes were kb, Mb, Gb and Tb respectively, rather than Ki, Mi, Gi and Ti. The proposal was not accepted at the time.\nThe Institute of Electrical and Electronics Engineers (IEEE) began to collaborate with the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC) to find acceptable names for binary prefixes. IEC proposed \"kibi\", \"mebi\", \"gibi\" and \"tebi\", with the symbols Ki, Mi, Gi and Ti respectively, in 1996.\nThe names for the new prefixes are derived from the original SI prefixes combined with the term \"binary\", but contracted, by taking the first two letters of the SI prefix and \"bi\" from binary. The first letter of each such prefix is therefore identical to the corresponding SI prefixes, except for \"K\", which is used interchangeably with \"k\", whereas in SI, only the lower-case k represents 1000.\nThe IEEE decided that their standards would use the prefixes \"kilo\", etc. with their metric definitions, but allowed the binary definitions to be used in an interim period as long as such usage was explicitly pointed out on a case-by-case basis.\nAdoption by IEC, NIST and ISO.\nIn January 1999, the IEC published the first international standard (IEC 60027-2 Amendment 2) with the new prefixes, extended up to \"pebi\" (Pi) and \"exbi\" (Ei).\nThe IEC 60027-2 Amendment 2 also states that the IEC position is the same as that of BIPM (the body that regulates the SI system); the SI prefixes retain their definitions in powers of 1000 and are never used to mean a power of 1024.\nIn usage, products and concepts typically described using powers of 1024 would continue to be, but with the new IEC prefixes. For example, a memory module of bytes () would be referred to as 512\u00a0MiB or 512 mebibytes instead of 512\u00a0MB or 512 megabytes. Conversely, since hard drives have historically been marketed using the SI convention that \"giga\" means , a \"500\u00a0GB\" hard drive would still be labeled as such. According to these recommendations, operating systems and other software would also use binary and SI prefixes in the same way, so the purchaser of a \"500\u00a0GB\" hard drive would find the operating system reporting either \"500\u00a0GB\" or \"466\u00a0GiB\", while bytes of RAM would be displayed as \"512\u00a0MiB\".\nThe second edition of the standard, published in 2000, defined them only up to \"exbi\", but in 2005, the third edition added prefixes \"zebi\" and \"yobi\", thus matching all SI prefixes with binary counterparts.\nThe harmonized ISO/IEC IEC 80000-13:2008 standard cancels and replaces subclauses 3.8 and 3.9 of IEC 60027-2:2005 (those defining prefixes for binary multiples). The only significant change is the addition of explicit definitions for some quantities. In 2009, the prefixes kibi-, mebi-, etc. were defined by ISO 80000-1 in their own right, independently of the kibibyte, mebibyte, and so on.\nThe BIPM standard JCGM 200:2012 \"International vocabulary of metrology \u2013 Basic and general concepts and associated terms (VIM), 3rd edition\" lists the IEC binary prefixes and states \"SI prefixes refer strictly to powers of 10, and should not be used for powers of 2. For example, 1 kilobit should not be used to represent bits (210 bits), which is 1 kibibit.\"\nOther standards bodies and organizations.\nThe IEC standard binary prefixes are now supported by other standardization bodies and technical organizations.\nThe United States National Institute of Standards and Technology (NIST) supports the ISO/IEC standards for\n\"Prefixes for binary multiples\" and has a web site documenting them, describing and justifying their use. NIST suggests that in English, the first syllable of the name of the binary-multiple prefix should be pronounced in the same way as the first syllable of the name of the corresponding SI prefix, and that the second syllable should be pronounced as \"bee\". NIST has stated the SI prefixes \"refer strictly to powers of 10\" and that the binary definitions \"should not be used\" for them.\nThe microelectronics industry standards body JEDEC describes the IEC prefixes in its online dictionary. The JEDEC standards for semiconductor memory use the customary prefix symbols K, M and G in the binary sense.\nOn 19 March 2005, the IEEE standard IEEE 1541-2002 (\"Prefixes for Binary Multiples\") was elevated to a full-use standard by the IEEE Standards Association after a two-year trial period. However, , the IEEE Publications division does not require the use of IEC prefixes in its major magazines such as \"Spectrum\" or \"Computer\". \nThe International Bureau of Weights and Measures (BIPM), which maintains the International System of Units (SI), expressly prohibits the use of SI prefixes to denote binary multiples, and recommends the use of the IEC prefixes as an alternative since units of information are not included in SI.\nThe Society of Automotive Engineers (SAE) prohibits the use of SI prefixes with anything but a power-of-1000 meaning, but does not recommend or otherwise cite the IEC binary prefixes.\nThe European Committee for Electrotechnical Standardization (CENELEC) adopted the IEC-recommended binary prefixes via the harmonization document HD\u00a060027-2:2003-03.\nThe European Union (EU) has required the use of the IEC binary prefixes since 2007.\nCurrent practice.\nMost computer hardware uses SI prefixes to state capacity and define other performance parameters such as data rate. Main and cache memories are notable exceptions.\nCapacities of main memory and cache memory are usually expressed with customary binary prefixes\nOn the other hand, flash memory, like that found in solid state drives, mostly uses SI prefixes to state capacity.\nSome operating systems and other software continue to use the customary binary prefixes in displays of memory, disk storage capacity, and file size, but SI prefixes in other areas such as network communication speeds and processor speeds.\nIn the following subsections, unless otherwise noted, examples are first given using the common prefixes used in each case, and then followed by interpretation using other notation where appropriate.\nOperating systems.\nPrior to the release of Macintosh System Software (1984), file sizes were typically reported by the operating system without any prefixes. Today, most operating systems report file sizes with prefixes.\nSoftware.\n, most software does not distinguish symbols for binary and decimal prefixes.\nThe IEC binary naming convention has been adopted by a few, but this is not used universally.\nOne of the stated goals of the introduction of the IEC prefixes was \"to preserve the SI prefixes as unambiguous decimal multipliers.\" Programs such as fdisk/cfdisk, parted, and apt-get use SI prefixes with their decimal meaning.\nExample of the use of IEC binary prefixes in the Linux operating system displaying traffic volume on a network interface in kibibytes (KiB) and mebibytes (MiB), as obtained with the ifconfig utility:\nSoftware that uses IEC binary prefixes for powers of 1024 \"and\" uses standard SI prefixes for powers of 1000 includes:\nSoftware that uses standard SI prefixes for powers of 1000, but \"not\" IEC binary prefixes for powers of 1024, includes:\nSoftware that supports decimal prefixes for powers of 1000 \"and\" binary prefixes for powers of 1024 (but does not follow SI or IEC nomenclature for this) includes:\nComputer hardware.\nHardware types that use powers-of-1024 multipliers, such as memory, continue to be marketed with customary binary prefixes.\nComputer memory.\nMeasurements of most types of electronic memory such as RAM and ROM are given using customary binary prefixes (kilo, mega, and giga). This includes some flash memory, like EEPROMs. For example, a \"512-megabyte\" memory module is 512\u00d7220 bytes (512 \u00d7 , or ).\nJEDEC Solid State Technology Association, the semiconductor engineering standardization body of the Electronic Industries Alliance (EIA), continues to include the customary binary definitions of kilo, mega and giga in their \"Terms, Definitions, and Letter Symbols\" document,\nand uses those definitions in later memory standards\nMany computer programming tasks reference memory in terms of powers of two because of the inherent binary design of current hardware addressing systems. For example, a 16-bit processor register can reference at most 65,536 items (bytes, words, or other objects); this is conveniently expressed as \"64K\" items. An operating system might map memory as 4096-byte pages, in which case exactly 8192 pages could be allocated within bytes of memory: \"8K\" (8192) pages of \"4 kilobytes\" (4096 bytes) each within \"32 megabytes\" (32\u00a0MiB) of memory.\nHard disk drives.\nAll hard disk drive manufacturers state capacity using SI prefixes.\nFlash drives.\nUSB flash drives, flash-based memory cards like CompactFlash or Secure Digital, and flash-based solid-state drives (SSDs) use SI prefixes;\nfor example, a \"256 MB\" flash card provides at least 256 million bytes (), not 256\u00d71024\u00d71024 ().\nThe flash memory chips inside these devices contain considerably more than the quoted capacities, but much like a traditional hard drive, some space is reserved for internal functions of the flash drive. These include wear leveling, error correction, sparing, and metadata needed by the device's internal firmware.\nFloppy drives.\nFloppy disks have existed in numerous physical and logical formats, and have been sized inconsistently. In part, this is because the end user capacity of a particular disk is a function of the controller hardware, so that the same disk could be formatted to a variety of capacities. In many cases, the media are marketed without any indication of the end user capacity, as for example, DSDD, meaning double-sided double-density.\nThe last widely adopted diskette was the 3\u00bd-inch high density. This has a formatted capacity of bytes or 1440\u00a0KB (1440 \u00d7 1024, using \"KB\" in the customary binary sense). These are marketed as \"HD\", or \"1.44\u00a0MB\" or both. This usage creates a third definition of \"megabyte\" as 1000\u00d71024 bytes.\nMost operating systems display the capacity using \"MB\" in the customary binary sense, resulting in a display of \"1.4\u00a0MB\" (). Some users have noticed the missing 0.04\u00a0MB and both Apple and Microsoft have support bulletins referring to them as 1.4\u00a0MB.\nThe earlier \"1200\u00a0KB\" (1200\u00d71024 bytes) 5\u00bc-inch diskette sold with the IBM PC AT was marketed as \"1.2\u00a0MB\" (). The largest 8-inch diskette formats could contain more than a megabyte, and the capacities of those devices were often irregularly specified in megabytes, also without controversy.\nOlder and smaller diskette formats were usually identified as an accurate number of (binary) KB, for example the Apple Disk II described as \"140KB\" had a 140\u00d71024-byte capacity, and the original \"360KB\" double sided, double density disk drive used on the IBM PC had a 360\u00d71024-byte capacity.\nIn many cases diskette hardware was marketed based on unformatted capacity, and the overhead required to format sectors on the media would reduce the nominal capacity as well (and this overhead typically varied based on the size of the formatted sectors), leading to more irregularities.\nOptical discs.\nThe capacities of most optical disc storage media like DVD, Blu-ray Disc, HD DVD and magneto-optical (MO) are given using SI decimal prefixes.\nA \"4.7\u00a0GB\" DVD has a nominal capacity of about 4.38\u00a0GiB. However, CD capacities are always given using customary binary prefixes. Thus a \"700-MB\" (or \"80-minute\") CD has a nominal capacity of about 700\u00a0MiB (approx 730\u00a0MB).\nTape drives and media.\nTape drive and media manufacturers use SI decimal prefixes to identify capacity.\nData transmission and clock rates.\nCertain units are always used with SI decimal prefixes even in computing contexts.\nTwo examples are hertz (Hz), which is used to measure the clock rates of electronic components, and bit/s, used to measure data transmission speed.\nBus clock speeds and therefore bandwidths are both quoted using SI decimal prefixes.\nUse by industry.\nIEC prefixes are used by Toshiba, IBM, HP to advertise or describe some of their products. According to one HP brochure, \"[t]o reduce confusion, vendors are pursuing one of two remedies: they are changing SI prefixes to the new binary prefixes, or they are recalculating the numbers as powers of ten.\" The IBM Data Center also uses IEC prefixes to reduce confusion. The IBM Style Guide reads To help avoid inaccuracy (especially with the larger prefixes) and potential ambiguity, the International Electrotechnical Commission (IEC) in 2000 adopted a set of prefixes specifically for binary multipliers (See IEC 60027-2). Their use is now supported by the United States National Institute of Standards and Technology (NIST) and incorporated into ISO 80000. They are also required by EU law and in certain contexts in the US.\nHowever, most documentation and products in the industry continue to use SI prefixes when referring to binary multipliers. In product documentation, follow the same standard that is used in the product itself (for example, in the interface or firmware). Whether you choose to use IEC prefixes for powers of 2 and SI prefixes for powers of 10, or use SI prefixes for a dual purpose ... be consistent in your usage and explain to the user your adopted system. "}
{"id": "4078", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=4078", "title": "National Baseball Hall of Fame and Museum", "text": "The National Baseball Hall of Fame and Museum is a history museum and hall of fame in Cooperstown, New York, operated by private interests. It serves as the central point of the history of baseball in the United States and displays baseball-related artifacts and exhibits, honoring those who have excelled in playing, managing, and serving the sport. The Hall's motto is \"Preserving History, Honoring Excellence, Connecting Generations\". Cooperstown is often used as shorthand (or a metonym) for the National Baseball Hall of Fame and Museum, similar to \"Canton\" for the Pro Football Hall of Fame in Canton, Ohio.\nThe Hall of Fame was established in 1939 by Stephen Carlton Clark, an heir to the Singer Sewing Machine fortune. Clark sought to bring tourists to a city hurt by the Great Depression, which reduced the local tourist trade, and Prohibition, which devastated the local hops industry. Clark constructed the Hall of Fame's building, and it was dedicated on June 12, 1939. (His granddaughter, Jane Forbes Clark, is the current chairman of the Board of Directors.) The erroneous claim that Civil War hero Abner Doubleday invented baseball in Cooperstown was instrumental in the early marketing of the Hall.\nAn expanded library and research facility opened in 1994. Dale Petroskey became the organization's president in 1999. In 2002, the Hall launched \"Baseball as America\", a traveling exhibit that toured ten American museums over six years. The Hall of Fame has since also sponsored educational programming on the Internet to bring the Hall of Fame to schoolchildren who might not visit. The Hall and Museum completed a series of renovations in spring 2005. The Hall of Fame also presents an annual exhibit at FanFest at the Major League Baseball All-Star Game.\nInductees.\nAmong baseball fans, \"Hall of Fame\" means not only the museum and facility in Cooperstown, New York, but the pantheon of players, managers, umpires, executives, and pioneers who have been inducted into the Hall. The first five men elected were Ty Cobb, Babe Ruth, Honus Wagner, Christy Mathewson and Walter Johnson, chosen in 1936; roughly 20 more were selected before the entire group was inducted at the Hall's 1939 opening. , 333 people had been elected to the Hall of Fame, including 234 former Major League Baseball players, 35 Negro league baseball players and executives, 23 managers, 10 umpires, and 36 pioneers, executives, and organizers; 114 members of the Hall of Fame have been inducted posthumously, including four who died after their selection was announced. Of the 35 Negro league members, 29 were inducted posthumously, including all 24 selected since the 1990s. The Hall of Fame includes one female member, Effa Manley.\nThe newest members inducted on July 21, 2019, are players Harold Baines, Roy Halladay, Edgar Mart\u00ednez, Mike Mussina, Mariano Rivera, and Lee Smith. Rivera was the first player ever to be elected unanimously. Derek Jeter, Marvin Miller, Ted Simmons, and Larry Walker were all originally scheduled to be inducted in 2020, but their induction ceremony has been postponed until July 25, 2021, due to the COVID-19 pandemic.\nSelection process.\nPlayers are currently inducted into the Hall of Fame through election by either the Baseball Writers' Association of America (or BBWAA), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball. Five years after retirement, any player with 10 years of major league experience who passes a screening committee (which removes from consideration players of clearly lesser qualification) is eligible to be elected by BBWAA members with 10 years' membership or more who also have been actively covering MLB at any time in the 10 years preceding the election (the latter requirement was added for the 2016 election). From a final ballot typically including 25\u201340 candidates, each writer may vote for up to 10 players; until the late 1950s, voters were advised to cast votes for the maximum 10 candidates. Any player named on 75% or more of all ballots cast is elected. A player who is named on fewer than 5% of ballots is dropped from future elections. In some instances, the screening committee had restored their names to later ballots, but in the mid-1990s, dropped players were made permanently ineligible for Hall of Fame consideration, even by the Veterans Committee. A 2001 change in the election procedures restored the eligibility of these dropped players; while their names will not appear on future BBWAA ballots, they may be considered by the Veterans Committee. Players receiving 5% or more of the votes but fewer than 75% are reconsidered annually until a maximum of ten years of eligibility (lowered from fifteen years for the 2015 election).\nUnder special circumstances, certain players may be deemed eligible for induction even though they have not met all requirements. Addie Joss was elected in 1978, despite only playing nine seasons before he died of meningitis. Additionally, if an otherwise eligible player dies before his fifth year of retirement, then that player may be placed on the ballot at the first election at least six months after his death. Roberto Clemente's induction in 1973 set the precedent when the writers chose to put him up for consideration after his death on New Year's Eve, 1972.\nThe five-year waiting period was established in 1954 after an evolutionary process. In 1936 all players were eligible, including active ones. From the 1937 election until the 1945 election, there was no waiting period, so any retired player was eligible, but writers were discouraged from voting for current major leaguers. Since there was no formal rule preventing a writer from casting a ballot for an active player, the scribes did not always comply with the informal guideline; Joe DiMaggio received a vote in 1945, for example. From the 1946 election until the 1954 election, an official one-year waiting period was in effect. (DiMaggio, for example, retired after the 1951 season and was first eligible in the 1953 election.) The modern rule establishing a wait of five years was passed in 1954, although those who had already been eligible under the old rule were grandfathered into the ballot, thus permitting Joe DiMaggio to be elected within four years of his retirement.\nContrary to popular belief, no formal exception was made for Lou Gehrig (other than to hold a special one-man election for him): there was no waiting period at that time, and Gehrig met all other qualifications, so he would have been eligible for the next regular election after he retired during the 1939 season. However, the BBWAA decided to hold a special election at the 1939 Winter Meetings in Cincinnati, specifically to elect Gehrig (most likely because it was known that he was terminally ill, making it uncertain that he would live long enough to see another election). Nobody else was on that ballot, and the numerical results have never been made public. Since no elections were held in 1940 or 1941, the special election permitted Gehrig to enter the Hall while still alive.\nIf a player fails to be elected by the BBWAA within 10 years of his retirement from active play, he may be selected by the Veterans Committee. Following changes to the election process for that body made in 2010 and 2016, it is now responsible for electing all otherwise eligible candidates who are not eligible for the BBWAA ballot \u2014 both long-retired players and non-playing personnel (managers, umpires, and executives). From 2011 to 2016, each candidate could be considered once every three years; now, the frequency depends on the era in which an individual made his greatest contributions. A more complete discussion of the new process is available below.\nFrom 2008 to 2010, following changes made by the Hall in July 2007, the main Veterans Committee, then made up of living Hall of Famers, voted only on players whose careers began in 1943 or later. These changes also established three separate committees to select other figures:\nPlayers of the Negro leagues have also been considered at various times, beginning in 1971. In 2005, the Hall completed a study on African American players between the late 19th century and the integration of the major leagues in 1947, and conducted a special election for such players in February 2006; seventeen figures from the Negro leagues were chosen in that election, in addition to the eighteen previously selected. Following the 2010 changes, Negro leagues figures were primarily considered for induction alongside other figures from the 1871\u20131946 era, called the \"Pre-Integration Era\" by the Hall; since 2016, Negro leagues figures are primarily considered alongside other figures from what the Hall calls the \"Early Baseball\" era (1871\u20131949).\nPredictably, the selection process catalyzes endless debate among baseball fans over the merits of various candidates. Even players elected years ago remain the subjects of discussions as to whether they deserved election. For example, Bill James' 1994 book \"Whatever Happened to the Hall of Fame?\" goes into detail about who he believes does and does not belong in the Hall of Fame.\nNon-induction of banned players.\nFollowing the banning of Pete Rose from MLB, the selection rules for the Baseball Hall of Fame were modified to prevent the induction of anyone on Baseball's \"permanently ineligible\" list, such as Rose or \"Shoeless Joe\" Jackson. Many others have been barred from participation in MLB, but none have Hall of Fame qualifications on the level of Jackson or Rose.\nJackson and Rose were both banned from MLB for life for actions related to gambling on their own teams\u2014Jackson was determined to have cooperated with those who conspired to intentionally lose the 1919 World Series, and for accepting payment for losing, and Rose voluntarily accepted a permanent spot on the ineligible list in return for MLB's promise to make no official finding in relation to alleged betting on the Cincinnati Reds when he was their manager in the 1980s. (Baseball's Rule 21, prominently posted in every clubhouse locker room, mandates permanent banishment from the MLB for having a gambling interest of any sort on a game in which a player or manager is directly involved.) Rose later admitted that he bet on the Reds in his 2004 autobiography. Baseball fans are deeply split on the issue of whether these two should remain banned or have their punishment revoked. Writer Bill James, though he advocates Rose eventually making it into the Hall of Fame, compared the people who want to put Jackson in the Hall of Fame to \"those women who show up at murder trials wanting to marry the cute murderer\".\nChanges to Veterans Committee process.\nThe actions and composition of the Veterans Committee have been at times controversial, with occasional selections of contemporaries and teammates of the committee members over seemingly more worthy candidates.\nIn 2001, the Veterans Committee was reformed to comprise the living Hall of Fame members and other honorees. The revamped Committee held three elections, in 2003 and 2007, for both players and non-players, and in 2005 for players only. No individual was elected in that time, sparking criticism among some observers who expressed doubt whether the new Veterans Committee would ever elect a player. The Committee members, most of whom were Hall members, were accused of being reluctant to elect new candidates in the hope of heightening the value of their own selection. After no one was selected for the third consecutive election in 2007, Hall of Famer Mike Schmidt noted, \"The same thing happens every year. The current members want to preserve the prestige as much as possible, and are unwilling to open the doors.\" In 2007, the committee and its selection processes were again reorganized; the main committee then included all living members of the Hall, and voted on a reduced number of candidates from among players whose careers began in 1943 or later. Separate committees, including sportswriters and broadcasters, would select umpires, managers and executives, as well as players from earlier eras.\nIn the first election to be held under the 2007 revisions, two managers and three executives were elected in December 2007 as part of the 2008 election process. The next Veterans Committee elections for players were held in December 2008 as part of the 2009 election process; the main committee did not select a player, while the panel for pre\u2013World War II players elected Joe Gordon in its first and ultimately only vote. The main committee voted as part of the election process for inductions in odd-numbered years, while the pre-World War II panel would vote every five years, and the panel for umpires, managers, and executives voted as part of the election process for inductions in even-numbered years.\nFurther changes to the Veterans Committee process were announced by the Hall on July 26, 2010, effective with the 2011 election.\nAll individuals eligible for induction but not eligible for BBWAA consideration were considered on a single ballot, grouped by the following eras in which they made their greatest contributions:\nThe Hall used the BBWAA's Historical Overview Committee to formulate the ballots for each era, consisting of 12 individuals for the Expansion Era and 10 for the other eras. The Hall's board of directors selected a committee of 16 voters for each era, made up of Hall of Famers, executives, baseball historians, and media members. Each committee met and voted at the Baseball Winter Meetings once every three years. The Expansion Era committee held its first vote in 2010 for 2011 induction, with longtime general manager Pat Gillick becoming the first individual elected under the new procedure. The Golden Era committee voted in 2011 for the induction class of 2012, with Ron Santo becoming the first player elected under the new procedure. The Pre-Integration Era committee voted in 2012 for the induction class of 2013, electing three figures. Subsequent elections rotated among the three committees in that order through the 2016 election.\nIn July 2016, however, the Hall of Fame announced a restructuring of the timeframes to be considered, with a much greater emphasis on modern eras. Four new committees were established:\nAll committees' ballots now include 10 candidates. At least one committee is scheduled to convene each December as part of the election process for the following calendar year's induction ceremony. Due to the COVID-19 pandemic, the December 2020 meetings of the Early Baseball and Golden Days committees were postponed, apparently pushing back the rotation of all committee meetings by a year.\nThe eligibility criteria for Era Committee consideration differ between players, managers, and executives.\nPlayers and managers with multiple teams.\nWhile the text on a player's or manager's plaque lists all teams for which the inductee was a member in that specific role, inductees are usually depicted wearing the cap of a specific team, though in a few cases, like umpires, they wear caps without logos. (Executives are not depicted wearing caps.) Additionally, as of 2015, inductee biographies on the Hall's website for all players and managers, and executives who were associated with specific teams, list a \"primary team\", which does not necessarily match the cap logo. The Hall selects the logo \"based on where that player makes his most indelible mark.\"\nAlthough the Hall always made the final decision on which logo was shown, until 2001 the Hall deferred to the wishes of players or managers whose careers were linked with multiple teams. Some examples of inductees associated with multiple teams are the following:\nIn all of the above cases, the \"primary team\" is the team for which the inductee spent the largest portion of his career except for Ryan, whose primary team is listed as the Angels despite playing one fewer season for that team than for the Astros.\nIn 2001, the Hall of Fame decided to change the policy on cap logo selection, as a result of rumors that some teams were offering compensation, such as number retirement, money, or organizational jobs, in exchange for the cap designation. (For example, though Wade Boggs denied the claims, some media reports had said that his contract with the Tampa Bay Devil Rays required him to request depiction in the Hall of Fame as a Devil Ray.) The Hall decided that it would no longer defer to the inductee, though the player's wishes would be considered, when deciding on the logo to appear on the plaque. Newly elected members affected by the change include the following:\nThe museum.\nAccording to the Hall of Fame, approximately 260,000 visitors enter the museum each year, and the running total has surpassed 17\u00a0million. These visitors see only a fraction of its 40,000 artifacts, 3\u00a0million library items (such as newspaper clippings and photos) and 140,000 baseball cards.\nThe Hall has seen a noticeable decrease in attendance in recent years. A 2013 story on ESPN.com about the village of Cooperstown and its relation to the game partially linked the reduced attendance with Cooperstown Dreams Park, a youth baseball complex about away in the town of Hartwick. The 22 fields at Dreams Park currently draw 17,000 players each summer for a week of intensive play; while the complex includes housing for the players, their parents and grandparents must stay elsewhere. According to the story,\nPrior to Dreams Park, a room might be filled for a week by several sets of tourists. Now, that room will be taken by just one family for the week, and that family may only go into Cooperstown and the Hall of Fame once. While there are other contributing factors (the recession and high gas prices among them), the Hall's attendance has tumbled since Dreams Park opened. The Hall drew 383,000 visitors in 1999. It drew 262,000 last year.\nNotable events.\n1982 unauthorized sales.\nA controversy erupted in 1982, when it emerged that some historic items given to the Hall had been sold on the collectibles market. The items had been lent to the Baseball Commissioner's office, gotten mixed up with other property owned by the Commissioner's office and employees of the office, and moved to the garage of Joe Reichler, an assistant to Commissioner Bowie Kuhn, who sold the items to resolve his personal financial difficulties. Under pressure from the New York Attorney General, the Commissioner's Office made reparations, but the negative publicity damaged the Hall of Fame's reputation, and made it more difficult for it to solicit donations.\n2014 commemorative coins.\nIn 2012, Congress passed and President Barack Obama signed a law ordering the United States Mint to produce and sell commemorative, non-circulating coins to benefit the private, non-profit Hall. The bill, , was introduced in the United States House of Representatives by Rep. Richard Hanna, a Republican from New York, and passed the House on October 26, 2011. The coins, which depict baseball gloves and balls, are the first concave designs produced by the Mint. The mintage included 50,000 gold coins, 400,000 silver coins, and 750,000 clad (nickel-copper) coins. The Mint released them on March 27, 2014, and the gold and silver editions quickly sold out. The Hall receives money from surcharges included in the sale price: a total of $9.5\u00a0million if all the coins are sold."}
{"id": "4079", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4079", "title": "BPP (complexity)", "text": "In computational complexity theory, bounded-error probabilistic polynomial time (BPP) is the class of decision problems solvable by a probabilistic Turing machine in polynomial time with an error probability bounded away from 1/3 for all instances.\nBPP is one of the largest \"practical\" classes of problems, meaning most problems of interest in BPP have efficient probabilistic algorithms that can be run quickly on real modern machines. BPP also contains P, the class of problems solvable in polynomial time with a deterministic machine, since a deterministic machine is a special case of a probabilistic machine.\nInformally, a problem is in BPP if there is an algorithm for it that has the following properties:\nDefinition.\nA language \"L\" is in BPP if and only if there exists a probabilistic Turing machine \"M\", such that\nUnlike the complexity class ZPP, the machine \"M\" is required to run for polynomial time on all inputs, regardless of the outcome of the random coin flips.\nAlternatively, BPP can be defined using only deterministic Turing machines. A language \"L\" is in BPP if and only if there exists a polynomial \"p\" and deterministic Turing machine \"M\", such that\nIn this definition, the string \"y\" corresponds to the output of the random coin flips that the probabilistic Turing machine would have made. For some applications this definition is preferable since it does not mention probabilistic Turing machines.\nIn practice, an error probability of might not be acceptable, however, the choice of in the definition is arbitrary. It can be any constant between 0 and (exclusive) and the set BPP will be unchanged. It does not even have to be constant: the same class of problems is defined by allowing error as high as \u2212 \"n\"\u2212\"c\" on the one hand, or requiring error as small as 2\u2212\"nc\" on the other hand, where \"c\" is any positive constant, and \"n\" is the length of input. The idea is that there is a probability of error, but if the algorithm is run many times, the chance that the majority of the runs are wrong drops off exponentially as a consequence of the Chernoff bound. This makes it possible to create a highly accurate algorithm by merely running the algorithm several times and taking a \"majority vote\" of the answers. For example, if one defined the class with the restriction that the algorithm can be wrong with probability at most , this would result in the same class of problems.\nProblems.\nAll problems in P are obviously also in BPP. However, many problems have been known to be in BPP but not known to be in P. The number of such problems is decreasing, and it is conjectured that P = BPP.\nFor a long time, one of the most famous problems known to be in BPP but not known to be in P was the problem of determining whether a given number is prime. However, in the 2002 paper \"PRIMES is in P\", Manindra Agrawal and his students Neeraj Kayal and Nitin Saxena found a deterministic polynomial-time algorithm for this problem, thus showing that it is in P.\nAn important example of a problem in BPP (in fact in co-RP) still not known to be in P is polynomial identity testing, the problem of determining whether a polynomial is identically equal to the zero polynomial, when you have access to the value of the polynomial for any given input, but not to the coefficients. In other words, is there an assignment of values to the variables such that when a nonzero polynomial is evaluated on these values, the result is nonzero? It suffices to choose each variable's value uniformly at random from a finite subset of at least \"d\" values to achieve bounded error probability, where \"d\" is the total degree of the polynomial.\nRelated classes.\nIf the access to randomness is removed from the definition of BPP, we get the complexity class P. In the definition of the class, if we replace the ordinary Turing machine with a quantum computer, we get the class BQP.\nAdding postselection to BPP, or allowing computation paths to have different lengths, gives the class BPPpath. BPPpath is known to contain NP, and it is contained in its quantum counterpart PostBQP.\nA Monte Carlo algorithm is a randomized algorithm which is likely to be correct. Problems in the class BPP have Monte Carlo algorithms with polynomial bounded running time. This is compared to a Las Vegas algorithm which is a randomized algorithm which either outputs the correct answer, or outputs \"fail\" with low probability. Las Vegas algorithms with polynomial bound running times are used to define the class ZPP. Alternatively, ZPP contains probabilistic algorithms that are always correct and have expected polynomial running time. This is weaker than saying it is a polynomial time algorithm, since it may run for super-polynomial time, but with very low probability.\nComplexity-theoretic properties.\nIt is known that BPP is closed under complement; that is, BPP = co-BPP. BPP is low for itself, meaning that a BPP machine with the power to solve BPP problems instantly (a BPP oracle machine) is not any more powerful than the machine without this extra power. In symbols, BPPBPP = BPP.\nThe relationship between BPP and NP is unknown: it is not known whether BPP is a subset of NP, NP is a subset of BPP or neither. If NP is contained in BPP, which is considered unlikely since it would imply practical solutions for NP-complete problems, then NP = RP and PH \u2286 BPP.\nIt is known that RP is a subset of BPP, and BPP is a subset of PP. It is not known whether those two are strict subsets, since we don't even know if P is a strict subset of PSPACE. BPP is contained in the second level of the polynomial hierarchy and therefore it is contained in PH. More precisely, the Sipser\u2013Lautemann theorem states that formula_1. As a result, P = NP leads to P = BPP since PH collapses to P in this case. Thus either P = BPP or P \u2260 NP or both.\nAdleman's theorem states that membership in any language in BPP can be determined by a family of polynomial-size Boolean circuits, which means BPP is contained in P/poly. Indeed, as a consequence of the proof of this fact, every BPP algorithm operating on inputs of bounded length can be derandomized into a deterministic algorithm using a fixed string of random bits. Finding this string may be expensive, however. Some weak separation results for Monte Carlo time classes were proven by , see also .\nClosure properties.\nThe class BPP is closed under complementation, union and intersection.\nRelativization.\nRelative to oracles, we know that there exist oracles A and B, such that PA = BPPA and PB \u2260 BPPB. Moreover, relative to a random oracle with probability 1, P = BPP and BPP is strictly contained in NP and co-NP.\nThere is even an oracle in which BPP=EXPNP (and hence P&lt;NP&lt;BPP=EXP=NEXP), which can be iteratively constructed as follows. For a fixed ENP (relativized) complete problem, the oracle will give correct answers with high probability if queried with the problem instance followed by a random string of length \"kn\" (\"n\" is instance length; \"k\" is an appropriate small constant). Start with \"n\"=1. For every instance of the problem of length \"n\" fix oracle answers (see lemma below) to fix the instance output. Next, provide the instance outputs for queries consisting of the instance followed by \"kn\"-length string, and then treat output for queries of length \u2264(\"k\"+1)\"n\" as fixed, and proceed with instances of length \"n\"+1.\nLemma: Given a problem (specifically, an oracle machine code and time constraint) in relativized ENP, for every partially constructed oracle and input of length \"n\", the output can be fixed by specifying 2\"O\"(\"n\") oracle answers.\nProof: The machine is simulated, and the oracle answers (that are not already fixed) are fixed step-by-step. There is at most one oracle query per deterministic computation step. For the relativized NP oracle, if possible fix the output to be yes by choosing a computation path and fixing the answers of the base oracle; otherwise no fixing is necessary, and either way there is at most 1 answer of the base oracle per step. Since there are 2\"O\"(\"n\") steps, the lemma follows.\nThe lemma ensures that (for a large enough \"k\"), it is possible to do the construction while leaving enough strings for the relativized ENP answers. Also, we can ensure that for the relativized ENP, linear time suffices, even for function problems (if given a function oracle and linear output size) and with exponentially small (with linear exponent) error probability. Also, this construction is effective in that given an arbitrary oracle A we can arrange the oracle B to have PA\u2264PB and EXPNPA=EXPNPB=BPPB. Also, for a ZPP=EXP oracle (and hence ZPP=BPP=EXP&lt;NEXP), one would fix the answers in the relativized E computation to a special nonanswer, thus ensuring that no fake answers are given.\nDerandomization.\nThe existence of certain strong pseudorandom number generators is conjectured by most experts of the field. This conjecture implies that randomness does not give additional computational power to polynomial time computation, that is, P = RP = BPP. Note that ordinary generators are not sufficient to show this result; any probabilistic algorithm implemented using a typical random number generator will always produce incorrect results on certain inputs irrespective of the seed (though these inputs might be rare).\nL\u00e1szl\u00f3 Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson showed that unless EXPTIME collapses to MA, BPP is contained in \nThe class i.o.-SUBEXP, which stands for infinitely often SUBEXP, contains problems which have sub-exponential time algorithms for infinitely many input sizes. They also showed that P = BPP if the exponential-time hierarchy, which is defined in terms of the polynomial hierarchy and E as EPH, collapses to E; however, note that the exponential-time hierarchy is usually conjectured \"not\" to collapse.\nRussell Impagliazzo and Avi Wigderson showed that if any problem in E, where \nhas circuit complexity 2\u03a9(\"n\") then P = BPP."}
{"id": "4080", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=4080", "title": "BQP", "text": "In computational complexity theory, bounded-error quantum polynomial time (BQP) is the class of decision problems solvable by a quantum computer in polynomial time, with an error probability of at most 1/3 for all instances. It is the quantum analogue to the complexity class BPP.\nA decision problem is a member of BQP if there exists a quantum algorithm (an algorithm that runs on a quantum computer) that solves the decision problem with high probability and is guaranteed to run in polynomial time. A run of the algorithm will correctly solve the decision problem with a probability of at least 2/3.\nDefinition.\nBQP can be viewed as the languages associated with certain bounded-error uniform families of quantum circuits. A language \"L\" is in BQP if and only if there exists a polynomial-time uniform family of quantum circuits formula_1, such that\nAlternatively, one can define BQP in terms of quantum Turing machines. A language \"L\" is in BQP if and only if there exists a polynomial quantum Turing machine that accepts \"L\" with an error probability of at most 1/3 for all instances.\nSimilarly to other \"bounded error\" probabilistic classes the choice of 1/3 in the definition is arbitrary. We can run the algorithm a constant number of times and take a majority vote to achieve any desired probability of correctness less than 1, using the Chernoff bound. The complexity class is unchanged by allowing error as high as 1/2 \u2212 \"n\"\u2212\"c\" on the one hand, or requiring error as small as 2\u2212\"nc\" on the other hand, where \"c\" is any positive constant, and \"n\" is the length of input.\nQuantum computation.\nThe number of qubits in the computer is allowed to be a polynomial function of the instance size. For example, algorithms are known for factoring an \"n\"-bit integer using just over 2\"n\" qubits (Shor's algorithm).\nUsually, computation on a quantum computer ends with a measurement. This leads to a collapse of quantum state to one of the basis states. It can be said that the quantum state is measured to be in the correct state with high probability.\nQuantum computers have gained widespread interest because some problems of practical interest are known to be in BQP, but suspected to be outside P. Some prominent examples are:\nRelationship to other complexity classes.\nBQP is defined for quantum computers; the corresponding complexity class for classical computers (or more formally for probabilistic Turing machines) is BPP. Just like P and BPP, BQP is low for itself, which means BQPBQP = BQP. Informally, this is true because polynomial time algorithms are closed under composition. If a polynomial time algorithm calls as a subroutine polynomially many polynomial time algorithms, the resulting algorithm is still polynomial time.\nBQP contains P and BPP and is contained in AWPP, PP and PSPACE.\nIn fact, BQP is low for PP, meaning that a PP machine achieves no benefit from being able to solve BQP problems instantly, an indication of the possible difference in power between these similar classes. The known relationships with classic complexity classes are:\nAs the problem of P \u225f PSPACE has not yet been solved, the proof of inequality between BQP and classes mentioned above is supposed to be difficult. The relation between BQP and NP is not known. In May 2018, computer scientists Ran Raz of Princeton University and Avishay Tal of Stanford University published a paper which showed that, relative to an oracle, BQP was not contained in PH.\nAdding postselection to BQP results in the complexity class PostBQP which is equal to PP."}
{"id": "4086", "revid": "1012370382", "url": "https://en.wikipedia.org/wiki?curid=4086", "title": "Brainfuck", "text": "Brainfuck is an esoteric programming language created in 1993 by Urban M\u00fcller.\nNotable for its extreme minimalism, the language consists of only eight simple commands and an instruction pointer. While it is fully Turing complete, it is not intended for practical use, but to challenge and amuse programmers. Brainfuck simply requires one to break commands into microscopic steps.\nThe language's name is a reference to the slang term \"brainfuck\", which refers to things so complicated or unusual that they exceed the limits of one's understanding.\nHistory.\nIn 1992, Urban M\u00fcller, a Swiss physics student, took over a small online archive for Amiga software. The archive grew more popular, and was soon mirrored around the world. Today, it is the world's largest Amiga archive, known as Aminet.\nM\u00fcller designed Brainfuck with the goal of implementing it with the smallest possible compiler, inspired by the 1024-byte compiler for the FALSE programming language. M\u00fcller's original compiler was implemented in machine language and compiled to a binary with a size of 296 bytes. He uploaded the first Brainfuck compiler to Aminet in 1993. The program came with a \"Readme\" file, which briefly described the language, and challenged the reader \"Who can program anything useful with it? :)\". M\u00fcller also included an interpreter and some quite elaborate examples. A second version of the compiler used only 240 bytes.\nAs Aminet grew, the compiler became popular among the Amiga community, and in time it was implemented for other platforms.\nP\u2032\u2032: Brainfuck's formal \"parent language\".\nExcept for its two I/O commands, Brainfuck is a minor variation of the formal programming language P\u2032\u2032 created by Corrado B\u00f6hm in 1964, which in turn is explicitly based on the Turing machine. In fact, using six symbols equivalent to the respective Brainfuck commands codice_1, codice_2, codice_3, codice_4, codice_5, codice_6, B\u00f6hm provided an explicit program for each of the basic functions that together serve to compute any computable function. So the first \"Brainfuck\" programs appear in B\u00f6hm's 1964 paper \u2013 and they were programs sufficient to prove Turing completeness.\nThe Infinite Abacus: Brainfuck's \"grand-parent\" language.\nA version with explicit memory addressing rather without stack and a conditional jump was introduced by Joachim Lambek in 1961 under the name of the Infinite Abacus, consisting of an infinite number of cells and two instructions:\nHe proves the Infinite Abacus can compute any computable recursive function by programming Kleene set of basic \u03bc-recursive function.\nHis machine was simulated by Melzac's machine modeling computation via arithmetic (rather than binary logic) mimicking a human operator moving pebbles on an abacus, hence the requirement that all numbers must be positive. Melzac, whose one instruction set computer is equivalent to an Infinite Abacus, gives programs for multiplication, gcd, nth prime number, representation in base b, sorting by magnitude, and shows how to simulate an arbitrary Turing machine.\nLanguage design.\nThe language consists of eight commands, listed below. A brainfuck program is a sequence of these commands, possibly interspersed with other characters (which are ignored). The commands are executed sequentially, with some exceptions: an instruction pointer begins at the first command, and each command it points to is executed, after which it normally moves forward to the next command. The program terminates when the instruction pointer moves past the last command.\nThe brainfuck language uses a simple machine model consisting of the program and instruction pointer, as well as a one-dimensional array of at least 30,000 byte cells initialized to zero; a movable data pointer (initialized to point to the leftmost byte of the array); and two streams of bytes for input and output (most often connected to a keyboard and a monitor respectively, and using the ASCII character encoding).\nCommands.\nThe eight language commands each consist of a single character:\ncodice_5 and codice_6 match as parentheses usually do: each codice_5 matches exactly one codice_6 and vice versa, the codice_5 comes first, and there can be no unmatched codice_5 or codice_6 between the two.\nBrainfuck programs can be translated into C using the following substitutions, assuming codice_18 is of type codice_19 and has been initialized to point to an array of zeroed bytes:\nAs the name suggests, Brainfuck programs tend to be difficult to comprehend. This is partly because any mildly complex task requires a long sequence of commands and partly because the program's text gives no direct indications of the program's state. These, as well as Brainfuck's inefficiency and its limited input/output capabilities, are some of the reasons it is not used for serious programming. Nonetheless, like any Turing complete language, Brainfuck is theoretically capable of computing any computable function or simulating any other computational model, if given access to an unlimited amount of memory. A variety of Brainfuck programs have been written. Although Brainfuck programs, especially complicated ones, are difficult to write, it is quite trivial to write an interpreter for Brainfuck in a more typical language such as C due to its simplicity. There even exist Brainfuck interpreters written in the Brainfuck language itself.\nBrainfuck is an example of a so-called Turing tarpit: It can be used to write \"any\" program, but it is not practical to do so, because Brainfuck provides so little abstraction that the programs get very long and/or complicated.\nExamples.\nAdding two values.\nAs a first, simple example, the following code snippet will add the current cell's value to the next cell: Each time the loop is executed, the current cell is decremented, the data pointer moves to the right, that next cell is incremented, and the data pointer moves left again. This sequence is repeated until the starting cell is 0.\n[-&gt;+&lt;]\nThis can be incorporated into a simple addition program as follows:\n Cell c0 =.\n&gt; +++++ Cell c1 = 5\n[ Start your loops with your cell pointer on the loop counter (c1 in our case)\n&lt; + Add 1 to c0\n&gt; - Subtract 1 from c1\n] End your loops with the cell pointer on the loop counter\nAt this point our program has added 5 to 2 leaving 7 in c0 and 0 in c1\nbut we cannot output this value to the terminal since it is not ASCII encoded.\nTo display the ASCII character \"7\" we must add 48 to the value 7.\nWe use a loop to compute 48 = 6 * 8.\n++ ++++ c1 = 8 and this will be our loop counter aga.\n&lt; +++ +++ Add 6 to c0\n&gt; - Subtract 1 from c1\n&lt; . Print out c0 which has the value 55 which translates to \"7\"!\nHello World!\nThe following program prints \"Hello World!\" and a newline to the screen:\n[ This program prints \"Hello World!\" and a newline to the screen, its\n length is 106 active command characters. [It is not the shortest.]\n This loop is an \"initial comment loop\", a simple way of adding a comment\n to a BF program such that you don't have to worry about any command\n characters. Any \".\", \",\", \"+\", \"-\", \"&lt;\" and \"&gt;\" characters are simply\n ignored, the \"[\" and \"]\" characters just have to be balanced. This\n loop and the commands it contains are ignored because the current cell\n defaults to a value of 0; the 0 value causes this loop to be skipped.\n++++++ Set Cell #0 to.\n &gt;++++ Add 4 to Cell #1; this will always set Cell #1 to 4\n [ as the cell will be cleared by the loop\n &gt;++ Add 2 to Cell #2\n &gt;+++ Add 3 to Cell #3\n &gt;+++ Add 3 to Cell #4\n &gt;+ Add 1 to Cell #5\n \u00ab\u00ab- Decrement the loop counter in Cell #1\n ] Loop till Cell #1 is zero; number of iterations is 4\n &gt;+ Add 1 to Cell #2\n &gt;+ Add 1 to Cell #3\n &gt;- Subtract 1 from Cell #4\n \u00bb+ Add 1 to Cell #6\n [&lt;] Move back to the first zero cell you find; this will\n be Cell #1 which was cleared by the previous loop\n &lt;- Decrement the loop Counter in Cell #0\n] Loop till Cell #0 is zero; number of iterations is 8\nThe result of this is:\nCell No : 0 1 2 3 4 5 6\nContents: 0 0 72 104 88 32 8\nPointer : ^\n\u00bb. Cell #2 has value 72 which is 'H'\n&gt;---. Subtract 3 from Cell #3 to get 101 which is 'e'\n+++++..+++. Likewise for 'llo' from Cell .\n\u00bb. Cell #5 is 32 for the space\n&lt;-. Subtract 1 from Cell #4 for 87 to give a 'W'\n&lt;. Cell #3 was set to 'o' from the end of 'Hello'\n+.------.--------. Cell #3 for 'rl' and '.\n\u00bb+. Add 1 to Cell #5 gives us an exclamation point\n&gt;++. And finally a newline from Cell #6\nFor \"readability\", this code has been spread across many lines, and blanks and comments have been added. Brainfuck ignores all characters except the eight commands codice_20 so no special syntax for comments is needed (as long as the comments do not contain the command characters). The code could just as well have been written as:\nROT13.\nThis program enciphers its input with the ROT13 cipher. To do this, it must map characters A-M (ASCII 65-77) to N-Z (78-90), and vice versa. Also it must map a-m (97-109) to n-z (110-122) and vice versa. It must map all other characters to themselves; it reads characters one at a time and outputs their enciphered equivalents until it reads an EOF (here assumed to be represented as either -1 or \"no change\"), at which point the program terminates.\nThe basic approach used is as follows. Calling the input character \"x\", divide \"x\"-1 by 32, keeping quotient and remainder. Unless the quotient is 2 or 3, just output \"x\", having kept a copy of it during the division. If the quotient is 2 or 3, divide the remainder ((\"x\"-1) modulo 32) by 13; if the quotient here is 0, output \"x\"+13; if 1, output \"x\"-13; if 2, output \"x\".\nRegarding the division algorithm, when dividing \"y\" by \"z\" to get a quotient \"q\" and remainder \"r\", there is an outer loop which sets \"q\" and \"r\" first to the quotient and remainder of 1/\"z\", then to those of 2/\"z\", and so on; after it has executed \"y\" times, this outer loop terminates, leaving \"q\" and \"r\" set to the quotient and remainder of \"y\"/\"z\". (The dividend \"y\" is used as a diminishing counter that controls how many times this loop is executed.) Within the loop, there is code to increment \"r\" and decrement \"y\", which is usually sufficient; however, every \"z\"th time through the outer loop, it is necessary to zero \"r\" and increment \"q\". This is done with a diminishing counter set to the divisor \"z\"; each time through the outer loop, this counter is decremented, and when it reaches zero, it is refilled by moving the value from \"r\" back into it.\n-,+[ Read first character and start outer character reading loop\n -[ Skip forward if character is 0\n \u00bb++++[&gt;++++++++&lt;-] Set up divisor (32) for division loop\n (MEMORY LAYOUT: dividend copy remainder divisor quotient zero zero)\n &lt;+&lt;-[ Set up dividend (x minus 1) and enter division loop\n &gt;+&gt;+&gt;-[\u00bb&gt;] Increase copy and remainder / reduce divisor / Normal case: skip forward\n &lt;]&lt;[ Zero that flag unless quotient was 2 or 3; zero quotient; check flag\n ++++++++++++&lt;[ If flag then set up divisor (13) for second division loop\n (MEMORY LAYOUT: zero copy dividend divisor remainder quotient zero zero)\n &gt;-[&gt;+\u00bb] Reduce divisor; Normal case: increase remainder\n &gt;[+[&lt;+&gt;-]&gt;+\u00bb] Special case: increase remainder / move it back to divisor / increase quotient\n \u00ab\u00ab&lt;- Decrease dividend\n ] End division loop\n \u00bb[&lt;+&gt;-] Add remainder back to divisor to get a useful 13\n &gt;[ Skip forward if quotient was 0\n -[ Decrement quotient and skip forward if quotient was 1\n -\u00ab[-]\u00bb Zero quotient and divisor if quotient was 2\n ]\u00ab[\u00ab-\u00bb-]\u00bb Zero divisor and subtract 13 from copy if quotient was 1\n ]\u00ab[\u00ab+\u00bb-] Zero divisor and add 13 to copy if quotient was 0\n ] End outer skip loop (jump to here if ((character minus 1)/32) was not 2 or 3)\n &lt;[-] Clear remainder from first division if second division was skipped\n &lt;.[-] Output ROT13ed character from copy and clear it\n &lt;-,+ Read next character\n] End character reading loop\nPortability issues.\nPartly because Urban M\u00fcller did not write a thorough language specification, the many subsequent brainfuck interpreters and compilers have come to use slightly different dialects of brainfuck.\nCell size.\nIn the classic distribution, the cells are of 8-bit size (cells are bytes), and this is still the most common size. However, to read non-textual data, a brainfuck program may need to distinguish an end-of-file condition from any possible byte value; thus 16-bit cells have also been used. Some implementations have used 32-bit cells, 64-bit cells, or bignum cells with practically unlimited range, but programs that use this extra range are likely to be slow, since storing the value formula_1 into a cell requires formula_2 time as a cell's value may only be changed by incrementing and decrementing.\nIn all these variants, the codice_21 and codice_22 commands still read and write data in bytes. In most of them, the cells wrap around, i.e. incrementing a cell which holds its maximal value (with the codice_1 command) will bring it to its minimal value and vice versa. The exceptions are implementations which are distant from the underlying hardware, implementations that use bignums, and implementations that try to enforce portability.\nIt is usually easy to write brainfuck programs that do not ever cause integer wraparound or overflow, and therefore don't depend on cell size. Generally this means avoiding increment of +255 (unsigned 8-bit wraparound), or avoiding overstepping the boundaries of [-128, +127] (signed 8-bit wraparound) (since there are no comparison operators, a program cannot distinguish between a signed and unsigned two's complement fixed-bit-size cell and negativeness of numbers is a matter of interpretation). For more details on integer wraparound, see the Integer overflow article.\nArray size.\nIn the classic distribution, the array has 30,000 cells, and the pointer begins at the leftmost cell. Even more cells are needed to store things like the millionth Fibonacci number, and the easiest way to make the language Turing complete is to make the array unlimited on the right.\nA few implementations extend the array to the left as well; this is an uncommon feature, and therefore portable brainfuck programs do not depend on it.\nWhen the pointer moves outside the bounds of the array, some implementations will give an error message, some will try to extend the array dynamically, some will not notice and will produce undefined behavior, and a few will move the pointer to the opposite end of the array. Some tradeoffs are involved: expanding the array dynamically to the right is the most user-friendly approach and is good for memory-hungry programs, but it carries a speed penalty. If a fixed-size array is used it is helpful to make it very large, or better yet let the user set the size. Giving an error message for bounds violations is very useful for debugging but even that carries a speed penalty unless it can be handled by the operating system's memory protections.\nEnd-of-line code.\nDifferent operating systems (and sometimes different programming environments) use subtly different versions of ASCII. The most important difference is in the code used for the end of a line of text. MS-DOS and Microsoft Windows use a CRLF, i.e. a 13 followed by a 10, in most contexts. UNIX and its descendants (including GNU/Linux and Mac OS X) and Amigas use just 10, and older Macs use just 13. It would be difficult if brainfuck programs had to be rewritten for different operating systems. However, a unified standard was easy to create. Urban M\u00fcller's compiler and his example programs use 10, on both input and output; so do a large majority of existing brainfuck programs; and 10 is also more convenient to use than CRLF. Thus, brainfuck implementations should make sure that brainfuck programs that assume newline = 10 will run properly; many do so, but some do not.\nThis assumption is also consistent with most of the world's sample code for C and other languages, in that they use '\\n', or 10, for their newlines. On systems that use CRLF line endings, the C standard library transparently remaps \"\\n\" to \"\\r\\n\" on output and \"\\r\\n\" to \"\\n\" on input for streams not opened in binary mode.\nEnd-of-file behavior.\nThe behavior of the codice_21 command when an end-of-file condition has been encountered varies. Some implementations set the cell at the pointer to 0, some set it to the C constant EOF (in practice this is usually -1), some leave the cell's value unchanged. There is no real consensus; arguments for the three behaviors are as follows.\nSetting the cell to 0 avoids the use of negative numbers, and makes it marginally more concise to write a loop that reads characters until EOF occurs. This is a language extension devised by Panu Kalliokoski.\nSetting the cell to -1 allows EOF to be distinguished from any byte value (if the cells are larger than bytes), which is necessary for reading non-textual data; also, it is the behavior of the C translation of codice_21 given in M\u00fcller's readme file. However, it is not obvious that those C translations are to be taken as normative.\nLeaving the cell's value unchanged is the behavior of Urban M\u00fcller's brainfuck compiler. This behavior can easily coexist with either of the others; for instance, a program that assumes EOF = 0 can set the cell to 0 before each codice_21 command, and will then work correctly on implementations that do either EOF = 0 or EOF = \"no change\". It is so easy to accommodate the \"no change\" behavior that any brainfuck programmer interested in portability should do so.\nImplementations.\nTyler Holewinski developed a .NET-based software framework, BrainF.NET, which by default runs brainfuck, but can also be used to derive various forms of the language, as well as add new commands, or modify the behavior of existing ones. BrainF.NET thereby allows for development of programs such as an IDE.\nAlthough it is trivial to make a naive brainfuck interpreter, writing an optimizing compiler or interpreter becomes more of a challenge and amusement much like writing programs in brainfuck itself is: to produce reasonably fast result, the compiler need to piece together the fragmentary instructions forced by the language. Possible optimizations range from simple run-length peephole optimizations on repeated commands and common loop patterns like , to more complicated ones like dead code elimination and constant folding.\nIn addition to optimization, other types of unusual brainfuck interpreters have also been written. Several brainfuck compilers have been made smaller than 200 bytes \u2013 one is only 100 bytes in x86 machine code.\nDerivatives.\nMany people have created brainfuck equivalents (languages with commands that directly map to brainfuck) or brainfuck derivatives (languages that extend its behavior or map it into new semantic territory).\nSome examples:"}
{"id": "4090", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4090", "title": "Binary and", "text": ""}
{"id": "4091", "revid": "379272", "url": "https://en.wikipedia.org/wiki?curid=4091", "title": "Bartolomeo Ammannati", "text": "Bartolomeo Ammannati (18 June 151113 April 1592) was an Italian architect and sculptor, born at Settignano, near Florence. He studied under Baccio Bandinelli and Jacopo Sansovino (assisting on the design of the Library of St. Mark's, the \"Biblioteca Marciana\", Venice) and closely imitated the style of Michelangelo.\nHe was more distinguished in architecture than in sculpture. He worked in Rome in collaboration with Vignola and Vasari), including designs for the Villa Giulia, but also for works and at Lucca. He labored during 1558\u20131570, in the refurbishment and enlargement of Pitti Palace, creating the courtyard consisting of three wings with rusticated facades, and one lower portico leading to the amphitheatre in the Boboli Gardens. His design mirrored the appearance of the main external fa\u00e7ade of Pitti. He was also named \"Consul\" of Accademia delle Arti del Disegno of Florence, which had been founded by the Duke Cosimo I in 1563.\nIn 1569, Ammanati was commissioned to build the Ponte Santa Trinita, a bridge over the Arno River. The three arches are elliptic, and though very light and elegant, has survived, when floods had damaged other Arno bridges at different times. Santa Trinita was destroyed in 1944, during World War II, and rebuilt in 1957.\nAmmannati designed what is considered a prototypic mannerist sculptural ensemble in the \"Fountain of Neptune\" (\"Fontana del Nettuno\"), prominently located in the Piazza della Signoria in the center of Florence. The assignment was originally given to the aged Bartolommeo Bandinelli; however when Bandinelli died, Ammannati's design, bested the submissions of Benvenuto Cellini and Vincenzo Danti, to gain the commission. From 1563 and 1565, Ammannati and his assistants, among them Giambologna, sculpted the block of marble that had been chosen by Bandinelli. He took Grand Duke Cosimo I as model for Neptune's face. The statue was meant to highlight Cosimo's goal of establishing a Florentine Naval force. The ungainly sea god was placed at the corner of the Palazzo Vecchio within sight of Michelangelo's David statue, and the then 87-year-old sculptor is said to have scoffed at Ammannati\u2014 saying that he had ruined a beautiful piece of marble\u2014 with the ditty: \"Ammannati, Ammanato, che bel marmo hai rovinato!\" Ammannati continued work on this fountain for a decade, adding around the perimeter a cornucopia of demigod figures: bronze reclining river gods, laughing satyrs and marble sea horses emerging from the water.\nIn 1550 Ammannati married Laura Battiferri, an elegant poet and an accomplished woman. Later in his life he had a religious crisis, influenced by Counter-Reformation piety, which resulted in condemning his own works depicting nudity, and he left all his possessions to the Jesuits.\nHe died in Florence in 1592."}
{"id": "4092", "revid": "1012810851", "url": "https://en.wikipedia.org/wiki?curid=4092", "title": "Bishop", "text": "A bishop is an ordained, consecrated, or appointed member of the Christian clergy who is generally entrusted with a position of authority and oversight.\nWithin the Catholic, Eastern Orthodox, Oriental Orthodox, Moravian, Anglican, Old Catholic and Independent Catholic churches, as well as the Assyrian Church of the East, bishops claim apostolic succession, a direct historical lineage dating back to the original Twelve Apostles. Within these churches, bishops are seen as those who possess the full priesthood and can ordain clergy, including other bishops. Some Protestant churches, including the Lutheran, Anglican and Methodist churches, have bishops serving similar functions as well, though not always understood to be within apostolic succession in the same way. A person ordained as a deacon, priest, and then bishop is understood to hold the fullness of the (ministerial) priesthood, given responsibility by Christ to govern, teach, and sanctify the Body of Christ. Priests, deacons and lay ministers co-operate and assist their bishops in pastoral ministry.\nTerm.\nThe English term \"bishop\" derives from the Greek word \"ep\u00edskopos\", meaning \"overseer\" in Greek, the early language of the Christian Church. In the early Christian era the term was not always clearly distinguished from \" presb\u00fdteros\" (literally: \"elder\" or \"senior\", origin of the modern English word \"priest\"), but is used in the sense of the order or office of bishop, distinct from that of presbyter, in the writings attributed to Ignatius of Antioch. (died c. 110).\nHistory.\nThe earliest organization of the Church in Jerusalem was, according to most scholars, similar to that of Jewish synagogues, but it had a council or college of ordained presbyters ( \"elders\"). In Acts 11:30 and Acts 15:22, we see a collegiate system of government in Jerusalem chaired by James the Just, according to tradition the first bishop of the city. In Acts 14:23, the Apostle Paul ordains presbyters in churches in Anatolia."}
{"id": "4093", "revid": "390477", "url": "https://en.wikipedia.org/wiki?curid=4093", "title": "Bertrand Andrieu", "text": "Bertrand Andrieu (24 November 1761\u00a0\u2013 6 December 1822) was a French engraver of medals. He was born in Bordeaux. In France, he was considered as the restorer of the art, which had declined after the time of Louis XIV. During the last twenty years of his life, the French government commissioned him to undertake every major work of importance."}
{"id": "4097", "revid": "38455", "url": "https://en.wikipedia.org/wiki?curid=4097", "title": "Bordeaux", "text": "Bordeaux ( , ; Gascon ) is a port city on the river Garonne in the Gironde department in Southwestern France.\nThe municipality (commune) of Bordeaux proper has a population of 257,804 (2019). Bordeaux is the centre of Bordeaux M\u00e9tropole that has a population of 796,273 (2019), the 5th largest in France after Paris, Lyon, Marseille and Lille with its immediate suburbs and closest satellite towns. The larger metropolitan area has a population of 1,247,977 (2017). It is the capital of the Nouvelle-Aquitaine region, as well as the prefecture of the Gironde department. Its inhabitants are called \"Bordelais\" (for men) or \"Bordelaises\" (women). The term \"Bordelais\" may also refer to the city and its surrounding region. Crossed by the Garonne River and bordering the Atlantic Coast, the metropolis, a perfect example of the Age of Enlightment, has been showcasing since the 18th century its blond and golden facades, its courtyards and monumental squares, as well as its lively streets accompanied by its French-style gardens.\nA world capital of wine with its castles and vineyards of the Bordeaux region that stand on the hillsides of the Gironde and is home to the world's main wine fair, Vinexpo. Bordeaux is also one of the centers of gastronomy and business tourism for the organization of international congresses. It is a central and strategic hub for the aeronautics, military and space sector, home to international companies such as Dassault Aviation, Ariane Group, Safran and Thal\u00e8s. The link with aviation dates back to 1910, the year the first airplane flew over the city. A crossroads of knowledge through university research, it is home to one of the only two megajoule lasers in the world, as well as a university population of nearly 100,000 students within the Bordeaux metropolis.\nBordeaux is an international tourist destination for its architectural and cultural heritage with more than 350 historic monuments, making it, after Paris, the city with the most listed or registered monuments in France. The \"Pearl of Aquitaine\" has been voted European Destination of the year in a 2015 online poll. The metropolis has also received awards and rankings by international organizations such as in 1957, Bordeaux was awarded the Europe Prize for its efforts in transmitting the European ideal. And in June 2007, with more than 1,810 hectares, it is the largest protected city in the world, with the inscription of the Port of the Moon by the World Heritage Committee, designated by the UNESCO General Assembly, on the World Heritage List for its ensemble of cultural and natural properties of exceptional interest for the common heritage of humanity. Bordeaux is also ranked as a Sufficiency city by the Globalization and World Cities Research Network.\nHistory.\n5th century BC to 11th century AD.\nAround 300 BC, the region was the settlement of a Celtic tribe, the Bituriges Vivisci, named the town Burdigala, probably of Aquitanian origin.\nIn 107 BC, the Battle of Burdigala was fought by the Romans who were defending the Allobroges, a Gallic tribe allied to Rome, and the Tigurini led by Divico. The Romans were defeated and their commander, the consul Lucius Cassius Longinus, was killed in battle.\nThe city came under Roman rule around 60 BC, and it became an important commercial centre for tin and lead. It continued to flourish, especially during the Severan dynasty (3rd century), and acquired the status of capital of Roman Aquitaine. During this period were built the amphitheatre and the monument \"Les Piliers de Tutelle\".\nIn 276, it was sacked by the Vandals. The Vandals attacked again in 409, followed by the Visigoths in 414, and the Franks in 498, and afterwards the city fell into a period of relative obscurity.\nIn the late 6th century the city re-emerged as the seat of a county and an archdiocese within the Merovingian kingdom of the Franks, but royal Frankish power was never strong. The city started to play a regional role as a major urban center on the fringes of the newly founded Frankish Duchy of Vasconia. Around 585 Gallactorius was made count of Bordeaux and fought the Basques.\nIn 732, the city was plundered by the troops of Abd er Rahman who stormed the fortifications and overwhelmed the Aquitanian garrison. Duke Eudes mustered a force to engage the Umayyads, eventually engaging them in the Battle of the River Garonne somewhere near the river Dordogne. The battle had a high death toll, and although Eudes was defeated he had enough troops to engage in the Battle of Poitiers and so retain his grip on Aquitaine.\nIn 773, following his father Eudes's death, the Aquitanian duke Hunald led a rebellion to which Charles responded by launching an expedition that captured Bordeaux. However, it was not retained for long, during the following year the Frankish commander clashed in battle with the Aquitanians but then left to take on hostile Burgundian authorities and magnates. In 745 Aquitaine faced another expedition where Charles's sons Pepin and Carloman challenged Hunald's power and defeated him. Hunald's son Waifer replaced him and confirmed Bordeaux as the capital city (along with Bourges in the north).\nDuring the last stage of the war against Aquitaine (760\u2013768), it was one of Waifer's last important strongholds to fall to the troops of King Pepin the Short. Charlemagne built the fortress of Fronsac (\"Frontiacus\", \"Franciacus\") near Bordeaux on a hill across the border with the Basques (\"Wascones\"), where Basque commanders came and pledged their loyalty (769).\nIn 778, Seguin (or Sihimin) was appointed count of Bordeaux, probably undermining the power of the Duke Lupo, and possibly leading to the Battle of Roncevaux Pass[9]-. In 814, Seguin was made Duke of Vasconia, but was deposed in 816 for failing to suppress a Basque rebellion. Under the Carolingians, sometimes the Counts of Bordeaux held the title concomitantly with that of Duke of Vasconia. They were to keep the Basques in check and defend the mouth of the Garonne from the Vikings when they appeared in c. 844. In Autumn 845, the Vikings were raiding Bordeaux and Saintes, count Seguin II marched on them but was captured and executed.\nAlthough the port of Bordeaux was a buzzing trade center, the stability and success of the city was threatened by Viking and Norman incursions and political instability. The restoration of the Ramnulfid Dukes of Aquitaine under William IV and his successors (known as the House of Poitiers) brought continuity of government.\n12th century to 15th century, the English era.\nFrom the 12th to the 15th century, Bordeaux flourished once more following the marriage of El\u00e9onore, Duchess of Aquitaine and the last of the House of Poitiers, to Henry II Plantagen\u00eat, Count of Anjou and the grandson of Henry I of England, who succeeded to the English crown months after their wedding, bringing into being the vast Angevin Empire, which stretched from the Pyrenees to Ireland. After granting a tax-free trade status with England, Henry was adored by the locals as they could be even more profitable in the wine trade, their main source of income, and the city benefited from imports of cloth and wheat. The belfry (Grosse Cloche) and city cathedral St-Andr\u00e9 were built, the latter in 1227, incorporating the artisan quarter of Saint-Paul. Under the terms of the Treaty of Br\u00e9tigny it became briefly the capital of an independent state under Edward, the Black Prince (1362\u20131372), but after the Battle of Castillon (1453) it was annexed by France.\n15th century to 17th century.\nIn 1462, Bordeaux created a local parliament.\nBordeaux adhered to the Fronde, being effectively annexed to the Kingdom of France only in 1653, when the army of Louis XIV entered the city.\n18th century, the golden era.\nThe 18th century saw another golden age of Bordeaux. The Port of the Moon supplied the majority of Europe with coffee, cocoa, sugar, cotton and indigo, becoming France's busiest port and the second busiest port in the world after London. Many downtown buildings (about 5,000), including those on the quays, are from this period.\nBordeaux was also a major trading centre for slaves. In total, the Bordeaux shipowners deported 150,000 Africans in some 500 expeditions.\nFrench Revolution : political disruption, and loss of the most profitable colony.\nAt the beginning of the French Revolution (1789), many local revolutionaries were members of the Girondists. This Party represented the provincial bourgeoisie, favorable towards abolishing aristocracy privileges, but opposed to the Revolution's social dimension. In 1793, the Montagnards led by Robespierre and Marat came to power. Fearing a bourgeois misappropriation of the Revolution, they executed a great number of Girondists. During the purge, the local Montagnard Section renamed the city of Bordeaux \"Commune-Franklin\" (Franklin-municipality) in homage to Benjamin Franklin.\nAt the same time, in 1791, a slave revolt broke out at Saint-Domingue (current Haiti), the most profitable of the French colonies. Three years later, the Montagnard Convention abolished slavery. In 1802, Napoleon revoked the manumission law but lost the war against the army of former slaves. In 1804, Haiti became independent. The loss of this \"Pearl\" of the West Indies Pearl generated the collapse of Bordeaux's port economy, which was dependent on the colonial trade and trade in slaves.\nTowards the end of the Peninsula War of 1814, the Duke of Wellington sent William Beresford with two divisions and seized Bordeaux, encountering little resistance. Bordeaux was largely anti-Bonapartist and the majority supported the Bourbons. The British troops were treated as liberators.\n19th century, rebirth of the economy.\nFrom the Bourbon Restoration, the economy of Bordeaux was rebuilt by traders and shipowners. They engaged to construct the first bridge of Bordeaux, and customs warehouses. The shipping traffic grew through the new African colonies.\nGeorges-Eug\u00e8ne Haussmann, a longtime prefect of Bordeaux, used Bordeaux's 18th-century large-scale rebuilding as a model when he was asked by Emperor Napoleon III to transform the quasi-medieval Paris into a \"modern\" capital that would make France proud. Victor Hugo found the town so beautiful he said: \"Take Versailles, add Antwerp, and you have Bordeaux\".\nIn 1870, at the beginning of the Franco-Prussian war against Prussia, the French government temporarily relocated to Bordeaux from Paris. That recurred during World War I and again very briefly during World War II, when it became clear that Paris would fall into German hands.\n20th century.\nDuring World War II, Bordeaux fell under the German occupation.\nIn May and June 1940, Bordeaux was the site of the life-saving actions of the Portuguese consul-general, Aristides de Sousa Mendes, who illegally granted thousands of Portuguese visas, which were needed to pass the Spanish border, to refugees fleeing the German occupation.\nFrom 1941 to 1943, the Italian Royal Navy established BETASOM, a submarine base at Bordeaux. Italian submarines participated in the Battle of the Atlantic from that base, which was also a major base for German U-boats as headquarters of 12th U-boat Flotilla. The massive, reinforced concrete U-boat pens have proved impractical to demolish and are now partly used as a cultural center for exhibitions.\n21st century, listed as World heritage.\nIn 2007, 40% of the city surface area, located around the Port of the Moon, was listed as World heritage sites. Unesco inscribed Bordeaux as \"an inhabited historic city, an outstanding urban and architectural ensemble, created in the age of the Enlightenment, whose values continued up to the first half of the 20th century, with more protected buildings than any other French city except Paris\".\nGeography.\nBordeaux is located close to the European Atlantic coast, in the southwest of France and in the north of the Aquitaine region. It is around southwest of Paris. The city is built on a bend of the river Garonne, and is divided into two parts: the right bank to the east and left bank in the west. Historically the left bank is more developed because when flowing outside the bend, the water makes a furrow of the required depth to allow the passing of merchant ships, which used to offload on this side of the river. But, today, the right bank is developing, including new urban projects. In Bordeaux, the Garonne River is accessible to ocean liners through the Gironde estuary. The right bank of the Garonne is a low-lying, often marshy plain.\nClimate.\nBordeaux's climate is classified as a temperate oceanic climate (K\u00f6ppen climate classification \"Cfb\"), or in the Trewartha climate classification system as temperate oceanic or Do climate. Bordeaux lies close to the humid subtropical climate zone, its summers not quite warm enough for that classification.\nWinters are cool because of the prevalence of westerly winds from the Atlantic. Summers are warm and long due to the influence from the Bay of Biscay (surface temperature reaches ). The average seasonal winter temperature is , but recent winters have been warmer than this. Frosts in the winter occur several times during a winter, but snowfall is very rare, occurring only once every three years. The average summer seasonal temperature is . The summer of 2003 set a record with an average temperature of . February 1956 was the coldest month on record with an average temperature of -2,00 \u00b0C at Bordeaux M\u00e9rignac-Airport.\nEconomy.\nBordeaux is a major centre for business in France as it has the sixth largest metropolitan population in France. It serves as a major regional center for trade, administration, services and industry.\n, the GDP of Bordeaux is \u20ac32.7 billion.\nWine.\nThe vine was introduced to the Bordeaux region by the Romans, probably in the mid-first century, to provide wine for local consumption, and wine production has been continuous in the region since.\nBordeaux wine growing area has about of vineyards, 57 appellations, 10,000 wine-producing estates (ch\u00e2teaux) and 13,000 grape growers. With an annual production of approximately 960 million bottles, the Bordeaux area produces large quantities of everyday wine as well as some of the most expensive wines in the world. Included among the latter are the area's five \"premier cru\" (first growth) red wines (four from M\u00e9doc and one, Ch\u00e2teau Haut-Brion, from Graves), established by the Bordeaux Wine Official Classification of 1855:\nBoth red and white wines are made in the Bordeaux region. Red Bordeaux wine is called claret in the United Kingdom. Red wines are generally made from a blend of grapes, and may be made from Cabernet Sauvignon, Merlot, Cabernet Franc, Petit verdot, Malbec, and, less commonly in recent years, Carm\u00e9n\u00e8re.\nWhite Bordeaux is made from Sauvignon blanc, S\u00e9millon, and Muscadelle. Sauternes is a sub-region of Graves known for its intensely sweet, white, dessert wines such as Ch\u00e2teau d'Yquem.\nBecause of a wine glut (wine lake) in the generic production, the price squeeze induced by an increasingly strong international competition, and vine pull schemes, the number of growers has recently dropped from 14,000 and the area under vine has also decreased significantly. In the meantime, the global demand for first growths and the most famous labels markedly increased and their prices skyrocketed.\nThe Cit\u00e9 du Vin, a museum as well as a place of exhibitions, shows, movie projections and academic seminars on the theme of wine opened its doors in June 2016.\nOthers.\nThe Laser M\u00e9gajoule will be one of the most powerful lasers in the world, allowing fundamental research and the development of the laser and plasma technologies. This project, carried by the French Ministry of Defence, involves an investment of 2\u00a0billion euros. The \"Road of the lasers\", a major project of regional planning, promotes regional investment in optical and laser related industries leading to the Bordeaux area having the most important concentration of optical and laser expertise in Europe.\nSome 20,000 people work for the aeronautic industry in Bordeaux. The city has some of the biggest companies including Dassault, EADS Sogerma, Snecma, Thales, SNPE, and others. The Dassault Falcon private jets are built there as well as the military aircraft Rafale and Mirage 2000, the Airbus A380 cockpit, the boosters of Ariane 5, and the M51 SLBM missile.\nTourism, especially wine tourism, is a major industry. Globelink.co.uk mentioned Bordeaux as the best tourist destination in Europe in 2015.\nAccess to the port from the Atlantic is via the Gironde estuary. Almost nine million tonnes of goods arrive and leave each year.\nMajor companies.\nThis list includes indigenous Bordeaux-based companies and companies that have major presence in Bordeaux, but are not necessarily headquartered there.\nPopulation.\nIn January 2017, there were 254,436 inhabitants in the city proper (commune) of Bordeaux. Bordeaux had its largest population of 267,409 in 1921. The majority of the population is French, but there are sizable groups of Italians, Spaniards (Up to 20% of the Bordeaux population claim some degree of Spanish heritage), Portuguese, Turks, Germans.\nThe built-up area has grown for more than a century beyond the municipal borders of Bordeaux due to urban sprawl, so that by January 2017 there were 1,247,977 people living in the overall metropolitan area (\"aire urbaine\") of Bordeaux, only a fifth of whom lived in the city proper.\nLargest communities of foreigners :\nflag united states\nPolitics.\nMunicipal administration.\nThe Mayor of the city is the environmentalist Pierre Hurmic.\nBordeaux is the capital of five cantons and the Prefecture of the Gironde and Aquitaine.\nThe town is divided into three districts, the first three of Gironde. The headquarters of Urban Community of Bordeaux M\u00e9riadeck is located in the neighbourhood and the city is at the head of the Chamber of Commerce and Industry that bears his name.\nThe number of inhabitants of Bordeaux is greater than 250,000 and less than 299,999 so the number of municipal councilors is 65. They are divided according to the following composition:\nMayors of Bordeaux.\nSince the Liberation (1944), there have been 6 mayors of Bordeaux:\nElections.\nPresidential elections of 2007.\nAt the 2007 presidential election, the Bordelais gave 31.37% of their votes to S\u00e9gol\u00e8ne Royal of the Socialist Party against 30.84% to Nicolas Sarkozy, president of the UMP. Then came Fran\u00e7ois Bayrou with 22.01%, followed by Jean-Marie Le Pen who recorded 5.42%. None of the other candidates exceeded the 5% mark. Nationally, Nicolas Sarkozy led with 31.18%, then S\u00e9gol\u00e8ne Royal with 25.87%, followed by Fran\u00e7ois Bayrou with 18.57%. After these came Jean-Marie Le Pen with 10.44%, none of the other candidates exceeded the 5% mark. In the second round, the city of Bordeaux gave S\u00e9gol\u00e8ne Royal 52.44% against 47.56% for Nicolas Sarkozy, the latter being elected President of the Republic with 53.06% against 46.94% for S\u00e9gol\u00e8ne Royal. The abstention rates for Bordeaux were 14.52% in the first round and 15.90% in the second round.\nParliamentary elections of 2007.\nIn the parliamentary elections of 2007, the left won eight constituencies against only three for the right. It should be added that after the partial 2008 elections, the eighth district of Gironde switched to the left, bringing the count to nine. In Bordeaux, the left was for the first time in its history the majority as it held two of three constituencies following the elections. In the first division of the Gironde, the outgoing UMP MP Chantal Bourragu\u00e9 was well ahead with 44.81% against 25.39% for the Socialist candidate Beatrice Desaigues. In the second round, it was Chantal Bourragu\u00e9 who was re-elected with 54.45% against 45.55% for his socialist opponent. In the second district of Gironde the UMP mayor and all new Minister of Ecology, Energy, Sustainable Development and the Sea Alain Jupp\u00e9 confronted the General Counsel PS Mich\u00e8le Delaunay. In the first round, Alain Jupp\u00e9 was well ahead with 43.73% against 31.36% for Mich\u00e8le Delaunay. In the second round, it was finally Mich\u00e8le Delaunay who won the election with 50.93% of the votes against 49.07% for Alain Jupp\u00e9, the margin being only 670 votes. The defeat of the so-called constituency \"Mayor\" showed that Bordeaux was rocking increasingly left. Finally, in the third constituency of the Gironde, No\u00ebl Mam\u00e8re was well ahead with 39.82% against 28.42% for the UMP candidate Elizabeth Vine. In the second round, No\u00ebl Mam\u00e8re was re-elected with 62.82% against 37.18% for his right-wing rival.\nMunicipal elections of 2008.\nIn 2008 municipal elections saw the clash between mayor of Bordeaux, Alain Jupp\u00e9 and the President of the Regional Council of Aquitaine Socialist Alain Rousset. The PS had put up a Socialist heavyweight in the Gironde and had put great hopes in this election after the victory of S\u00e9gol\u00e8ne Royal and Mich\u00e8le Delaunay in 2007. However, after a rather exciting campaign it was Alain Jupp\u00e9 who was widely elected in the first round with 56.62%, far ahead of Alain Rousset who has managed to get 34.14%. At present, of the eight cantons that has Bordeaux, five are held by the PS and three by the UMP, the left eating a little each time into the right's numbers.\nEuropean elections of 2009.\nIn the European elections of 2009, Bordeaux voters largely voted for the UMP candidate Dominique Baudis, who won 31.54% against 15.00% for PS candidate Kader Arif. The candidate of Europe Ecology Jos\u00e9 Bov\u00e9 came second with 22.34%. None of the other candidates reached the 10% mark. The 2009 European elections were like the previous ones in eight constituencies. Bordeaux is located in the district \"Southwest\", here are the results:\nUMP candidate Dominique Baudis: 26.89%. His party gained four seats. PS candidate Kader Arif: 17.79%, gaining two seats in the European Parliament. Europe Ecology candidate Bove: 15.83%, obtaining two seats. MoDem candidate Robert Rochefort: 8.61%, winning a seat. Left Front candidate Jean-Luc M\u00e9lenchon: 8.16%, gaining the last seat. At regional elections in 2010, the Socialist incumbent president Alain Rousset won the first round by totaling 35.19% in Bordeaux, but this score was lower than the plan for Gironde and Aquitaine. Xavier Darcos, Minister of Labour followed with 28.40% of the votes, scoring above the regional and departmental average. Then came Monique De Marco, Green candidate with 13.40%, followed by the member of Pyrenees-Atlantiques and candidate of the MoDem Jean Lassalle who registered a low 6.78% while qualifying to the second round on the whole Aquitaine, closely followed by Jacques Colombier, candidate of the National Front, who gained 6.48%. Finally the candidate of the Left Front G\u00e9rard Boulanger with 5.64%, no other candidate above the 5% mark. In the second round, Alain Rousset had a tidal wave win as national totals rose to 55.83%. If Xavier Darcos largely lost the election, he nevertheless achieved a score above the regional and departmental average obtaining 33.40%. Jean Lassalle, who qualified for the second round, passed the 10% mark by totaling 10.77%. The ballot was marked by abstention amounting to 55.51% in the first round and 53.59% in the second round.\n2017 elections.\nBordeaux voted for Emmanuel Macron in the presidential election. In the 2017 parliamentary election, La R\u00e9publique En Marche! won most of the constituencies in Bordeaux.\n2019 European elections.\nBordeaux voted in the 2019 European Parliament election in France.\nMunicipal elections of 2020.\nAfter 73 years of right-of-centre rule, the ecologist Pierre Hurmic (EELV) came in ahead of Nicolas Florian (LR/LaREM).\nParliamentary representation.\nThe city area is represented by the following constituencies: Gironde's 1st, Gironde's 2nd, Gironde's 3rd, Gironde's 4th, Gironde's 5th, Gironde's 6th, Gironde's 7th.\nEducation.\nUniversity.\nDuring Antiquity, a first university had been created by the Roman in 286. The city was an important administrative centre and the new university had to train administrators. Only rhetoric and grammar were taught. Ausonius and Sulpicius Severus were two of the teachers.\nIn 1441, when Bordeaux was an English town, the Pope Eugene IV created a university by demand of the archbishop Pey Berland. In 1793, during the French Revolution, the National Convention abolished the university, and replace them with the \u00c9cole centrale in 1796. In Bordeaux, this one was located in the former buildings of the college of Guyenne.\nIn 1808, the University reappeared with Napoleon. Bordeaux accommodates approximately 70,000 students on one of the largest campuses of Europe (235\u00a0ha).\nThe University of Bordeaux is divided into four:\nSchools.\nBordeaux has numerous public and private schools offering undergraduate and postgraduate programs.\nEngineering schools:\nBusiness and management schools:\nOther:\nWeekend education.\nThe \"\u00c9cole Complem\u00e9ntaire Japonaise de Bordeaux\" (\u30dc\u30eb\u30c9\u30fc\u65e5\u672c\u8a9e\u88dc\u7fd2\u6388\u696d\u6821 \"Borud\u014d Nihongo Hosh\u016b Jugy\u014d K\u014d\"), a part-time Japanese supplementary school, is held in the \"Salle de L'Athenee Municipal\" in Bordeaux.\nMain sights.\nHeritage and architecture.\nBordeaux is classified \"City of Art and History\". The city is home to 362 \"monuments historiques\" (only Paris has more in France) with some buildings dating back to Roman times. Bordeaux, Port of the moon, has been inscribed on UNESCO World Heritage List as \"an outstanding urban and architectural ensemble\".\nBordeaux is home to one of Europe's biggest 18th-century architectural urban areas, making it a sought-after destination for tourists and cinema production crews. It stands out as one of the first French cities, after Nancy, to have entered an era of urbanism and metropolitan big scale projects, with the team Gabriel father and son, architects for King Louis XV, under the supervision of two intendants (Governors), first Nicolas-Fran\u00e7ois Dupr\u00e9 de Saint-Maur then the Marquis de Tourny.\nSaint-Andr\u00e9 Cathedral, Saint-Michel Basilica and Saint-Seurin Basilica are part of the World Heritage Sites of the Routes of Santiago de Compostela in France. The organ in Saint-Louis-des-Chartrons is registered on the French monuments historiques.\nBuildings.\nMain sights include:\nMemory of slavery.\nSlavery was part of a growing drive for the city. Firstly, during the 18th and 19th centuries, Bordeaux was an important slave port, which saw some 500 slave expeditions that cause the deportation of 150,000 Africans by Bordeaux shipowners. Secondly, even though the \"Triangular trade\" represented only 5% of Bordeaux's wealth, the city's direct trade with the Caribbean, that accounted for the other 95%, concerns the colonial stuffs made by the slave (sugar, coffee, cocoa). And thirdly, in that same period, a major migratory movement by Aquitanians took place to the Caribbean colonies, with Saint-Domingue (now Haiti) being the most popular destination. 40% of the white population of the island came from Aquitaine. They prospered with plantations incomes, until the first slave revolts which concluded in 1848 in the final abolition of slavery in France.\nToday a lot of traces and memorial sites are visible in the city. Moreover, in May 2009, the Museum of Aquitaine opened the spaces dedicated to \"Bordeaux in the 18th century, trans-Atlantic trading and slavery\". This work, richly illustrated with original documents, contributes to disseminate the state of knowledge on this question, presenting above all the facts and their chronology.\nThe region of Bordeaux was also the land of several prominent abolitionists, as Montesquieu, Laffon deLad\u00e9bat and Elis\u00e9e Reclus. Others were members of the Society of the Friends of the Blacks as the revolutionaries Boyer-Fonfr\u00e8de, Gensonn\u00e9, Guadet and Ducos.\nPont Jacques Chaban-Delmas.\nEurope's longest-span vertical-lift bridge, the Pont Jacques Chaban-Delmas, was opened in 2013 in Bordeaux, spanning the River Garonne. The central lift span is and can be lifted vertically up to to let tall ships pass underneath. The \u20ac160 million bridge was inaugurated by President Fran\u00e7ois Hollande and Mayor Alain Jupp\u00e9 on 16 March 2013. The bridge was named after the late Jacques Chaban-Delmas, who was a former Prime Minister and Mayor of Bordeaux.\nShopping.\nBordeaux has many shopping options. In the heart of Bordeaux is \"Rue Sainte-Catherine\". This pedestrian-only shopping street has of shops, restaurants and caf\u00e9s; it is also one of the longest shopping streets in Europe. \"Rue Sainte-Catherine\" starts at \"Place de la Victoire\" and ends at \"Place de la Com\u00e9die\" by the \"Grand Th\u00e9\u00e2tre\". The shops become progressively more upmarket as one moves towards \"Place de la Com\u00e9die\" and the nearby \"Cours de l'Intendance\" is where one finds the more exclusive shops and boutiques.\nCulture.\nBordeaux is also the first city in France to have created, in the 1980s, an architecture exhibition and research centre, \"Arc en r\u00eave\". Bordeaux offers a large number of cinemas, theatres, and is the home of the Op\u00e9ra national de Bordeaux. There are many music venues of varying capacity. The city also offers several festivals throughout the year.\nTransport.\nRoad.\nBordeaux is an important road and motorway junction. The city is connected to Paris by the A10 motorway, with Lyon by the A89, with Toulouse by the A62, and with Spain by the A63. There is a ring road called the \"Rocade\" which is often very busy. Another ring road is under consideration.\nBordeaux has five road bridges that cross the Garonne, the Pont de pierre built in the 1820s and three modern bridges built after 1960: the Pont Saint Jean, just south of the Pont de pierre (both located downtown), the Pont d'Aquitaine, a suspended bridge downstream from downtown, and the Pont Fran\u00e7ois Mitterrand, located upstream of downtown. These two bridges are part of the ring road around Bordeaux. A fifth bridge, the Pont Jacques-Chaban-Delmas, was constructed in 2009\u20132012 and opened to traffic in March 2013. Located halfway between the Pont de pierre and the Pont d'Aquitaine and serving downtown rather than highway traffic, it is a vertical-lift bridge with a height comparable to the Pont de pierre in closed position, and to the Pont d'Aquitaine in open position. All five road bridges, including the two highway bridges, are open to cyclists and pedestrians as well.\nAnother bridge, the Pont Jean-Jacques Bosc, is to be built in 2018.\nLacking any steep hills, Bordeaux is relatively friendly to cyclists. Cycle paths (separate from the roadways) exist on the highway bridges, along the riverfront, on the university campuses, and incidentally elsewhere in the city. Cycle lanes and bus lanes that explicitly allow cyclists exist on many of the city's boulevards. A paid bicycle-sharing system with automated stations has been established in 2010.\nRail.\nThe main railway station, Gare de Bordeaux Saint-Jean, near the center of the city, has 12\u00a0million passengers a year. It is served by the French national (SNCF) railway's high speed train, the TGV, that gets to Paris in two hours, with connections to major European centers such as Lille, Brussels, Amsterdam, Cologne, Geneva and London. The TGV also serves Toulouse and Irun (Spain) from Bordeaux. A regular train service is provided to Nantes, Nice, Marseille and Lyon. The Gare Saint-Jean is the major hub for regional trains (TER) operated by the SNCF to Arcachon, Limoges, Agen, P\u00e9rigueux, Langon, Pau, Le M\u00e9doc, Angoul\u00eame and Bayonne.\nHistorically the train line used to terminate at a station on the right bank of the river Garonne near the Pont de Pierre, and passengers crossed the bridge to get into the city. Subsequently, a double-track steel railway bridge was constructed in the 1850s, by Gustave Eiffel, to bring trains across the river direct into Gare de Bordeaux Saint-Jean. The old station was later converted and in 2010 comprised a cinema and restaurants.\nThe two-track Eiffel bridge with a speed limit of became a bottleneck and a new bridge was built, opening in 2009. The new bridge has four tracks and allows trains to pass at . During the planning there was much lobbying by the Eiffel family and other supporters to preserve the old bridge as a footbridge across the Garonne, with possibly a museum to document the history of the bridge and Gustave Eiffel's contribution. The decision was taken to save the bridge, but by early 2010 no plans had been announced as to its future use. The bridge remains intact, but unused and without any means of access.\nSince July 2017, the LGV Sud Europe Atlantique is fully operational and makes Bordeaux city 2h04 from Paris.\nAir.\nBordeaux is served by Bordeaux\u2013M\u00e9rignac Airport, located from the city centre in the suburban city of M\u00e9rignac.\nTrams, buses and boats.\nBordeaux has an important public transport system called Transports Bordeaux M\u00e9tropole (TBM). This company is run by the Keolis group. The network consists of:\nThis network is operated from 5\u00a0am to 2\u00a0am.\nThere had been several plans for a subway network to be set up, but they stalled for both geological and financial reasons. Work on the Tramway de Bordeaux system was started in the autumn of 2000, and services started in December 2003 connecting Bordeaux with its suburban areas. The tram system uses Alstom APS a form of ground-level power supply technology developed by French company Alstom and designed to preserve the aesthetic environment by eliminating overhead cables in the historic city. Conventional overhead cables are used outside the city. The system was controversial for its considerable cost of installation, maintenance and also for the numerous initial technical problems that paralysed the network. Many streets and squares along the tramway route became pedestrian areas, with limited access for cars.\nThe planned Bordeaux tramway system is to link with the airport to the city centre towards the end of 2019.\nTaxis.\nThere are more than 400 taxicabs in Bordeaux.\nPublic transportation statistics.\nThe average amount of time people spend commuting with public transit in Bordeaux, for example to and from work, on a weekday is 51 min. 12.% of public transit riders, ride for more than 2 hours every day. The average amount of time people wait at a stop or station for public transit is 13 min, while 15.5% of riders wait for over 20 minutes on average every day. The average distance people usually ride in a single trip with public transit is , while 8% travel for over in a single direction.\nSport.\nThe 41,458-capacity Nouveau Stade de Bordeaux is the largest stadium in Bordeaux. The stadium was opened in 2015 and replaced the Stade Chaban-Delmas, which was a venue for the FIFA World Cup in 1938 and 1998, as well as the 2007 Rugby World Cup. In the 1938 FIFA World Cup, it hosted a violent quarter-final known as the Battle of Bordeaux. The ground was formerly known as the \"Stade du Parc Lescure\" until 2001, when it was renamed in honour of the city's long-time mayor, Jacques Chaban-Delmas.\nThere are two major sport teams in Bordeaux, Girondins de Bordeaux is the football team, playing in Ligue 1 in the French football championship. Union Bordeaux B\u00e8gles is a rugby team in the Top 14 in the Ligue Nationale de Rugby.\nSkateboarding, rollerblading, and BMX biking are activities enjoyed by many young inhabitants of the city. Bordeaux is home to a beautiful quay which runs along the Garonne river. On the quay there is a skate-park divided into three sections. One section is for Vert tricks, one for street style tricks, and one for little action sports athletes with easier features and softer materials. The skate-park is very well maintained by the municipality.\nBordeaux is also the home to one of the strongest cricket teams in France and are champions of the South West League.\nThere is a wooden velodrome, V\u00e9lodrome du Lac, in Bordeaux which hosts international cycling competition in the form of UCI Track Cycling World Cup events.\nThe 2015 Trophee Eric Bompard was in Bordeaux. But the Free Skate was cancelled in all of the divisions due to the Paris and aftermath. The Short Program occurred hours before the bombing. French skaters Chafik Besseghier (68.36) in 10th place, Romain Ponsart (62.86) in 11th. Mae-Berenice-Meite (46.82) in 11th and Laurine Lecavelier (46.53) in 12th. Vanessa James/Morgan Cipres (65.75) in 2nd.\nBetween 1951 and 1955, an annual Formula 1 motor race was held on a 2.5-kilometre circuit which looped around the Esplanade des Quinconces and along the waterfront, attracting drivers such as Juan Manuel Fangio, Stirling Moss, Jean Behra and Maurice Trintignant.\nInternational relationship.\nTwin towns \u2013 sister cities.\nBordeaux is twinned with:"}
{"id": "4098", "revid": "27992515", "url": "https://en.wikipedia.org/wiki?curid=4098", "title": "Puzzle Bobble", "text": ", also known as Bust-a-Move, is a 1994 tile-matching puzzle arcade game developed and published by Taito. It is based on the 1986 arcade game \"Bubble Bobble\", featuring characters and themes from that game. Its characteristically cute Japanese animation and music, along with its play mechanics and level designs, made it successful as an arcade title and spawned several sequels and ports to home gaming systems.\nGameplay.\nAt the start of each round, the rectangular playing arena contains a prearranged pattern of colored \"bubbles\". At the bottom of the screen, the player controls a device called a \"pointer\", which aims and fires bubbles up the screen. The color of bubbles fired is randomly generated and chosen from the colors of bubbles still left on the screen. \nThe objective of the game is to clear all the bubbles from the arena without any bubble crossing the bottom line. Bubbles will fire automatically if the player remains idle. After clearing the arena, the next round begins with a new pattern of bubbles to clear. The game consists of 32 levels. The fired bubbles travel in straight lines (possibly bouncing off the sidewalls of the arena), stopping when they touch other bubbles or reach the top of the arena. If a bubble touches identically-colored bubbles, forming a group of three or more, those bubbles\u2014as well as any bubbles hanging from them\u2014are removed from the field of play, and points are awarded.After every few shots, the \"ceiling\" of the playing arena drops downwards slightly, along with all the bubbles stuck to it. The number of shots between each drop of the ceiling is influenced by the number of bubble colors remaining. The closer the bubbles get to the bottom of the screen, the faster the music plays and if they cross the line at the bottom then the game is over.\nRelease.\nTwo different versions of the original game were released. \"Puzzle Bobble\" was originally released in Japan only in June 1994 by Taito Corporation, running on Taito's B System hardware (with the preliminary title \"Bubble Buster\"). Then, 6 months later in December, the international Neo Geo version of \"Puzzle Bobble\" was released. It was almost identical aside from being in stereo and having some different sound effects and translated text.\nReception.\nIn Japan, \"Game Machine\" listed the Neo Geo version of \"Puzzle Bobble\" on their February 15, 1995 issue as being the second most-popular arcade game at the time.\nReviewing the Super NES version, Mike Weigand of \"Electronic Gaming Monthly\" called it \"a thoroughly enjoyable and incredibly addicting puzzle game\". He considered the two player mode the highlight, but also said that the one player mode provides a solid challenge. \"GamePro\" gave it a generally negative review, saying it \"starts out fun but ultimately lacks intricacy and longevity.\" They elaborated that in one player mode all the levels feel the same, and that two player matches are over too quickly to build up any excitement. They also criticized the lack of any 3D effects in the graphics. \"Next Generation\" reviewed the SNES version of the game, and stated that \"It's very simple, using only the control pad and one button to fire, and it's addictive as hell.\"\nA reviewer for \"Next Generation\", while questioning the continued viability of the action puzzle genre, admitted that the game is \"very simple and \"very\" addictive\". He remarked that though the 3DO version makes no significant additions, none are called for by a game with such simple enjoyment. \"GamePro\"s brief review of the 3DO version commented, \"The move-and-shoot controls are very responsive and the simple visuals and music are well done. This is one puzzler that isn't a bust.\" Edge magazine ranked the game 73rd on their 100 Best Video Games in 2007.\nLegacy.\nThe simplicity of the concept has led to many clones, both commercial and otherwise. 1996's \"Snood\" replaced the bubbles with small creatures and has been successful in its own right. \"Worms Blast\" was Team 17's take on the concept. Mobile clones include \"Bubble Witch Saga\" and \"Bubble Shooter\". \"Frozen Bubble\" is a free software clone."}
{"id": "4099", "revid": "2087512", "url": "https://en.wikipedia.org/wiki?curid=4099", "title": "Bone", "text": "A bone is a rigid tissue that constitutes part of the vertebrate skeleton in animals. Bones protect the various organs of the body, produce red and white blood cells, store minerals, provide structure and support for the body, and enable mobility. Bones come in a variety of shapes and sizes and have a complex internal and external structure. They are lightweight yet strong and hard, and serve multiple functions.\nBone tissue (osseous tissue) is a hard tissue, a type of specialized connective tissue. It has a honeycomb-like matrix internally, which helps to give the bone rigidity. Bone tissue is made up of different types of bone cells. Osteoblasts and osteocytes are involved in the formation and mineralization of bone; osteoclasts are involved in the resorption of bone tissue. Modified (flattened) osteoblasts become the lining cells that form a protective layer on the bone surface. The mineralized matrix of bone tissue has an organic component of mainly collagen called \"ossein\" and an inorganic component of bone mineral made up of various salts. Bone tissue is a mineralized tissue of two types, cortical bone and cancellous bone. Other types of tissue found in bones include bone marrow, endosteum, periosteum, nerves, blood vessels and cartilage.\nIn the human body at birth, there are approximately 270 bones present; many of these fuse together during development, leaving a total of 206 separate bones in the adult, not counting numerous small sesamoid bones. The largest bone in the body is the femur or thigh-bone, and the smallest is the stapes in the middle ear.\nThe Greek word for bone is \u1f40\u03c3\u03c4\u03ad\u03bf\u03bd (\"osteon\"), hence the many terms that use it as a prefix\u2014such as osteopathy.\nStructure.\nBone is not uniformly solid, but consists of a flexible matrix (about 30%) and bound minerals (about 70%) which are intricately woven and endlessly remodeled by a group of specialized bone cells. Their unique composition and design allows bones to be relatively hard and strong, while remaining lightweight.\nBone matrix is 90 to 95% composed of elastic collagen fibers, also known as ossein, and the remainder is ground substance. The elasticity of collagen improves fracture resistance. The matrix is hardened by the binding of inorganic mineral salt, calcium phosphate, in a chemical arrangement known as calcium hydroxylapatite. It is the bone mineralization that give bones rigidity.\nBone is actively constructed and remodeled throughout life by special bone cells known as osteoblasts and osteoclasts. Within any single bone, the tissue is woven into two main patterns, known as cortical and cancellous bone, and each with different appearance and characteristics.\nCortical bone.\nThe hard outer layer of bones is composed of cortical bone, which is also called compact bone as it is much denser than cancellous bone. It forms the hard exterior (cortex) of bones. The cortical bone gives bone its smooth, white, and solid appearance, and accounts for 80% of the total bone mass of an adult human skeleton. It facilitates bone's main functions\u2014to support the whole body, to protect organs, to provide levers for movement, and to store and release chemical elements, mainly calcium. It consists of multiple microscopic columns, each called an osteon or Haversian system. Each column is multiple layers of osteoblasts and osteocytes around a central canal called the haversian canal. Volkmann's canals at right angles connect the osteons together. The columns are metabolically active, and as bone is reabsorbed and created the nature and location of the cells within the osteon will change. Cortical bone is covered by a periosteum on its outer surface, and an endosteum on its inner surface. The endosteum is the boundary between the cortical bone and the cancellous bone. The primary anatomical and functional unit of cortical bone is the osteon.\nCancellous bone.\nCancellous bone, also called trabecular or spongy bone, is the internal tissue of the skeletal bone and is an open cell porous network. Cancellous bone has a higher surface-area-to-volume ratio than cortical bone and it is less dense. This makes it weaker and more flexible. The greater surface area also makes it suitable for metabolic activities such as the exchange of calcium ions. Cancellous bone is typically found at the ends of long bones, near joints and in the interior of vertebrae. Cancellous bone is highly vascular and often contains red bone marrow where hematopoiesis, the production of blood cells, occurs. The primary anatomical and functional unit of cancellous bone is the trabecula. The trabeculae are aligned towards the mechanical load distribution that a bone experiences within long bones such as the femur. As far as short bones are concerned, trabecular alignment has been studied in the vertebral pedicle. Thin formations of osteoblasts covered in endosteum create an irregular network of spaces, known as trabeculae. Within these spaces are bone marrow and hematopoietic stem cells that give rise to platelets, red blood cells and white blood cells. Trabecular marrow is composed of a network of rod- and plate-like elements that make the overall organ lighter and allow room for blood vessels and marrow. Trabecular bone accounts for the remaining 20% of total bone mass but has nearly ten times the surface area of compact bone.\nThe words \"cancellous\" and \"trabecular\" refer to the tiny lattice-shaped units (trabeculae) that form the tissue. It was first illustrated accurately in the engravings of Cris\u00f3stomo Martinez.\nBone marrow.\nBone marrow, also known as myeloid tissue in red bone marrow, can be found in almost any bone that holds cancellous tissue. In newborns, all such bones are filled exclusively with red marrow or hematopoietic marrow, but as the child ages the hematopoietic fraction decreases in quantity and the fatty/ yellow fraction called marrow adipose tissue (MAT) increases in quantity. In adults, red marrow is mostly found in the bone marrow of the femur, the ribs, the vertebrae and pelvic bones.\nBone cells.\nBone is a metabolically active tissue composed of several types of cells. These cells include osteoblasts, which are involved in the creation and mineralization of bone tissue, osteocytes, and osteoclasts, which are involved in the reabsorption of bone tissue. Osteoblasts and osteocytes are derived from osteoprogenitor cells, but osteoclasts are derived from the same cells that differentiate to form macrophages and monocytes. Within the marrow of the bone there are also hematopoietic stem cells. These cells give rise to other cells, including white blood cells, red blood cells, and platelets.\nOsteoblast.\n Osteoblasts are mononucleate bone-forming cells. They are located on the surface of osteon seams and make a protein mixture known as osteoid, which mineralizes to become bone. The osteoid seam is a narrow region of newly formed organic matrix, not yet mineralized, located on the surface of a bone. Osteoid is primarily composed of Type I collagen. Osteoblasts also manufacture hormones, such as prostaglandins, to act on the bone itself. The osteoblast creates and repairs new bone by actually building around itself. First, the osteoblast puts up collagen fibers. These collagen fibers are used as a framework for the osteoblasts' work. The osteoblast then deposits calcium phosphate which is hardened by hydroxide and bicarbonate ions. The brand-new bone created by the osteoblast is called osteoid. Once the osteoblast is finished working it is actually trapped inside the bone once it hardens. When the osteoblast becomes trapped, it becomes known as an osteocyte. Other osteoblasts remain on the top of the new bone and are used to protect the underlying bone, these become known as lining cells.\nOsteocyte.\nOsteocytes are cells of mesenchymal origin and originate from osteoblasts that have migrated into and become trapped and surrounded by bone matrix that they themselves produced. The spaces the cell body of osteocytes occupy within the mineralized collagen type I matrix are known as lacunae, while the osteocyte cell processes occupy channels called canaliculi. The many processes of osteocytes reach out to meet osteoblasts, osteoclasts, bone lining cells, and other osteocytes probably for the purposes of communication. Osteocytes remain in contact with other osteocytes in the bone through gap junctions\u2014coupled cell processes which pass through the canalicular channels. \nOsteoclast.\nOsteoclasts are very large multinucleate cells that are responsible for the breakdown of bones by the process of bone resorption. New bone is then formed by the osteoblasts. Bone is constantly remodeled by the resorption of osteoclasts and created by osteoblasts. Osteoclasts are large cells with multiple nuclei located on bone surfaces in what are called \"Howship's lacunae\" (or \"resorption pits\"). These lacunae are the result of surrounding bone tissue that has been reabsorbed. Because the osteoclasts are derived from a monocyte stem-cell lineage, they are equipped with phagocytic-like mechanisms similar to circulating macrophages. Osteoclasts mature and/or migrate to discrete bone surfaces. Upon arrival, active enzymes, such as tartrate-resistant acid phosphatase, are secreted against the mineral substrate. The reabsorption of bone by osteoclasts also plays a role in calcium homeostasis.\nComposition.\nBones consist of living cells (osteoblasts and osteocytes) embedded in a mineralized organic matrix. The primary inorganic component of human bone is hydroxyapatite, the dominant bone mineral, having the nominal composition of Ca10(PO4)6(OH)2. The organic components of this matrix consist mainly of type I collagen\u2014\"organic\" referring to materials produced as a result of the human body\u2014and inorganic components, which alongside the dominant hydroxyapatite phase, include other compounds of calcium and phosphate including salts. Approximately 30% of the acellular component of bone consists of organic matter, while roughly 70% by mass is attributed to the inorganic phase. The collagen fibers give bone its tensile strength, and the interspersed crystals of hydroxyapatite give bone its compressive strength. These effects are synergistic. The exact composition of the matrix may be subject to change over time due to nutrition and biomineralization, with the ratio of calcium to phosphate varying between 1.3 and 2.0 (per weight), and trace minerals such as magnesium, sodium, potassium and carbonate also being found.\nType I collagen composes 90\u201395% of the organic matrix, with remainder of the matrix being a homogenous liquid called ground substance consisting of proteoglycans such as hyaluronic acid and chondroitin sulfate, as well as non-collagenous proteins such as osteocalcin, osteopontin or bone sialoprotein. Collagen consists of strands of repeating units, which give bone tensile strength, and are arranged in an overlapping fashion that prevents shear stress. The function of ground substance is not fully known. Two types of bone can be identified microscopically according to the arrangement of collagen: woven and lamellar.\nWoven bone is produced when osteoblasts produce osteoid rapidly, which occurs initially in all fetal bones, but is later replaced by more resilient lamellar bone. In adults woven bone is created after fractures or in Paget's disease. Woven bone is weaker, with a smaller number of randomly oriented collagen fibers, but forms quickly; it is for this appearance of the fibrous matrix that the bone is termed \"woven\". It is soon replaced by lamellar bone, which is highly organized in concentric sheets with a much lower proportion of osteocytes to surrounding tissue. Lamellar bone, which makes its first appearance in humans in the fetus during the third trimester, is stronger and filled with many collagen fibers parallel to other fibers in the same layer (these parallel columns are called osteons). In cross-section, the fibers run in opposite directions in alternating layers, much like in plywood, assisting in the bone's ability to resist torsion forces. After a fracture, woven bone forms initially and is gradually replaced by lamellar bone during a process known as \"bony substitution.\" Compared to woven bone, lamellar bone formation takes place more slowly. The orderly deposition of collagen fibers restricts the formation of osteoid to about 1 to 2\u00a0\u00b5m per day. Lamellar bone also requires a relatively flat surface to lay the collagen fibers in parallel or concentric layers.\nDeposition.\nThe extracellular matrix of bone is laid down by osteoblasts, which secrete both collagen and ground substance. These synthesise collagen within the cell, and then secrete collagen fibrils. The collagen fibers rapidly polymerise to form collagen strands. At this stage they are not yet mineralised, and are called \"osteoid\". Around the strands calcium and phosphate precipitate on the surface of these strands, within days to weeks becoming crystals of hydroxyapatite.\nIn order to mineralise the bone, the osteoblasts secrete vesicles containing alkaline phosphatase. This cleaves the phosphate groups and acts as the foci for calcium and phosphate deposition. The vesicles then rupture and act as a centre for crystals to grow on. More particularly, bone mineral is formed from globular and plate structures.\nTypes.\nThere are five types of bones in the human body: long, short, flat, irregular, and sesamoid.\nTerminology.\nIn the study of anatomy, anatomists use a number of anatomical terms to describe the appearance, shape and function of bones. Other anatomical terms are also used to describe the location of bones. Like other anatomical terms, many of these derive from Latin and Greek. Some anatomists still use Latin to refer to bones. The term \"osseous\", and the prefix \"osteo-\", referring to things related to bone, are still used commonly today.\nSome examples of terms used to describe bones include the term \"foramen\" to describe a hole through which something passes, and a \"canal\" or \"meatus\" to describe a tunnel-like structure. A protrusion from a bone can be called a number of terms, including a \"condyle\", \"crest\", \"spine\", \"eminence\", \"tubercle\" or \"tuberosity\", depending on the protrusion's shape and location. In general, long bones are said to have a \"head\", \"neck\", and \"body\".\nWhen two bones join together, they are said to \"articulate\". If the two bones have a fibrous connection and are relatively immobile, then the joint is called a \"suture\".\nDevelopment.\nThe formation of bone is called ossification. During the fetal stage of development this occurs by two processes: intramembranous ossification and endochondral ossification. Intramembranous ossification involves the formation of bone from connective tissue whereas endochondral ossification involves the formation of bone from cartilage.\nIntramembranous ossification mainly occurs during formation of the flat bones of the skull but also the mandible, maxilla, and clavicles; the bone is formed from connective tissue such as mesenchyme tissue rather than from cartilage. The process includes: the development of the ossification center, calcification, trabeculae formation and the development of the periosteum.\nEndochondral ossification occurs in long bones and most other bones in the body; it involves the development of bone from cartilage. This process includes the development of a cartilage model, its growth and development, development of the primary and secondary ossification centers, and the formation of articular cartilage and the epiphyseal plates.\nEndochondral ossification begins with points in the cartilage called \"primary ossification centers.\" They mostly appear during fetal development, though a few short bones begin their primary ossification after birth. They are responsible for the formation of the diaphyses of long bones, short bones and certain parts of irregular bones. Secondary ossification occurs after birth, and forms the epiphyses of long bones and the extremities of irregular and flat bones. The diaphysis and both epiphyses of a long bone are separated by a growing zone of cartilage (the epiphyseal plate). At skeletal maturity (18 to 25 years of age), all of the cartilage is replaced by bone, fusing the diaphysis and both epiphyses together (epiphyseal closure). In the upper limbs, only the diaphyses of the long bones and scapula are ossified. The epiphyses, carpal bones, coracoid process, medial border of the scapula, and acromion are still cartilaginous.\nThe following steps are followed in the conversion of cartilage to bone:\nFunction.\nBones have a variety of functions:\nMechanical.\nBones serve a variety of mechanical functions. Together the bones in the body form the skeleton. They provide a frame to keep the body supported, and an attachment point for skeletal muscles, tendons, ligaments and joints, which function together to generate and transfer forces so that individual body parts or the whole body can be manipulated in three-dimensional space (the interaction between bone and muscle is studied in biomechanics).\nBones protect internal organs, such as the skull protecting the brain or the ribs protecting the heart and lungs. Because of the way that bone is formed, bone has a high compressive strength of about , poor tensile strength of 104\u2013121 MPa, and a very low shear stress strength (51.6 MPa). This means that bone resists pushing (compressional) stress well, resist pulling (tensional) stress less well, but only poorly resists shear stress (such as due to torsional loads). While bone is essentially brittle, bone does have a significant degree of elasticity, contributed chiefly by collagen.\nMechanically, bones also have a special role in hearing. The ossicles are three small bones in the middle ear which are involved in sound transduction.\nSynthetic.\nThe cancellous part of bones contain bone marrow. Bone marrow produces blood cells in a process called hematopoiesis. Blood cells that are created in bone marrow include red blood cells, platelets and white blood cells. Progenitor cells such as the hematopoietic stem cell divide in a process called mitosis to produce precursor cells. These include precursors which eventually give rise to white blood cells, and erythroblasts which give rise to red blood cells. Unlike red and white blood cells, created by mitosis, platelets are shed from very large cells called megakaryocytes. This process of progressive differentiation occurs within the bone marrow. After the cells are matured, they enter the circulation. Every day, over 2.5 billion red blood cells and platelets, and 50\u2013100 billion granulocytes are produced in this way.\nAs well as creating cells, bone marrow is also one of the major sites where defective or aged red blood cells are destroyed.\nMetabolic.\nDetermined by the species, age, and the type of bone, bone cells make up to 15 percent of the bone. Growth factor storage\u2014mineralized bone matrix stores important growth factors such as insulin-like growth factors, transforming growth factor, bone morphogenetic proteins and others.\nRemodeling.\nBone is constantly being created and replaced in a process known as remodeling. This ongoing turnover of bone is a process of resorption followed by replacement of bone with little change in shape. This is accomplished through osteoblasts and osteoclasts. Cells are stimulated by a variety of signals, and together referred to as a remodeling unit. Approximately 10% of the skeletal mass of an adult is remodelled each year. The purpose of remodeling is to regulate calcium homeostasis, repair microdamaged bones from everyday stress, and to shape the skeleton during growth. Repeated stress, such as weight-bearing exercise or bone healing, results in the bone thickening at the points of maximum stress (Wolff's law). It has been hypothesized that this is a result of bone's piezoelectric properties, which cause bone to generate small electrical potentials under stress.\nThe action of osteoblasts and osteoclasts are controlled by a number of chemical enzymes that either promote or inhibit the activity of the bone remodeling cells, controlling the rate at which bone is made, destroyed, or changed in shape. The cells also use paracrine signalling to control the activity of each other. For example, the rate at which osteoclasts resorb bone is inhibited by calcitonin and osteoprotegerin. Calcitonin is produced by parafollicular cells in the thyroid gland, and can bind to receptors on osteoclasts to directly inhibit osteoclast activity. Osteoprotegerin is secreted by osteoblasts and is able to bind RANK-L, inhibiting osteoclast stimulation.\nOsteoblasts can also be stimulated to increase bone mass through increased secretion of osteoid and by inhibiting the ability of osteoclasts to break down osseous tissue. Increased secretion of osteoid is stimulated by the secretion of growth hormone by the pituitary, thyroid hormone and the sex hormones (estrogens and androgens). These hormones also promote increased secretion of osteoprotegerin. Osteoblasts can also be induced to secrete a number of cytokines that promote reabsorption of bone by stimulating osteoclast activity and differentiation from progenitor cells. Vitamin D, parathyroid hormone and stimulation from osteocytes induce osteoblasts to increase secretion of RANK-ligand and interleukin 6, which cytokines then stimulate increased reabsorption of bone by osteoclasts. These same compounds also increase secretion of macrophage colony-stimulating factor by osteoblasts, which promotes the differentiation of progenitor cells into osteoclasts, and decrease secretion of osteoprotegerin.\nBone volume.\nBone volume is determined by the rates of bone formation and bone resorption. Recent research has suggested that certain growth factors may work to locally alter bone formation by increasing osteoblast activity. Numerous bone-derived growth factors have been isolated and classified via bone cultures. These factors include insulin-like growth factors I and II, transforming growth factor-beta, fibroblast growth factor, platelet-derived growth factor, and bone morphogenetic proteins. Evidence suggests that bone cells produce growth factors for extracellular storage in the bone matrix. The release of these growth factors from the bone matrix could cause the proliferation of osteoblast precursors. Essentially, bone growth factors may act as potential determinants of local bone formation. Research has suggested that cancellous bone volume in postmenopausal osteoporosis may be determined by the relationship between the total bone forming surface and the percent of surface resorption.\nClinical significance.\nA number of diseases can affect bone, including arthritis, fractures, infections, osteoporosis and tumours. Conditions relating to bone can be managed by a variety of doctors, including rheumatologists for joints, and orthopedic surgeons, who may conduct surgery to fix broken bones. Other doctors, such as rehabilitation specialists may be involved in recovery, radiologists in interpreting the findings on imaging, and pathologists in investigating the cause of the disease, and family doctors may play a role in preventing complications of bone disease such as osteoporosis.\nWhen a doctor sees a patient, a history and exam will be taken. Bones are then often imaged, called radiography. This might include ultrasound X-ray, CT scan, MRI scan and other imaging such as a Bone scan, which may be used to investigate cancer. Other tests such as a blood test for autoimmune markers may be taken, or a synovial fluid aspirate may be taken.\nFractures.\nIn normal bone, fractures occur when there is significant force applied, or repetitive trauma over a long time. Fractures can also occur when a bone is weakened, such as with osteoporosis, or when there is a structural problem, such as when the bone remodels excessively (such as Paget's disease) or is the site of the growth of cancer. Common fractures include wrist fractures and hip fractures, associated with osteoporosis, vertebral fractures associated with high-energy trauma and cancer, and fractures of long-bones. Not all fractures are painful. When serious, depending on the fractures type and location, complications may include flail chest, compartment syndromes or fat embolism.\nCompound fractures involve the bone's penetration through the skin. Some complex fractures can be treated by the use of bone grafting procedures that replace missing bone portions.\nFractures and their underlying causes can be investigated by X-rays, CT scans and MRIs. Fractures are described by their location and shape, and several classification systems exist, depending on the location of the fracture. A common long bone fracture in children is a Salter\u2013Harris fracture. When fractures are managed, pain relief is often given, and the fractured area is often immobilised. This is to promote bone healing. In addition, surgical measures such as internal fixation may be used. Because of the immobilisation, people with fractures are often advised to undergo rehabilitation.\nTumours.\nThere are several types of tumour that can affect bone; examples of benign bone tumours include osteoma, osteoid osteoma, osteochondroma, osteoblastoma, enchondroma, giant cell tumour of bone, and aneurysmal bone cyst.\nCancer.\nCancer can arise in bone tissue, and bones are also a common site for other cancers to spread (metastasise) to. Cancers that arise in bone are called \"primary\" cancers, although such cancers are rare. Metastases within bone are \"secondary\" cancers, with the most common being breast cancer, lung cancer, prostate cancer, thyroid cancer, and kidney cancer. Secondary cancers that affect bone can either destroy bone (called a \"lytic\" cancer) or create bone (a \"sclerotic\" cancer). Cancers of the bone marrow inside the bone can also affect bone tissue, examples including leukemia and multiple myeloma. Bone may also be affected by cancers in other parts of the body. Cancers in other parts of the body may release parathyroid hormone or parathyroid hormone-related peptide. This increases bone reabsorption, and can lead to bone fractures.\nBone tissue that is destroyed or altered as a result of cancers is distorted, weakened, and more prone to fracture. This may lead to compression of the spinal cord, destruction of the marrow resulting in bruising, bleeding and immunosuppression, and is one cause of bone pain. If the cancer is metastatic, then there might be other symptoms depending on the site of the original cancer. Some bone cancers can also be felt.\nCancers of the bone are managed according to their type, their stage, prognosis, and what symptoms they cause. Many primary cancers of bone are treated with radiotherapy. Cancers of bone marrow may be treated with chemotherapy, and other forms of targeted therapy such as immunotherapy may be used. Palliative care, which focuses on maximising a person's quality of life, may play a role in management, particularly if the likelihood of survival within five years is poor.\nOsteoporosis.\nOsteoporosis is a disease of bone where there is reduced bone mineral density, increasing the likelihood of fractures. Osteoporosis is defined in women by the World Health Organization as a bone mineral density of 2.5 standard deviations below peak bone mass, relative to the age and sex-matched average. This density is measured using dual energy X-ray absorptiometry (DEXA), with the term \"established osteoporosis\" including the presence of a fragility fracture. Osteoporosis is most common in women after menopause, when it is called \"postmenopausal osteoporosis\", but may develop in men and premenopausal women in the presence of particular hormonal disorders and other chronic diseases or as a result of smoking and medications, specifically glucocorticoids. Osteoporosis usually has no symptoms until a fracture occurs. For this reason, DEXA scans are often done in people with one or more risk factors, who have developed osteoporosis and are at risk of fracture.\nOsteoporosis treatment includes advice to stop smoking, decrease alcohol consumption, exercise regularly, and have a healthy diet. Calcium and trace mineral supplements may also be advised, as may Vitamin D. When medication is used, it may include bisphosphonates, Strontium ranelate, and hormone replacement therapy.\nOsteopathic medicine.\nOsteopathic medicine is a school of medical thought originally developed based on the idea of the link between the musculoskeletal system and overall health, but now very similar to mainstream medicine. , over 77,000 physicians in the United States are trained in osteopathic medical schools.\nOsteology.\nThe study of bones and teeth is referred to as osteology. It is frequently used in anthropology, archeology and forensic science for a variety of tasks. This can include determining the nutritional, health, age or injury status of the individual the bones were taken from. Preparing fleshed bones for these types of studies can involve the process of maceration.\nTypically anthropologists and archeologists study bone tools made by \"Homo sapiens\" and \"Homo neanderthalensis\". Bones can serve a number of uses such as projectile points or artistic pigments, and can also be made from external bones such as antlers.\nOther animals.\nBird skeletons are very lightweight. Their bones are smaller and thinner, to aid flight. Among mammals, bats come closest to birds in terms of bone density, suggesting that small dense bones are a flight adaptation. Many bird bones have little marrow due to their being hollow.\nA bird's beak is primarily made of bone as projections of the mandibles which are covered in keratin.\nA deer's antlers are composed of bone which is an unusual example of bone being outside the skin of the animal once the velvet is shed.\nThe extinct predatory fish \"Dunkleosteus\" had sharp edges of hard exposed bone along its jaws.\nMany animals possess an exoskeleton that is not made of bone. These include insects and crustaceans.\nThe proportion of cortical bone that is 80% in the human skeleton may be much lower in other animals, especially in marine mammals and marine turtles, or in various Mesozoic marine reptiles, such as ichthyosaurs, among others.\nMany animals, particularly herbivores, practice osteophagy\u2014the eating of bones. This is presumably carried out in order to replenish lacking phosphate.\nMany bone diseases that affect humans also affect other vertebrates\u2014an example of one disorder is skeletal fluorosis.\nSociety and culture.\nBones from slaughtered animals have a number of uses. In prehistoric times, they have been used for making bone tools. They have further been used in bone carving, already important in prehistoric art, and also in modern time as crafting materials for buttons, beads, handles, bobbins, calculation aids, head nuts, dice, poker chips, pick-up sticks, ornaments, etc. A special genre is scrimshaw.\nBone glue can be made by prolonged boiling of ground or cracked bones, followed by filtering and evaporation to thicken the resulting fluid. Historically once important, bone glue and other animal glues today have only a few specialized uses, such as in antiques restoration. Essentially the same process, with further refinement, thickening and drying, is used to make gelatin.\nBroth is made by simmering several ingredients for a long time, traditionally including bones.\nBone char, a porous, black, granular material primarily used for filtration and also as a black pigment, is produced by charring mammal bones.\nOracle bone script was a writing system used in Ancient China based on inscriptions in bones. Its name originates from oracle bones, which were mainly ox clavicle. The Ancient Chinese (mainly in the Shang dynasty), would write their questions on the oracle bone, and burn the bone, and where the bone cracked would be the answer for the questions. \nTo point the bone at someone is considered bad luck in some cultures, such as Australian aborigines, such as by the Kurdaitcha.\nThe wishbones of fowl have been used for divination, and are still customarily used in a tradition to determine which one of two people pulling on either prong of the bone may make a wish.\nVarious cultures throughout history have adopted the custom of shaping an infant's head by the practice of artificial cranial deformation. A widely practised custom in China was that of foot binding to limit the normal growth of the foot."}
{"id": "4100", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4100", "title": "Bretwalda", "text": "Bretwalda (also brytenwalda and bretenanwealda, sometimes capitalised) is an Old English word. The first record comes from the late 9th-century \"Anglo-Saxon Chronicle\". It is given to some of the rulers of Anglo-Saxon kingdoms from the 5th century onwards who had achieved overlordship of some or all of the other Anglo-Saxon kingdoms. It is unclear whether the word dates back to the 5th century and was used by the kings themselves or whether it is a later, 9th-century, invention. The term \"bretwalda\" also appears in a 10th-century charter of \u00c6thelstan. The literal meaning of the word is disputed and may translate to either 'wide-ruler' or 'Britain-ruler'.\nThe rulers of Mercia were generally the most powerful of the Anglo-Saxon kings from the mid 7th century to the early 9th century but are not accorded the title of \"bretwalda\" by the \"Chronicle\", which had an anti-Mercian bias. The \"Annals of Wales\" continued to recognise the kings of Northumbria as \"Kings of the Saxons\" until the death of Osred I of Northumbria in 716.\nEtymology.\nThe first syllable of the term \"bretwalda\" may be related to \"Briton\" or \"Britain\". The second element is taken to mean 'ruler' or 'sovereign', though is more literally 'wielder'. Thus, this interpretation would mean 'sovereign of Britain' or 'wielder of Britain'. The word may be a compound containing the Old English adjective \"brytten\" (from the verb \"breotan\" meaning 'to break' or 'to disperse'), an element also found in the terms \"bryten rice\" ('kingdom'), \"bryten-grund\" ('the wide expanse of the earth') and \"bryten cyning\" ('king whose authority was widely extended'). Though the origin is ambiguous, the draughtsman of the charter issued by \u00c6thelstan used the term in a way that can only mean 'wide-ruler'.\nThe latter etymology was first suggested by John Mitchell Kemble who alluded that \"of six manuscripts in which this passage occurs, one only reads \"Bretwalda\": of the remaining five, four have \"Bryten-walda\" or \"-wealda\", and one \"Breten-anweald\", which is precisely synonymous with Brytenwealda\"; that \u00c6thelstan was called \"brytenwealda ealles \u00f0yses ealondes\", which Kemble translates as 'ruler of all these islands'; and that \"bryten-\" is a common prefix to words meaning 'wide or general dispersion' and that the similarity to the word \"bretwealh\" ('Briton') is \"merely accidental\".\nContemporary use.\nThe first recorded use of the term \"Bretwalda\" comes from a West Saxon chronicle of the late 9th century that applied the term to Ecgberht, who ruled Wessex from 802 to 839. The chronicler also wrote down the names of seven kings that Bede listed in his \"Historia ecclesiastica gentis Anglorum\" in 731. All subsequent manuscripts of the \"Chronicle\" use the term \"Brytenwalda\", which may have represented the original term or derived from a common error.\nThere is no evidence that the term was a title that had any practical use, with implications of formal rights, powers and office, or even that it had any existence before the 9th-century. Bede wrote in Latin and never used the term and his list of kings holding \"imperium\" should be treated with caution, not least in that he overlooks kings such as Penda of Mercia, who clearly held some kind of dominance during his reign. Similarly, in his list of bretwaldas, the West Saxon chronicler ignored such Mercian kings as Offa.\nThe use of the term \"Bretwalda\" was the attempt by a West Saxon chronicler to make some claim of West Saxon kings to the whole of Great Britain. The concept of the overlordship of the whole of Britain was at least recognised in the period, whatever was meant by the term. Quite possibly it was a survival of a Roman concept of \"Britain\": it is significant that, while the hyperbolic inscriptions on coins and titles in charters often included the title \"rex Britanniae\", when England was unified the title used was \"rex Angulsaxonum\", ('king of the Anglo-Saxons'.)\nModern interpretation by historians.\nFor some time, the existence of the word \"bretwalda\" in the \"Anglo-Saxon Chronicle\", which was based in part on the list given by Bede in his \"Historia Ecclesiastica\", led historians to think that there was perhaps a \"title\" held by Anglo-Saxon overlords. This was particularly attractive as it would lay the foundations for the establishment of an English monarchy. The 20th-century historian Frank Stenton said of the Anglo-Saxon chronicler that \"his inaccuracy is more than compensated by his preservation of the English title applied to these outstanding kings\". He argued that the term \"bretwalda\" \"falls into line with the other evidence which points to the Germanic origin of the earliest English institutions\".\nOver the later 20th century, this assumption was increasingly challenged. Patrick Wormald interpreted it as \"less an objectively realized office than a subjectively perceived status\" and emphasised the partiality of its usage in favour of Southumbrian rulers. In 1991, Steven Fanning argued that \"it is unlikely that the term ever existed as a title or was in common usage in Anglo-Saxon England\". The fact that Bede never mentioned a special title for the kings in his list implies that he was unaware of one. In 1995, Simon Keynes observed that \"if Bede's concept of the Southumbrian overlord, and the chronicler's concept of the 'Bretwalda', are to be regarded as artificial constructs, which have no validity outside the context of the literary works in which they appear, we are released from the assumptions about political development which they seem to involve... we might ask whether kings in the eighth and ninth centuries were quite so obsessed with the establishment of a pan-Southumbrian state\".\nModern interpretations view the concept of \"bretwalda\" overlordship as complex and an important indicator of how a 9th-century chronicler interpreted history and attempted to insert the increasingly powerful Saxon kings into that history.\nOverlordship.\nA complex array of dominance and subservience existed during the Anglo-Saxon period. A king who used charters to grant land in another kingdom indicated such a relationship. If a king held sway over a large kingdom, such as when the Mercians dominated the East Anglians, the relationship would have been more equal than in the case of the Mercian dominance of the Hwicce, which was a comparatively small kingdom. Mercia was arguably the most powerful Anglo-Saxon kingdom for much of the late 7th though 8th centuries, though Mercian kings are missing from the two main \"lists\". For Bede, Mercia was a traditional enemy of his native Northumbria and he regarded powerful kings such as the pagan Penda as standing in the way of the Christian conversion of the Anglo-Saxons. Bede omits them from his list, even though it is evident that Penda held a considerable degree of power. Similarly powerful Mercia kings such as Offa are missed out of the West Saxon \"Anglo-Saxon Chronicle\", which sought to demonstrate the legitimacy of their kings to rule over other Anglo-Saxon peoples."}
{"id": "4101", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4101", "title": "Brouwer fixed-point theorem", "text": "Brouwer's fixed-point theorem is a fixed-point theorem in topology, named after L. E. J. (Bertus) Brouwer. It states that for any continuous function formula_1 mapping a compact convex set to itself there is a point formula_2 such that formula_3. The simplest forms of Brouwer's theorem are for continuous functions formula_1 from a closed interval formula_5 in the real numbers to itself or from a closed disk formula_6 to itself. A more general form than the latter is for continuous functions from a convex compact subset formula_7 of Euclidean space to itself.\nAmong hundreds of fixed-point theorems, Brouwer's is particularly well known, due in part to its use across numerous fields of mathematics.\nIn its original field, this result is one of the key theorems characterizing the topology of Euclidean spaces, along with the Jordan curve theorem, the hairy ball theorem and the Borsuk\u2013Ulam theorem.\nThis gives it a place among the fundamental theorems of topology. The theorem is also used for proving deep results about differential equations and is covered in most introductory courses on differential geometry.\nIt appears in unlikely fields such as game theory. In economics, Brouwer's fixed-point theorem and its extension, the Kakutani fixed-point theorem, play a central role in the proof of existence of general equilibrium in market economies as developed in the 1950s by economics Nobel prize winners Kenneth Arrow and G\u00e9rard Debreu.\nThe theorem was first studied in view of work on differential equations by the French mathematicians around Henri Poincar\u00e9 and Charles \u00c9mile Picard. Proving results such as the Poincar\u00e9\u2013Bendixson theorem requires the use of topological methods. This work at the end of the 19th century opened into several successive versions of the theorem. The general case was first proved in 1910 by Jacques Hadamard and by Luitzen Egbertus Jan Brouwer.\nStatement.\nThe theorem has several formulations, depending on the context in which it is used and its degree of generalization. The simplest is sometimes given as follows:\nThis can be generalized to an arbitrary finite dimension:\nA slightly more general version is as follows:\nAn even more general form is better known under a different name:\nImportance of the pre-conditions.\nThe theorem holds only for sets that are \"compact\" (thus, in particular, bounded and closed) and \"convex\" (or homeomorphic to convex). The following examples show why the pre-conditions are important.\nBoundedness.\nConsider the function\nwhich is a continuous function from formula_9 to itself. As it shifts every point to the right, it cannot have a fixed point. The space formula_9 is convex and closed, but not bounded.\nClosedness.\nConsider the function\nwhich is a continuous function from the open interval (\u22121,1) to itself. In this interval, it shifts every point to the right, so it cannot have a fixed point. The space (\u22121,1) is convex and bounded, but not closed. The function \"f\" have a fixed point for the closed interval [\u22121,1], namely \"f\"(1) = 1.\nConvexity.\nConvexity is not strictly necessary for BFPT. Because the properties involved (continuity, being a fixed point) are invariant under homeomorphisms, BFPT is equivalent to forms in which the domain is required to be a closed unit ball formula_12. For the same reason it holds for every set that is homeomorphic to a closed ball (and therefore also closed, bounded, connected, without holes, etc.).\nThe following example shows that BFPT doesn't work for domains with holes. Consider the function formula_13,\nwhich is a continuous function from the unit circle to itself. Since \"-x\u2260x\" holds for any point of the unit circle, \"f\" has no fixed point. The analogous example works for the \"n\"-dimensional sphere (or any symmetric domain that does not contain the origin). The unit circle is closed and bounded, but it has a hole (and so it is not convex) . The function \"f\" have a fixed point for the unit disc, since it takes the origin to itself.\nA formal generalization of BFPT for \"hole-free\" domains can be derived from the Lefschetz fixed-point theorem.\nNotes.\nThe continuous function in this theorem is not required to be bijective or even surjective.\nIllustrations.\nThe theorem has several \"real world\" illustrations. Here are some examples.\nIntuitive approach.\nExplanations attributed to Brouwer.\nThe theorem is supposed to have originated from Brouwer's observation of a cup of coffee.\nIf one stirs to dissolve a lump of sugar, it appears there is always a point without motion.\nHe drew the conclusion that at any moment, there is a point on the surface that is not moving.\nThe fixed point is not necessarily the point that seems to be motionless, since the centre of the turbulence moves a little bit.\nThe result is not intuitive, since the original fixed point may become mobile when another fixed point appears.\nBrouwer is said to have added: \"I can formulate this splendid result different, I take a horizontal sheet, and another identical one which I crumple, flatten and place on the other. Then a point of the crumpled sheet is in the same place as on the other sheet.\"\nBrouwer \"flattens\" his sheet as with a flat iron, without removing the folds and wrinkles. Unlike the coffee cup example, the crumpled paper example also demonstrates that more than one fixed point may exist. This distinguishes Brouwer's result from other fixed-point theorems, such as Stefan Banach's, that guarantee uniqueness.\nOne-dimensional case.\nIn one dimension, the result is intuitive and easy to prove. The continuous function \"f\" is defined on a closed interval [\"a\",\u00a0\"b\"] and takes values in the same interval. Saying that this function has a fixed point amounts to saying that its graph (dark green in the figure on the right) intersects that of the function defined on the same interval [\"a\",\u00a0\"b\"] which maps \"x\" to \"x\" (light green).\nIntuitively, any continuous line from the left edge of the square to the right edge must necessarily intersect the green diagonal. To prove this, consider the function \"g\" which maps \"x\" to \"f\"(\"x\")\u00a0-\u00a0\"x\". It is \u2265\u00a00 on \"a\" and \u2264\u00a00 on\u00a0\"b\". By the intermediate value theorem, \"g\" has a zero in [\"a\",\u00a0\"b\"]; this zero is a fixed point.\nBrouwer is said to have expressed this as follows: \"Instead of examining a surface, we will prove the theorem about a piece of string. Let us begin with the string in an unfolded state, then refold it. Let us flatten the refolded string. Again a point of the string has not changed its position with respect to its original position on the unfolded string.\"\nHistory.\nThe Brouwer fixed point theorem was one of the early achievements of algebraic topology, and is the basis of more general fixed point theorems which are important in functional analysis. The case \"n\" = 3 first was proved by Piers Bohl in 1904 (published in \"Journal f\u00fcr die reine und angewandte Mathematik\"). It was later proved by L. E. J. Brouwer in 1909. Jacques Hadamard proved the general case in 1910, and Brouwer found a different proof in the same year. Since these early proofs were all non-constructive indirect proofs, they ran contrary to Brouwer's intuitionist ideals. Although the existence of a fixed point is not constructive in the sense of constructivism in mathematics, methods to approximate fixed points guaranteed by Brouwer's theorem are now known.\nPrehistory.\nTo understand the prehistory of Brouwer's fixed point theorem one needs to pass through differential equations. At the end of the 19th century, the old problem of the stability of the solar system returned into the focus of the mathematical community.\nIts solution required new methods. As noted by Henri Poincar\u00e9, who worked on the three-body problem, there is no hope to find an exact solution: \"Nothing is more proper to give us an idea of the hardness of the three-body problem, and generally of all problems of Dynamics where there is no uniform integral and the Bohlin series diverge.\"\nHe also noted that the search for an approximate solution is no more efficient: \"the more we seek to obtain precise approximations, the more the result will diverge towards an increasing imprecision\".\nHe studied a question analogous to that of the surface movement in a cup of coffee. What can we say, in general, about the trajectories on a surface animated by a constant flow? Poincar\u00e9 discovered that the answer can be found in what we now call the topological properties in the area containing the trajectory. If this area is compact, i.e. both closed and bounded, then the trajectory either becomes stationary, or it approaches a limit cycle. Poincar\u00e9 went further; if the area is of the same kind as a disk, as is the case for the cup of coffee, there must necessarily be a fixed point. This fixed point is invariant under all functions which associate to each point of the original surface its position after a short time interval\u00a0\"t\". If the area is a circular band, or if it is not closed, then this is not necessarily the case.\nTo understand differential equations better, a new branch of mathematics was born. Poincar\u00e9 called it \"analysis situs\". The French Encyclop\u00e6dia Universalis defines it as the branch which \"treats the properties of an object that are invariant if it is deformed in any continuous way, without tearing\". In 1886, Poincar\u00e9 proved a result that is equivalent to Brouwer's fixed-point theorem, although the connection with the subject of this article was not yet apparent. A little later, he developed one of the fundamental tools for better understanding the analysis situs, now known as the fundamental group or sometimes the Poincar\u00e9 group. This method can be used for a very compact proof of the theorem under discussion.\nPoincar\u00e9's method was analogous to that of \u00c9mile Picard, a contemporary mathematician who generalized the Cauchy\u2013Lipschitz theorem. Picard's approach is based on a result that would later be formalised by another fixed-point theorem, named after Banach. Instead of the topological properties of the domain, this theorem uses the fact that the function in question is a contraction.\nFirst proofs.\nAt the dawn of the 20th century, the interest in analysis situs did not stay unnoticed. However, the necessity of a theorem equivalent to the one discussed in this article was not yet evident. Piers Bohl, a Latvian mathematician, applied topological methods to the study of differential equations. In 1904 he proved the three-dimensional case of our theorem, but his publication was not noticed.\nIt was Brouwer, finally, who gave the theorem its first patent of nobility. His goals were different from those of Poincar\u00e9. This mathematician was inspired by the foundations of mathematics, especially mathematical logic and topology. His initial interest lay in an attempt to solve Hilbert's fifth problem. In 1909, during a voyage to Paris, he met Henri Poincar\u00e9, Jacques Hadamard, and \u00c9mile Borel. The ensuing discussions convinced Brouwer of the importance of a better understanding of Euclidean spaces, and were the origin of a fruitful exchange of letters with Hadamard. For the next four years, he concentrated on the proof of certain great theorems on this question. In 1912 he proved the hairy ball theorem for the two-dimensional sphere, as well as the fact that every continuous map from the two-dimensional ball to itself has a fixed point. These two results in themselves were not really new. As Hadamard observed, Poincar\u00e9 had shown a theorem equivalent to the hairy ball theorem. The revolutionary aspect of Brouwer's approach was his systematic use of recently developed tools such as homotopy, the underlying concept of the Poincar\u00e9 group. In the following year, Hadamard generalised the theorem under discussion to an arbitrary finite dimension, but he employed different methods. Hans Freudenthal comments on the respective roles as follows: \"Compared to Brouwer's revolutionary methods, those of Hadamard were very traditional, but Hadamard's participation in the birth of Brouwer's ideas resembles that of a midwife more than that of a mere spectator.\"\nBrouwer's approach yielded its fruits, and in 1910 he also found a proof that was valid for any finite dimension, as well as other key theorems such as the invariance of dimension. In the context of this work, Brouwer also generalized the Jordan curve theorem to arbitrary dimension and established the properties connected with the degree of a continuous mapping. This branch of mathematics, originally envisioned by Poincar\u00e9 and developed by Brouwer, changed its name. In the 1930s, analysis situs became algebraic topology.\nReception.\nThe theorem proved its worth in more than one way. During the 20th century numerous fixed-point theorems were developed, and even a branch of mathematics called fixed-point theory.\nBrouwer's theorem is probably the most important. It is also among the foundational theorems on the topology of topological manifolds and is often used to prove other important results such as the Jordan curve theorem.\nBesides the fixed-point theorems for more or less contracting functions, there are many that have emerged directly or indirectly from the result under discussion. A continuous map from a closed ball of Euclidean space to its boundary cannot be the identity on the boundary. Similarly, the Borsuk\u2013Ulam theorem says that a continuous map from the \"n\"-dimensional sphere to Rn has a pair of antipodal points that are mapped to the same point. In the finite-dimensional case, the Lefschetz fixed-point theorem provided from 1926 a method for counting fixed points. In 1930, Brouwer's fixed-point theorem was generalized to Banach spaces. This generalization is known as Schauder's fixed-point theorem, a result generalized further by S. Kakutani to multivalued functions. One also meets the theorem and its variants outside topology. It can be used to prove the Hartman-Grobman theorem, which describes the qualitative behaviour of certain differential equations near certain equilibria. Similarly, Brouwer's theorem is used for the proof of the Central Limit Theorem. The theorem can also be found in existence proofs for the solutions of certain partial differential equations.\nOther areas are also touched. In game theory, John Nash used the theorem to prove that in the game of Hex there is a winning strategy for white. In economics, P. Bich explains that certain generalizations of the theorem show that its use is helpful for certain classical problems in game theory and generally for equilibria (Hotelling's law), financial equilibria and incomplete markets.\nBrouwer's celebrity is not exclusively due to his topological work. The proofs of his great topological theorems are not constructive, and Brouwer's dissatisfaction with this is partly what led him to articulate the idea of constructivity. He became the originator and zealous defender of a way of formalising mathematics that is known as intuitionism, which at the time made a stand against set theory. Brouwer disavowed his original proof of the fixed-point theorem. The first algorithm to approximate a fixed point was proposed by Herbert Scarf. A subtle aspect of Scarf's algorithm is that it finds a point that is by a function \"f\", but in general cannot find a point that is close to an actual fixed point. In mathematical language, if is chosen to be very small, Scarf's algorithm can be used to find a point \"x\" such that \"f\"(\"x\") is very close to \"x\", i.e., formula_14. But Scarf's algorithm cannot be used to find a point \"x\" such that \"x\" is very close to a fixed point: we cannot guarantee formula_15 where formula_16 Often this latter condition is what is meant by the informal phrase \"approximating a fixed point\".\nProof outlines.\nA proof using degree.\nBrouwer's original 1911 proof relied on the notion of the degree of a continuous mapping. Modern accounts of the proof can also be found in the literature.\nLet formula_17 denote the closed unit ball in formula_18 centered at the origin. Suppose for simplicitly that formula_19 is continuously differentiable. A regular value of formula_1 is a point formula_21 such that the Jacobian of formula_1 is non-singular at every point of the preimage of formula_23. In particular, by the inverse function theorem, every point of the preimage of formula_1 lies in formula_25 (the interior of formula_26). The degree of formula_1 at a regular value formula_21 is defined as the sum of the signs of the Jacobian determinant of formula_1 over the preimages of formula_23 under formula_1:\nThe degree is, roughly speaking, the number of \"sheets\" of the preimage \"f\" lying over a small open set around \"p\", with sheets counted oppositely if they are oppositely oriented. This is thus a generalization of winding number to higher dimensions.\nThe degree satisfies the property of \"homotopy invariance\": let formula_1 and formula_34 be two continuously differentiable functions, and formula_35 for formula_36. Suppose that the point formula_23 is a regular value of formula_38 for all \"t\". Then formula_39.\nIf there is no fixed point of the boundary of formula_26, then the function \nis well-defined, and\nformula_42\ndefines a homotopy from the identity function to it. The identity function has degree one at every point. In particular, the identity function has degree one at the origin, so formula_34 also has degree one at the origin. As a consequence, the preimage formula_44 is not empty. The elements of formula_44 are precisely the fixed points of the original function \"f\".\nThis requires some work to make fully general. The definition of degree must be extended to singular values of \"f\", and then to continuous functions. The more modern advent of homology theory simplifies the construction of the degree, and so has become a standard proof in the literature.\nA proof using homology.\nThe proof uses the observation that the boundary of the \"n\"-disk \"D\"\"n\" is \"S\"\"n\"\u22121, the (\"n\" \u2212 1)-sphere.\nSuppose, for contradiction, that a continuous function \"f\"\u00a0:\u00a0\"D\"\"n\"\u00a0\u2192\u00a0\"D\"\"n\" has \"no\" fixed point. This means that, for every point x in \"D\"\"n\", the points \"x\" and \"f\"(\"x\") are distinct. Because they're distinct, for every point x in \"D\"\"n\", we can construct a unique ray from \"f\"(\"x\") to \"x\" and follow the ray until it intersects the boundary \"S\"\"n\"\u22121 (see illustration). By calling this intersection point \"F\"(\"x\"), we define a function \"F\"\u00a0:\u00a0\"D\"\"n\"\u00a0\u2192\u00a0\"S\"\"n\"\u22121 sending each point in the disk to its corresponding intersection point on the boundary. As a special case, whenever x itself is on the boundary, then the intersection point \"F\"(\"x\") must be \"x\".\nConsequently, F is a special type of continuous function known as a retraction: every point of the codomain (in this case \"S\"\"n\"\u22121) is a fixed point of \"F\".\nIntuitively it seems unlikely that there could be a retraction of \"D\"\"n\" onto \"S\"\"n\"\u22121, and in the case \"n\" = 1, the impossibility is more basic, because \"S\"0 (i.e., the endpoints of the closed interval \"D\"1) is not even connected. The case \"n\" = 2 is less obvious, but can be proven by using basic arguments involving the fundamental groups of the respective spaces: the retraction would induce an injective group homomorphism from the fundamental group of \"S\"1 to that of \"D\"2, but the first group is isomorphic to Z while the latter group is trivial, so this is impossible. The case \"n\" = 2 can also be proven by contradiction based on a theorem about non-vanishing vector fields.\nFor \"n\" &gt; 2, however, proving the impossibility of the retraction is more difficult. One way is to make use of homology groups: the homology \"H\"\"n\"\u2009\u2212\u20091(\"D\"\"n\") is trivial, while \"H\"\"n\"\u2009\u2212\u20091(\"S\"\"n\"\u22121) is infinite cyclic. This shows that the retraction is impossible, because again the retraction would induce an injective group homomorphism from the latter to the former group.\nA proof using Stokes' theorem.\nTo prove that a continuous map formula_46 has fixed points, one can assume that it is smooth, because if a map has no fixed points then formula_47, its convolution with an appropriate mollifier (a smooth function of sufficiently small support and integral one), will produce a smooth function with no fixed points. As in the proof using homology, the problem is reduced to proving that there is no smooth retraction formula_46 from the ball formula_49 onto its boundary formula_50. If formula_51 is a volume form on the boundary then by Stokes' Theorem,\ngiving a contradiction.\nMore generally, this shows that there is no smooth retraction from any non-empty smooth orientable compact manifold onto its boundary. The proof using Stokes' theorem is closely related to the proof using homology, because the form formula_51 generates the de Rham cohomology group formula_54 which is isomorphic to the homology group formula_55 by de Rham's Theorem.\nA combinatorial proof.\nThe BFPT can be proved using Sperner's lemma. We now give an outline of the proof for the special case in which \"f\" is a function from the standard \"n\"-simplex, formula_56 to itself, where\nFor every point formula_58 also formula_59 Hence the sum of their coordinates is equal:\nHence, by the pigeonhole principle, for every formula_58 there must be an index formula_62 such that the formula_63th coordinate of formula_64 is greater than or equal to the formula_63th coordinate of its image under \"f\":\nMoreover, if formula_64 lies on a \"k\"-dimensional sub-face of formula_56 then by the same argument, the index formula_63 can be selected from among the coordinates which are not zero on this sub-face.\nWe now use this fact to construct a Sperner coloring. For every triangulation of formula_56 the color of every vertex formula_64 is an index formula_63 such that formula_73\nBy construction, this is a Sperner coloring. Hence, by Sperner's lemma, there is an \"n\"-dimensional simplex whose vertices are colored with the entire set of available colors.\nBecause \"f\" is continuous, this simplex can be made arbitrarily small by choosing an arbitrarily fine triangulation. Hence, there must be a point formula_64 which satisfies the labeling condition in all coordinates: formula_75 for all formula_76\nBecause the sum of the coordinates of formula_64 and formula_78 must be equal, all these inequalities must actually be equalities. But this means that:\nThat is, formula_64 is a fixed point of formula_81\nA proof by Hirsch.\nThere is also a quick proof, by Morris Hirsch, based on the impossibility of a differentiable retraction. The indirect proof starts by noting that the map \"f\" can be approximated by a smooth map retaining the property of not fixing a point; this can be done by using the Weierstrass approximation theorem, for example. One then defines a retraction as above which must now be differentiable. Such a retraction must have a non-singular value, by Sard's theorem, which is also non-singular for the restriction to the boundary (which is just the identity). Thus the inverse image would be a 1-manifold with boundary. The boundary would have to contain at least two end points, both of which would have to lie on the boundary of the original ball\u2014which is impossible in a retraction.\nR. Bruce Kellogg, Tien-Yien Li, and James A. Yorke turned Hirsch's proof into a computable proof by observing that the retract is in fact defined everywhere except at the fixed points. For almost any point, \"q\", on the boundary, (assuming it is not a fixed point) the one manifold with boundary mentioned above does exist and the only possibility is that it leads from \"q\" to a fixed point. It is an easy numerical task to follow such a path from \"q\" to the fixed point so the method is essentially computable. gave a conceptually similar path-following version of the homotopy proof which extends to a wide variety of related problems.\nA proof using oriented area.\nA variation of the preceding proof does not employ the Sard's theorem, and goes as follows. If formula_82 is a smooth retraction, one considers the smooth deformation formula_83 and the smooth function\nDifferentiating under the sign of integral it is not difficult to check that \"\"(\"t\") = 0 for all \"t\", so \"\u03c6\" is a constant function, which is a contradiction because \"\u03c6\"(0) is the \"n\"-dimensional volume of the ball, while \"\u03c6\"(1) is zero. The geometric idea is that \"\u03c6\"(\"t\") is the oriented area of \"g\"\"t\"(\"B\") (that is, the Lebesgue measure of the image of the ball via \"g\"\"t\", taking into account multiplicity and orientation), and should remain constant (as it is very clear in the one-dimensional case). On the other hand, as the parameter \"t\" passes form 0 to 1 the map \"g\"\"t\" transforms continuously from the identity map of the ball, to the retraction \"r\", which is a contradiction since the oriented area of the identity coincides with the volume of the ball, while the oriented area of \"r\" is necessarily 0, as its image is the boundary of the ball, a set of null measure.\nA proof using the game hex.\nA quite different proof given by David Gale is based on the game of Hex. The basic theorem about Hex is that no game can end in a draw. This is equivalent to the Brouwer fixed-point theorem for dimension 2. By considering \"n\"-dimensional versions of Hex, one can prove in general that Brouwer's theorem is equivalent to the determinacy theorem for Hex.\nA proof using the Lefschetz fixed-point theorem.\nThe Lefschetz fixed-point theorem says that if a continuous map \"f\" from a finite simplicial complex \"B\" to itself has only isolated fixed points, then the number of fixed points counted with multiplicities (which may be negative) is equal to the Lefschetz number\nand in particular if the Lefschetz number is nonzero then \"f\" must have a fixed point. If \"B\" is a ball (or more generally is contractible) then the Lefschetz number is one because the only non-zero homology group is :formula_86 and \"f\" acts as the identity on this group, so \"f\" has a fixed point.\nA proof in a weak logical system.\nIn reverse mathematics, Brouwer's theorem can be proved in the system WKL0, and conversely over the base system RCA0 Brouwer's theorem for a square implies the weak K\u00f6nig's lemma, so this gives a precise description of the strength of Brouwer's theorem.\nGeneralizations.\nThe Brouwer fixed-point theorem forms the starting point of a number of more general fixed-point theorems.\nThe straightforward generalization to infinite dimensions, i.e. using the unit ball of an arbitrary Hilbert space instead of Euclidean space, is not true. The main problem here is that the unit balls of infinite-dimensional Hilbert spaces are not compact. For example, in the Hilbert space \u21132 of square-summable real (or complex) sequences, consider the map \"f\" : \u21132 \u2192 \u21132 which sends a sequence (\"x\"\"n\") from the closed unit ball of \u21132 to the sequence (\"y\"\"n\") defined by\nIt is not difficult to check that this map is continuous, has its image in the unit sphere of \u21132, but does not have a fixed point.\nThe generalizations of the Brouwer fixed-point theorem to infinite dimensional spaces therefore all include a compactness assumption of some sort, and also often an assumption of convexity. See fixed-point theorems in infinite-dimensional spaces for a discussion of these theorems.\nThere is also finite-dimensional generalization to a larger class of spaces: If formula_88 is a product of finitely many chainable continua, then every continuous function formula_89 has a fixed point, where a chainable continuum is a (usually but in this case not necessarily metric) compact Hausdorff space of which every open cover has a finite open refinement formula_90, such that formula_91 if and only if formula_92. Examples of chainable continua include compact connected linearly ordered spaces and in particular closed intervals of real numbers.\nThe Kakutani fixed point theorem generalizes the Brouwer fixed-point theorem in a different direction: it stays in R\"n\", but considers upper hemi-continuous set-valued functions (functions that assign to each point of the set a subset of the set). It also requires compactness and convexity of the set.\nThe Lefschetz fixed-point theorem applies to (almost) arbitrary compact topological spaces, and gives a condition in terms of singular homology that guarantees the existence of fixed points; this condition is trivially satisfied for any map in the case of \"D\"\"n\"."}
{"id": "4106", "revid": "1544984", "url": "https://en.wikipedia.org/wiki?curid=4106", "title": "Benzoic acid", "text": "Benzoic acid is a white (or colorless) solid with the formula C6H5CO2H. It is the simplest aromatic carboxylic acid. The name is derived from gum benzoin, which was for a long time its only source. Benzoic acid occurs naturally in many plants and serves as an intermediate in the biosynthesis of many secondary metabolites. Salts of benzoic acid are used as food preservatives. Benzoic acid is an important precursor for the industrial synthesis of many other organic substances. The salts and esters of benzoic acid are known as benzoates .\nHistory.\nBenzoic acid was discovered in the sixteenth century. The dry distillation of gum benzoin was first described by Nostradamus (1556), and then by Alexius Pedemontanus (1560) and Blaise de Vigen\u00e8re (1596).\nJustus von Liebig and Friedrich W\u00f6hler determined the composition of benzoic acid. These latter also investigated how hippuric acid is related to benzoic acid.\nIn 1875 Salkowski discovered the antifungal abilities of benzoic acid, which was used for a long time in the preservation of benzoate-containing cloudberry fruits.\nIt is also one of the chemical compounds found in castoreum. This compound is gathered from the castor sacs of the North American beaver.\nProduction.\nIndustrial preparations.\nBenzoic acid is produced commercially by partial oxidation of toluene with oxygen. The process is catalyzed by cobalt or manganese naphthenates. The process uses abundant materials, and proceeds in high yield.\nThe first industrial process involved the reaction of benzotrichloride (trichloromethyl benzene) with calcium hydroxide in water, using iron or iron salts as catalyst. The resulting calcium benzoate is converted to benzoic acid with hydrochloric acid. The product contains significant amounts of chlorinated benzoic acid derivatives. For this reason, benzoic acid for human consumption was obtained by dry distillation of gum benzoin. Food-grade benzoic acid is now produced synthetically.\nLaboratory synthesis.\nBenzoic acid is cheap and readily available, so the laboratory synthesis of benzoic acid is mainly practiced for its pedagogical value. It is a common undergraduate preparation.\nBenzoic acid can be purified by recrystallization from water because of its high solubility in hot water and poor solubility in cold water. The avoidance of organic solvents for the recrystallization makes this experiment particularly safe. This process usually gives a yield of around 65%\nBy hydrolysis.\nLike other nitriles and amides, benzonitrile and benzamide can be hydrolyzed to benzoic acid or its conjugate base in acid or basic conditions.\nFrom Grignard reagent.\nBromobenzene can be converted to benzoic acid by \"carboxylation\" of the intermediate phenylmagnesium bromide. This synthesis offers a convenient exercise for students to carry out a Grignard reaction, an important class of carbon\u2013carbon bond forming reaction in organic chemistry.\nOxidation of benzyl compounds.\nBenzyl alcohol and benzyl chloride and virtually all benzyl derivatives are readily oxidized to benzoic acid.\nUses.\nBenzoic acid is mainly consumed in the production of phenol by oxidative decarboxylation at 300\u2212400\u00a0\u00b0C:\nThe temperature required can be lowered to 200\u00a0\u00b0C by the addition of catalytic amounts of copper (II) salts. The phenol can be converted to cyclohexanol, which is a starting material for nylon synthesis.\nPrecursor to plasticizers.\nBenzoate plasticizers, such as the glycol-, diethyleneglycol-, and triethyleneglycol esters, are obtained by transesterification of methyl benzoate with the corresponding diol. Alternatively these species arise by treatment of benzoyl chloride with the diol. These plasticizers are used similarly to those derived from terephthalic acid ester.\nPrecursor to sodium benzoate and related preservatives.\nBenzoic acid and its salts are used as a food preservatives, represented by the E numbers E210, E211, E212, and E213. Benzoic acid inhibits the growth of mold, yeast and some bacteria. It is either added directly or created from reactions with its sodium, potassium, or calcium salt. The mechanism starts with the absorption of benzoic acid into the cell. If the intracellular pH changes to 5 or lower, the anaerobic fermentation of glucose through phosphofructokinase is decreased by 95%. The efficacy of benzoic acid and benzoate is thus dependent on the pH of the food. Acidic food and beverage like fruit juice (citric acid), sparkling drinks (carbon dioxide), soft drinks (phosphoric acid), pickles (vinegar) or other acidified food are preserved with benzoic acid and benzoates.\nTypical levels of use for benzoic acid as a preservative in food are between 0.05 and 0.1%. Foods in which benzoic acid may be used and maximum levels for its application are controlled by local food laws.\nConcern has been expressed that benzoic acid and its salts may react with ascorbic acid (vitamin C) in some soft drinks, forming small quantities of carcinogenic benzene.\nMedicinal.\nBenzoic acid is a constituent of Whitfield's ointment which is used for the treatment of fungal skin diseases such as tinea, ringworm, and athlete's foot. As the principal component of gum benzoin, benzoic acid is also a major ingredient in both tincture of benzoin and Friar's balsam. Such products have a long history of use as topical antiseptics and inhalant decongestants.\nBenzoic acid was used as an expectorant, analgesic, and antiseptic in the early 20th century.\nLaboratory investigations as well as very recent theoretical work have highlighted that derivatives of benzoic acid are promising for inhibiting the coronavirus (SARS-CoV).\nBenzoyl chloride.\nBenzoic acid is a precursor to benzoyl chloride, C6H5C(O)Cl by treatment with thionyl chloride, phosgene or one of the chlorides of phosphorus. Benzoyl chloride is an important starting material for several benzoic acid derivates like benzyl benzoate, which is used in artificial flavours and insect repellents.\nNiche and laboratory uses.\nIn teaching laboratories, benzoic acid is a common standard for calibrating a bomb calorimeter.\nBiology and health effects.\nBenzoic acid occurs naturally as do its esters in many plant and animal species. Appreciable amounts are found in most berries (around 0.05%). Ripe fruits of several \"Vaccinium\" species (e.g., cranberry, \"V. vitis macrocarpon\"; bilberry, \"V. myrtillus\") contain as much as 0.03\u20130.13% free benzoic acid. Benzoic acid is also formed in apples after infection with the fungus \"Nectria galligena\". Among animals, benzoic acid has been identified primarily in omnivorous or phytophageous species, e.g., in viscera and muscles of the rock ptarmigan (\"Lagopus muta\") as well as in gland secretions of male muskoxen (\"Ovibos moschatus\") or Asian bull elephants (\"Elephas maximus\"). Gum benzoin contains up to 20% of benzoic acid and 40% benzoic acid esters.\nIn terms of its biosynthesis, benzoate is produced in plants from cinnamic acid. A pathway has been identified from phenol via 4-hydroxybenzoate.\nReactions.\nReactions of benzoic acid can occur at either the aromatic ring or at the carboxyl group:\nAromatic ring.\nElectrophilic aromatic substitution reaction will take place mainly in 3-position due to the electron-withdrawing carboxylic group; i.e. benzoic acid is \"meta\" directing.\nCarboxyl group.\nReactions typical for carboxylic acids apply also to benzoic acid.\nSafety and mammalian metabolism.\nIt is excreted as hippuric acid. Benzoic acid is metabolized by butyrate-CoA ligase into an intermediate product, benzoyl-CoA, which is then metabolized by glycine \"N\"-acyltransferase into hippuric acid. Humans metabolize toluene and benzoic acid which is excreted as hippuric acid.\nFor humans, the World Health Organization's International Programme on Chemical Safety (IPCS) suggests a provisional tolerable intake would be 5\u00a0mg/kg body weight per day. Cats have a significantly lower tolerance against benzoic acid and its salts than rats and mice. Lethal dose for cats can be as low as 300\u00a0mg/kg body weight. The oral for rats is 3040\u00a0mg/kg, for mice it is 1940\u20132263\u00a0mg/kg.\nIn Taipei, Taiwan, a city health survey in 2010 found that 30% of dried and pickled food products had benzoic acid."}
{"id": "4107", "revid": "41182775", "url": "https://en.wikipedia.org/wiki?curid=4107", "title": "Boltzmann distribution", "text": "In statistical mechanics and mathematics, a Boltzmann distribution (also called Gibbs distribution) is a probability distribution or probability measure that gives the probability that a system will be in a certain state as a function of that state's energy and the temperature of the system. The distribution is expressed in the form:\nwhere is the probability of the system being in state , is the energy of that state, and a constant of the distribution is the product of Boltzmann's constant and thermodynamic temperature . The symbol formula_2 denotes proportionality (see for the proportionality constant).\nThe term \"system\" here has a very wide meaning; it can range from a single atom to a macroscopic system such as a natural gas storage tank. Because of this the Boltzmann distribution can be used to solve a very wide variety of problems. The distribution shows that states with lower energy will always have a higher probability of being occupied .\nThe \"ratio\" of probabilities of two states is known as the Boltzmann factor and characteristically only depends on the states' energy difference:\nThe Boltzmann distribution is named after Ludwig Boltzmann who first formulated it in 1868 during his studies of the statistical mechanics of gases in thermal equilibrium. Boltzmann's statistical work is borne out in his paper \u201cOn the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium\"\nThe distribution was later investigated extensively, in its modern generic form, by Josiah Willard Gibbs in 1902.\nThe generalized Boltzmann distribution is a sufficient and necessary condition for the equivalence between the statistical mechanics definition of entropy (The Gibbs entropy formula formula_4) and the thermodynamic definition of entropy (formula_5, and the fundamental thermodynamic relation).\nThe Boltzmann distribution should not be confused with the Maxwell\u2013Boltzmann distribution. The former gives the probability that a system will be in a certain state as a function of that state's energy; in contrast, the latter is used to describe particle speeds in idealized gases.\nThe distribution.\nThe Boltzmann distribution is a probability distribution that gives the probability of a certain state as a function of that state's energy and temperature of the system to which the distribution is applied. It is given as\nwhere \"pi\" is the probability of state \"i\", \"\u03b5i\" the energy of state \"i\", \"k\" the Boltzmann constant, \"T\" the temperature of the system and \"M\" is the number of all states accessible to the system of interest. Implied parentheses around the denominator \"kT\" are omitted for brevity. The normalization denominator \"Q\" (denoted by some authors by \"Z\") is the canonical partition function\nIt results from the constraint that the probabilities of all accessible states must add up to 1.\nThe Boltzmann distribution is the distribution that maximizes the entropy\nsubject to the constraint that formula_9 equals a particular mean energy value (which can be proven using Lagrange multipliers).\nThe partition function can be calculated if we know the energies of the states accessible to the system of interest. For atoms the partition function values can be found in the NIST Atomic Spectra Database.\nThe distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy. It can also give us the quantitative relationship between the probabilities of the two states being occupied. The ratio of probabilities for states \"i\" and \"j\" is given as\nwhere \"pi\" is the probability of state \"i\", \"pj\" the probability of state \"j\", and \"\u03b5i\" and \"\u03b5j\" are the energies of states \"i\" and \"j\", respectively.\nThe Boltzmann distribution is often used to describe the distribution of particles, such as atoms or molecules, over energy states accessible to them. If we have a system consisting of many particles, the probability of a particle being in state \"i\" is practically the probability that, if we pick a random particle from that system and check what state it is in, we will find it is in state \"i\". This probability is equal to the number of particles in state \"i\" divided by the total number of particles in the system, that is the fraction of particles that occupy state \"i\".\nwhere \"Ni\" is the number of particles in state \"i\" and \"N\" is the total number of particles in the system. We may use the Boltzmann distribution to find this probability that is, as we have seen, equal to the fraction of particles that are in state i. So the equation that gives the fraction of particles in state \"i\" as a function of the energy of that state is \nThis equation is of great importance to spectroscopy. In spectroscopy we observe a spectral line of atoms or molecules that we are interested in going from one state to another. In order for this to be possible, there must be some particles in the first state to undergo the transition. We may find that this condition is fulfilled by finding the fraction of particles in the first state. If it is negligible, the transition is very likely not to be observed at the temperature for which the calculation was done. In general, a larger fraction of molecules in the first state means a higher number of transitions to the second state. This gives a stronger spectral line. However, there are other factors that influence the intensity of a spectral line, such as whether it is caused by an allowed or a forbidden transition.\nThe Boltzmann distribution is related to the softmax function commonly used in machine learning.\nIn statistical mechanics.\nThe Boltzmann distribution appears in statistical mechanics when considering isolated (or nearly-isolated) systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble, but also some special cases (derivable from the canonical ensemble) also show the Boltzmann distribution in different aspects:\nAlthough these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed:\nIn mathematics.\nIn more general mathematical settings, the Boltzmann distribution is also known as the Gibbs measure. In statistics and machine learning, it is called a log-linear model. In deep learning, the Boltzmann distribution is used in the sampling distribution of stochastic neural networks such as the Boltzmann machine, Restricted Boltzmann machine, Energy-Based models and deep Boltzmann machine.\nIn economics.\nThe Boltzmann distribution can be introduced to allocate permits in emissions trading. The new allocation method using the Boltzmann distribution can describe the most probable, natural, and unbiased distribution of emissions permits among multiple countries. Simple and versatile, this new method holds potential for many economic and environmental applications.\nThe Boltzmann distribution has the same form as the multinomial logit model. As a discrete choice model, this is very well known in economics since Daniel McFadden made the connection to random utility maximization."}
{"id": "4109", "revid": "13568690", "url": "https://en.wikipedia.org/wiki?curid=4109", "title": "Leg theory", "text": "Leg theory is a bowling tactic in the sport of cricket. The term \"leg theory\" is somewhat archaic and seldom used any longer, but the basic tactic remains a play in modern cricket.\nSimply put, leg theory involves concentrating the bowling attack at or near the line of leg stump. This may or may not be accompanied by a concentration of fielders on the leg side. The line of attack aims to cramp the batsman, making him play the ball with the bat close to the body. This makes it difficult to hit the ball freely and score runs, especially on the off side. Since a leg theory attack means the batsman is more likely to hit the ball on the leg side, additional fielders on that side of the field can be effective in preventing runs and taking catches.\nStifling the batsman in this manner can lead to impatience and frustration, resulting in rash play by the batsman which in turn can lead to a quick dismissal.\nLeg theory can be a moderately successful tactic when used with both fast bowling and spin bowling, particularly leg spin to right-handed batsmen or off spin to left-handed batsmen. However, because it relies on lack of concentration or discipline by the batsman, it can be risky against patient and skilled players, especially batsmen who are strong on the leg side. The English opening bowlers Sydney Barnes and Frank Foster used leg theory with some success in Australia in 1911\u201312. In England, at around the same time Fred Root was one of the main proponents of the same tactic.\nConcentrating attack on the leg stump is considered by many cricket fans and commentators to lead to boring play, as it stifles run scoring and encourages batsmen to play conservatively.\nFast leg theory.\nIn 1930, England captain Douglas Jardine, together with Nottinghamshire's captain Arthur Carr and his bowlers Harold Larwood and Bill Voce, developed a variant of leg theory in which the bowlers bowled fast, short-pitched balls that would rise into the batsman's body, together with a heavily stacked ring of close fielders on the leg side. The idea was that when the batsman defended against the ball, he would be likely to deflect the ball into the air for a catch.\nJardine called this modified form of the tactic \"fast leg theory\". On the 1932-33 English tour of Australia, Larwood and Voce bowled fast leg theory at the Australian batsmen. It turned out to be extremely dangerous, and most Australian players sustained injuries from being hit by the ball. Wicket-keeper Bert Oldfield's skull was fractured by a ball hitting his head (although the ball had first glanced off the bat and Larwood had an orthodox field), almost precipitating a riot by the Australian crowd.\nThe Australian press dubbed the tactic \"Bodyline\", and claimed it was a deliberate attempt by the English team to intimidate and injure the Australian players. Reports of the controversy reaching England at the time described the bowling as \"fast leg theory\", which sounded to many people to be a harmless and well-established tactic. This led to a serious misunderstanding amongst the English public and the Marylebone Cricket Club - the administrators of English cricket - of the dangers posed by Bodyline. The English press and cricket authorities declared the Australian protests to be a case of sore losing and \"squealing\".\nIt was only with the return of the English team and the subsequent use of Bodyline against English players in England by the touring West Indian cricket team in 1933 that demonstrated to the country the dangers it posed. The MCC subsequently revised the Laws of Cricket to prevent the use of \"fast leg theory\" tactics in future, also limiting the traditional tactic."}
{"id": "4110", "revid": "1010275017", "url": "https://en.wikipedia.org/wiki?curid=4110", "title": "Blythe Danner", "text": "Blythe Katherine Danner (born February 3, 1943) is an American actress. She is the recipient of several accolades, including two Primetime Emmy Awards for Best Supporting Actress in a Drama Series for her role as Izzy Huffstodt on \"Huff\" (2004\u20132006), and a Tony Award for Best Actress for her performance in \"Butterflies Are Free\" on Broadway (1969\u20131972). Danner was twice nominated for the Primetime Emmy for Outstanding Guest Actress in a Comedy Series for portraying Marilyn Truman on \"Will &amp; Grace\" (2001\u201306; 2018\u201320), and the Primetime Emmy for Outstanding Lead Actress in a Miniseries or Movie for her roles in \"We Were the Mulvaneys\" (2002) and \"Back When We Were Grownups\" (2004). For the latter, she also received a Golden Globe Award nomination.\nDanner played Dina Byrnes in \"Meet the Parents\" (2000) and its sequels \"Meet the Fockers\" (2004) and \"Little Fockers\" (2010). She has collaborated on several occasions with Woody Allen, appearing in three of his films: \"Another Woman\" (1988), \"Alice\" (1990), and \"Husbands and Wives\" (1992). Her other notable film credits include \"1776\" (1972), \"Hearts of the West\" (1975), \"The Great Santini\" (1979), \"Mr. and Mrs. Bridge\" (1990), \"The Prince of Tides\" (1991), \"To Wong Foo, Thanks for Everything! Julie Newmar\" (1995), \"The Myth of Fingerprints\" (1997), \"The X-Files\" (1998), \"Forces of Nature\" (1999), \"The Last Kiss\" (2006), \"Paul\" (2011), \"Hello I Must Be Going\" (2012), \"I'll See You in My Dreams\" (2015), and \"What They Had\" (2018).\nDanner is the sister of Harry Danner and the widow of Bruce Paltrow. She is the mother of actress Gwyneth Paltrow and director Jake Paltrow.\nEarly life.\nDanner was born in Philadelphia, Pennsylvania, the daughter of Katharine (n\u00e9e Kile; 1909\u20132006) and Harry Earl Danner, a bank executive. She has a brother, opera singer and actor Harry Danner; a sister, performer-turned-director Dorothy \"Dottie\" Danner; and a maternal half-brother, violin maker William Moennig. Danner has Pennsylvania Dutch (German), and some English and Irish, ancestry; her maternal grandmother was a German immigrant, and one of her paternal great-grandmothers was born in Barbados (to a family of European descent).\nDanner graduated from George School, a Quaker high school located near Newtown, Bucks County, Pennsylvania in 1960.\nCareer.\nA graduate of Bard College, Danner's first roles included the 1967 musical \"Mata Hari\" (closed out of town), and the 1968 Off-Broadway production of \"Summertree\". Her early Broadway appearances included \"Cyrano de Bergerac\" (1968) and her Theatre World Award-winning performance in \"The Miser\" (1969). She won the Tony Award for Best Featured Actress in a Play for portraying a free-spirited divorc\u00e9e in \"Butterflies Are Free\" (1970).\nIn 1972, Danner portrayed Martha Jefferson in the film version of \"1776\". That same year, she played the unknowing wife of a husband who committed murder, opposite Peter Falk and John Cassavetes, in the \"Columbo\" episode \"Etude in Black\".\nHer earliest starring film role was opposite Alan Alda in \"To Kill a Clown\" (1972). Danner appeared in the episode of \"M*A*S*H\" entitled \"The More I See You\", playing the love interest of Alda's character Hawkeye Pierce. She played lawyer Amanda Bonner in television's \"Adam's Rib\", also opposite Ken Howard as Adam Bonner. She played Zelda Fitzgerald in \"F. Scott Fitzgerald and 'The Last of the Belles\"' (1974). She was the eponymous heroine in the film \"Lovin' Molly\" (1974) (directed by Sidney Lumet). She appeared in \"Futureworld\", playing Tracy Ballard with co-star Peter Fonda (1976). In the 1982 TV movie \"Inside the Third Reich\", she played the wife of Albert Speer. In the film version of Neil Simon's semi-autobiographical play \"Brighton Beach Memoirs\" (1986), she portrayed a middle-aged Jewish mother. She has appeared in two films based on the novels of Pat Conroy, \"The Great Santini\" (1979) and \"The Prince of Tides\" (1991), as well as two television movies adapted from books by Anne Tyler, \"Saint Maybe\" and \"Back When We Were Grownups\", both for the Hallmark Hall of Fame.\nDanner appeared opposite Robert De Niro in the 2000 comedy hit \"Meet the Parents\", and its sequels, \"Meet the Fockers\" (2004) and \"Little Fockers\" (2010).\nFrom 2001 to 2006, she regularly appeared on NBC's sitcom \"Will &amp; Grace\" as Will Truman's mother Marilyn. From 2004 to 2006, she starred in the main cast of the comedy-drama series \"Huff\". In 2005, she was nominated for three Primetime Emmy Awards for her work on \"Will &amp; Grace\", \"Huff\", and the television film \"Back When We Were Grownups\", winning for her role in \"Huff\". The following year, she won a second consecutive Emmy Award for \"Huff\". For 25 years, she has been a regular performer at the Williamstown Summer Theater Festival, where she also serves on the Board of Directors.\nIn 2006, Danner was awarded an inaugural Katharine Hepburn Medal by Bryn Mawr College's Katharine Houghton Hepburn Center. In 2015, Danner was inducted into the American Theater Hall of Fame.\nEnvironmental activism.\nDanner has been involved in environmental issues such as recycling and conservation for over 30 years. She has been active with INFORM, Inc., is on the Board of Environmental Advocates of New York and the Board of Directors of the Environmental Media Association, and won the 2002 EMA Board of Directors Ongoing Commitment Award. In 2011, Danner joined Moms Clean Air Force, to help call on parents to join in the fight against toxic air pollution.\nHealth care activism.\nAfter the death of her husband Bruce Paltrow from oral cancer, she became involved with the nonprofit Oral Cancer Foundation. In 2005, she filmed a public service announcement to raise public awareness of the disease and the need for early detection. She has since appeared on morning talk shows and given interviews in such magazines as \"People\". The Bruce Paltrow Oral Cancer Fund, administered by the Oral Cancer Foundation, raises funding for oral cancer research and treatment, with a particular focus on those communities in which healthcare disparities exist.\nShe has also appeared in commercials for Prolia, a brand of denosumab used in the treatment of osteoporosis.\nPersonal life.\nDanner was married to producer and director Bruce Paltrow, who died of oral cancer in 2002. She and Paltrow had two children together, actress Gwyneth Paltrow and director Jake Paltrow.\nDanner's niece is the actress Katherine Moennig, the daughter of her maternal half-brother William.\nDanner co-starred with her daughter in the 1992 television film \"Cruel Doubt\" and again in the 2003 film \"Sylvia\", in which she portrayed Aurelia Plath, mother to Gwyneth's title role of Sylvia Plath.\nDanner is a practitioner of transcendental meditation, which she has described as \"very helpful and comforting.\""}
{"id": "4111", "revid": "35209614", "url": "https://en.wikipedia.org/wiki?curid=4111", "title": "Bioleaching", "text": "Bioleaching is the extraction of metals from their ores through the use of living organisms. This is much cleaner than the traditional heap leaching using cyanide. Bioleaching is one of several applications within biohydrometallurgy and several methods are used to recover copper, zinc, lead, arsenic, antimony, nickel, molybdenum, gold, silver, and cobalt.\nProcess.\nBioleaching can involve numerous ferrous iron and sulfur oxidizing bacteria, including \"Acidithiobacillus ferrooxidans\" (formerly known as \"Thiobacillus ferrooxidans\") and \"Acidithiobacillus thiooxidans \" (formerly known as \"Thiobacillus thiooxidans\"). As a general principle, Fe3+ ions are used to oxidize the ore. This step is entirely independent of microbes. The role of the bacteria is the further oxidation of the ore, but also the regeneration of the chemical oxidant Fe3+ from Fe2+. For example, bacteria catalyse the breakdown of the mineral pyrite (FeS2) by oxidising the sulfur and metal (in this case ferrous iron, (Fe2+)) using oxygen. This yields soluble products that can be further purified and refined to yield the desired metal.\nPyrite leaching (FeS2):\nIn the first step, disulfide is spontaneously oxidized to thiosulfate by ferric ion (Fe3+), which in turn is reduced to give ferrous ion (Fe2+):\nThe ferrous ion is then oxidized by bacteria using oxygen:\nThiosulfate is also oxidized by bacteria to give sulfate:\nThe ferric ion produced in reaction (2) oxidized more sulfide as in reaction (1), closing the cycle and given the net reaction:\nThe net products of the reaction are soluble ferrous sulfate and sulfuric acid.\nThe microbial oxidation process occurs at the cell membrane of the bacteria. The electrons pass into the cells and are used in biochemical processes to produce energy for the bacteria while reducing oxygen to water. The critical reaction is the oxidation of sulfide by ferric iron. The main role of the bacterial step is the regeneration of this reactant.\nThe process for copper is very similar, but the efficiency and kinetics depend on the copper mineralogy. The most efficient minerals are supergene minerals such as chalcocite, Cu2S and covellite, CuS. The main copper mineral chalcopyrite (CuFeS2) is not leached very efficiently, which is why the dominant copper-producing technology remains flotation, followed by smelting and refining. The leaching of CuFeS2 follows the two stages of being dissolved and then further oxidised, with Cu2+ ions being left in solution.\nChalcopyrite leaching:\nnet reaction:\nIn general, sulfides are first oxidized to elemental sulfur, whereas disulfides are oxidized to give thiosulfate, and the processes above can be applied to other sulfidic ores. Bioleaching of non-sulfidic ores such as pitchblende also uses ferric iron as an oxidant (e.g., UO2 + 2 Fe3+ ==&gt; UO22+ + 2 Fe2+). In this case, the sole purpose of the bacterial step is the regeneration of Fe3+. Sulfidic iron ores can be added to speed up the process and provide a source of iron. Bioleaching of non-sulfidic ores by layering of waste sulfides and elemental sulfur, colonized by \"Acidithiobacillus\" spp., has been accomplished, which provides a strategy for accelerated leaching of materials that do not contain sulfide minerals.\nFurther processing.\nThe dissolved copper (Cu2+) ions are removed from the solution by ligand exchange solvent extraction, which leaves other ions in the solution. The copper is removed by bonding to a ligand, which is a large molecule consisting of a number of smaller groups, each possessing a lone electron pair. The ligand-copper complex is extracted from the solution using an organic solvent such as kerosene:\nThe ligand donates electrons to the copper, producing a complex - a central metal atom (copper) bonded to the ligand. Because this complex has no charge, it is no longer attracted to polar water molecules and dissolves in the kerosene, which is then easily separated from the solution. Because the initial reaction is reversible, it is determined by pH. Adding concentrated acid reverses the equation, and the copper ions go back into an aqueous solution.\nThen the copper is passed through an electro-winning process to increase its purity: An electric current is passed through the resulting solution of copper ions. Because copper ions have a 2+ charge, they are attracted to the negative cathodes and collect there.\nThe copper can also be concentrated and separated by displacing the copper with Fe from scrap iron:\nThe electrons lost by the iron are taken up by the copper. Copper is the oxidising agent (it accepts electrons), and iron is the reducing agent (it loses electrons).\nTraces of precious metals such as gold may be left in the original solution. Treating the mixture with sodium cyanide in the presence of free oxygen dissolves the gold. The gold is removed from the solution by adsorbing (taking it up on the surface) to charcoal.\nWith fungi.\nSeveral species of fungi can be used for bioleaching. Fungi can be grown on many different substrates, such as electronic scrap, catalytic converters, and fly ash from municipal waste incineration. Experiments have shown that two fungal strains (\"Aspergillus niger, Penicillium simplicissimum\") were able to mobilize Cu and Sn by 65%, and Al, Ni, Pb, and Zn by more than 95%. \"Aspergillus niger\" can produce some organic acids such as citric acid. This form of leaching does not rely on microbial oxidation of metal but rather uses microbial metabolism as source of acids that directly dissolve the metal.\nFeasibility.\nEconomic feasibility.\nBioleaching is in general simpler and, therefore, cheaper to operate and maintain than traditional processes, since fewer specialists are needed to operate complex chemical plants. And low concentrations are not a problem for bacteria because they simply ignore the waste that surrounds the metals, attaining extraction yields of over 90% in some cases. These microorganisms actually gain energy by breaking down minerals into their constituent elements. The company simply collects the ions out of the solution after the bacteria have finished. There is a limited amount of ores.\nBioleaching can be used to extract metals from low concentration ores as gold that are too poor for other technologies. It can be used to partially replace the extensive crushing and grinding that translates to prohibitive cost and energy consumption in a conventional process. Because the lower cost of bacterial leaching outweighs the time it takes to extract the metal.\nHigh concentration ores like copper is more economical to smelt rather than to use bioleaching because the profit obtained from the speed and yield of smelting justifies its cost due bacterial leaching process being very slow compared to smelting. This brings in less profit as well as introducing a significant delay in cash flow for new plants. Nonetheless, at the largest copper mine of the world, Escondida in Chile the process seems to be favorable.\nEconomically it is also very expensive and many companies once started can not keep up with the demand and end up in debt.\nIn space.\nIn 2020 scientists showed, with an experiment with different gravity environments on the ISS, that microorganisms could be employed to mine useful elements from basaltic rocks via bioleaching in space.\nEnvironmental impact.\nThe process is more environmentally friendly than traditional extraction methods. For the company this can translate into profit, since the necessary limiting of sulfur dioxide emissions during smelting is expensive. Less landscape damage occurs, since the bacteria involved grow naturally, and the mine and surrounding area can be left relatively untouched. As the bacteria breed in the conditions of the mine, they are easily cultivated and recycled.\nToxic chemicals are sometimes produced in the process. Sulfuric acid and H+ ions that have been formed can leak into the ground and surface water turning it acidic, causing environmental damage. Heavy ions such as iron, zinc, and arsenic leak during acid mine drainage. When the pH of this solution rises, as a result of dilution by fresh water, these ions precipitate, forming \"Yellow Boy\" pollution. For these reasons, a setup of bioleaching must be carefully planned, since the process can lead to a biosafety failure. Unlike other methods, once started, bioheap leaching cannot be quickly stopped, because leaching would still continue with rainwater and natural bacteria. Projects like Finnish Talvivaara proved to be environmentally and economically disastrous."}
{"id": "4113", "revid": "38464105", "url": "https://en.wikipedia.org/wiki?curid=4113", "title": "Bouldering", "text": "Bouldering is a form of free climbing that is performed on small rock formations or artificial rock walls without the use of ropes or harnesses. While bouldering can be done without any equipment, most climbers use climbing shoes to help secure footholds, chalk to keep their hands dry and to provide a firmer grip, and bouldering mats to prevent injuries from falls. Unlike free solo climbing, which is also performed without ropes, bouldering problems (the sequence of moves that a climber performs to complete the climb) are usually less than 6 meters (20\u00a0ft.) tall. Traverses, which are a form of boulder problem, require the climber to climb horizontally from one end to another. Artificial climbing walls allow boulderers to climb indoors in areas without natural boulders. In addition, bouldering competitions take place in both indoor and outdoor settings.\nThe sport was originally a method of training for roped climbs and mountaineering, so climbers could practice specific moves at a safe distance from the ground. Additionally, the sport served to build stamina and increase finger strength. Throughout the 20th century, bouldering evolved into a separate discipline. Individual problems are assigned ratings based on difficulty. Although there have been various rating systems used throughout the history of bouldering, modern problems usually use either the V-scale or the Fontainebleau scale.\nThe growing popularity of bouldering has caused several environmental concerns, including soil erosion and trampled vegetation, as climbers often hike off-trail to reach bouldering sites. This has caused some landowners to restrict access or prohibit bouldering altogether.\nOutdoor bouldering.\nThe characteristics of boulder problems depend largely on the type of rock being climbed. For example, granite often features long cracks and slabs while sandstone rocks are known for their steep overhangs and frequent horizontal breaks. Limestone and volcanic rock are also used for bouldering.\nThere are many prominent bouldering areas throughout the United States, including Hueco Tanks in Texas, Mount Evans in Colorado, and The Buttermilks in Bishop, California. Squamish, British Columbia is one of the most popular bouldering areas in Canada. Europe is also home to a number of bouldering sites, such as Fontainebleau in France, Albarrac\u00edn in Spain, and various mountains throughout Switzerland. Africa's most prominent bouldering areas include the more established Rocklands, South Africa, the newer Oukaimeden in Morocco or more recently opened areas like Chimanimani in Zimbabwe.\nIndoor bouldering.\nArtificial climbing walls are used to simulate boulder problems in an indoor environment, usually at climbing gyms. These walls are constructed with wooden panels, polymer cement panels, concrete shells, or precast molds of actual rock walls. Holds, usually made of plastic, are then bolted onto the wall to create problems. The walls often feature steep overhanging surfaces which force the climber to employ highly technical movements while supporting much of their weight with their upper body strength. However, in more recent times, many problems set on flat walls now require the climber to execute a series of coordinated movements in order to complete the route. These routes were likely to have originated at the Stuntwerk gym in Germany, and the style of climbing can be said to closely resemble the sport of Parkour. The IFSC Climbing World Championships have noticeably included more of such problems in their competitions as of late.\nClimbing gyms often feature multiple problems within the same section of wall. In the US the most common method route-setters use to designate the intended problem is by placing colored tape next to each hold. For example, red tape would indicate one bouldering problem while green tape would be used to set a different problem in the same area. Across much of the rest of the world problems and grades are usually designated using a set color of plastic hold to indicate problems and their difficulty levels. Using colored holds to set has certain advantages, the most notable of which are that it makes it more obvious where the holds for a problem are, and that there is no chance of tape being accidentally kicked off footholds. Smaller, resource-poor climbing gyms may prefer taped problems because large, expensive holds can be used in multiple routes by marking them with more than one color of tape.\nGrading.\nBouldering problems are assigned numerical difficulty ratings by route-setters and climbers. The two most widely used rating systems are the V-scale and the Fontainbleau system.\nThe V-scale, which originated in the United States, is an open-ended rating system with higher numbers indicating a higher degree of difficulty. The V1 rating indicates that a problem can be completed by a novice climber in good physical condition after several attempts. The scale begins at V0, and as of 2013, the highest V rating that has been assigned to a bouldering problem is V17. Some climbing gyms also use a VB grade to indicate beginner problems.\nThe Fontainebleau scale follows a similar system, with each numerical grade divided into three ratings with the letters \"a\", \"b\", and \"c\". For example, Fontainebleau 7A roughly corresponds with V6, while Fontainebleau 7C+ is equivalent to V10. In both systems, grades are further differentiated by appending \"+\" to indicate a small increase in difficulty. Despite this level of specificity, ratings of individual problems are often controversial, as ability level is not the only factor that affects how difficult a problem may be for a particular climber. Height, arm length, flexibility, and other body characteristics can also be relevant to perceived difficulty.\nHighball bouldering.\nHighball bouldering is simply climbing high, difficult, long, and tall boulders. Using the same protection as standard bouldering, climbers venture up house-sized rocks that test not only their physical skill and strength but mental focus. Highballing, like most of climbing, is open to interpretation. Most climbers say anything above 15 feet is a highball and can range in height up to 35\u201340 feet where highball bouldering then turns into free soloing.\nHighball bouldering may have begun in 1961 when John Gill, without top-rope rehearsal, bouldered a steep face on a 37-foot (11 meter) granite spire called \"The Thimble\". The difficulty level of this ascent (V4/5 or 5.12a) was extraordinary for that time. Gill's achievement initiated a wave of climbers making ascents of large boulders. Later, with the introduction and evolution of crash pads, climbers were able to push the limits of highball bouldering ever higher.\nIn 2002 Jason Kehl completed the first highball at double-digit V-difficulty, called Evilution, a 55-foot (16.8 meter) boulder in the Buttermilks of California, earning the grade of V12. This climb marked the beginning of a new generation of highball climbing that pushed not only height, but great difficulty.It is not unusual for climbers to rehearse such risky problems on top-rope, although this practice is not a settled issue.\nGroundbreaking ascents in this style include; \"Ambrosia\", a 55-foot (16.8 meter) boulder in Bishop, California, climbed by Kevin Jorgeson in 2015 sporting the grade of V11.\n\"Too Big to Flail,\" V10, another 55 foot (16.8 meter) line in Bishop, California, climbed by Alex Honnold in 2016.\n\"Livin' Large\", a 35-foot V15 in Rocklands, South Africa, found and established by Nalle Hukkataival in 2009, which has been repeated by only one person, Jimmy Webb.\n\"The Process\" is a 55-foot V16 in Bishop, California, first climbed by Daniel Woods in 2015. The line was worked with another climber, Dan Beal, but a hold broke after Woods's top and the climb has yet to see a second ascent as of Sep 28, 2017.\nCompetitions.\nTraditionally, competition in bouldering was informal, with climbers working out problems near the limits of their abilities, then challenging their peers to repeat these accomplishments. However, modern climbing gyms allow for a more formal competitive structure.\nThe International Federation of Sport Climbing (IFSC) employs an indoor format (although competitions can also take place in an outdoor setting) that breaks the competition into three rounds: qualifications, semi-finals, and finals. The rounds feature different sets of four to six boulder problems, and each competitor has a fixed amount of time to attempt each problem. At the end of each round, competitors are ranked by the number of completed problems with ties settled by the total number of attempts taken to solve the problems.\nSome competitions only permit climbers a fixed number of attempts at each problem with a timed rest period in between. In an open-format competition, all climbers compete simultaneously, and are given a fixed amount of time to complete as many problems as possible. More points are awarded for more difficult problems, while points are deducted for multiple attempts on the same problem.\nIn 2012, the IFSC submitted a proposal to the International Olympic Committee (IOC) to include lead climbing in the 2020 Summer Olympics. The proposal was later revised to an \"overall\" competition, which would feature bouldering, lead climbing, and speed climbing. In May 2013, the IOC announced that climbing would not be added to the 2020 Olympic program.\nIn 2016, the International Olympic Committee (IOC) officially approved climbing as an Olympic sport \"in order to appeal to younger audiences.\" The Olympics will feature the earlier proposed overall competition. Medalists will be competing in all three categories for a best overall score. The score will be calculated by the multiplication of the positions that the climbers have attained in each discipline of climbing.\nHistory.\nRock climbing first appeared as a sport in the late-1800s. Early records describe climbers engaging in what is now referred to as bouldering, not as a separate discipline, but as a playful form of training for larger ascents. It was during this time that the words \"bouldering\" and \"problem\" first appeared in British climbing literature. Oscar Eckenstein was an early proponent of the activity in the British Isles. In the early 20th century, the Fontainebleau area of France established itself as a prominent climbing area, where some of the first dedicated \"bleausards\" (or \"boulderers\") emerged. One of those athletes, Pierre Allain, invented the specialized shoe used for rock climbing.\nIn the late 1950s through the 1960s, American mathematician John Gill pushed the sport further and contributed several important innovations, distinguishing bouldering as a separate discipline in the process. Gill previously pursued gymnastics, a sport which had an established scale of difficulty for movements and body positions, and shifted the focus of bouldering from reaching the summit to navigating a set of holds. Gill developed a rating system that was closed-ended: B1 problems were as difficult as the most challenging roped routes of the time, B2 problems were more difficult, and B3 problems had been completed once.\nGill introduced chalk as a method of keeping the climber's hands dry, promoted a dynamic climbing style, and emphasized the importance of strength training to complement skill. As Gill improved in ability and influence, his ideas became the norm.\nIn the 1980s, two important training tools emerged. One important training tool was bouldering mats, also referred to as \"crash pads\", which protected against injuries from falling and enabled boulderers to climb in areas that would have been too dangerous otherwise. The second important tool was indoor climbing walls, which helped spread the sport to areas without outdoor climbing and allowed serious climbers to train year-round.\nAs the sport grew in popularity, new bouldering areas were developed throughout Europe and the United States, and more athletes began participating in bouldering competitions. The visibility of the sport greatly increased in the early 2000s, as YouTube videos and climbing blogs helped boulderers around the world to quickly learn techniques, find hard problems, and announce newly completed projects.\nNotable ascents.\n (unconfirmed):\nEquipment.\nUnlike other climbing sports, bouldering can be performed safely and effectively with very little equipment, an aspect which makes the discipline highly appealing, but opinions differ. While bouldering pioneer John Sherman asserted that \"The only gear really needed to go bouldering is boulders,\" others suggest the use of climbing shoes and a chalkbag \u2013 a small pouch where ground-up chalk is kept \u2013 as the bare minimum, and more experienced boulderers typically bring multiple pairs of climbing shoes, chalk, brushes, crash pads, and a skincare kit. \nClimbing shoes have the most direct impact on performance. Besides protecting the climber's feet from rough surfaces, climbing shoes are designed to help the climber secure footholds. Climbing shoes typically fit much tighter than other athletic footwear and often curl the toes downwards to enable precise footwork. They are manufactured in a variety of different styles to perform in different situations. For example, High-top shoes provide better protection for the ankle, while low-top shoes provide greater flexibility and freedom of movement. Stiffer shoes excel at securing small edges, whereas softer shoes provide greater sensitivity. The front of the shoe, called the \"toe box\", can be asymmetric, which performs well on overhanging rocks, or symmetric, which is better suited for vertical problems and slabs.\u2009\u2009\nTo absorb sweat, most boulderers use gymnastics chalk on their hands, stored in a chalkbag, which can be tied around the waist (also called sport climbing chalkbags), allowing the climber to reapply chalk during the climb. There are also versions of floor chalkbags (also called bouldering chalkbags), which are usually bigger than sport climbing chalkbags and are meant to be kept on the floor while climbing; this is because boulders do not usually have so many movements as to require chalking up more than once. Different sizes of brushes are used to remove excess chalk and debris from boulders in between climbs; they are often attached to the end of a long straight object in order to reach higher holds. Crash pads, also referred to as bouldering mats, are foam cushions placed on the ground to protect climbers from falls.\nSafety.\nBoulder problems are generally shorter than from ground to top. This makes the sport significantly safer than free solo climbing, which is also performed without ropes, but with no upper limit on the height of the climb. However, minor injuries are common in bouldering, particularly sprained ankles and wrists. Two factors contribute to the frequency of injuries in bouldering: first, boulder problems typically feature more difficult moves than other climbing disciplines, making falls more common. Second, without ropes to arrest the climber's descent, every fall will cause the climber to hit the ground.\nTo prevent injuries, boulderers position crash pads near the boulder to provide a softer landing, as well as one or more spotters (people watching out for the climber to fall in convenient position) to help redirect the climber towards the pads. Upon landing, boulderers employ falling techniques similar to those used in gymnastics: spreading the impact across the entire body to avoid bone fractures, and positioning limbs to allow joints to move freely throughout the impact.\nTechnique.\nAlthough every type of rock climbing requires a high level of strength and technique, bouldering is the most dynamic form of the sport, requiring the highest level of power and placing considerable strain on the body. Training routines that strengthen fingers and forearms are useful in preventing injuries such as tendonitis and ruptured ligaments.\nHowever, as with other forms of climbing, bouldering technique begins with proper footwork. Leg muscles are significantly stronger than arm muscles; thus, proficient boulderers use their arms to maintain balance and body positioning as much as possible, relying on their legs to push them up the rock. Boulderers also keep their arms straight with their shoulders engaged whenever feasible, allowing their bones to support their body weight rather than their muscles.\nBouldering movements are described as either \"static\" or \"dynamic\". Static movements are those that are performed slowly, with the climber's position controlled by maintaining contact on the boulder with the other three limbs. Dynamic movements use the climber's momentum to reach holds that would be difficult or impossible to secure statically, with an increased risk of falling if the movement is not performed accurately.\nEnvironmental impact.\nBouldering can damage vegetation that grows on rocks, such as moss and lichens. This can occur as a result of the climber intentionally cleaning the boulder, or unintentionally from repeated use of handholds and footholds. Vegetation on the ground surrounding the boulder can also be damaged from overuse, particularly by climbers laying down crash pads. Soil erosion can occur when boulderers trample vegetation while hiking off of established trails, or when they unearth small rocks near the boulder in an effort to make the landing zone safer in case of a fall. The repeated use of white climbing chalk can damage the rock surface of boulders and cliffs, particularly sandstone and other porous rock types, and the scrubbing of rocks to remove chalk can also degrade the rock surface. In order to prevent chalk from damaging the surface of the rock, it is important to remove it gently with a brush after a rock climbing session. Other environmental concerns include littering, improperly disposed feces, and graffiti. These issues have caused some land managers to prohibit bouldering, as was the case in Tea Garden, a popular bouldering area in Rocklands, South Africa."}
{"id": "4115", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4115", "title": "Boiling point", "text": "The boiling point of a substance is the temperature at which the vapor pressure of a liquid equals the pressure surrounding the liquid and the liquid changes into a vapor.\nThe boiling point of a liquid varies depending upon the surrounding environmental pressure. A liquid in a partial vacuum has a lower boiling point than when that liquid is at atmospheric pressure. A liquid at high pressure has a higher boiling point than when that liquid is at atmospheric pressure. For example, water boils at at sea level, but at at altitude. For a given pressure, different liquids will boil at different temperatures.\nThe normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, one atmosphere. At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of one bar.\nThe heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).\nLiquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.\nSaturation temperature and pressure.\nA \"saturated liquid\" contains as much thermal energy as it can without boiling (or conversely a \"saturated vapor\" contains as little thermal energy as it can without condensing).\nSaturation temperature means \"boiling point\". The saturation temperature is the temperature for a corresponding saturation pressure at which a liquid boils into its vapor phase. The liquid can be said to be saturated with thermal energy. Any addition of thermal energy results in a phase transition.\nIf the pressure in a system remains constant (isobaric), a vapor at saturation temperature will begin to condense into its liquid phase as thermal energy (heat) is removed. Similarly, a liquid at saturation temperature and pressure will boil into its vapor phase as additional thermal energy is applied.\nThe boiling point corresponds to the temperature at which the vapor pressure of the liquid equals the surrounding environmental pressure. Thus, the boiling point is dependent on the pressure. Boiling points may be published with respect to the NIST, USA standard pressure of 101.325 kPa (or 1 atm), or the IUPAC standard pressure of 100.000 kPa. At higher elevations, where the atmospheric pressure is much lower, the boiling point is also lower. The boiling point increases with increased pressure up to the critical point, where the gas and liquid properties become identical. The boiling point cannot be increased beyond the critical point. Likewise, the boiling point decreases with decreasing pressure until the triple point is reached. The boiling point cannot be reduced below the triple point.\nIf the heat of vaporization and the vapor pressure of a liquid at a certain temperature are known, the boiling point can be calculated by using the Clausius\u2013Clapeyron equation, thus:\nwhere:\nSaturation pressure is the pressure for a corresponding saturation temperature at which a liquid boils into its vapor phase. Saturation pressure and saturation temperature have a direct relationship: as saturation pressure is increased, so is saturation temperature.\nIf the temperature in a system remains constant (an \"isothermal\" system), vapor at saturation pressure and temperature will begin to condense into its liquid phase as the system pressure is increased. Similarly, a liquid at saturation pressure and temperature will tend to flash into its vapor phase as system pressure is decreased.\nThere are two conventions regarding the \"standard boiling point of water\": The \"normal boiling point\" is at a pressure of 1 atm (i.e., 101.325 kPa). The IUPAC recommended \"standard boiling point of water\" at a standard pressure of 100 kPa (1 bar) is . For comparison, on top of Mount Everest, at elevation, the pressure is about and the boiling point of water is .\nThe Celsius temperature scale was defined until 1954 by two points: 0\u00a0\u00b0C being defined by the water freezing point and 100\u00a0\u00b0C being defined by the water boiling point at standard atmospheric pressure.\nRelation between the normal boiling point and the vapor pressure of liquids.\nThe higher the vapor pressure of a liquid at a given temperature, the lower the normal boiling point (i.e., the boiling point at atmospheric pressure) of the liquid.\nThe vapor pressure chart to the right has graphs of the vapor pressures versus temperatures for a variety of liquids. As can be seen in the chart, the liquids with the highest vapor pressures have the lowest normal boiling points.\nFor example, at any given temperature, methyl chloride has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (\u221224.2\u00a0\u00b0C), which is where the vapor pressure curve of methyl chloride (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.\nThe critical point of a liquid is the highest temperature (and pressure) it will actually boil at.\nSee also Vapour pressure of water.\nProperties of the elements.\nThe element with the lowest boiling point is helium. Both the boiling points of rhenium and tungsten exceed 5000 K at standard pressure; because it is difficult to measure extreme temperatures precisely without bias, both have been cited in the literature as having the higher boiling point.\nBoiling point as a reference property of a pure compound.\nAs can be seen from the above plot of the logarithm of the vapor pressure vs. the temperature for any given pure chemical compound, its normal boiling point can serve as an indication of that compound's overall volatility. A given pure compound has only one normal boiling point, if any, and a compound's normal boiling point and melting point can serve as characteristic physical properties for that compound, listed in reference books. The higher a compound's normal boiling point, the less volatile that compound is overall, and conversely, the lower a compound's normal boiling point, the more volatile that compound is overall. Some compounds decompose at higher temperatures before reaching their normal boiling point, or sometimes even their melting point. For a stable compound, the boiling point ranges from its triple point to its critical point, depending on the external pressure. Beyond its triple point, a compound's normal boiling point, if any, is higher than its melting point. Beyond the critical point, a compound's liquid and vapor phases merge into one phase, which may be called a superheated gas. At any given temperature, if a compound's normal boiling point is lower, then that compound will generally exist as a gas at atmospheric external pressure. If the compound's normal boiling point is higher, then that compound can exist as a liquid or solid at that given temperature at atmospheric external pressure, and will so exist in equilibrium with its vapor (if volatile) if its vapors are contained. If a compound's vapors are not contained, then some volatile compounds can eventually evaporate away in spite of their higher boiling points.\nIn general, compounds with ionic bonds have high normal boiling points, if they do not decompose before reaching such high temperatures. Many metals have high boiling points, but not all. Very generally\u2014with other factors being equal\u2014in compounds with covalently bonded molecules, as the size of the molecule (or molecular mass) increases, the normal boiling point increases. When the molecular size becomes that of a macromolecule, polymer, or otherwise very large, the compound often decomposes at high temperature before the boiling point is reached. Another factor that affects the normal boiling point of a compound is the polarity of its molecules. As the polarity of a compound's molecules increases, its normal boiling point increases, other factors being equal. Closely related is the ability of a molecule to form hydrogen bonds (in the liquid state), which makes it harder for molecules to leave the liquid state and thus increases the normal boiling point of the compound. Simple carboxylic acids dimerize by forming hydrogen bonds between molecules. A minor factor affecting boiling points is the shape of a molecule. Making the shape of a molecule more compact tends to lower the normal boiling point slightly compared to an equivalent molecule with more surface area.\nMost volatile compounds (anywhere near ambient temperatures) go through an intermediate liquid phase while warming up from a solid phase to eventually transform to a vapor phase. By comparison to boiling, a sublimation is a physical transformation in which a solid turns directly into vapor, which happens in a few select cases such as with carbon dioxide at atmospheric pressure. For such compounds, a sublimation point is a temperature at which a solid turning directly into vapor has a vapor pressure equal to the external pressure.\nImpurities and mixtures.\nIn the preceding section, boiling points of pure compounds were covered. Vapor pressures and boiling points of substances can be affected by the presence of dissolved impurities (solutes) or other miscible compounds, the degree of effect depending on the concentration of the impurities or other compounds. The presence of non-volatile impurities such as salts or compounds of a volatility far lower than the main component compound decreases its mole fraction and the solution's volatility, and thus raises the normal boiling point in proportion to the concentration of the solutes. This effect is called boiling point elevation. As a common example, salt water boils at a higher temperature than pure water.\nIn other mixtures of miscible compounds (components), there may be two or more components of varying volatility, each having its own pure component boiling point at any given pressure. The presence of other volatile components in a mixture affects the vapor pressures and thus boiling points and dew points of all the components in the mixture. The dew point is a temperature at which a vapor condenses into a liquid. Furthermore, at any given temperature, the composition of the vapor is different from the composition of the liquid in most such cases. In order to illustrate these effects between the volatile components in a mixture, a boiling point diagram is commonly used. Distillation is a process of boiling and [usually] condensation which takes advantage of these differences in composition between liquid and vapor phases."}
{"id": "4116", "revid": "18426370", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": "The Big Bang theory is a cosmological model of the observable universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from an initial state of high density and temperature, and offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure.\nCrucially, the theory is compatible with Hubble-Lema\u00eetre law \u2013 the observation that the farther away galaxies are, the faster they are moving away from Earth. Extrapolating this cosmic expansion backwards in time using the known laws of physics, the theory describes a high density state preceded by a singularity in which space and time lose meaning. There is no evidence of any phenomena prior to the singularity. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8\u00a0billion years ago, which is thus considered the age of the universe.\nAfter its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later atoms. Giant clouds of these primordial elements \u2013 mostly hydrogen, with some helium and lithium \u2013 later coalesced through gravity, forming early stars and galaxies, the descendants of which are visible today. Besides these primordial building materials, astronomers observe the gravitational effects of an unknown dark matter surrounding galaxies. Most of the gravitational potential in the universe seems to be in this form, and the Big Bang theory and various observations indicate that this excess gravitational potential is not created by baryonic matter, such as normal atoms. Measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence.\nGeorges Lema\u00eetre first noted in 1927 that an expanding universe could be traced back in time to an originating single point, which he called the \"primeval atom\". Edwin Hubble confirmed through analysis of galactic redshifts in 1929 that galaxies are indeed drifting apart; this is important observational evidence for an expanding universe. For several decades, the scientific community was divided between supporters of the Big Bang and the rival steady-state model. In 1964, the CMB was discovered, which was crucial evidence in favor of the hot Big Bang model, since that theory predicted a uniform background radiation throughout the universe. A wide range of empirical evidence strongly favors the Big Bang, which is now universally accepted. \nFeatures of the model.\nThe Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundances of the light elements, the CMB, large-scale structure, and Hubble's law. The theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The universality of physical laws is one of the underlying principles of the theory of relativity. The cosmological principle states that on large scales the universe is homogeneous and isotropic.\nThese ideas were initially taken as postulates, but later efforts were made to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine-structure constant over much of the age of the universe is of order 10\u22125. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\nThe large-scale universe appears isotropic as viewed from Earth. If it is indeed isotropic, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10\u22125 via observations of the temperature of the CMB. At the scale of the CMB horizon, the universe has been measured to be homogeneous with an upper bound on the order of 10% inhomogeneity, as of 1995.\nExpansion of space.\nThe expansion of the Universe was inferred from early twentieth century astronomical observations and is an essential ingredient of the Big Bang theory. Mathematically, general relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, are specified using a coordinate chart or \"grid\" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker (FLRW) metric. This metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their \"coordinate\" distance (comoving distance) remains constant, the \"physical\" distance between two such co-moving points expands proportionally with the scale factor of the universe.\nThe Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distances between comoving points. In other words, the Big Bang is not an explosion \"in space\", but rather an expansion \"of space\". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales\u2014local concentrations of matter such as our galaxy do not necessarily expand with the same speed as the whole Universe.\nHorizons.\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not yet had time to reach us. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe.\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\nThermalisation.\nSome processes in the early universe occurred too slowly, compared to the expansion rate of the universe, to reach approximate thermodynamic equilibrium. Others were fast enough to reach thermalisation. The parameter usually used to find out whether a process in the very early universe has reached thermal equilibrium is the ratio between the rate of the process (usually rate of collisions between particles) and the Hubble parameter. The larger the ratio, the more time particles had to thermalise before they were too far away from each other.\nTimeline.\nAccording to the Big Bang theory, the universe at the beginning was very hot and very compact, and since then it has been expanding and cooling down.\nSingularity.\nExtrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This irregular behavior, known as the gravitational singularity, indicates that general relativity is not an adequate description of the laws of physics in this regime. Models based on general relativity alone can not extrapolate toward the singularity \u2014 beyond the end of the so-called Planck epoch.\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the Standard Model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event \u2014 known as the \"age of the universe\" \u2014 is 13.799 \u00b1 0.021\u00a0billion years.\nDespite being extremely dense at this time\u2014far denser than is usually required to form a black hole\u2014the universe did not re-collapse into a singularity. This may be explained by considering that commonly used calculations and limits for gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not apply to rapidly expanding space such as the Big Bang. Likewise, since the early universe did not immediately collapse into a multitude of black holes, matter at that time must have been very evenly distributed with a negligible density gradient.\nInflation and baryogenesis.\nThe earliest phases of the Big Bang are subject to much speculation, since astronomical data about them are not available. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures, and was very rapidly expanding and cooling. The period from 0 to 10\u221243 seconds into the expansion, the Planck epoch, was a phase in which the four fundamental forces \u2014 the electromagnetic force, the strong nuclear force, the weak nuclear force, and the gravitational force, were unified as one. In this stage, the characteristic scale length of the universe was the planck length, , and consequently had a temperature of approximately 1032 degrees Celsius. Even the very concept of a particle breaks down in these conditions. A proper understanding of this period awaits the development of a theory of quantum gravity. The Planck epoch was succeeded by the grand unification epoch beginning at 10\u221243 seconds, where gravitation separated from the other forces as the universe's temperature fell.\nAt approximately 10\u221237 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially, unconstrained by the light speed invariance, and temperatures dropped by a factor of 100,000. Microscopic quantum fluctuations that occurred because of Heisenberg's uncertainty principle were amplified into the seeds that would later form the large-scale structure of the universe. At a time around 10\u221236 seconds, the Electroweak epoch begins when the strong nuclear force separates from the other forces, with only the electromagnetic force and weak nuclear force remaining unified.\nInflation stopped at around the 10\u221233 to 10\u221232 seconds mark, with the universe's volume having increased by a factor of at least 1078. Reheating occurred until the universe obtained the temperatures required for the production of a quark\u2013gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle\u2013antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons\u2014of the order of one part in 30\u00a0million. This resulted in the predominance of matter over antimatter in the present universe.\nCooling.\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry-breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form, with the electromagnetic force and weak nuclear force separating at about 10\u221212 seconds. After about 10\u221211 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10\u22126 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton\u2013antiproton pairs (similarly for neutrons\u2013antineutrons), so a mass annihilation immediately followed, leaving just one in 1010 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\nA few minutes into the expansion, when the temperature was about a billion kelvin and the density of matter in the universe was comparable to the current density of Earth's atmosphere, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis (BBN). Most protons remained uncombined as hydrogen nuclei.\nAs the universe cooled, the rest energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years, the electrons and nuclei combined into atoms (mostly hydrogen), which were able to emit radiation. This relic radiation, which continued through space largely unimpeded, is known as the cosmic microwave background.\nStructure formation.\nOver a long period of time, the slightly denser regions of the uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from the Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an \"extended model\" which includes hot dark matter in the form of neutrinos, then if the \"physical baryon density\" formula_1 is estimated at about 0.023 (this is different from the 'baryon density' formula_2 expressed as a fraction of the total matter/energy density, which is about 0.046), and the corresponding cold dark matter density formula_3 is about 0.11, the corresponding neutrino density formula_4 is estimated to be less than 0.0062.\nCosmic acceleration.\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate.\nDark energy in its simplest formulation takes the form of the cosmological constant term in Einstein field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theoretically.\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the \u039bCDM model of cosmology, which uses the independent frameworks of quantum mechanics and general relativity. There are no easily testable models that would describe the situation prior to approximately 10\u221215 seconds. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.\nHistory.\nEtymology.\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a talk for a March 1949 BBC Radio broadcast, saying: \"These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.\"\nIt is popularly reported that Hoyle, who favored an alternative \"steady-state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.\nDevelopment.\nThe Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912, Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Einstein field equations, showing that the universe might be expanding in contrast to the static universe model advocated by Albert Einstein at that time.\nIn 1924, American astronomer Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Starting that same year, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929, Hubble discovered a correlation between distance and recessional velocity\u2014now known as Hubble's law. By that time, Lema\u00eetre had already shown that this was expected, given the cosmological principle.\nIndependently deriving Friedmann's equations in 1927, Georges Lema\u00eetre, a Belgian physicist and Roman Catholic priest, proposed that the inferred recession of the nebulae was due to the expansion of the universe. In 1931, Lema\u00eetre went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\nIn the 1920s and 1930s, almost every major cosmologist preferred an eternal steady-state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady-state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Lema\u00eetre, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, \"viz\"., that matter is eternal. A beginning in time was \"repugnant\" to him. Lema\u00eetre, however, disagreed:\nDuring the 1930s, other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard C. Tolman) and Fritz Zwicky's tired light hypothesis.\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady-state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lema\u00eetre's Big Bang theory, advocated and developed by George Gamow, who introduced BBN and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lema\u00eetre's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over steady state. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.\nIn 1968 and 1970, Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\nIn the mid-1990s, observations of certain globular clusters appeared to indicate that they were about 15\u00a0billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as the Cosmic Background Explorer (COBE), the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\nObservational evidence.\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis (BBN). More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the \"four pillars\" of the Big Bang theory.\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently the subject of most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.\nHubble's law and the expansion of space.\nObservations of distant galaxies and quasars show that these objects are redshifted: the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nformula_5\nwhere\nHubble's law has two possible explanations. Either we are at the center of an explosion of galaxies\u2014which is untenable under the assumption of the Copernican principle\u2014or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Friedmann in 1922 and Lema\u00eetre in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lema\u00eetre, Robertson, and Walker.\nThe theory requires the relation formula_9 to hold at all times, where formula_7 is the comoving distance, \"v\" is the recessional velocity, and formula_6, formula_12, and formula_7 vary as the universe expands (hence we write formula_8 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity formula_6. However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.\nThat space is undergoing metric expansion is shown by direct observational evidence of the cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.\nMeasurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the CMB over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.\nCosmic microwave background radiation.\nIn 1964, Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s, the radiation was found to be approximately consistent with a blackbody spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725\u00a0K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded the 1978 Nobel Prize in Physics.\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\nIn 1989, NASA launched COBE, which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 104, and measured a residual temperature of 2.726\u00a0K (more recent measurements have revised this figure down slightly to 2.7255\u00a0K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 105. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000\u20132001, several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The \"Planck\" space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.\nAbundance of primordial elements.\nUsing the Big Bang model, it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for &lt;chem&gt;^4He/H&lt;/chem&gt;, about 10\u22123 for &lt;chem&gt;^2H/H&lt;/chem&gt;, about 10\u22124 for &lt;chem&gt;^3He/H&lt;/chem&gt; and about 10\u22129 for &lt;chem&gt;^7Li/H&lt;/chem&gt;.\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for &lt;chem&gt;^4He&lt;/chem&gt;, and off by a factor of two for &lt;chem&gt;^7Li&lt;/chem&gt; (this anomaly is known as the cosmological lithium problem); in the latter two cases, there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by BBN is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20\u201330% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than &lt;chem&gt;^3He&lt;/chem&gt;, and in constant ratios, too.\nGalactic evolution and distribution.\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently, appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\nPrimordial gas clouds.\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during BBN.\nOther lines of evidence.\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars. It is also in good agreement with age estimates based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background. The agreement of independent measurements of this age supports the Lambda-CDM (\u039bCDM) model, since the model is used to relate some of the measurements to an age estimate, and all estimates turn out to agree. Still, some observations of objects from the relatively early universe (in particular quasar APM 08279+5255) raise concern as to whether these objects had enough time to form so early in the \u039bCDM model.\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev\u2013Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\nFuture observations.\nFuture gravitational-wave observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\nProblems and related issues in physics.\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.\nBaryon asymmetry.\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\nDark energy.\nMeasurements of the redshift\u2013magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\nDark matter.\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\nHorizon problem.\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit\u2014the particle horizon\u2014on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\nA resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to a cosmic scale. These fluctuations served as the seeds for all the current structures in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.\nIf inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\nMagnetic monopoles.\nThe magnetic monopole objection was raised in the late 1970s. Grand Unified theories (GUTs) predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\nFlatness problem.\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a FLRW. The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be \"flat\". Observations indicate the universe is consistent with being flat.\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10\u221243 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 1014 of its critical value, or it would not exist as it does today.\nUltimate fate of the universe.\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started\u2014a Big Crunch.\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would very gradually asymptotically approach absolute zero\u2014a Big Freeze. Moreover, if protons are unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The \u039bCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\nMisconceptions.\nOne of the common misconceptions about the Big Bang model is that it fully explains the origin of the universe. However, the Big Bang model does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state. It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\nHubble's law predicts that galaxies that are beyond Hubble distance recede faster than the speed of light. However, special relativity does not apply beyond motion through space. Hubble's law describes velocity that results from expansion \"of\" space, rather than \"through\" space.\nAstronomers often refer to the cosmological redshift as a Doppler shift which can lead to a misconception. Although similar, the cosmological redshift is not identical to the classically derived Doppler redshift because most elementary derivations of the Doppler redshift do not accommodate the expansion of space. Accurate derivation of the cosmological redshift requires the use of general relativity, and while a treatment using simpler Doppler effect arguments gives nearly identical results for nearby galaxies, interpreting the redshift of more distant galaxies as due to the simplest Doppler redshift treatments can cause confusion.\nPre\u2013Big Bang cosmology.\nThe Big Bang explains the evolution of the universe from a density and temperature that is well beyond humanity's capability to replicate, so extrapolations to most extreme conditions and earliest times are necessarily more speculative. Lema\u00eetre called this initial state the \"\"primeval atom\" while Gamow called the material \"ylem\"\". How the initial state of the universe originated is still an open question, but the Big Bang model does constrain some of its characteristics. For example, specific laws of nature most likely came to existence in a random way, but as inflation models show, some combinations of these are far more probable. A topologically flat universe implies a balance between gravitational potential energy and other forms, requiring no additional energy to be created.\nThe Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time, and such an infinite energy density may be a physical impossibility. However, the physical theories of general relativity and quantum mechanics as currently realized are not applicable before the Planck epoch, and correcting this will require the development of a correct treatment of quantum gravity. Certain quantum gravity treatments, such as the Wheeler\u2013DeWitt equation, imply that time itself could be an emergent property. As such, physics may conclude that time did not exist before the Big Bang.\nWhile it is not known what could have preceded the hot dense state of the early universe or how and why it originated, or even whether such questions are sensible, speculation abounds as the subject of \"cosmogony\".\nSome speculative proposals in this regard, each of which entails untested hypotheses, are:\nProposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.\nReligious and philosophical interpretations.\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, while others argue that Big Bang cosmology makes the notion of a creator superfluous."}
{"id": "4119", "revid": "7430752", "url": "https://en.wikipedia.org/wiki?curid=4119", "title": "Bock", "text": "Bock is a strong lager (traditionally, and sometimes still, an ale, as in the case of at least some weizenbocks), of German origin. Several substyles exist, including:\nOriginally a dark beer, a modern bock can range from light copper to brown in colour. The style is very popular, with many examples brewed internationally.\nHistory.\nThe style known now as \"bock\" was a dark, malty, lightly hopped ale first brewed in the 14th century by German brewers in the Hanseatic town of Einbeck in Lower Saxony. \nThe style from Einbeck was later adopted in Bavaria by Munich brewers in the 17th century and adapted to the new lager style of brewing. Due to their Bavarian accent, citizens of Munich pronounced \"Einbeck\" as \"ein Bock\" (\"a billy goat\"), and thus the beer became known as \"bock\". As a visual pun, a goat often appears on bock labels.\nBock is historically associated with special occasions, often religious festivals such as Christmas, Easter or Lent (the latter as \"\"). Bocks have a long history of being brewed and consumed by Bavarian monks as a source of nutrition during times of fasting.\nStyles of bock.\nTraditional bock.\nTraditional bock is a sweet, relatively strong (6.3\u20137.2% by volume), lightly hopped (20\u201327 IBUs) lager. The beer should be clear, and color can range from light copper to brown, with a bountiful and persistent off-white head. The aroma should be malty and toasty, possibly with hints of alcohol, but no detectable hops or fruitiness. The mouthfeel is smooth, with low to moderate carbonation and no astringency. The taste is rich and toasty, sometimes with a bit of caramel. Again, hop presence is low to undetectable, providing just enough bitterness so that the sweetness is not cloying and the aftertaste is muted. \nThe following commercial products are indicative of the style: Point Bock (Stevens Point Brewery) Einbecker Ur-Bock Dunkel, Pennsylvania Brewing St. Nick Bock, Aass Bock, Great Lakes Rockefeller Bock, Stegmaier Brewhouse Bock, and Nashville Brewing Company's Nashville Bock.\nMaibock.\nThe maibock style, also known as helles bock or heller bock, is a helles lager brewed to bock strength; therefore, still as strong as traditional bock, but lighter in colour and with more hop presence. \nIt is a fairly recent development compared to other styles of bock beers, frequently associated with springtime and the month of May. Colour can range from deep gold to light amber with a large, creamy, persistent white head, and moderate to moderately high carbonation, while alcohol content ranges from 6.3% to 7.4% by volume. The flavour is typically less malty than a traditional bock, and may be drier, hoppier, and more bitter, but still with a relatively low hop flavour, with a mild spicy or peppery quality from the hops, increased carbonation and alcohol content. \nThe following commercial products are indicative of the style: Ayinger Maibock, Mahr's Bock, Hacker-Pschorr Hubertus Bock, Capital Maibock, Einbecker Mai-Urbock, Hofbr\u00e4u Maibock, Victory St. Boisterous, Gordon Biersch Blonde Bock, Smuttynose Maibock, Old Dominion Brewing Company Big Thaw Bock, [Brewery 85's Quittin' Time], Rogue Dead Guy Ale, Franconia Brewing Company Maibock Ale, Church Street maibock, and Tr\u00f6egs Cultivator.\nDoppelbock.\n\"Doppelbock\" or \"double bock\" is a stronger version of traditional bock that was first brewed in Munich by the Paulaner Friars, a Franciscan order founded by St. Francis of Paula. \nHistorically, doppelbock was high in alcohol and sweet. The story is told that it served as \"liquid bread\" for the Friars during times of fasting, when solid food was not permitted. However, historian Mark Dredge, in his book \"A Brief History of Lager\", says that this story is myth, and that the monks produced doppelbock to supplement their order's vegetarian diet all year. \nToday, doppelbock is still strong \u2014 ranging from 7%\u201312% or more by volume. It is clear, with colour ranging from dark gold, for the paler version, to dark brown with ruby highlights for darker version. It has a large, creamy, persistent head (although head retention may be impaired by alcohol in the stronger versions). The aroma is intensely malty, with some toasty notes, and possibly some alcohol presence as well; darker versions may have a chocolate-like or fruity aroma. The flavour is very rich and malty, with toasty notes and noticeable alcoholic strength, and little or no detectable hops (16\u201326 IBUs). \nPaler versions may have a drier finish. The monks who originally brewed doppelbock named their beer \"Salvator\" (literally \"Savior\", but actually a malapropism for \"Sankt Vater\", \"St. Father\", originally brewed for the feast of St. Francis of Paola on 2 April which often falls into Lent), which today is trademarked by Paulaner. \nBrewers of modern doppelbocks often add \"-ator\" to their beer's name as a signpost of the style; there are 200 \"-ator\" doppelbock names registered with the German patent office. \nThe following are representative examples of the style: Paulaner Salvator, Ayinger Celebrator, Weihenstephaner Korbinian, Andechser Doppelbock Dunkel, Spaten Optimator, Augustiner Brau Maximator, Tucher Bajuvator, Weltenburger Kloster Asam-Bock, Capital Autumnal Fire, EKU 28, Eggenberg Urbock 23\u00ba, Bell's Consecrator, Moretti La Rossa, Samuel Adams Double Bock, Tr\u00f6egs Tr\u00f6egenator Double Bock, Wasatch Brewery Devastator, Great Lakes Doppelrock, Abita Andygator, Wolverine State Brewing Company Predator, Burly Brewing's Burlynator, Monteith's Doppel Bock, and Christian Moerlein Emancipator Doppelbock.\nEisbock.\nEisbock is a traditional specialty beer of the Kulmbach district of Bavaria, Germany that is made by partially freezing a doppelbock and removing the water ice to concentrate the flavour and alcohol content, which ranges from 9% to 13% by volume. \nIt is clear, with a colour ranging from deep copper to dark brown in colour, often with ruby highlights. Although it can pour with a thin off-white head, head retention is frequently impaired by the higher alcohol content. The aroma is intense, with no hop presence, but frequently can contain fruity notes, especially of prunes, raisins, and plums. Mouthfeel is full and smooth, with significant alcohol, although this should not be hot or sharp. The flavour is rich and sweet, often with toasty notes, and sometimes hints of chocolate, always balanced by a significant alcohol presence. \nThe following are representative examples of the style: Colorado Team Brew \"Warning Sign\", Kulmbacher Reichelbr\u00e4u Eisbock, Eggenberg, Schneider Aventinus Eisbock, Urbock Dunkel Eisbock, Franconia Brewing Company Ice Bock 17%.\nThe strongest ice-beer, Strength in Numbers, was a one-time collaboration in 2020 between Schorschbrau of Germany and BrewDog of Scotland, who had competed with each other in the early years of the 21st century to produce the world's strongest beer. \"Strength in Numbers\" was created using traditional ice distialltion, reaching a final strength of 57.8% ABV.\nWeizenbock.\nWeizenbock is a style of bock that replaces some of the barley in the grain bill with 40\u201360% wheat. It was first produced in Bavaria in 1907 by G. Schneider &amp; Sohn and was named \"Aventinus\" after a Bavarian historian. The style combines darker Munich malts and top-fermenting wheat beer yeast, brewed at the strength of a doppelbock."}
{"id": "4122", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=4122", "title": "B roll", "text": ""}
{"id": "4124", "revid": "9021902", "url": "https://en.wikipedia.org/wiki?curid=4124", "title": "Bantu languages", "text": "The Bantu languages (English: , Proto-Bantu: *bant\u028a\u0300) are a large family of languages spoken by the Bantu peoples throughout sub-Saharan Africa.\nThe total number of Bantu languages ranges in the hundreds, depending on the definition of \"language\" versus \"dialect\", and is estimated at between 440 and 680 distinct languages. For Bantuic, Linguasphere (Part 2, Transafrican phylosector, phylozone 99) has 260 outer languages (which are equivalent to languages, inner languages being dialects). McWhorter points out, using a comparison of 16 languages from Bangi-Moi, Bangi-Ntamba, Koyo-Mboshi, Likwala-Sangha, Ngondi-Ngiri and Northern Mozambiqean, mostly from Guthrie Zone C, that many varieties are mutually intelligible.\nThe total number of Bantu speakers is in the hundreds of millions, estimated around 350 million in the mid-2010s (roughly 30% of the total population of Africa or roughly 5% of world population).\nBantu languages are largely spoken southeast of Cameroon, throughout Central Africa, Southeast Africa and Southern Africa. About one-sixth of the Bantu speakers, and about one-third of Bantu languages, are found in the Democratic Republic of the Congo alone (c. 60 million speakers as of 2015). See list of Bantu peoples.\nThe Bantu language with the largest total number of speakers is Swahili; however, the majority of its speakers use it as a second language (L1: c. 16 million, L2: 80 million, as of 2015).\nOther major Bantu languages include Zulu, with 27 million speakers (15.7 million L2) and Shona, with about 11 million speakers (if Manyika and Ndau are included). \"Ethnologue\" separates the largely mutually intelligible Kinyarwanda and Kirundi, which, if grouped together, have 20 million speakers.\nName.\nThe similarity among dispersed Bantu languages had been observed as early as the 17th century.\nThe term \"Bantu\" as a name for the group was coined (as \"B\u00e2-ntu\") by Wilhelm Bleek in 1857 or 1858, and popularised in his \"Comparative Grammar\" of 1862.\nHe coined the term to represent the word for 'people' in loosely reconstructed Proto-Bantu, from the plural noun class prefix \"*ba-\" categorizing 'people', and the root \"*nt\u028a\u0300-\" 'some (entity), any' (e.g. Zulu \"umuntu\" 'person', \"abantu\" 'people'). \nThere is no indigenous term for the group, as Bantu-speaking populations refer to themselves by their endonyms, but did not have a concept for the larger ethno-linguistic phylum. Bleek's coinage was inspired by the anthropological observation of groups frequently self-identifying as 'people' or 'the true people' (as is the case, for example, with the term \"Khoekhoe\", but this is a \"kare\" 'praise address' and not an ethnic name).\nThe term \"narrow Bantu\", excluding those languages classified as Bantoid by Guthrie (1948), was introduced in the 1960s.\nThe prefix \"ba-\" specifically refers to people. Endonymically, the term for cultural objects, including language, is formed with the \"ki-\" noun class (Nguni \"\u00edsi-\"), as in \"Kiswahili,\" 'coast language and culture,' and \"isiZulu,\" 'Zulu language and culture'.\nIn the 1980s, South African linguists suggested referring to these languages as \"KiNtu.\" The word \"kintu\" exists in some places, but it means 'thing', with no relation to the concept of 'language'. In addition, delegates at the African Languages Association of Southern Africa conference in 1984 reported that, in some places, the term \"Kintu\" has a derogatory significance. This is because \"kintu\" refers to 'things' and is used as a dehumanizing term for people who have lost their dignity.\nIn addition, \"Kintu\" is a figure in some mythologies.\nIn the 1990s, the term \"Kintu\" was still occasionally used by South African linguists. But in contemporary decolonial South African linguistics, the term \"Ntu languages\" is used.\nOrigin.\nThe Bantu languages descend from a common Proto-Bantu language, which is believed to have been spoken in what is now Cameroon in Central Africa. An estimated 2,500\u20133,000 years ago (1000 BC to 500 BC), speakers of the Proto-Bantu language began a series of migrations eastward and southward, carrying agriculture with them. This Bantu expansion came to dominate Sub-Saharan Africa east of Cameroon, an area where Bantu peoples now constitute nearly the entire population. Some other sources estimate the Bantu Expansion started closer to 3000 BC.\nThe technical term Bantu, meaning \"human beings\" or simply \"people\", was first used by Wilhelm Bleek (1827\u20131875), as the concept is reflected in many of the languages of this group. A common characteristic of Bantu languages is that they use words such as \"muntu\" or \"mutu\" for \"human being\" or in simplistic terms \"person\", and the plural prefix for human nouns starting with \"mu-\" (class 1) in most languages is \"ba-\" (class 2), thus giving \"bantu\" for \"people\". Bleek, and later Carl Meinhof, pursued extensive studies comparing the grammatical structures of Bantu languages.\nClassification.\nThe most widely used classification is an alphanumeric coding system developed by Malcolm Guthrie in his 1948 classification of the Bantu languages. It is mainly geographic. The term 'narrow Bantu' was coined by the \"Benue\u2013Congo Working Group\" to distinguish Bantu as recognized by Guthrie, from the Bantoid languages not recognized as Bantu by Guthrie.\nIn recent times, the distinctiveness of Narrow Bantu as opposed to the other Southern Bantoid languages has been called into doubt (cf. Piron 1995, Williamson &amp; Blench 2000, Blench 2011), but the term is still widely used.\nThere is no true genealogical classification of the (Narrow) Bantu languages. Until recently most attempted classifications only considered languages that happen to fall within traditional Narrow Bantu, but there seems to be a continuum with the related languages of South Bantoid.\nAt a broader level, the family is commonly split in two depending on the reflexes of proto-Bantu tone patterns: Many Bantuists group together parts of zones A through D (the extent depending on the author) as \"Northwest Bantu\" or \"Forest Bantu\", and the remainder as \"Central Bantu\" or \"Savanna Bantu\". The two groups have been described as having mirror-image tone systems: where Northwest Bantu has a high tone in a cognate, Central Bantu languages generally have a low tone, and vice versa.\nNorthwest Bantu is more divergent internally than Central Bantu, and perhaps less conservative due to contact with non-Bantu Niger\u2013Congo languages; Central Bantu is likely the innovative line cladistically. Northwest Bantu is clearly not a coherent family, but even for Central Bantu the evidence is lexical, with little evidence that it is a historically valid group.\nAnother attempt at a detailed genetic classification to replace the Guthrie system is the 1999 \"Tervuren\" proposal of Bastin, Coupez, and Mann. However, it relies on lexicostatistics, which, because of its reliance on overall similarity rather than shared innovations, may predict spurious groups of conservative languages that are not closely related. Meanwhile, \"Ethnologue\" has added languages to the Guthrie classification which Guthrie overlooked, while removing the Mbam languages (much of zone A), and shifting some languages between groups (much of zones D and E to a new zone J, for example, and part of zone L to K, and part of M to F) in an apparent effort at a semi-genetic, or at least semi-areal, classification. This has been criticized for sowing confusion in one of the few unambiguous ways to distinguish Bantu languages. Nurse &amp; Philippson (2006) evaluate many proposals for low-level groups of Bantu languages, but the result is not a complete portrayal of the family. \"Glottolog\" has incorporated many of these into their classification.\nThe languages that share Dahl's law may also form a valid group, Northeast Bantu. The infobox at right lists these together with various low-level groups that are fairly uncontroversial, though they continue to be revised. The development of a rigorous genealogical classification of many branches of Niger\u2013Congo, not just Bantu, is hampered by insufficient data.\nComputational phylogenetic analyses of Bantu include Currie et al. (2013), Grollemund et al. (2015), Rexova et al. 2006, Holden et al., 2016, and Whiteley et al. 2018.\nGrollemund (2012).\nSimplified phylogeny of northwestern branches of Bantu by Grollemund (2012):\nLanguage structure.\nGuthrie reconstructed both the phonemic inventory and the vocabulary of Proto-Bantu.\nThe most prominent grammatical characteristic of Bantu languages is the extensive use of affixes (see Sotho grammar and Ganda noun classes for detailed discussions of these affixes). Each noun belongs to a class, and each language may have several numbered classes, somewhat like grammatical gender in European languages. The class is indicated by a prefix that is part of the noun, as well as agreement markers on verb and qualificative roots connected with the noun. Plural is indicated by a change of class, with a resulting change of prefix. All Bantu languages are agglutinative.\nThe verb has a number of prefixes, though in the western languages these are often treated as independent words. In Swahili, for example, \"Kitoto kidogo kimekisoma\" (for comparison, \"Kamwana kadoko kariverenga\" in Shona language) means 'The small child has read it [a book]'. \"Kitoto\" 'child' governs the adjective prefix \"ki-\" (representing the diminutive form of the word) and the verb subject prefix \"a-\". Then comes perfect tense \"-me-\" and an object marker \"-ki-\" agreeing with implicit \"kitabu\" 'book' (from Arabic \"kitab\"). Pluralizing to 'children' gives \"Vitoto vidogo vimekisoma\" (\"Vana vadoko variverenga\" in Shona), and pluralizing to 'books' (\"vitabu\") gives \"Watoto wadogo wamevisoma\".\nBantu words are typically made up of open syllables of the type CV (consonant-vowel) with most languages having syllables exclusively of this type. The Bushong language recorded by Vansina, however, has final consonants, while slurring of the final syllable (though written) is reported as common among the Tonga of Malawi. The morphological shape of Bantu words is typically CV, VCV, CVCV, VCVCV, etc.; that is, any combination of CV (with possibly a V- syllable at the start). In other words, a strong claim for this language family is that almost all words end in a vowel, precisely because closed syllables (CVC) are not permissible in most of the documented languages, as far as is understood.\nThis tendency to avoid consonant clusters in some positions is important when words are imported from English or other non-Bantu languages. An example from Chewa: the word \"school\", borrowed from English, and then transformed to fit the sound patterns of this language, is \"sukulu\". That is, \"sk-\" has been broken up by inserting an epenthetic \"-u-\"; \"-u\" has also been added at the end of the word. Another example is \"buledi\" for \"bread\". Similar effects are seen in loanwords for other non-African CV languages like Japanese. However, a clustering of sounds at the beginning of a syllable can be readily observed in such languages as Shona, and the Makua languages.\nWith few exceptions, notably Swahili, Bantu languages are tonal and have two to four register tones.\nReduplication.\nReduplication is a common morphological phenomenon in Bantu languages and is usually used to indicate frequency or intensity of the action signalled by the (unreduplicated) verb stem.\nWell-known words and names that have reduplication include:\nRepetition emphasizes the repeated word in the context that it is used. For instance, \"Mwenda pole hajikwai,\" while, \"Pole pole ndio mwendo,\" has two to emphasize the consistency of slowness of the pace. The meaning of the former in translation is, \"He who goes slowly doesn't trip,\" and that of the latter is, \"A slow but steady pace wins the race.\" Haraka haraka would mean hurrying just for the sake of hurrying, reckless hurry, as in \"Njoo! Haraka haraka\" [come here! Hurry, hurry].\nIn contrast, there are some words in some of the languages in which reduplication has the opposite meaning. It usually denotes short durations, and or lower intensity of the action and also means a few repetitions or a little bit more.\nNoun class.\nThe following is a list of nominal classes in Bantu Languages:\nBy country.\nFollowing is an incomplete list of the principal Bantu languages of each country. Included are those languages that constitute at least 1% of the population and have at least 10% the number of speakers of the largest Bantu language in the country.\nAn attempt at a full list of Bantu languages (with various conflations and a puzzlingly diverse nomenclature) can be found in \"The Bantu Languages of Africa\", 1959.\nMost languages are best known in English without the class prefix (\"Swahili\", \"Tswana\", \"Ndebele\"), but are sometimes seen with the (language-specific) prefix (\"Kiswahili\", \"Setswana\", \"Sindebele\"). In a few cases prefixes are used to distinguish languages with the same root in their name, such as Tshiluba and Kiluba (both \"Luba\"), Umbundu and Kimbundu (both \"Mbundu\"). The bare (prefixless) form typically does not occur in the language itself, but is the basis for other words based on the ethnicity. So, in the country of Botswana the people are the \"Batswana\", one person is a \"Motswana\", and the language is \"Setswana\"; and in Uganda, centred on the kingdom of \"Buganda\", the dominant ethnicity are the \"Baganda\" (sg. \"Muganda\"), whose language is \"Luganda\".\nLingua franca\nAngola\nBotswana\nBurundi\nCameroon\nCentral African Republic\nDemocratic Republic of the Congo\nEquatorial Guinea\nEswatini (formerly Swaziland)\nGabon\nKenya\nLesotho\nMalawi\nMozambique\nNamibia\nRepublic of the Congo (Congo-Brazzaville)\nRwanda\nSomalia\nSouth Africa\nAccording to the South African National Census of 2011\nTOTAL Nguni: 22,406,O49 (61.98%)\nTOTAL Sotho-Tswana: 13,744,775 (38.02%)\nTOTAL OFFICIAL INDIGENOUS LANGUAGE SPEAKERS: 36,150,824 (69.83%)\nTanzania\nUganda\nZambia\nZimbabwe\nGeographic areas.\nMap 1 shows Bantu languages in Africa and map 2 a magnification of the Benin, Nigeria and Cameroon area, as of July 2017.\nBantu words popularised in western cultures.\nA case has been made out for borrowings of many place-names and even misremembered rhymes \u2013 chiefly from one of the Luba varieties \u2013 in the USA.\nSome words from various Bantu languages have been borrowed into western languages. These include: \nWriting systems.\nAlong with the Latin script and Arabic script orthographies, there are also some modern indigenous writing systems used for Bantu languages:"}
{"id": "4126", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=4126", "title": "Ballroom dancing", "text": ""}
{"id": "4127", "revid": "3672900", "url": "https://en.wikipedia.org/wiki?curid=4127", "title": "Bearing", "text": "Bearing may refer to:"}
{"id": "4129", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=4129", "title": "BOMARC", "text": ""}
{"id": "4130", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4130", "title": "CIM-10 Bomarc", "text": "The Boeing CIM-10 Bomarc (IM-99 Weapon System prior to September 1962) was a supersonic ramjet powered long-range surface-to-air missile (SAM) used during the Cold War for the air defense of North America. In addition to being the first operational long-range SAM and the first operational pulse doppler aviation radar, it was the only SAM deployed by the United States Air Force.\nStored horizontally in a launcher shelter with movable roof, the missile was erected, fired vertically using rocket boosters to high altitude, and then tipped over into a horizontal Mach 2.5 cruise powered by ramjet engines. This lofted trajectory allowed the missile to operate at a maximum range as great as 430\u00a0mi (700\u00a0km). Controlled from the ground for most of its flight, when it reached the target area it was commanded to begin a dive, activating an onboard active radar homing seeker for terminal guidance. A radar proximity fuse detonated the warhead, either a large conventional explosive or the W40 nuclear warhead.\nThe Air Force originally planned for a total of 52 sites covering most of the major cities and industrial regions in the US. The US Army was deploying their own systems at the same time, and the two services fought constantly both in political circles and in the press. Development dragged on, and by the time it was ready for deployment in the late 1950s, the nuclear threat had moved from manned bombers to the intercontinental ballistic missile (ICBM). By this time the Army had successfully deployed the much shorter range Nike Hercules that they claimed filled any possible need through the 1960s, in spite of Air Force claims to the contrary.\nAs testing continued, the Air Force reduced its plans to sixteen sites, and then again to eight with an additional two sites in Canada. The first US site was declared operational in 1959, but with only a single working missile. Bringing the rest of the missiles into service took years, by which time the system was obsolete. Deactivations began in 1969 and by 1972 all Bomarc sites had been shut down. A small number were used as target drones, and only a few remain on display today.\nDesign and development.\nBomarc A.\nIn 1946, Boeing started to study surface-to-air guided missiles under the United States Army Air Forces project MX-606. By 1950, Boeing had launched more than 100 test rockets in various configurations, all under the designator XSAM-A-1 GAPA (Ground-to-Air Pilotless Aircraft). Because these tests were very promising, Boeing received a USAF contract in 1949 to develop a pilotless interceptor (a term then used by the USAF for air-defense guided missiles) under project MX-1599.\nThe MX-1599 missile was to be a ramjet-powered, nuclear-armed long-range surface-to-air missile to defend the Continental United States from high-flying bombers. The Michigan Aerospace Research Center (MARC) was added to the project soon afterward, and this gave the new missile its name Bomarc (for Boeing and MARC). In 1951, the USAF decided to emphasize its point of view that missiles were nothing else than pilotless aircraft by assigning aircraft designators to its missile projects, and anti-aircraft missiles received F-for-Fighter designations. The Bomarc became the F-99.\nTest flights of XF-99 test vehicles began in September 1952 and continued through early 1955. The XF-99 tested only the liquid-fueled booster rocket, which would accelerate the missile to ramjet ignition speed. In February 1955, tests of the XF-99A propulsion test vehicles began. These included live ramjets, but still had no guidance system or warhead. The designation YF-99A had been reserved for the operational test vehicles. In August 1955, the USAF discontinued the use of aircraft-like type designators for missiles, and the XF-99A and YF-99A became XIM-99A and YIM-99A, respectively. Originally the USAF had allocated the designation IM-69, but this was changed (possibly at Boeing's request to keep number 99) to IM-99 in October 1955.\nIn October 1957, the first YIM-99A production-representative prototype flew with full guidance, and succeeded to pass the target within destructive range. In late 1957, Boeing received the production contract for the IM-99A Bomarc A interceptor missile, and in September 1959, the first IM-99A squadron became operational.\nThe IM-99A had an operational radius of and was designed to fly at Mach\u00a02.5\u20132.8 at a cruising altitude of . It was long and weighed . Its armament was either a conventional warhead or a W40 nuclear warhead (7\u201310 kiloton yield). A liquid-fuel rocket engine boosted the Bomarc to Mach 2, when its Marquardt RJ43-MA-3 ramjet engines, fueled by 80-octane gasoline, would take over for the remainder of the flight. This was the same model of engine used to power the Lockheed X-7, the Lockheed AQM-60 Kingfisher drone used to test air defenses, and the Lockheed D-21 launched from the back of an M-21, although the Bomarc and Kingfisher engines used different materials due to the longer duration of their flights.\nOperational units.\nThe operational IM-99A missiles were based horizontally in semi-hardened shelters, nicknamed \"coffins\". After the launch order, the shelter's roof would slide open, and the missile raised to the vertical. After the missile was supplied with fuel for the booster rocket, it would be launched by the Aerojet General LR59-AJ-13 booster. After sufficient speed was reached, the Marquardt RJ43-MA-3 ramjets would ignite and propel the missile to its cruise speed of Mach\u00a02.8 at an altitude of .\nWhen the Bomarc was within of the target, its own Westinghouse AN/DPN-34 radar guided the missile to the interception point. The maximum range of the IM-99A was , and it was fitted with either a conventional high-explosive or a 10\u00a0kiloton W-40 nuclear fission warhead.\nThe Bomarc relied on the Semi-Automatic Ground Environment (SAGE), an automated control system used by NORAD for detecting, tracking and intercepting enemy bomber aircraft. SAGE allowed for remote launching of the Bomarc missiles, which were housed in a constant combat-ready basis in individual launch shelters in remote areas. At the height of the program, there were 14 Bomarc sites located in the US and two in Canada.\nBomarc B.\nThe liquid-fuel booster of the Bomarc A had several drawbacks. It took two minutes to fuel before launch, which could be a long time in high-speed intercepts, and its hypergolic propellants (hydrazine and nitric acid) were very dangerous to handle, leading to several serious accidents.\nAs soon as high-thrust solid-fuel rockets became a reality in the mid-1950s, the USAF began to develop a new solid-fueled Bomarc variant, the IM-99B Bomarc\u00a0B. It used a Thiokol XM51 booster, and also had improved Marquardt RJ43-MA-7 (and finally the RJ43-MA-11) ramjets. The first IM-99B was launched in May 1959, but problems with the new propulsion system delayed the first fully successful flight until July 1960, when a supersonic MQM-15A Regulus II drone was intercepted. Because the new booster took up less space in the missile, more ramjet fuel could be carried, increasing the range to . The terminal homing system was also improved, using the world's first pulse Doppler search radar, the Westinghouse AN/DPN-53. All Bomarc\u00a0Bs were equipped with the W-40 nuclear warhead. In June 1961, the first IM-99B squadron became operational, and Bomarc\u00a0B quickly replaced most Bomarc\u00a0A missiles. On 23 March 1961, a Bomarc\u00a0B successfully intercepted a Regulus\u00a0II cruise missile flying at , thus achieving the highest interception in the world up to that date.\nBoeing built 570 Bomarc missiles between 1957 and 1964, 269 CIM-10A, 301 CIM-10B.\nIn September 1958 Air Research &amp; Development Command decided to transfer the Bomarc program from its testing at Cape Canaveral Air Force Station to a new facility on Santa Rosa Island, immediately south of Eglin AFB Hurlburt Field on the Gulf of Mexico. To operate the facility and to provide training and operational evaluation in the missile program, Air Defense Command established the 4751st Air Defense Wing (Missile) (4751st ADW) on 15 January 1958. The first launch from Santa Rosa took place on 15 January 1959.\nOperational history.\nIn 1955, to support a program which called for 40 squadrons of BOMARC (120 missiles to a squadron for a total of 4,800 missiles), ADC reached a decision on the location of these 40 squadrons and suggested operational dates for each. The sequence was as follows: ... l. McGuire 1/60 2. Suffolk 2/60 3. Otis 3/60 4. Dow 4/60 5. Niagara Falls 1/61 6. Plattsburgh 1/61 7. Kinross 2/61 8. K.I. Sawyer 2/61 9. Langley 2/61 10. Truax 3/61 11. Paine 3/61 12. Portland 3/61 ... At the end of 1958, ADC plans called for construction of the following BOMARC bases in the following order: l. McGuire 2. Suffolk 3. Otis 4. Dow 5. Langley 6. Truax 7. Kinross 8. Duluth 9. Ethan Allen 10. Niagara Falls 11. Paine 12. Adair 13. Travis 14. Vandenberg 15. San Diego 16. Malmstrom 17. Grand Forks 18. Minot 19. Youngstown 20. Seymour-Johnson 21. Bunker Hill 22. Sioux Falls 23. Charleston 24. McConnell 25. Holloman 26. McCoy 27. Amarillo 28. Barksdale 29. Williams.\nUnited States.\nThe first USAF operational Bomarc squadron was the 46th Air Defense Missile Squadron (ADMS), organized on 1 January 1959 and activated on 25 March. The 46th ADMS was assigned to the New York Air Defense Sector at McGuire Air Force Base, New Jersey. The training program, under the 4751st Air Defense Wing used technicians acting as instructors and was established for a four-month duration. Training included missile maintenance; SAGE operations and launch procedures, including the launch of an unarmed missile at Eglin. In September 1959 the squadron assembled at their permanent station, the Bomarc site near McGuire AFB, and trained for operational readiness. The first Bomarc-A were used at McGuire on 19 September 1959 with Kincheloe AFB getting the first operational IM-99Bs. While several of the squadrons replicated earlier fighter interceptor unit numbers, they were all new organizations with no previous historical counterpart.\nADC's initial plans called for some 52 Bomarc sites around the United States with 120 missiles each but as defense budgets decreased during the 1950s the number of sites dropped substantially. Ongoing development and reliability problems didn't help, nor did Congressional debate over the missile's usefulness and necessity. In June 1959, the Air Force authorized 16 Bomarc sites with 56 missiles each; the initial five would get the IM-99A with the remainder getting the IM-99B. However, in March 1960, HQ USAF cut deployment to eight sites in the United States and two in Canada.\nBomarc incident.\nWithin a year of operations, a Bomarc\u00a0A with a nuclear warhead caught fire at McGuire AFB on 7 June 1960 after its on-board helium tank exploded. While the missile's explosives did not detonate, the heat melted the warhead and released plutonium, which the fire crews spread. The Air Force and the Atomic Energy Commission cleaned up the site and covered it with concrete. This was the only major incident involving the weapon system. The site remained in operation for several years following the fire. Since its closure in 1972, the area has remained off limits, primarily due to low levels of plutonium contamination. Between 2002 and 2004, 21,998 cubic yards of contaminated debris and soils were shipped to what was then known as Envirocare, located in Utah.\nModification and deactivation.\nIn 1962, the US Air Force started using modified A-models as drones; following the October 1962 tri-service redesignation of aircraft and weapons systems they became CQM-10As. Otherwise the air defense missile squadrons maintained alert while making regular trips to Santa Rosa Island for training and firing practice. After the inactivation of the 4751st ADW(M) on 1 July 1962 and transfer of Hurlburt to Tactical Air Command for air commando operations the 4751st Air Defense Squadron (Missile) remained at Hurlburt and Santa Rosa Island for training purposes.\nIn 1964, the liquid-fueled Bomarc-A sites and squadrons began to be deactivated. The sites at Dow and Suffolk County closed first. The remainder continued to be operational for several more years while the government started dismantling the air defense missile network. Niagara Falls was the first BOMARC B installation to close, in December 1969; the others remained on alert through 1972. In April 1972, the last Bomarc B in U.S. Air Force service was retired at McGuire and the 46th ADMS inactivated and the base was deactivated.\nIn the era of the intercontinental ballistic missiles the Bomarc, designed to intercept relatively slow manned bombers, had become a useless asset. The remaining Bomarc missiles were used by all armed services as high-speed target drones for tests of other air-defense missiles. The Bomarc A and Bomarc B targets were designated as CQM-10A and CQM-10B, respectively.\nFollowing the accident, the McGuire complex has never been sold or converted to other uses and remains in Air Force ownership, making it the most intact site of the eight in the US. It has been nominated to the National Register of Historic Sites. Although a number of IM-99/CIM-10 Bomarcs have been placed on public display, because of concerns about the possible environmental hazards of the thoriated magnesium structure of the airframe several have been removed from public view.\nRuss Sneddon, director of the Air Force Armament Museum, Eglin Air Force Base, Florida provided information about missing CIM-10 exhibit airframe serial 59\u20132016, one of the museum's original artifacts from its founding in 1975 and donated by the 4751st Air Defense Squadron at Hurlburt Field, Eglin Auxiliary Field 9, Eglin AFB. As of December 2006, the suspect missile was stored in a secure compound behind the Armaments Museum. In December 2010, the airframe was still on premises, but partly dismantled.\nCanada.\nThe Bomarc Missile Program was highly controversial in Canada. The Progressive Conservative government of Prime Minister John Diefenbaker initially agreed to deploy the missiles, and shortly thereafter controversially scrapped the Avro Arrow, a supersonic manned interceptor aircraft, arguing that the missile program made the Arrow unnecessary.\nInitially, it was unclear whether the missiles would be equipped with nuclear warheads. By 1960 it became known that the missiles were to have a nuclear payload, and a debate ensued about whether Canada should accept nuclear weapons. Ultimately, the Diefenbaker government decided that the Bomarcs should not be equipped with nuclear warheads. The dispute split the Diefenbaker Cabinet, and led to the collapse of the government in 1963. The Official Opposition and Liberal Party leader Lester B. Pearson originally was against nuclear missiles, but reversed his personal position and argued in favor of accepting nuclear warheads. He won the 1963 election, largely on the basis of this issue, and his new Liberal government proceeded to accept nuclear-armed Bomarcs, with the first being deployed on 31 December 1963. When the nuclear warheads were deployed, Pearson's wife, Maryon, resigned her honorary membership in the anti-nuclear weapons group, Voice of Women.\nCanadian operational deployment of the Bomarc involved the formation of two specialized Surface/Air Missile squadrons. The first to begin operations was No. 446 SAM Squadron at RCAF Station North Bay, which was the command and control center for both squadrons. With construction of the compound and related facilities completed in 1961, the squadron received its Bomarcs in 1961, without nuclear warheads. The squadron became fully operational from 31 December 1963, when the nuclear warheads arrived, until disbanding on 31 March 1972. All the warheads were stored separately and under control of Detachment 1 of the USAF 425th Munitions Maintenance Squadron. During operational service, the Bomarcs were maintained on stand-by, on a 24-hour basis, but were never fired, although the squadron test-fired the missiles at Eglin AFB, Florida on annual winter retreats.\nNo. 447 SAM Squadron operating out of RCAF Station La Macaza, Quebec, was activated on 15 September 1962 although warheads were not delivered until late 1963. The squadron followed the same operational procedures as No. 446, its sister squadron. With the passage of time the operational capability of the 1950s-era Bomarc system no longer met modern requirements; the Department of National Defence deemed that the Bomarc missile defense was no longer a viable system, and ordered both squadrons to be stood down in 1972. The bunkers and ancillary facilities remain at both former sites.\nOperators.\nLocations under construction but not activated. Each site was programmed for 28 IM-99B missiles:\nSurviving missiles.\nBelow is a list of museums or sites which have a Bomarc missile on display:\nImpact on popular music.\nThe Bomarc missile captured the imagination of the American and Canadian popular music industry, giving rise to a pop music group, the Bomarcs (composed mainly of servicemen stationed on a Florida radar site that tracked Bomarcs), a record label, Bomarc Records, and a moderately successful Canadian pop group, The Beau Marks."}
{"id": "4132", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4132", "title": "Branco River", "text": "The Branco River (; Engl: \"White River\") is the principal affluent of the Rio Negro from the north.\nBasin.\nThe river drains the Guayanan Highlands moist forests ecoregion.\nIt is enriched by many streams from the Tepui highlands which separate Venezuela and Guyana from Brazil. Its two upper main tributaries are the Uraricoera and the Takutu. The latter almost links its sources with those of the Essequibo; during floods headwaters of the Branco and those of the Essequibo are connected, allowing a level of exchange in the aquatic fauna (such as fish) between the two systems.\nThe Branco flows nearly south, and finds its way into the Negro through several channels and a chain of lagoons similar to those of the latter river. It is long, up to its Uraricoera confluence. It has numerous islands, and, above its mouth, it is broken by a bad series of rapids.\nWater chemistry.\nAs suggested by its name, the Branco (literally \"white\" in Portuguese) has whitish water that may appear almost milky due to the inorganic sediments it carries. It is traditionally considered a whitewater river, although the major seasonal fluctuations in its physico-chemical characteristics makes a classification difficult and some consider it clearwater. Especially the river's upper parts at the headwaters are clear and flow through rocky country, leading to the suggestion that sediments mainly originate from the lower parts. Furthermore, its chemistry and color may contradict each other compared to the traditional Amazonian river classifications. The Branco River has pH 6\u20137 and low levels of dissolved organic carbon.\nAlfred Russel Wallace mentioned the coloration in \"On the Rio Negro\", a paper read at the 13 June 1853 meeting of the Royal Geographical Society, in which he said: \"[The Rio Branco] is white to a remarkable degree, its waters being actually milky in appearance\". Alexander von Humboldt attributed the color to the presence of silicates in the water, principally mica and talc. There is a visible contrast with the waters of the Rio Negro at the confluence of the two rivers. The Rio Negro is a blackwater river with dark tea-colored acidic water (pH 3.5\u20134.5) that contains high levels of dissolved organic carbon.\nRiver capture.\nUntil approximately 20,000 years ago the headwaters of the Branco River flowed not into the Amazon, but via the Takutu Graben in the Rupununi area of Guyana towards the Caribbean. Currently in the rainy season much of the Rupununi area floods, with water draining both to the Amazon (via the Branco River) and the Essequibo River."}
{"id": "4146", "revid": "1544984", "url": "https://en.wikipedia.org/wiki?curid=4146", "title": "Bus", "text": "A bus (contracted from omnibus, with variants multibus, motorbus, autobus, etc.) is a road vehicle designed to carry many passengers. Buses can have a capacity as high as 300 passengers. The most common type is the single-deck rigid bus, with larger loads carried by double-decker and articulated buses, and smaller loads carried by midibuses and minibuses while coaches are used for longer-distance services. Many types of buses, such as city transit buses and inter-city coaches, charge a fare. Other types, such as elementary or secondary school buses or shuttle buses within a post-secondary education campus do not charge a fare. In many jurisdictions, bus drivers require a special licence above and beyond a regular driver's licence.\nBuses may be used for scheduled bus transport, scheduled coach transport, school transport, private hire, or tourism; promotional buses may be used for political campaigns and others are privately operated for a wide range of purposes, including rock and pop band tour vehicles.\nHorse-drawn buses were used from the 1820s, followed by steam buses in the 1830s, and electric trolleybuses in 1882. The first internal combustion engine buses, or motor buses, were used in 1895. Recently, interest has been growing in hybrid electric buses, fuel cell buses, and electric buses, as well as buses powered by compressed natural gas or biodiesel. As of the 2010s, bus manufacturing is increasingly globalised, with the same designs appearing around the world.\nName.\nBus is a clipped form of the Latin adjectival form \"omnibus\" (\"for all\"), the dative plural of \"omnis-e\" (\"all\"). The theoretical full name is in French \"voiture omnibus\" (\"vehicle for all\"). The name originates from a mass-transport service started in 1823 by a French corn-mill owner named in Richebourg, a suburb of Nantes. A by-product of his mill was hot water, and thus next to it he established a spa business. In order to encourage customers he started a horse-drawn transport service from the city centre of Nantes to his establishment. The first vehicles stopped in front of the shop of a hatter named Omn\u00e9s, which displayed a large sign inscribed \"Omnes Omnibus\", a pun on his Latin-sounding surname, \"omnes\" being the male and female nominative, vocative and accusative form of the Latin adjective \"omnis-e\" (\"all\"), combined with \"omnibus\", the dative plural form meaning \"for all\", thus giving his shop the name \"Omn\u00e9s for all\", or \"everything for everyone\". His transport scheme was a huge success, although not as he had intended as most of his passengers did not visit his spa. He turned the transport service into his principal lucrative business venture and closed the mill and spa. Nantes citizens soon gave the nickname \"omnibus\" to the vehicle. Having invented the successful concept Baudry moved to Paris and launched the first omnibus service there in April 1828. A similar service was introduced in London in 1829.\nHistory.\nSteam buses.\nRegular intercity bus services by steam-powered buses were pioneered in England in the 1830s by Walter Hancock and by associates of Sir Goldsworthy Gurney, among others, running reliable services over road conditions which were too hazardous for horse-drawn transportation.\nThe first mechanically propelled omnibus appeared on the streets of London on 22 April 1833. Steam carriages were much less likely to overturn, they travelled faster than horse-drawn carriages, they were much cheaper to run, and caused much less damage to the road surface due to their wide tyres.\nHowever, the heavy road tolls imposed by the turnpike trusts discouraged steam road vehicles and left the way clear for the horse bus companies, and from 1861 onwards, harsh legislation virtually eliminated mechanically propelled vehicles from the roads of Great Britain for 30 years, the Locomotive Act of that year imposing restrictive speed limits on \"road locomotives\" of 5\u00a0mph in towns and cities, and 10\u00a0mph in the country.\nTrolleybuses.\nIn parallel to the development of the bus was the invention of the electric trolleybus, typically fed through trolley poles by overhead wires. The Siemens brothers, William in England and Ernst Werner in Germany, collaborated on the development of the trolleybus concept. Sir William first proposed the idea in an article to the \"Journal of the Society of Arts\" in 1881 as an \"...arrangement by which an ordinary omnibus...would have a suspender thrown at intervals from one side of the street to the other, and two wires hanging from these suspenders; allowing contact rollers to run on these two wires, the current could be conveyed to the tram-car, and back again to the dynamo machine at the station, without the necessity of running upon rails at all.\"\nThe first such vehicle, the Electromote, was made by his brother Dr. Ernst Werner von Siemens and presented to the public in 1882 in Halensee, Germany. Although this experimental vehicle fulfilled all the technical criteria of a typical trolleybus, it was dismantled in the same year after the demonstration.\nMax Schiemann opened a passenger-carrying trolleybus in 1901 near Dresden, in Germany. Although this system operated only until 1904, Schiemann had developed what is now the standard trolleybus current collection system. In the early days, a few other methods of current collection were used. Leeds and Bradford became the first cities to put trolleybuses into service in Great Britain on 20 June 1911.\nMotor buses.\nIn Siegerland, Germany, two passenger bus lines ran briefly, but unprofitably, in 1895 using a six-passenger motor carriage developed from the 1893 Benz Viktoria. Another commercial bus line using the same model Benz omnibuses ran for a short time in 1898 in the rural area around Llandudno, Wales.\nDaimler also produced one of the earliest motor-bus models in 1898, selling a double-decker bus to the Motor Traction Company which was first used on the streets of London on 23 April 1898. The vehicle had a maximum speed of and accommodated up to 20 passengers, in an enclosed area below and on an open-air platform above. With the success and popularity of this bus, Daimler expanded production, selling more buses to companies in London and, in 1899, to Stockholm and Speyer. Daimler also entered into a partnership with the British company Milnes and developed a new double-decker in 1902 that became the market standard.\nThe first mass-produced bus model was the B-type double-decker bus, designed by Frank Searle and operated by the London General Omnibus Company \u2013 it entered service in 1910, and almost 3,000 had been built by the end of the decade. Hundreds saw military service on the Western Front during the First World War.\nThe Yellow Coach Manufacturing Company, which rapidly became a major manufacturer of buses in the US, was founded in Chicago in 1923 by John D. Hertz. General Motors purchased a majority stake in 1925 and changed its name to the Yellow Truck and Coach Manufacturing Company. They then purchased the balance of the shares in 1943 to form the GM Truck and Coach Division.\nModels expanded in the 20th century, leading to the widespread introduction of the contemporary recognizable form of full-sized buses from the 1950s. The AEC Routemaster, developed in the 1950s, was a pioneering design and remains an icon of London to this day. The innovative design used lightweight aluminium and techniques developed in aircraft production during World War II. As well as a novel weight-saving integral design, it also introduced for the first time on a bus independent front suspension, power steering, a fully automatic gearbox, and power-hydraulic braking.\nTypes.\nFormats include single-decker bus, double-decker bus (both usually with a rigid chassis) and articulated bus (or 'bendy-bus') the prevalence of which varies from country to country. High-capacity bi-articulated buses are also manufactured, and passenger-carrying trailers\u2014either towed behind a rigid bus (a bus trailer) or hauled as a trailer by a truck (a trailer bus). Smaller midibuses have a lower capacity and open-top buses are typically used for leisure purposes. In many new fleets, particularly in local transit systems, a shift to low-floor buses is occurring, primarily for easier accessibility. Coaches are designed for longer-distance travel and are typically fitted with individual high-backed reclining seats, seat belts, toilets, and audio-visual entertainment systems, and can operate at higher speeds with more capacity for luggage. Coaches may be single- or double-deckers, articulated, and often include a separate luggage compartment under the passenger floor. Guided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes.\nBus manufacturing may be by a single company (an integral manufacturer), or by one manufacturer's building a bus body over a chassis produced by another manufacturer.\nDesign.\nAccessibility.\nTransit buses used to be mainly high-floor vehicles. However, they are now increasingly of low-floor design and optionally also 'kneel' air suspension and have electrically or hydraulically extended under-floor ramps to provide level access for wheelchair users and people with baby carriages. Prior to more general use of such technology, these wheelchair users could only use specialist para-transit mobility buses.\nAccessible vehicles also have wider entrances and interior gangways and space for wheelchairs. Interior fittings and destination displays may also be designed to be usable by the visually impaired. Coaches generally use wheelchair lifts instead of low-floor designs. In some countries, vehicles are required to have these features by disability discrimination laws.\nConfiguration.\nBuses were initially configured with an engine in the front and an entrance at the rear. With the transition to one-man operation, many manufacturers moved to mid- or rear-engined designs, with a single door at the front or multiple doors. The move to the low-floor design has all but eliminated the mid-engined design, although some coaches still have mid-mounted engines. Front-engined buses still persist for niche markets such as American school buses, some minibuses, and buses in less developed countries, which may be derived from truck chassis, rather than purpose-built bus designs. Most buses have two axles, articulated buses have three.\nGuidance.\nGuided buses are fitted with technology to allow them to run in designated guideways, allowing the controlled alignment at bus stops and less space taken up by guided lanes than conventional roads or bus lanes. Guidance can be mechanical, optical, or electromagnetic. Extensions of the guided technology include the Guided Light Transit and Translohr systems, although these are more often termed 'rubber-tyred trams' as they have limited or no mobility away from their guideways.\nLiveries.\nTransit buses are normally painted to identify the operator or a route, function, or to demarcate low-cost or premium service buses. Liveries may be painted onto the vehicle, applied using adhesive vinyl technologies, or using decals. Vehicles often also carry bus advertising or part or all of their visible surfaces (as mobile billboard). Campaign buses may be decorated with key campaign messages; these can be to promote an event or initiative.\nPropulsion.\nThe most common power source since the 1920s has been the diesel engine. Early buses, known as trolleybuses, were powered by electricity supplied from overhead lines. Nowadays, electric buses often carry their own battery, which is sometimes recharged on stops/stations to keep the size of the battery small/lightweight. Currently, interest exists in hybrid electric buses, fuel cell buses, electric buses, and ones powered by compressed natural gas or biodiesel. Gyrobuses, which are powered by the momentum stored by a flywheel, were tried in the 1940s.\nDimensions.\nUnited Kingdom and European Union:\nUnited States, Canada and Mexico:\nManufacture.\nEarly bus manufacturing grew out of carriage coach building, and later out of automobile or truck manufacturers. Early buses were merely a bus body fitted to a truck chassis. This body+chassis approach has continued with modern specialist manufacturers, although there also exist integral designs such as the Leyland National where the two are practically inseparable. Specialist builders also exist and concentrate on building buses for special uses or modifying standard buses into specialised products.\nIntegral designs have the advantages that they have been well-tested for strength and stability, and also are off-the-shelf. However, two incentives cause use of the chassis+body model. First, it allows the buyer and manufacturer both to shop for the best deal for their needs, rather than having to settle on one fixed design\u2014the buyer can choose the body and the chassis separately. Second, over the lifetime of a vehicle (in constant service and heavy traffic), it will likely get minor damage now and again, and being able easily to replace a body panel or window etc. can vastly increase its service life and save the cost and inconvenience of removing it from service.\nAs with the rest of the automotive industry, into the 20th century, bus manufacturing increasingly became globalized, with manufacturers producing buses far from their intended market to exploit labour and material cost advantages. As with the cars, new models are often exhibited by manufacturers at prestigious industry shows to gain new orders. A typical city bus costs almost US$450,000.\nUses.\nPublic transport.\nTransit buses, used on public transport bus services, have utilitarian fittings designed for efficient movement of large numbers of people, and often have multiple doors. Coaches are used for longer-distance routes. High-capacity bus rapid transit services may use the bi-articulated bus or tram-style buses such as the Wright StreetCar and the Irisbus Civis.\nBuses and coach services often operate to a predetermined published public transport timetable defining the route and the timing, but smaller vehicles may be used on more flexible demand responsive transport services.\nTourism.\nBuses play a major part in the tourism industry. Tour buses around the world allow tourists to view local attractions or scenery. These are often open-top buses, but can also be regular buses or coaches.\nIn local sightseeing, City Sightseeing is the largest operator of local tour buses, operating on a franchised basis all over the world. Specialist tour buses are also often owned and operated by safari parks and other theme parks or resorts. Longer-distance tours are also carried out by bus, either on a turn up and go basis or through a tour operator, and usually allow disembarkation from the bus to allow touring of sites of interest on foot. These may be day trips or longer excursions incorporating hotel stays. Tour buses often carry a tour guide, although the driver or a recorded audio commentary may also perform this function. The tour operator may be a subsidiary of a company that operates buses and coaches for other uses or an independent company that charters buses or coaches. Commuter transport operators may also use their coaches to conduct tours within the target city between the morning and evening commuter transport journey.\nBuses and coaches are also a common component of the wider package holiday industry, providing private airport transfers (in addition to general airport buses) and organised tours and day trips for holidaymakers on the package.\nTour buses can also be hired as chartered buses by groups for sightseeing at popular holiday destinations. These private tour buses may offer specific stops, such as all the historical sights, or allow the customers to choose their own itineraries. Tour buses come with professional and informed staff and insurance, and maintain state governed safety standards. Some provide other facilities like entertainment units, luxurious reclining seats, large scenic windows, and even lavatories.\nPublic long-distance coach networks are also often used as a low-cost method of travel by students or young people travelling the world. Some companies such as Topdeck Travel were set up specifically to use buses to drive the hippie trail or travel to places such as North Africa.\nIn many tourist or travel destinations, a bus is part of the tourist attraction, such as the North American tourist trolleys, London's AEC Routemaster heritage routes, or the customised buses of Malta, Asia, and the Americas. Another example of tourist stops is the homes of celebrities, such as tours based near Hollywood. There are several such services between 6000 and 7000 Hollywood Boulevard in Los Angeles.\nStudent transport.\nIn some countries, particularly the US and Canada, buses used to transport schoolchildren have evolved into a specific design with specified mandatory features. American states have also adopted laws regarding motorist conduct around school buses, including large fines and possibly prison for passing a stopped school bus in the process of loading or offloading children passengers. These school buses may have school bus yellow livery and crossing guards. Other countries may mandate the use of seat belts. As a minimum, many countries require a bus carrying students to display a , and may also adopt yellow liveries. Student transport often uses older buses cascaded from service use, retrofitted with more seats or seatbelts. Student transport may be operated by local authorities or private contractors. Schools may also own and operate their own buses for other transport needs, such as class field trips, or transport to associated sports, music, or other school events.\nPrivate charter.\nDue to the costs involved in owning, operating, and driving buses and coaches, many bus and coach use a private hire of vehicles from charter bus companies, either for a day or two or a longer contract basis, where the charter company provides the vehicles and qualified drivers.\nCharter bus operators may be completely independent businesses, or charter hire may be a subsidiary business of a public transport operator that might maintain a separate fleet or use surplus buses, coaches, and dual-purpose coach-seated buses. Many private taxicab companies also operate larger minibus vehicles to cater for group fares. Companies, private groups, and social clubs may hire buses or coaches as a cost-effective method of transporting a group to an event or site, such as a group meeting, racing event, or organised recreational activity such as a summer camp. Schools often hire charter bus services on regular basis for transportation of children to and from their homes. Chartered buses are also used by education institutes for transport to conventions, exhibitions, and field trips. Entertainment or event companies may also hire temporary shuttles buses for transport at events such as festivals or conferences. Party buses are used by companies in a similar manner to limousine hire, for luxury private transport to social events or as a touring experience. Sleeper buses are used by bands or other organisations that tour between entertainment venues and require mobile rest and recreation facilities. Some couples hire preserved buses for their wedding transport, instead of the traditional car. Buses are often hired for parades or processions. Victory parades are often held for triumphant sports teams, who often tour their home town or city in an open-top bus. Sports teams may also contract out their transport to a team bus, for travel to away games, to a competition or to a final event. These buses are often specially decorated in a livery matching the team colours. Private companies often contract out private shuttle bus services, for transport of their customers or patrons, such as hotels, amusement parks, university campuses, or private airport transfer services. This shuttle usage can be as transport between locations, or to and from parking lots. High specification luxury coaches are often chartered by companies for executive or VIP transport. Charter buses may also be used in tourism and for promotion (See Tourism and Promotion sections).\nPrivate ownership.\nMany organisations, including the police, not for profit, social or charitable groups with a regular need for group transport may find it practical or cost-effective to own and operate a bus for their own needs. These are often minibuses for practical, tax and driver licensing reasons, although they can also be full-size buses. Cadet or scout groups or other youth organizations may also own buses. Companies such as railroads, construction contractors, and agricultural firms may own buses to transport employees to and from remote job sites. Specific charities may exist to fund and operate bus transport, usually using specially modified mobility buses or otherwise accessible buses (See Accessibility section). Some use their contributions to buy vehicles and provide volunteer drivers.\nAirport operators make use of special airside airport buses for crew and passenger transport in the secure airside parts of an airport. Some public authorities, police forces, and military forces make use of armoured buses where there is a special need to provide increased passenger protection. The United States Secret Service acquired two in 2010 for transporting dignitaries needing special protection. Police departments make use of police buses for a variety of reasons, such as prisoner transport, officer transport, temporary detention facilities, and as command and control vehicles. Some fire departments also use a converted bus as a command post while those in cold climates might retain a bus as a heated shelter at fire scenes. Many are drawn from retired school or service buses.\nPromotion.\nBuses are often used for advertising, political campaigning, , public relations, or promotional purposes. These may take the form of temporary charter hire of service buses, or the temporary or permanent conversion and operation of buses, usually of second-hand buses. Extreme examples include converting the bus with displays and decorations or awnings and fittings. Interiors may be fitted out for exhibition or information purposes with special equipment or audio visual devices.\nBus advertising takes many forms, often as interior and exterior adverts and all-over advertising liveries. The practice often extends into the exclusive private hire and use of a bus to promote a brand or product, appearing at large public events, or touring busy streets. The bus is sometimes staffed by promotions personnel, giving out free gifts. Campaign buses are often specially decorated for a political campaign or other social awareness information campaign, designed to bring a specific message to different areas, or used to transport campaign personnel to local areas/meetings. Exhibition buses are often sent to public events such as fairs and festivals for purposes such as recruitment campaigns, for example by private companies or the armed forces. Complex urban planning proposals may be organised into a mobile exhibition bus for the purposes of public consultation.\nGoods transport.\nIn some sparsely populated areas, it is common to use brucks, buses with a cargo area to transport both passengers and cargo at the same time. They are especially common in the Nordic countries.\nAround the world.\nHistorically, the types and features of buses have developed according to local needs. Buses were fitted with technology appropriate to the local climate or passenger needs, such as air conditioning in Asia, or cycle mounts on North American buses. The bus types in use around the world where there was little mass production were often sourced second hand from other countries, such as the Malta bus, and buses in use in Africa. Other countries such as Cuba required novel solutions to import restrictions, with the creation of the \"camellos\" (camel bus), a specially manufactured trailer bus.\nAfter the Second World War, manufacturers in Europe and the Far East, such as Mercedes-Benz buses and Mitsubishi Fuso expanded into other continents influencing the use of buses previously served by local types. Use of buses around the world has also been influenced by colonial associations or political alliances between countries. Several of the Commonwealth nations followed the British lead and sourced buses from British manufacturers, leading to a prevalence of double-decker buses. Several Eastern Bloc countries adopted trolleybus systems, and their manufacturers such as Trolza exported trolleybuses to other friendly states. In the 1930s, Italy designed the world's only triple decker bus for the busy route between Rome and Tivoli that could carry eighty-eight passengers. It was unique not only in being a triple decker but having a separate smoking compartment on the third level.\nKnight Bus in Harry Potter and the Prisoner of Azkaban is a full-sized triple decker bus used only as a prop for the movie.\nThe buses to be found in countries around the world often reflect the quality of the local road network, with high floor resilient truck-based designs prevalent in several less developed countries where buses are subject to tough operating conditions. Population density also has a major impact, where dense urbanisation such as in Japan and the far east has led to the adoption of high capacity long multi-axle buses, often double-deckers while South America and China are implementing large numbers of articulated buses for bus rapid transit schemes.\nBus expositions.\nEuro Bus Expo is a trade show, which is held biennially at the UK's National Exhibition Centre in Birmingham. As the official show of the Confederation of Passenger Transport, the UK's trade association for the bus, coach and light rail industry, the three-day event offers visitors from Europe and beyond the chance to see and experience the very latest vehicles and product and service innovations right across the industry.\nBusworld Kortrijk in Kortrijk, Belgium, is the leading bus trade fair in Europe. It is also held biennially.\nUse of retired buses.\nMost public or private buses and coaches, once they have reached the end of their service with one or more operators, are sent to the wrecking yard for breaking up for scrap and spare parts. Some buses which are not economical to keep running as service buses are often converted for use other than revenue-earning transport. Much like old cars and trucks, buses often pass through a dealership where they can be bought privately or at auction.\nBus operators often find it economical to convert retired buses to use as permanent training buses for driver training, rather than taking a regular service bus out of use. Some large operators have also converted retired buses into tow bus vehicles, to act as tow trucks. With the outsourcing of maintenance staff and facilities, the increase in company health and safety regulations, and the increasing curb weights of buses, many operators now contract their towing needs to a professional vehicle recovery company.\nSome retired buses have been converted to static or mobile caf\u00e9s, often using historic buses as a tourist attraction. There are also catering buses: buses converted into a mobile canteen and break room. These are commonly seen at external filming locations to feed the cast and crew, and at other large events to feed staff. Another use is as an emergency vehicle, such as high-capacity ambulance bus or mobile command centre.\nSome organisations adapt and operate playbuses or learning buses to provide a playground or learning environments to children who might not have access to proper play areas. An ex-London AEC Routemaster bus has been converted to a mobile theatre and catwalk fashion show.\nSome buses meet a destructive end by being entered in banger races or at demolition derbys. A larger number of old retired buses have also been converted into mobile holiday homes and campers.\nBus preservation.\nRather than being scrapped or converted for other uses, sometimes retired buses are saved for preservation. This can be done by individuals, volunteer preservation groups or charitable trusts, museums, or sometimes by the operators themselves as part of a heritage fleet. These buses often need to be restored to their original condition and will have their livery and other details such as internal notices and rollsigns restored to be authentic to a specific time in the bus's history. Some buses that undergo preservation are rescued from a state of great disrepair, but others enter preservation with very little wrong with them. As with other historic vehicles, many preserved buses either in a working or static state form part of the collections of transport museums. Working buses will often be exhibited at rallies and events, and they are also used as charter buses. While many preserved buses are quite old or even vintage, in some cases relatively new examples of a bus type can enter restoration. In-service examples are still in use by other operators. This often happens when a change in design or operating practice, such as the switch to one person operation or low floor technology, renders some buses redundant while still relatively new."}
{"id": "4147", "revid": "1012935100", "url": "https://en.wikipedia.org/wiki?curid=4147", "title": "Bali", "text": "Bali () () is a province of Indonesia and the westernmost of the Lesser Sunda Islands. East of Java and west of Lombok, the province includes the island of Bali and a few smaller neighbouring islands, notably Nusa Penida, Nusa Lembongan, and Nusa Ceningan. The provincial capital, Denpasar, is the most populous city in the Lesser Sunda Islands and the second-largest, after Makassar, in Eastern Indonesia. Bali is Indonesia's main tourist destination, with a significant rise in tourism since the 1980s. Tourism-related business makes up 80% of its economy.\nBali is the only Hindu-majority province in Indonesia, with 82.5% of the population adhering to Balinese Hinduism. It is also renowned for its highly developed arts, including traditional and modern dance, sculpture, painting, leather, metalworking, and music. The Indonesian International Film Festival is held every year in Bali. Other international events held in Bali include the Miss World 2013 and 2018 Annual Meetings of the International Monetary Fund and the World Bank Group. In March 2017, TripAdvisor named Bali as the world's top destination in its Traveller's Choice award, followed in January 2021 with the same name. \nBali is part of the Coral Triangle, the area with the highest biodiversity of marine species especially fish and turtles. In this area alone, over 500 reef-building coral species can be found. For comparison, this is about seven times as many as in the entire Caribbean. Bali is the home of the Subak irrigation system, a UNESCO World Heritage Site. It is also home to a unified confederation of kingdoms composed of 10 traditional royal Balinese houses, each house ruling a specific geographic area. The confederation is the successor of the Bali Kingdom. The royal houses are not recognised by the government of Indonesia; however, they originated before Dutch colonisation.\nHistory.\nAncient.\nBali was inhabited around 2000 BCE by Austronesian people who migrated originally from the island of Taiwan to Southeast Asia and Oceania through Maritime Southeast Asia. Culturally and linguistically, the Balinese are closely related to the people of the Indonesian archipelago, Malaysia, the Philippines and Oceania. Stone tools dating from this time have been found near the village of Cekik in the island's west.\nIn ancient Bali, nine Hindu sects existed, namely Pasupata, Bhairawa, Siwa Shidanta, Vaishnava, Bodha, Brahma, Resi, Sora and Ganapatya. Each sect revered a specific deity as its personal Godhead.\nInscriptions from 896 and 911 do not mention a king, until 914, when Sri Kesarivarma is mentioned. They also reveal an independent Bali, with a distinct dialect, where Buddhism and Sivaism were practised simultaneously. Mpu Sindok's great-granddaughter, Mahendradatta (Gunapriyadharmapatni), married the Bali king Udayana Warmadewa (Dharmodayanavarmadeva) around 989, giving birth to Airlangga around 1001. This marriage also brought more Hinduism and Javanese culture to Bali. Princess Sakalendukirana appeared in 1098. Suradhipa reigned from 1115 to 1119, and Jayasakti from 1146 until 1150. Jayapangus appears on inscriptions between 1178 and 1181, while Adikuntiketana and his son Paramesvara in 1204.\nBalinese culture was strongly influenced by Indian, Chinese, and particularly Hindu culture, beginning around the 1st century AD. The name \"Bali dwipa\" (\"Bali island\") has been discovered from various inscriptions, including the Blanjong pillar inscription written by Sri Kesari Warmadewa in 914 AD and mentioning Walidwipa. It was during this time that the people developed their complex irrigation system \"subak\" to grow rice in wet-field cultivation. Some religious and cultural traditions still practised today can be traced to this period.\nThe Hindu Majapahit Empire (1293\u20131520 AD) on eastern Java founded a Balinese colony in 1343. The uncle of Hayam Wuruk is mentioned in the charters of 1384\u201386. Mass Javanese immigration to Bali occurred in the next century when the Majapahit Empire fell in 1520. Bali's government then became an independent collection of Hindu kingdoms which led to a Balinese national identity and major enhancements in culture, arts, and economy. The nation with various kingdoms became independent for up to 386 years until 1906 when the Dutch subjugated and repulsed the natives for economic control and took it over.\nPortuguese contacts.\nThe first known European contact with Bali is thought to have been made in 1512, when a Portuguese expedition led by Antonio Abreu and Francisco Serr\u00e3o sighted its northern shores. It was the first expedition of a series of bi-annual fleets to the Moluccas, that throughout the 16th century usually travelled along the coasts of the Sunda Islands. Bali was also mapped in 1512, in the chart of Francisco Rodrigues, aboard the expedition. In 1585, a ship foundered off the Bukit Peninsula and left a few Portuguese in the service of Dewa Agung.\nDutch East Indies.\nIn 1597, the Dutch explorer Cornelis de Houtman arrived at Bali, and the Dutch East India Company was established in 1602. The Dutch government expanded its control across the Indonesian archipelago during the second half of the 19th century. Dutch political and economic control over Bali began in the 1840s on the island's north coast when the Dutch pitted various competing for Balinese realms against each other. In the late 1890s, struggles between Balinese kingdoms in the island's south were exploited by the Dutch to increase their control.\nIn June 1860, the famous Welsh naturalist, Alfred Russel Wallace, travelled to Bali from Singapore, landing at Buleleng on the north coast of the island. Wallace's trip to Bali was instrumental in helping him devise his Wallace Line theory. The Wallace Line is a faunal boundary that runs through the strait between Bali and Lombok. It is a boundary between species. In his travel memoir \"The Malay Archipelago,\" Wallace wrote of his experience in Bali, of which has a strong mention of the unique Balinese irrigation methods:\nI was both astonished and delighted; for as my visit to Java was some years later, I had never beheld so beautiful and well-cultivated a district out of Europe. A slightly undulating plain extends from the seacoast about inland, where it is bounded by a fine range of wooded and cultivated hills. Houses and villages, marked out by dense clumps of coconut palms, tamarind and other fruit trees, are dotted about in every direction; while between them extend luxurious rice-grounds, watered by an elaborate system of irrigation that would be the pride of the best cultivated parts of Europe. \nThe Dutch mounted large naval and ground assaults at the Sanur region in 1906 and were met by the thousands of members of the royal family and their followers who rather than yield to the superior Dutch force committed ritual suicide (\"puputan\") to avoid the humiliation of surrender. Despite Dutch demands for surrender, an estimated 200 Balinese killed themselves rather than surrender. In the Dutch intervention in Bali, a similar mass suicide occurred in the face of a Dutch assault in Klungkung. Afterwards, the Dutch governours exercised administrative control over the island, but local control over religion and culture generally remained intact. Dutch rule over Bali came later and was never as well established as in other parts of Indonesia such as Java and Maluku.\nIn the 1930s, anthropologists Margaret Mead and Gregory Bateson, artists Miguel Covarrubias and Walter Spies, and musicologist Colin McPhee all spent time here. Their accounts of the island and its peoples created a western image of Bali as \"an enchanted land of aesthetes at peace with themselves and nature\". Western tourists began to visit the island. The sensuous image of Bali was enhanced in the West by a quasi-pornographic 1932 documentary \"Virgins of Bali\" about a day in the lives of two teenage Balinese girls whom the film's narrator Deane Dickason notes in the first scene \"bathe their shamelessly nude bronze bodies\". Under the looser version of the Hays code that existed up to 1934, nudity involving \"civilised\" (i.e. white) women was banned, but permitted with \"uncivilised\" (i.e. all non-white women), a loophole that was exploited by the producers of \"Virgins of Bali\". The film, which mostly consisted of scenes of topless Balinese women was a great success in 1932, and almost single-handedly made Bali into a popular spot for tourists.\nImperial Japan occupied Bali during World War II. It was not originally a target in their Netherlands East Indies Campaign, but as the airfields on Borneo were inoperative due to heavy rains, the Imperial Japanese Army decided to occupy Bali, which did not suffer from comparable weather. The island had no regular Royal Netherlands East Indies Army (KNIL) troops. There was only a Native Auxiliary Corps \"Prajoda\" (Korps Prajoda) consisting of about 600 native soldiers and several Dutch KNIL officers under the command of KNIL Lieutenant Colonel W.P. Roodenburg. On 19 February 1942, the Japanese forces landed near the town of Sanoer [Sanur]. The island was quickly captured.\nDuring the Japanese occupation, a Balinese military officer, Gusti Ngurah Rai, formed a Balinese 'freedom army'. The harshness of Japanese occupation forces made them more resented than the Dutch colonial rulers.\nIndependence from the Dutch.\nIn 1945, Bali was liberated by the British 5th infantry Division under the command of Major-General Robert Mansergh who took the Japanese surrender. Once the Japanese forces had been repatriated the island was handed over to the Dutch the following year.\nIn 1946, the Dutch constituted Bali as one of the 13 administrative districts of the newly proclaimed State of East Indonesia, a rival state to the Republic of Indonesia, which was proclaimed and headed by Sukarno and Hatta. Bali was included in the \"Republic of the United States of Indonesia\" when the Netherlands recognised Indonesian independence on 29 December 1949. The first governor of Bali, Anak Agung Bagus Suteja, was appointed by President Sukarno in 1958, when Bali became a province.\nContemporary.\nThe 1963 eruption of Mount Agung killed thousands, created economic havoc and forced many displaced Balinese to be transmigrated to other parts of Indonesia. Mirroring the widening of social divisions across Indonesia in the 1950s and early 1960s, Bali saw conflict between supporters of the traditional caste system, and those rejecting this system. Politically, the opposition was represented by supporters of the Indonesian Communist Party (PKI) and the Indonesian Nationalist Party (PNI), with tensions and ill-feeling further increased by the PKI's land reform programs. An attempted coup in Jakarta was put down by forces led by General Suharto.\nThe army became the dominant power as it instigated a violent anti-communist purge, in which the army blamed the PKI for the coup. Most estimates suggest that at least 500,000 people were killed across Indonesia, with an estimated 80,000 killed in Bali, equivalent to 5% of the island's population. With no Islamic forces involved as in Java and Sumatra, upper-caste PNI landlords led the extermination of PKI members.\nAs a result of the 1965\u201366 upheavals, Suharto was able to manoeuvre Sukarno out of the presidency. His \"New Order\" government re-established relations with Western countries. The pre-War Bali as \"paradise\" was revived in a modern form. The resulting large growth in tourism has led to a dramatic increase in Balinese standards of living and significant foreign exchange earned for the country. A bombing in 2002 by militant Islamists in the tourist area of Kuta killed 202 people, mostly foreigners. This attack, and another in 2005, severely reduced tourism, producing much economic hardship to the island.\nOn 27 November 2017, Mount Agung erupted five times, causing evacuation of thousands, disruption of air travel and environmental damage. Further eruptions also occurred between 2018 and 2019.\nGeography.\nThe island of Bali lies east of Java, and is approximately 8 degrees south of the equator. Bali and Java are separated by the Bali Strait. East to west, the island is approximately wide and spans approximately north to south; administratively it covers , or without Nusa Penida District; its population density is roughly .\nBali's central mountains include several peaks over in elevation and active volcanoes such as Mount Batur. The highest is Mount Agung (), known as the \"mother mountain\", which is an active volcano rated as one of the world's most likely sites for a massive eruption within the next 100 years. In late 2017 Mount Agung started erupting and large numbers of people were evacuated, temporarily closing the island's airport. Mountains range from centre to the eastern side, with Mount Agung the easternmost peak. Bali's volcanic nature has contributed to its exceptional fertility and its tall mountain ranges provide the high rainfall that supports the highly productive agriculture sector. South of the mountains is a broad, steadily descending area where most of Bali's large rice crop is grown. The northern side of the mountains slopes more steeply to the sea and is the main coffee-producing area of the island, along with rice, vegetables and cattle. The longest river, Ayung River, flows approximately (see List of rivers of Bali).\nThe island is surrounded by coral reefs. Beaches in the south tend to have white sand while those in the north and west have black sand. Bali has no major waterways, although the Ho River is navigable by small \"sampan\" boats. Black sand beaches between Pasut and Klatingdukuh are being developed for tourism, but apart from the seaside temple of Tanah Lot, they are not yet used for significant tourism.\nThe largest city is the provincial capital, Denpasar, near the southern coast. Its population is around 491,500 (2002). Bali's second-largest city is the old colonial capital, Singaraja, which is located on the north coast and is home to around 100,000 people. Other important cities include the beach resort, Kuta, which is practically part of Denpasar's urban area, and Ubud, situated at the north of Denpasar, is the island's cultural centre.\nThree small islands lie to the immediate south-east and all are administratively part of the Klungkung regency of Bali: Nusa Penida, Nusa Lembongan and Nusa Ceningan. These islands are separated from Bali by the Badung Strait.\nTo the east, the Lombok Strait separates Bali from Lombok and marks the biogeographical division between the fauna of the Indomalayan realm and the distinctly different fauna of Australasia. The transition is known as the Wallace Line, named after Alfred Russel Wallace, who first proposed a transition zone between these two major biomes. When sea levels dropped during the Pleistocene ice age, Bali was connected to Java and Sumatra and to the mainland of Asia and shared the Asian fauna, but the deep water of the Lombok Strait continued to keep Lombok Island and the Lesser Sunda archipelago isolated.\nClimate.\nBeing just 8 degrees south of the equator, Bali has a fairly even climate all year round. Average year-round temperature stands at around with a humidity level of about 85%.\nDay time temperatures at low elevations vary between , but the temperatures decrease significantly with increasing elevation.\nThe west monsoon is in place from approximately October to April, and this can bring significant rain, particularly from December to March. During the rainy season, there are comparatively fewer tourists seen in Bali. During the Easter and Christmas holidays, the weather is very unpredictable. Outside of the monsoon period, humidity is relatively low and any rain is unlikely in lowland areas.\nEcology.\nBali lies just to the west of the Wallace Line, and thus has a fauna that is Asian in character, with very little Australasian influence, and has more in common with Java than with Lombok. An exception is the yellow-crested cockatoo, a member of a primarily Australasian family. There are around 280 species of birds, including the critically endangered Bali myna, which is endemic. Others include barn swallow, black-naped oriole, black racket-tailed treepie, crested serpent-eagle, crested treeswift, dollarbird, Java sparrow, lesser adjutant, long-tailed shrike, milky stork, Pacific swallow, red-rumped swallow, sacred kingfisher, sea eagle, woodswallow, savanna nightjar, stork-billed kingfisher, yellow-vented bulbul and great egret.\nUntil the early 20th century, Bali was possibly home to several large mammals: leopard and the endemic Bali tiger. The banteng still occurs in its domestic form, whereas leopards are found only in neighbouring Java, and the Bali tiger is extinct. The last definite record of a tiger on Bali dates from 1937, when one was shot, though the subspecies may have survived until the 1940s or 1950s. Pleistocene and Holocene megafaunas include banteng and giant tapir (based on speculations that they might have reached up to the Wallace Line), elephants, and rhinoceros.\nSquirrels are quite commonly encountered, less often is the Asian palm civet, which is also kept in coffee farms to produce kopi luwak. Bats are well represented, perhaps the most famous place to encounter them remaining is the Goa Lawah (Temple of the Bats) where they are worshipped by the locals and also constitute a tourist attraction. They also occur in other cave temples, for instance at Gangga Beach. Two species of monkey occur. The crab-eating macaque, known locally as \"kera\", is quite common around human settlements and temples, where it becomes accustomed to being fed by humans, particularly in any of the three \"monkey forest\" temples, such as the popular one in the Ubud area. They are also quite often kept as pets by locals. The second monkey, endemic to Java and some surrounding islands such as Bali, is far rarer and more elusive and is the Javan langur, locally known as \"lutung\". They occur in a few places apart from the West Bali National Park. They are born an orange colour, though by their first year they would have already changed to a more blackish colouration. In Java, however, there is more of a tendency for this species to retain its juvenile orange colour into adulthood, and a mixture of black and orange monkeys can be seen together as a family. Other rarer mammals include the leopard cat, Sunda pangolin and black giant squirrel.\nSnakes include the king cobra and reticulated python. The water monitor can grow to at least in length and and can move quickly.\nThe rich coral reefs around the coast, particularly around popular diving spots such as Tulamben, Amed, Menjangan or neighbouring Nusa Penida, host a wide range of marine life, for instance hawksbill turtle, giant sunfish, giant manta ray, giant moray eel, bumphead parrotfish, hammerhead shark, reef shark, barracuda, and sea snakes. Dolphins are commonly encountered on the north coast near Singaraja and Lovina.\nA team of scientists conducted a survey from 29 April 2011 to 11 May 2011 at 33 sea sites around Bali. They discovered 952 species of reef fish of which 8 were new discoveries at Pemuteran, Gilimanuk, Nusa Dua, Tulamben and Candidasa, and 393 coral species, including two new ones at Padangbai and between Padangbai and Amed. The average coverage level of healthy coral was 36% (better than in Raja Ampat and Halmahera by 29% or in Fakfak and Kaimana by 25%) with the highest coverage found in Gili Selang and Gili Mimpang in Candidasa, Karangasem regency.\nAmong the larger trees the most common are: banyan trees, jackfruit, coconuts, bamboo species, acacia trees and also endless rows of coconuts and banana species. Numerous flowers can be seen: hibiscus, frangipani, bougainvillea, poinsettia, oleander, jasmine, water lily, lotus, roses, begonias, orchids and hydrangeas exist. On higher grounds that receive more moisture, for instance around Kintamani, certain species of fern trees, mushrooms and even pine trees thrive well. Rice comes in many varieties. Other plants with agricultural value include: salak, mangosteen, corn, kintamani orange, coffee and water spinach.\nEnvironment.\nOver-exploitation by the tourist industry has led to 200 out of 400 rivers on the island drying up. Research suggests that the southern part of Bali would face a water shortage. To ease the shortage, the central government plans to build a water catchment and processing facility at Petanu River in Gianyar. The 300 litres capacity of water per second will be channelled to Denpasar, Badung and Gianyar in 2013.\nA 2010 Environment Ministry report on its environmental quality index gave Bali a score of 99.65, which was the highest score of Indonesia's 33 provinces. The score considers the level of total suspended solids, dissolved oxygen and chemical oxygen demand in water.\nErosion at Lebih Beach has seen of land lost every year. Decades ago, this beach was used for holy pilgrimages with more than 10,000 people, but they have now moved to Masceti Beach.\nIn 2017, a year when Bali received nearly 5.7 million tourists, government officials declared a \u201cgarbage emergency\u201d in response to the covering of 3.6 mile stretch of coastline in plastic waste brought in by the tide, amid concerns that the pollution could dissuade visitors from returning. Indonesia is one of the world's worst plastic polluters, with some estimates suggesting the country is the source of around 10 per cent of the world's plastic waste. Indonesia's capital city Jakarta features several large rubbish dumps and it is common to see swaths of plastics bobbing on the city's few waterways.\nAdministrative divisions.\nThe province is divided into eight regencies (\"kabupaten\") and one city (\"kota\"). These are, with their areas and populations:\nEconomy.\nIn 1970s, the Balinese economy was largely agriculture-based in terms of both output and employment. Tourism is now the largest single industry in terms of income, and as a result, Bali is one of Indonesia's wealthiest regions. In 2003, around 80% of Bali's economy was tourism related. By end of June 2011, the rate of non-performing loans of all banks in Bali were 2.23%, lower than the average of Indonesian banking industry non-performing loan rates (about 5%). The economy, however, suffered significantly as a result of the Islamists' terrorist bombings in 2002 and 2005. The tourism industry has since recovered from these events.\nAgriculture.\nAlthough tourism produces the GDP's largest output, agriculture is still the island's biggest employer. Fishing also provides a significant number of jobs. Bali is also famous for its artisans who produce a vast array of handicrafts, including batik and ikat cloth and clothing, wooden carvings, stone carvings, painted art and silverware. Notably, individual villages typically adopt a single product, such as wind chimes or wooden furniture.\nThe Arabica coffee production region is the highland region of Kintamani near Mount Batur. Generally, Balinese coffee is processed using the wet method. This results in a sweet, soft coffee with good consistency. Typical flavours include lemon and other citrus notes. Many coffee farmers in Kintamani are members of a traditional farming system called Subak Abian, which is based on the Hindu philosophy of \"Tri Hita Karana\". According to this philosophy, the three causes of happiness are good relations with God, other people, and the environment. The Subak Abian system is ideally suited to the production of fair trade and organic coffee production. Arabica coffee from Kintamani is the first product in Indonesia to request a geographical indication.\nTourism.\nIn 1963 the Bali Beach Hotel in Sanur was built by Sukarno and boosted tourism in Bali. Before the construction of the Bali Beach Hotel, there were only three significant tourist-class hotels on the island. Construction of hotels and restaurants began to spread throughout Bali. Tourism further increased on Bali after the Ngurah Rai International Airport opened in 1970. The Buleleng regency government encouraged the tourism sector as one of the mainstays for economic progress and social welfare.\nThe tourism industry is primarily focused in the south, while also significant in the other parts of the island. The main tourist locations are the town of Kuta (with its beach), and its outer suburbs of Legian and Seminyak (which were once independent townships), the east coast town of Sanur (once the only tourist hub), Ubud towards the centre of the island, to the south of the Ngurah Rai International Airport, Jimbaran and the newer developments of Nusa Dua and Pecatu.\nThe United States government lifted its travel warnings in 2008. The Australian government issued an advisory on Friday, 4 May 2012, with the overall level of this advisory lowered to 'Exercise a high degree of caution'. The Swedish government issued a new warning on Sunday, 10 June 2012 because of one tourist who died from methanol poisoning. Australia last issued an advisory on Monday, 5 January 2015 due to new terrorist threats.\nAn offshoot of tourism is the growing real estate industry. Bali's real estate has been rapidly developing in the main tourist areas of Kuta, Legian, Seminyak and Oberoi. Most recently, high-end 5-star projects are under development on the Bukit peninsula, on the south side of the island. Expensive villas are being developed along the cliff sides of south Bali, with commanding panoramic ocean views. Foreign and domestic, many Jakarta individuals and companies are fairly active, investment into other areas of the island also continues to grow. Land prices, despite the worldwide economic crisis, have remained stable.\nIn the last half of 2008, Indonesia's currency had dropped approximately 30% against the US dollar, providing many overseas visitors improved value for their currencies.\nBali's tourism economy survived the Islamists terrorist bombings of 2002 and 2005, and the tourism industry has slowly recovered and surpassed its pre terrorist bombing levels; the long-term trend has been a steady increase of visitor arrivals. In 2010, Bali received 2.57\u00a0million foreign tourists, which surpassed the target of 2.0\u20132.3\u00a0million tourists. The average occupancy of starred hotels achieved 65%, so the island still should be able to accommodate tourists for some years without any addition of new rooms/hotels, although at the peak season some of them are fully booked.\nBali received the Best Island award from Travel and Leisure in 2010. Bali won because of its attractive surroundings (both mountain and coastal areas), diverse tourist attractions, excellent international and local restaurants, and the friendliness of the local people. The Balinese culture and its religion are also considered as the main factor of the award. One of the most prestigious events that symbolize a strong relationship between a god and its followers is Kecak dance. According to BBC Travel released in 2011, Bali is one of the World's Best Islands, ranking second after Santorini, Greece.\nIn 2006, Elizabeth Gilbert's memoir \"Eat, Pray, Love\" was published, and in August 2010 it was adapted into the film \"Eat Pray Love\". It took place at Ubud and Padang-Padang Beach at Bali. Both the book and the film fuelled a boom in tourism in Ubud, the hill town and cultural and tourist centre that was the focus of Gilbert's quest for balance and love through traditional spirituality and healing.\nIn January 2016, after musician David Bowie died, it was revealed that in his will, Bowie asked for his ashes to be scattered in Bali, conforming to Buddhist rituals. He had visited and performed in several Southeast Asian cities early in his career, including Bangkok and Singapore.\nSince 2011, China has displaced Japan as the second-largest supplier of tourists to Bali, while Australia still tops the list while India has also emerged as a greater supply of tourists.\nChinese tourists increased by 17% from last year due to the impact of ACFTA and new direct flights to Bali.\nIn January 2012, Chinese tourists increased by 222.18% compared to January 2011, while Japanese tourists declined by 23.54% year on year.\nBali authorities reported the island had 2.88\u00a0million foreign tourists and 5\u00a0million domestic tourists in 2012, marginally surpassing the expectations of 2.8\u00a0million foreign tourists.\nBased on a Bank Indonesia survey in May 2013, 34.39 per cent of tourists are upper-middle class, spending between $1,286 to $5,592, and are dominated by Australia, India, France, China, Germany and the UK. Some Chinese tourists have increased their levels of spending from previous years. 30.26 percent of tourists are middle class, spending between $662 to $1,285. In 2017 it was expected that Chinese tourists would outnumber Australian tourists.\nIn January 2020, 10,000 Chinese tourists canceled trips to Bali due to the COVID-19 pandemic.\nTransportation.\nThe Ngurah Rai International Airport is located near Jimbaran, on the isthmus at the southernmost part of the island. Lt. Col. Wisnu Airfield is on the north-west Bali.\nA coastal road circles the island, and three major two-lane arteries cross the central mountains at passes reaching to 1,750\u00a0m in height (at Penelokan). The Ngurah Rai Bypass is a four-lane expressway that partly encircles Denpasar. Bali has no railway lines. There is a car ferry between Gilimanuk on the west coast of Bali to Ketapang on Java.\nIn December 2010 the Government of Indonesia invited investors to build a new Tanah Ampo Cruise Terminal at Karangasem, Bali with a projected worth of $30\u00a0million. On 17 July 2011 the first cruise ship (Sun Princess) anchored about away from the wharf of Tanah Ampo harbour. The current pier is only but will eventually be extended to to accommodate international cruise ships. The harbour is safer than the existing facility at Benoa and has a scenic backdrop of east Bali mountains and green rice fields. The tender for improvement was subject to delays, and as of July 2013 the situation was unclear with cruise line operators complaining and even refusing to use the existing facility at Tanah Ampo.\nA Memorandum of Understanding has been signed by two ministers, Bali's Governor and Indonesian Train Company to build of railway along the coast around the island. As of July 2015, no details of this proposed railways have been released. In 2019 it was reported in \"Gapura Bali\" that Wayan Koster, governor of Bali, \"is keen to improve Bali's transportation infrastructure and is considering plans to build an electric rail network across the island\".\nOn 16 March 2011 (Tanjung) Benoa port received the \"Best Port Welcome 2010\" award from London's \"Dream World Cruise Destination\" magazine. Government plans to expand the role of Benoa port as export-import port to boost Bali's trade and industry sector. In 2013, The Tourism and Creative Economy Ministry advised that 306 cruise liners were scheduled to visit Indonesia, an increase of 43 per cent compared to the previous year.\nIn May 2011, an integrated Aerial Traffic Control System (ATCS) was implemented to reduce traffic jams at four crossing points: Ngurah Rai statue, Dewa Ruci Kuta crossing, Jimbaran crossing and Sanur crossing. ATCS is an integrated system connecting all traffic lights, CCTVs and other traffic signals with a monitoring office at the police headquarters. It has successfully been implemented in other ASEAN countries and will be implemented at other crossings in Bali.\nOn 21 December 2011 construction started on the Nusa Dua-Benoa-Ngurah Rai International Airport toll road which will also provide a special lane for motorcycles. This has been done by seven state-owned enterprises led by PT Jasa Marga with 60% of shares. PT Jasa Marga Bali Tol will construct the toll road (totally with access road). The construction is estimated to cost Rp.2.49\u00a0trillion ($273.9\u00a0million). The project goes through of mangrove forest and through of beach, both within area. The elevated toll road is built over the mangrove forest on 18,000 concrete pillars which occupied 2 hectares of mangroves forest. This was compensated by the planting of 300,000 mangrove trees along the road. On 21 December 2011 the Dewa Ruci underpass has also started on the busy Dewa Ruci junction near Bali Kuta Galeria with an estimated cost of Rp136\u00a0billion ($14.9\u00a0million) from the state budget. On 23 September 2013, the Bali Mandara Toll Road was opened, with the Dewa Ruci Junction (Simpang Siur) underpass being opened previously.\nTo solve chronic traffic problems, the province will also build a toll road connecting Serangan with Tohpati, a toll road connecting Kuta, Denpasar and Tohpati and a flyover connecting Kuta and Ngurah Rai Airport.\nDemographics.\nThe population of Bali was 3,890,757 as of the 2010 Census, and 4,148,588 at the 2015 Intermediate Census; the latest estimate (for mid 2019) is 4,362,000. There are an estimated 30,000 expatriates living in Bali.\nEthnic origins.\nA DNA study in 2005 by Karafet et al. found that 12% of Balinese Y-chromosomes are of likely Indian origin, while 84% are of likely Austronesian origin, and 2% of likely Melanesian origin.\nCaste system.\nPre-modern Bali had four castes, as Jeff Lewis and Belinda Lewis state, but with a \"very strong tradition of communal decision-making and interdependence\". The four castes have been classified as Soedra (Shudra), Wesia (Vaishyas), Satrias (Kshatriyas) and Brahmana (Brahmin).\nThe 19th-century scholars such as Crawfurd and Friederich suggested that the Balinese caste system had Indian origins, but Helen Creese states that scholars such as Brumund who had visited and stayed on the island of Bali suggested that his field observations conflicted with the \"received understandings concerning its Indian origins\". In Bali, the Shudra (locally spelt \"Soedra\") have typically been the temple priests, though depending on the demographics, a temple priest may also be from the other three castes. In most regions, it has been the Shudra who typically make offerings to the gods on behalf of the Hindu devotees, chant prayers, recite \"meweda\" (Vedas), and set the course of Balinese temple festivals.\nReligion.\nUnlike most of Muslim-majority Indonesia, about 83.5% of Bali's population adheres to Balinese Hinduism, formed as a combination of existing local beliefs and Hindu influences from mainland Southeast Asia and South Asia. Minority religions include Islam (13.37%), Christianity (2.47%), and Buddhism (0.5%).\nThe general beliefs and practices of \"Agama Hindu Dharma\" mix ancient traditions and contemporary pressures placed by Indonesian laws that permit only monotheist belief under the national ideology of \"Pancasila\". Traditionally, Hinduism in Indonesia had a pantheon of deities and that tradition of belief continues in practice; further, Hinduism in Indonesia granted freedom and flexibility to Hindus as to when, how and where to pray. However, officially, the Indonesian government considers and advertises Indonesian Hinduism as a monotheistic religion with certain officially recognised beliefs that comply with its national ideology. Indonesian school textbooks describe Hinduism as having one supreme being, Hindus offering three daily mandatory prayers, and Hinduism as having certain common beliefs that in part parallel those of Islam. Scholars contest whether these Indonesian government recognised and assigned beliefs to reflect the traditional beliefs and practices of Hindus in Indonesia before Indonesia gained independence from Dutch colonial rule.\nBalinese Hinduism has roots in Indian Hinduism and Buddhism, that arrived through Java. Hindu influences reached the Indonesian Archipelago as early as the first century. Historical evidence is unclear about the diffusion process of cultural and spiritual ideas from India. Java legends refer to Saka-era, traced to 78 CE. Stories from the Mahabharata Epic have been traced in Indonesian islands to the 1st century; however, the versions mirror those found in southeast Indian peninsular region (now Tamil Nadu and southern Karnataka Andhra Pradesh).\nThe Bali tradition adopted the pre-existing animistic traditions of the indigenous people. This influence strengthened the belief that the gods and goddesses are present in all things. Every element of nature, therefore, possesses its power, which reflects the power of the gods. A rock, tree, dagger, or woven cloth is a potential home for spirits whose energy can be directed for good or evil. Balinese Hinduism is deeply interwoven with art and ritual. Ritualising states of self-control are a notable feature of religious expression among the people, who for this reason have become famous for their graceful and decorous behaviour.\nApart from the majority of Balinese Hindus, there also exist Chinese immigrants whose traditions have melded with that of the locals. As a result, these Sino-Balinese not only embrace their original religion, which is a mixture of Buddhism, Christianity, Taoism and Confucianism but also find a way to harmonise it with the local traditions. Hence, it is not uncommon to find local Sino-Balinese during the local temple's \"odalan\". Moreover, Balinese Hindu priests are invited to perform rites alongside a Chinese priest in the event of the death of a Sino-Balinese. Nevertheless, the Sino-Balinese claim to embrace Buddhism for administrative purposes, such as their Identity Cards. The Roman Catholic community has a diocese, the Diocese of Denpasar that encompasses the province of Bali and West Nusa Tenggara and has its cathedral located in Denpasar.\nLanguage.\nBalinese and Indonesian are the most widely spoken languages in Bali, and the vast majority of Balinese people are bilingual or trilingual. The most common spoken language around the tourist areas is Indonesian, as many people in the tourist sector are not solely Balinese, but migrants from Java, Lombok, Sumatra, and other parts of Indonesia. There are several indigenous Balinese languages, but most Balinese can also use the most widely spoken option: modern common Balinese. The usage of different Balinese languages was traditionally determined by the Balinese caste system and by clan membership, but this tradition is diminishing. Kawi and Sanskrit are also commonly used by some Hindu priests in Bali, as Hindu literature was mostly written in Sanskrit.\nEnglish and Chinese are the next most common languages (and the primary foreign languages) of many Balinese, owing to the requirements of the tourism industry, as well as the English-speaking community and huge Chinese-Indonesian population. Other foreign languages, such as Japanese, Korean, French, Russian or German are often used in multilingual signs for foreign tourists.\nCulture.\nBali is renowned for its diverse and sophisticated art forms, such as painting, sculpture, woodcarving, handcrafts, and performing arts. Balinese cuisine is also distinctive. Balinese percussion orchestra music, known as \"gamelan\", is highly developed and varied. Balinese performing arts often portray stories from Hindu epics such as the Ramayana but with heavy Balinese influence. Famous Balinese dances include \"pendet\", \"legong\", \"baris\", \"topeng\", \"barong\", \"gong keybar\", and \"kecak\" (the monkey dance). Bali boasts one of the most diverse and innovative performing arts cultures in the world, with paid performances at thousands of temple festivals, private ceremonies, or public shows.\nFestivals.\nThroughout the year, there are a number of festivals celebrated locally or island-wide according to the traditional calendars.\nThe Hindu New Year, \"Nyepi\", is celebrated in the spring by a day of silence. On this day everyone stays at home and tourists are encouraged (or required) to remain in their hotels. On the day before New Year, large and colourful sculptures of \"Ogoh-ogoh\" monsters are paraded and burned in the evening to drive away evil spirits. Other festivals throughout the year are specified by the Balinese \"pawukon\" calendrical system.\nCelebrations are held for many occasions such as a tooth-filing (coming-of-age ritual), cremation or \"odalan\" (temple festival). One of the most important concepts that Balinese ceremonies have in common is that of \"d\u00e9sa kala patra\", which refers to how ritual performances must be appropriate in both the specific and general social context. Many of the ceremonial art forms such as \"wayang kulit\" and \"topeng\" are highly improvisatory, providing flexibility for the performer to adapt the performance to the current situation. Many celebrations call for a loud, boisterous atmosphere with much activity and the resulting aesthetic, \"ram\u00e9\", is distinctively Balinese. Often two or more \"gamelan\" ensembles will be performing well within earshot, and sometimes compete with each other to be heard. Likewise, the audience members talk amongst themselves, get up and walk around, or even cheer on the performance, which adds to the many layers of activity and the liveliness typical of \"ram\u00e9\".\n\"Kaja\" and \"kelod\" are the Balinese equivalents of North and South, which refer to one's orientation between the island's largest mountain Gunung Agung (\"kaja\"), and the sea (\"kelod\"). In addition to spatial orientation, \"kaja\" and \"kelod\" have the connotation of good and evil; gods and ancestors are believed to live on the mountain whereas demons live in the sea. Buildings such as temples and residential homes are spatially oriented by having the most sacred spaces closest to the mountain and the unclean places nearest to the sea.\nMost temples have an inner courtyard and an outer courtyard which are arranged with the inner courtyard furthest \"kaja\". These spaces serve as performance venues since most Balinese rituals are accompanied by any combination of music, dance and drama. The performances that take place in the inner courtyard are classified as \"wali\", the most sacred rituals which are offerings exclusively for the gods, while the outer courtyard is where \"bebali\" ceremonies are held, which are intended for gods and people. Lastly, performances meant solely for the entertainment of humans take place outside the walls of the temple and are called \"bali-balihan\". This three-tiered system of classification was standardised in 1971 by a committee of Balinese officials and artists to better protect the sanctity of the oldest and most sacred Balinese rituals from being performed for a paying audience.\nTourism, Bali's chief industry, has provided the island with a foreign audience that is eager to pay for entertainment, thus creating new performance opportunities and more demand for performers. The impact of tourism is controversial since before it became integrated into the economy, the Balinese performing arts did not exist as a capitalist venture, and were not performed for entertainment outside of their respective ritual context. Since the 1930s sacred rituals such as the \"barong\" dance have been performed both in their original contexts, as well as exclusively for paying tourists. This has led to new versions of many of these performances which have developed according to the preferences of foreign audiences; some villages have a \"barong\" mask specifically for non-ritual performances as well as an older mask which is only used for sacred performances.\nBalinese society continues to revolve around each family's ancestral village, to which the cycle of life and religion is closely tied. Coercive aspects of traditional society, such as customary law sanctions imposed by traditional authorities such as village councils (including \"kasepekang\", or shunning) have risen in importance as a consequence of the democratisation and decentralisation of Indonesia since 1998.\nOther than Balinese sacred rituals and festivals, the government presents Bali Arts Festival to showcase Bali's performing arts and various artworks produced by the local talents that they have. It is held once a year, from the second week of June until the end of July. Southeast Asia's biggest annual festival of words and ideas Ubud Writers and Readers Festival is held at Ubud in October, which is participated by the world's most celebrated writers, artists, thinkers and performers.\nBeauty pageant.\nBali was the host of Miss World 2013 (63rd edition of the Miss World pageant). It was the first time Indonesia hosted an international beauty pageant.\nSports.\nBali is a major world surfing destination with popular breaks dotted across the southern coastline and around the offshore island of Nusa Lembongan.\nAs part of the Coral Triangle, Bali, including Nusa Penida, offers a wide range of dive sites with varying types of reefs, and tropical aquatic life.\nBali was the host of 2008 Asian Beach Games. It was the second time Indonesia hosted an Asia-level multi-sport event, after Jakarta held the 1962 Asian Games.\nIn football, Bali is home to Bali United football club, which plays in Liga 1.\nThe team was relocated from Samarinda, East Kalimantan to Gianyar, Bali. Harbiansyah Hanafiah, the main commissioner of Bali United explained that he changed the name and moved the home base because there was no representative from Bali in the highest football tier in Indonesia. Another reason was due to local fans in Samarinda preferring to support Pusamania Borneo F.C. rather than Persisam.\nHeritage sites.\nIn June 2012, Subak, the irrigation system for paddy fields in Jatiluwih, central Bali was enlisted as a Natural UNESCO world heritage site."}
{"id": "4149", "revid": "38995231", "url": "https://en.wikipedia.org/wiki?curid=4149", "title": "Bulgarian language", "text": "Bulgarian (, ; , ) is a South Slavic language spoken in Southeastern Europe, primarily in Bulgaria. It is the language of Bulgarians.\nAlong with the closely related Macedonian language (collectively forming the East South Slavic languages), it is a member of the Balkan sprachbund and South Slavic dialect continuum of the Indo-European language family. The two languages have several characteristics that set them apart from all other Slavic languages: changes include the elimination of case declension, the development of a suffixed definite article and the lack of a verb infinitive, but it retains and has further developed the Proto-Slavic verb system (albeit analytically). One such major development is the innovation of evidential verb forms to encode for the source of information: witnessed, inferred, or reported.\nIt is the official language of Bulgaria, and since 2007 has been among the official languages of the European Union. It is also spoken by minorities in several other countries.\nHistory.\nOne can divide the development of the Bulgarian language into several periods.\n\"Bulgarian\" was the first \"Slavic\" language attested in writing. As Slavic linguistic unity lasted into late antiquity, the oldest manuscripts initially referred to this language as \u0467\u0437\ua651\u043a\u044a \u0441\u043b\u043e\u0432\u0463\u043d\u044c\u0441\u043a\u044a, \"the Slavic language\". In the Middle Bulgarian period this name was gradually replaced by the name \u0467\u0437\ua651\u043a\u044a \u0431\u043b\u044a\u0433\u0430\u0440\u044c\u0441\u043a\u044a, the \"Bulgarian language\". In some cases, this name was used not only with regard to the contemporary Middle Bulgarian language of the copyist but also to the period of Old Bulgarian. A most notable example of anachronism is the Service of Saint Cyril from Skopje (\u0421\u043a\u043e\u043f\u0441\u043a\u0438 \u043c\u0438\u043d\u0435\u0439), a 13th-century Middle Bulgarian manuscript from northern Macedonia according to which St. Cyril preached with \"Bulgarian\" books among the Moravian Slavs. The first mention of the language as the \"Bulgarian language\" instead of the \"Slavonic language\" comes in the work of the Greek clergy of the Archbishopric of Ohrid in the 11th century, for example in the Greek hagiography of Clement of Ohrid by Theophylact of Ohrid (late 11th century).\nDuring the Middle Bulgarian period, the language underwent dramatic changes, losing the Slavonic case system, but preserving the rich verb system (while the development was exactly the opposite in other Slavic languages) and developing a definite article. It was influenced by its non-Slavic neighbors in the Balkan language area (mostly grammatically) and later also by Turkish, which was the official language of the Ottoman Empire, in the form of the Ottoman Turkish language, mostly lexically. As a national revival occurred toward the end of the period of Ottoman rule (mostly during the 19th century), a modern Bulgarian literary language gradually emerged that drew heavily on Church Slavonic/Old Bulgarian (and to some extent on literary Russian, which had preserved many lexical items from Church Slavonic) and later reduced the number of Turkish and other Balkan loans. Today one difference between Bulgarian dialects in the country and literary spoken Bulgarian is the significant presence of Old Bulgarian words and even word forms in the latter. Russian loans are distinguished from Old Bulgarian ones on the basis of the presence of specifically Russian phonetic changes, as in \u043e\u0431\u043e\u0440\u043e\u0442 (turnover, rev), \u043d\u0435\u043f\u043e\u043d\u044f\u0442\u0435\u043d (incomprehensible), \u044f\u0434\u0440\u043e (nucleus) and others. Many other loans from French, English and the classical languages have subsequently entered the language as well.\nModern Bulgarian was based essentially on the Eastern dialects of the language, but its pronunciation is in many respects a compromise between East and West Bulgarian (see especially the phonetic sections below). Following the efforts of some figures of the National awakening of Bulgaria (most notably Neofit Rilski and Ivan Bogorov), there had been many attempts to codify a standard Bulgarian language; however, there was much argument surrounding the choice of norms. Between 1835 and 1878 more than 25 proposals were put forward and \"linguistic chaos\" ensued. Eventually the eastern dialects prevailed,\nand in 1899 the Bulgarian Ministry of Education officially codified a standard Bulgarian language based on the Drinov-Ivanchev orthography.\nGeographic distribution.\nBulgarian is the official language of Bulgaria, where it is used in all spheres of public life. As of 2011, it is spoken as a first language by about 6million people in the country, or about four out of every five Bulgarian citizens.\nThere is also a significant Bulgarian diaspora abroad. One of the main historically established communities are the Bessarabian Bulgarians, whose settlement in the Bessarabia region of nowadays Moldavia and Ukraine dates mostly to the early 19th century. There were Bulgarian speakers in Ukraine at the 2001 census, in Moldova as of the 2014 census (of which were habitual users of the language), and presumably a significant proportion of the 13,200 ethnic Bulgarians residing in neighbouring Transnistria in 2016.\nAnother community abroad are the Banat Bulgarians, who migrated in the 17th century to the Banat region now split between Romania, Serbia and Hungary. They speak the Banat Bulgarian dialect, which has had its own written standard and a historically important literary tradition.\nThere are Bulgarian speakers in neighbouring countries as well. The regional dialects of Bulgarian and Macedonian form a dialect continuum, and there is no well-defined boundary where one language ends and the other begins. Within the limits of the republic of North Macedonia a strong separate Macedonian identity has emerged since the Second World War, even though there still are a small number of citizens who identify their language as Bulgarian. Beyond the borders of North Macedonia, the situation is more fluid, and the pockets of speakers of the related regional dialects in Albania and in Greece variously identify their language as Macedonian or as Bulgarian. In Serbia, there were speakers as of 2011, mainly concentrated in the so-called Western Outlands along the border with Bulgaria. Bulgarian is also spoken in Turkey: natively by Pomaks, and as a second language by many Bulgarian Turks who emigrated from Bulgaria, mostly during the \"Big Excursion\" of 1989.\nThe language is also represented among the diaspora in Western Europe and North America, which has been steadily growing since the 1990s. Countries with significant numbers of speakers include Germany, Spain, Italy, the United Kingdom ( speakers in England and Wales as of 2011), France, the United States, and Canada ( in 2011).\nDialects.\nThe language is mainly split into two broad dialect areas, based on the different reflexes of the Common Slavic yat vowel (\u0462). This split, which occurred at some point during the Middle Ages, led to the development of Bulgaria's:\nThe literary language norm, which is generally based on the Eastern dialects, also has the Eastern alternating reflex of \"yat\". However, it has not incorporated the general Eastern umlaut of \"all\" synchronic or even historic \"ya\" sounds into \"e\" before front vowels \u2013 e.g. \u043f\u043e\u043b\u044f\u043d\u0430 (\"polyana\") vs. \u043f\u043e\u043b\u0435\u043d\u0438 (\"poleni\") \"meadow \u2013 meadows\" or even \u0436\u0430\u0431\u0430 (\"zhaba\") vs. \u0436\u0435\u0431\u0438 (\"zhebi\") \"frog \u2013 frogs\", even though it co-occurs with the yat alternation in almost all Eastern dialects that have it (except a few dialects along the yat border, e.g. in the Pleven region).\nMore examples of the \"yat\" umlaut in the literary language are:\nUntil 1945, Bulgarian orthography did not reveal this alternation and used the original Old Slavic Cyrillic letter \"yat\" (\u0462), which was commonly called \u0434\u0432\u043e\u0439\u043d\u043e \u0435 (\"dvoyno e\") at the time, to express the historical \"yat\" vowel or at least root vowels displaying the \"ya \u2013 e\" alternation. The letter was used in each occurrence of such a root, regardless of the actual pronunciation of the vowel: thus, both \"mlyako\" and \"mlekar\" were spelled with (\u0462). Among other things, this was seen as a way to \"reconcile\" the Western and the Eastern dialects and maintain language unity at a time when much of Bulgaria's Western dialect area was controlled by Serbia and Greece, but there were still hopes and occasional attempts to recover it. With the 1945 orthographic reform, this letter was abolished and the present spelling was introduced, reflecting the alternation in pronunciation.\nThis had implications for some grammatical constructions:\nSometimes, with the changes, words began to be spelled as other words with different meanings, e.g.:\nIn spite of the literary norm regarding the yat vowel, many people living in Western Bulgaria, including the capital Sofia, will fail to observe its rules. While the norm requires the realizations \"vidyal\" vs. \"videli\" (he has seen; they have seen), some natives of Western Bulgaria will preserve their local dialect pronunciation with \"e\" for all instances of \"yat\" (e.g. \"videl\", \"videli\"). Others, attempting to adhere to the norm, will actually use the \"ya\" sound even in cases where the standard language has \"e\" (e.g. \"vidyal\", \"vidyali\"). The latter hypercorrection is called \u0441\u0432\u0440\u044a\u0445\u044f\u043a\u0430\u043d\u0435 (\"svrah-yakane\" \u2248\"over-\"ya\"-ing\").\nBulgarian is the only Slavic language whose literary standard does not naturally contain the iotated sound (or its palatalized variant , except in non-Slavic foreign-loaned words). The sound is common in all modern Slavic languages (e.g. Czech \"medv\u011bd\" \"bear\", Polish \"pi\u0119\u0107\" \"five\", Serbo-Croatian jelen\" \"deer\", Ukrainian \"\u043d\u0435\u043c\u0430\u0454 \"there is not...\", Macedonian \"\u043f\u0438\u0448\u0443\u0432\u0430\u045a\u0435\" \"writing\", etc.), as well as some Western Bulgarian dialectal forms \u2013 e.g. \"\u043e\u0440\u0430\u0300\u043d\u2019\u0435\" (standard Bulgarian: \"\u043e\u0440\u0430\u043d\u0435\" , \"ploughing\"), however it is not represented in standard Bulgarian speech or writing. Even where occurs in other Slavic words, in Standard Bulgarian it is usually transcribed and pronounced as pure \u2013 e.g. Boris Yeltsin is \"Eltsin\" (), Yekaterinburg is \"Ekaterinburg\" () and Sarajevo is \"Saraevo\" (), although - because the sound is contained in a stressed syllable at the beginning of the word - Jelena Jankovi\u0107 is \"Yelena\" \u2013 .\nRelationship to Macedonian.\nUntil the period immediately following the Second World War, all Bulgarian and the majority of foreign linguists referred to the South Slavic dialect continuum spanning the area of modern Bulgaria, North Macedonia and parts of Northern Greece as a group of Bulgarian dialects. In contrast, Serbian sources tended to label them \"south Serbian\" dialects. Some local naming conventions included \"bolg\u00e1rski\", \"bug\u00e1rski\" and so forth. The codifiers of the standard Bulgarian language, however, did not wish to make any allowances for a pluricentric \"Bulgaro-Macedonian\" compromise. In 1870 Marin Drinov, who played a decisive role in the standardization of the Bulgarian language, rejected the proposal of Parteniy Zografski and Kuzman Shapkarev for a mixed eastern and western Bulgarian/Macedonian foundation of the standard Bulgarian language, stating in his article in the newspaper Makedoniya: \"Such an artificial assembly of written language is something impossible, unattainable and never heard of.\"\nAfter 1944 the People's Republic of Bulgaria and the Socialist Federal Republic of Yugoslavia began a policy of making Macedonia into the connecting link for the establishment of a new Balkan Federative Republic and stimulating here a development of distinct Macedonian consciousness. With the proclamation of the Socialist Republic of Macedonia as part of the Yugoslav federation, the new authorities also started measures that would overcome the pro-Bulgarian feeling among parts of its population and in 1945 a separate Macedonian language was codified. After 1958, when the pressure from Moscow decreased, Sofia reverted to the view that the Macedonian language did not exist as a separate language. Nowadays, Bulgarian and Greek linguists, as well as some linguists from other countries, still consider the various Macedonian dialects as part of the broader Bulgarian pluricentric dialectal continuum. Outside Bulgaria and Greece, Macedonian is generally considered an autonomous language within the South Slavic dialect continuum. Sociolinguists agree that the question whether Macedonian is a dialect of Bulgarian or a language is a political one and cannot be resolved on a purely linguistic basis, because dialect continua do not allow for either/or judgments.\nAlphabet.\nIn 886 AD, the Bulgarian Empire introduced the Glagolitic alphabet which was devised by the Saints Cyril and Methodius in the 850s. The Glagolitic alphabet was gradually superseded in later centuries by the Cyrillic script, developed around the Preslav Literary School, Bulgaria in the 9th century.\nSeveral Cyrillic alphabets with 28 to 44 letters were used in the beginning and the middle of the 19th century during the efforts on the codification of Modern Bulgarian until an alphabet with 32 letters, proposed by Marin Drinov, gained prominence in the 1870s. The alphabet of Marin Drinov was used until the orthographic reform of 1945, when the letters yat (uppercase \u0462, lowercase \u0463) and yus (uppercase \u046a, lowercase \u046b) were removed from its alphabet, reducing the number of letters to 30.\nWith the accession of Bulgaria to the European Union on 1 January 2007, Cyrillic became the third official script of the European Union, following the Latin and Greek scripts.\nPhonology.\nBulgarian possesses a phonology similar to that of the rest of the South Slavic languages, notably lacking Serbo-Croatian's phonemic vowel length and tones and alveo-palatal affricates. The eastern dialects exhibit palatalization of consonants before front vowels ( and ) and reduction of vowel phonemes in unstressed position (causing mergers of and , and , and ) - both patterns have partial parallels in Russian and lead to a partly similar sound. The western dialects are like Macedonian and Serbo-Croatian in that they do not have allophonic palatalization and have only little vowel reduction.\nBulgarian has six vowel phonemes, but at least eight distinct phones can be distinguished when reduced allophones are taken into consideration.\nGrammar.\nThe parts of speech in Bulgarian are divided in ten types, which are categorized in two broad classes: mutable and immutable. The difference is that mutable parts of speech vary grammatically, whereas the immutable ones do not change, regardless of their use. The five classes of mutables are: \"nouns\", \"adjectives\", \"numerals\", \"pronouns\" and \"verbs\". Syntactically, the first four of these form the group of the noun or the nominal group. The immutables are: \"adverbs\", \"prepositions\", \"conjunctions\", \"particles\" and \"interjections\". Verbs and adverbs form the group of the verb or the verbal group.\nNominal morphology.\nNouns and adjectives have the categories grammatical gender, number, case (only vocative) and definiteness in Bulgarian. Adjectives and adjectival pronouns agree with nouns in number and gender. Pronouns have gender and number and retain (as in nearly all Indo-European languages) a more significant part of the case system.\nNominal inflection.\nGender.\nThere are three grammatical genders in Bulgarian: \"masculine\", \"feminine\" and \"neuter\". The gender of the noun can largely be inferred from its ending: nouns ending in a consonant (\"zero ending\") are generally masculine (for example, 'city', 'son', 'man'; those ending in\u00a0\u2013\u0430/\u2013\u044f (-a/-ya) ( 'woman', 'daughter', 'street') are normally feminine; and nouns ending in\u00a0\u2013\u0435,\u00a0\u2013\u043e are almost always neuter ( 'child', 'lake'), as are those rare words (usually loanwords) that end in\u00a0\u2013\u0438,\u00a0\u2013\u0443, and\u00a0\u2013\u044e ( 'tsunami', 'taboo', 'menu'). Perhaps the most significant exception from the above are the relatively numerous nouns that end in a consonant and yet are feminine: these comprise, firstly, a large group of nouns with zero ending expressing quality, degree or an abstraction, including all nouns ending on\u00a0\u2013\u043e\u0441\u0442/\u2013\u0435\u0441\u0442 -{ost/est} ( 'wisdom', 'vileness', 'loveliness', 'sickness', 'love'), and secondly, a much smaller group of irregular nouns with zero ending which define tangible objects or concepts ( 'blood', 'bone', 'evening', 'night'). There are also some commonly used words that end in a vowel and yet are masculine: 'father', 'grandfather', / 'uncle', and others.\nThe plural forms of the nouns do not express their gender as clearly as the singular ones, but may also provide some clues to it: the ending (-i) is more likely to be used with a masculine or feminine noun ( 'facts', 'sicknesses'), while one in belongs more often to a neuter noun ( 'lakes'). Also, the plural ending occurs only in masculine nouns.\nNumber.\nTwo numbers are distinguished in Bulgarian\u2013singular and plural. A variety of plural suffixes is used, and the choice between them is partly determined by their ending in singular and partly influenced by gender; in addition, irregular declension and alternative plural forms are common. Words ending in (which are usually feminine) generally have the plural ending , upon dropping of the singular ending. Of nouns ending in a consonant, the feminine ones also use , whereas the masculine ones usually have for polysyllables and for monosyllables (however, exceptions are especially common in this group). Nouns ending in (most of which are neuter) mostly use the suffixes (both of which require the dropping of the singular endings) and .\nWith cardinal numbers and related words such as ('several'), masculine nouns use a special count form in , which stems from the Proto-Slavonic dual: ('two/three chairs') versus ('these chairs'); cf. feminine ('two/three/these books') and neuter ('two/three/these beds'). However, a recently developed language norm requires that count forms should only be used with masculine nouns that do not denote persons. Thus, ('two/three students') is perceived as more correct than , while the distinction is retained in cases such as ('two/three pencils') versus ('these pencils').\nCase.\nCases exist only in the personal and some other pronouns (as they do in many other modern Indo-European languages), with nominative, accusative, dative and vocative forms. Vestiges are present in a number of phraseological units and sayings. The major exception are vocative forms, which are still in use for masculine (with the endings -\u0435, -\u043e and -\u044e) and feminine nouns (-[\u044c/\u0439]\u043e and -\u0435) in the singular.\nDefiniteness (article).\nIn modern Bulgarian, definiteness is expressed by a definite article which is postfixed to the noun, much like in the Scandinavian languages or Romanian (indefinite: , 'person'; definite: , \"\"the\" person\") or to the first nominal constituent of definite noun phrases (indefinite: , 'a good person'; definite: , \"\"the\" good person\"). There are four singular definite articles. Again, the choice between them is largely determined by the noun's ending in the singular. Nouns that end in a consonant and are masculine use \u2013\u044a\u0442/\u2013\u044f\u0442, when they are grammatical subjects, and \u2013\u0430/\u2013\u044f elsewhere. Nouns that end in a consonant and are feminine, as well as nouns that end in \u2013\u0430/\u2013\u044f (most of which are feminine, too) use \u2013\u0442\u0430. Nouns that end in \u2013\u0435/\u2013\u043e use \u2013\u0442\u043e.\nThe plural definite article is \u2013\u0442\u0435 for all nouns except for those whose plural form ends in \u2013\u0430/\u2013\u044f; these get \u2013\u0442\u0430 instead. When postfixed to adjectives the definite articles are \u2013\u044f\u0442/\u2013\u044f for masculine gender (again, with the longer form being reserved for grammatical subjects), \u2013\u0442\u0430 for feminine gender, \u2013\u0442\u043e for neuter gender, and \u2013\u0442\u0435 for plural.\nAdjective and numeral inflection.\nBoth groups agree in gender and number with the noun they are appended to. They may also take the definite article as explained above.\nPronouns.\nPronouns may vary in gender, number, and definiteness, and are the only parts of speech that have retained case inflections. Three cases are exhibited by some groups of pronouns \u2013 nominative, accusative and dative. The distinguishable types of pronouns include the following: personal, relative, reflexive, interrogative, negative, indefinitive, summative and possessive.\nVerbal morphology and grammar.\nThe Bulgarian verb can take up to 3,000 distinct forms, as it varies in person, number, voice, aspect, mood, tense and in some cases gender.\nFinite verbal forms.\nFinite verbal forms are \"simple\" or \"compound\" and agree with subjects in person (first, second and third) and number (singular, plural). In addition to that, past compound forms using participles vary in gender (masculine, feminine, neuter) and voice (active and passive) as well as aspect (perfective/aorist and imperfective).\nAspect.\nBulgarian verbs express lexical aspect: perfective verbs signify the completion of the action of the verb and form past perfective (aorist) forms; imperfective ones are neutral with regard to it and form past imperfective forms. Most Bulgarian verbs can be grouped in perfective-imperfective pairs (imperfective/perfective: \"come\", \"arrive\"). Perfective verbs can be usually formed from imperfective ones by suffixation or prefixation, but the resultant verb often deviates in meaning from the original. In the pair examples above, aspect is stem-specific and therefore there is no difference in meaning.\nIn Bulgarian, there is also grammatical aspect. Three grammatical aspects are distinguishable: neutral, perfect and pluperfect. The neutral aspect comprises the three simple tenses and the future tense. The pluperfect is manifest in tenses that use double or triple auxiliary \"be\" participles like the past pluperfect subjunctive. Perfect constructions use a single auxiliary \"be\".\nMood.\nThe traditional interpretation is that in addition to the four moods (\u043d\u0430\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u044f ) shared by most other European languages \u2013 indicative (\u0438\u0437\u044f\u0432\u0438\u0442\u0435\u043b\u043d\u043e, ) imperative (\u043f\u043e\u0432\u0435\u043b\u0438\u0442\u0435\u043b\u043d\u043e ), subjunctive ( ) and conditional (\u0443\u0441\u043b\u043e\u0432\u043d\u043e, ) \u2013 in Bulgarian there is one more to describe a general category of unwitnessed events \u2013 the inferential (\u043f\u0440\u0435\u0438\u0437\u043a\u0430\u0437\u043d\u043e ) mood. However, most contemporary Bulgarian linguists usually exclude the subjunctive mood and the inferential mood from the list of Bulgarian moods (thus placing the number of Bulgarian moods at a total of 3: indicative, imperative and conditional) and don't consider them to be moods but view them as verbial morphosyntactic constructs or separate gramemes of the verb class. The possible existence of a few other moods has been discussed in the literature. Most Bulgarian school grammars teach the traditional view of 4 Bulgarian moods (as described above, but excluding the subjunctive and including the inferential).\nTense.\nThere are three grammatically distinctive positions in time \u2013 present, past and future \u2013 which combine with aspect and mood to produce a number of formations. Normally, in grammar books these formations are viewed as separate tenses \u2013 i. e. \"past imperfect\" would mean that the verb is in past tense, in the imperfective aspect, and in the indicative mood (since no other mood is shown). There are more than 40 different tenses across Bulgarian's two aspects and five moods.\nIn the indicative mood, there are three simple tenses:\nIn the indicative there are also the following compound tenses:\nThe four perfect constructions above can vary in aspect depending on the aspect of the main-verb participle; they are in fact pairs of imperfective and perfective aspects. Verbs in forms using past participles also vary in voice and gender.\nThere is only one simple tense in the imperative mood, the present, and there are simple forms only for the second-person singular, -\u0438/-\u0439 (-i, -y/i), and plural, -\u0435\u0442\u0435/-\u0439\u0442\u0435 (-ete, -yte), e.g. \u0443\u0447\u0430 ('to study'): , sg., , pl.; 'to play': , . There are compound imperative forms for all persons and numbers in the present compound imperative (, ), the present perfect compound imperative (, ) and the rarely used present pluperfect compound imperative (, ).\nThe conditional mood consists of five compound tenses, most of which are not grammatically distinguishable. The present, future and past conditional use a special past form of the stem \u0431\u0438- (bi \u2013 \"be\") and the past participle (, , 'I would study'). The past future conditional and the past future perfect conditional coincide in form with the respective indicative tenses.\nThe subjunctive mood is rarely documented as a separate verb form in Bulgarian, (being, morphologically, a sub-instance of the quasi-infinitive construction with the particle \u0434\u0430 and a normal finite verb form), but nevertheless it is used regularly. The most common form, often mistaken for the present tense, is the present subjunctive ( , 'I had better go'). The difference between the present indicative and the present subjunctive tense is that the subjunctive can be formed by \"both\" perfective and imperfective verbs. It has completely replaced the infinitive and the supine from complex expressions (see below). It is also employed to express opinion about \"possible\" future events. The past perfect subjunctive ( , 'I'd had better be gone') refers to \"possible\" events in the past, which \"did not\" take place, and the present pluperfect subjunctive ( ), which may be used about both past and future events arousing feelings of incontinence, suspicion, etc. and has no perfect English translation.\nThe inferential mood has five pure tenses. Two of them are simple \u2013 \"past aorist inferential\" and \"past imperfect inferential\" \u2013 and are formed by the past participles of perfective and imperfective verbs, respectively. There are also three compound tenses \u2013 \"past future inferential\", \"past future perfect inferential\" and \"past perfect inferential\". All these tenses' forms are gender-specific in the singular. There are also conditional and compound-imperative crossovers. The existence of inferential forms has been attributed to Turkic influences by most Bulgarian linguists. Morphologically, they are derived from the perfect.\nNon-finite verbal forms.\nBulgarian has the following participles:\nThe participles are inflected by gender, number, and definiteness, and are coordinated with the subject when forming compound tenses (see tenses above). When used in attributive role the inflection attributes are coordinated with the noun that is being attributed.\nReflexive verbs.\nBulgarian uses reflexive verbal forms (i.e. actions which are performed by the agent onto him- or herself) which behave in a similar way as they do in many other Indo-European languages, such as French and Spanish. The reflexive is expressed by the invariable particle se, originally a clitic form of the accusative reflexive pronoun. Thus \u2013\nWhen the action is performed on others, other particles are used, just like in any normal verb, e.g. \u2013\nSometimes, the reflexive verb form has a similar but not necessarily identical meaning to the non-reflexive verb \u2013\nIn other cases, the reflexive verb has a completely different meaning from its non-reflexive counterpart \u2013\nWhen the action is performed on an indirect object, the particles change to si and its derivatives \u2013\nIn some cases, the particle \"si\" is ambiguous between the indirect object and the possessive meaning \u2013\nThe difference between transitive and intransitive verbs can lead to significant differences in meaning with minimal change, e.g. \u2013\nThe particle \"si\" is often used to indicate a more personal relationship to the action, e.g. \u2013\nAdverbs.\nThe most productive way to form adverbs is to derive them from the neuter singular form of the corresponding adjective\u2014e.g. (fast), (hard), (strange)\u2014but adjectives ending in use the masculine singular form (i.e. ending in ), instead\u2014e.g. (heroically), (bravely, like a man), (skillfully). The same pattern is used to form adverbs from the (adjective-like) ordinal numerals, e.g. (firstly), (secondly), (thirdly), and in some cases from (adjective-like) cardinal numerals, e.g. (twice as/double), (three times as), (five times as).\nThe remaining adverbs are formed in ways that are no longer productive in the language. A small number are original (not derived from other words), for example: (here), (there), (inside), (outside), (very/much) etc. The rest are mostly fossilized case forms, such as:\nAdverbs can sometimes be reduplicated to emphasize the qualitative or quantitative properties of actions, moods or relations as performed by the subject of the sentence: \"\" (\"rather slowly\"), \"\" (\"with great difficulty\"), \"\" (\"quite\", \"thoroughly\").\nSyntax.\nBulgarian employs clitic doubling, mostly for emphatic purposes. For example, the following constructions are common in colloquial Bulgarian:\nThe phenomenon is practically obligatory in the spoken language in the case of inversion signalling information structure (in writing, clitic doubling may be skipped in such instances, with a somewhat bookish effect):\nSometimes, the doubling signals syntactic relations, thus:\nThis is contrasted with:\nIn this case, clitic doubling can be a colloquial alternative of the more formal or bookish passive voice, which would be constructed as follows:\nClitic doubling is also fully obligatory, both in the spoken and in the written norm, in clauses including several special expressions that use the short accusative and dative pronouns such as \"\" (I feel like playing), \u0441\u0442\u0443\u0434\u0435\u043d\u043e \u043c\u0438 \u0435 (I am cold), and \u0431\u043e\u043b\u0438 \u043c\u0435 \u0440\u044a\u043a\u0430\u0442\u0430 (my arm hurts):\nExcept the above examples, clitic doubling is considered inappropriate in a formal context.\nOther features.\nQuestions.\nQuestions in Bulgarian which do not use a question word (such as who? what? etc.) are formed with the particle \u043b\u0438 after the verb; a subject is not necessary, as the verbal conjugation suggests who is performing the action:\nWhile the particle generally goes after the verb, it can go after a noun or adjective if a contrast is needed:\nA verb is not always necessary, e.g. when presenting a choice:\nRhetorical questions can be formed by adding to a question word, thus forming a \"double interrogative\" \u2013\nThe same construction +\u043d\u0435 ('no') is an emphasized positive \u2013\nSignificant verbs.\n\u0421\u044a\u043c.\nThe verb \u2013 'to be' is also used as an auxiliary for forming the perfect, the passive and the conditional:\nTwo alternate forms of exist:\n\u0429\u0435.\nThe impersonal verb (lit. 'it wants') is used to for forming the (positive) future tense:\nThe negative future is formed with the invariable construction (see below):\nThe past tense of this verb \u2013 \u0449\u044f\u0445 is conjugated to form the past conditional ('would have' \u2013 again, with \u0434\u0430, since it is \"irrealis\"):\n\u0418\u043c\u0430\u043c and \u043d\u044f\u043c\u0430\u043c.\nThe verbs ('to have') and ('to not have'):\nConjunctions and particles.\nBut.\nIn Bulgarian, there are several conjunctions all translating into English as \"but\", which are all used in distinct situations. They are (), (), (), (), and () (and () \u2013 \"however\", identical in use to ).\nWhile there is some overlapping between their uses, in many cases they are specific. For example, is used for a choice \u2013 \u2013 \"not this one, but that one\" (compare Spanish ), while is often used to provide extra information or an opinion \u2013 \u2013 \"I said it, but I was wrong\". Meanwhile, provides contrast between two situations, and in some sentences can even be translated as \"although\", \"while\" or even \"and\" \u2013 \u2013 \"I'm working, and he's daydreaming\".\nVery often, different words can be used to alter the emphasis of a sentence \u2013 e.g. while and both mean \"I smoke, but I shouldn't\", the first sounds more like a statement of fact (\"...but I mustn't\"), while the second feels more like a \"judgement\" (\"...but I oughtn't\"). Similarly, and both mean \"I don't want to, but he does\", however the first emphasizes the fact that \"he\" wants to, while the second emphasizes the \"wanting\" rather than the person.\n is interesting in that, while it feels archaic, it is often used in poetry and frequently in children's stories, since it has quite a moral/ominous feel to it.\nSome common expressions use these words, and some can be used alone as interjections:\nVocative particles.\nBulgarian has several abstract particles which are used to strengthen a statement. These have no precise translation in English. The particles are strictly informal and can even be considered rude by some people and in some situations. They are mostly used at the end of questions or instructions.\nModal particles.\nThese are \"tagged\" on to the beginning or end of a sentence to express the mood of the speaker in relation to the situation. They are mostly interrogative or slightly imperative in nature. There is no change in the grammatical mood when these are used (although they may be expressed through different grammatical moods in other languages).\nIntentional particles.\nThese express intent or desire, perhaps even pleading. They can be seen as a sort of cohortative side to the language. (Since they can be used by themselves, they could even be considered as verbs in their own right.) They are also highly informal.\nThese particles can be combined with the vocative particles for greater effect, e.g. (let me see), or even exclusively in combinations with them, with no other elements, e.g. (come on!); (I told you not to!).\nPronouns of quality.\nBulgarian has several pronouns of quality which have no direct parallels in English \u2013 \"kakav\" (what sort of); \"takuv\" (this sort of); \"onakuv\" (that sort of \u2013 colloq.); \"nyakakav\" (some sort of); \"nikakav\" (no sort of); \"vsyakakav\" (every sort of); and the relative pronoun \"kakavto\" (the sort of ... that ... ). The adjective \"ednakuv\" (\"the same\") derives from the same radical.\nExample phrases include:\nAn interesting phenomenon is that these can be strung along one after another in quite long constructions, e.g.\nAn extreme (colloquial) sentence, with almost no \"physical\" meaning in it whatsoever \u2013 yet which \"does\" have perfect meaning to the Bulgarian ear \u2013 would be :\n\u2014Note: the subject of the sentence is simply the pronoun \"taya\" (lit. \"this one here\"; colloq. \"she\").\nAnother interesting phenomenon that is observed in colloquial speech is the use of \"takova\" (neuter of \"takyv\") not only as a substitute for an adjective, but also as a substitute for a verb. In that case the base form \"takova\" is used as the third person singular in the present indicative and all other forms are formed by analogy to other verbs in the language. Sometimes the \"verb\" may even acquire a derivational prefix that changes its meaning. Examples:\nAnother use of \"takova\" in colloquial speech is the word \"takovata\", which can be used as a substitution for a noun, but also, if the speaker doesn't remember or is not sure how to say something, they might say \"takovata\" and then pause to think about it:\nSimilar \"meaningless\" expressions are extremely common in spoken Bulgarian, especially when the speaker is finding it difficult to describe something.\nVocabulary.\nMost of the vocabulary of modern Bulgarian consists of terms inherited from Proto-Slavic and local Bulgarian innovations and formations of those through the mediation of Old and Middle Bulgarian. The native terms in Bulgarian account for 70% to 80% of the lexicon.\nThe remaining 25% to 30% are loanwords from a number of languages, as well as derivations of such words. Bulgarian adopted also a few words of Thracian and Bulgar origin. The languages which have contributed most to Bulgarian are Russian, French and to a lesser extent English and Ottoman Turkish. Also Latin and Greek are the source of many words, used mostly in international terminology. Many Latin terms entered the language through Romanian, Aromanian, and Megleno-Romanian during Bulgarian Empires (present-day Bulgaria was part of the Roman Empire), loanwords of Greek origin in Bulgarian are a product of the influence of the liturgical language of the Orthodox Church. Many of the numerous loanwords from another Turkic language, Ottoman Turkish (and, via Ottoman Turkish, from Arabic and Persian) which were adopted into Bulgarian during the long period of Ottoman rule, have been replaced with native terms. In addition, both specialized (usually coming from the field of science) and commonplace English words (notably abstract, commodity/service-related or technical terms) have also penetrated Bulgarian since the second half of the 20th century, especially since 1989. A noteworthy portion of this English-derived terminology has attained some unique features in the process of its introduction to native speakers, and this has resulted in peculiar derivations that set the newly formed loanwords apart from the original words (mainly in pronunciation), although many loanwords are completely identical to the source words. A growing number of international neologisms are also being widely adopted, causing controversy between younger generations who, in general, are raised in the era of digital globalization, and the older, more conservative educated purists. Prior to standardization in the 19th century, after a period of Ottoman Turkish as a lingua franca for about 5 centuries, vernacular Bulgarian is estimated to have consisted of 50% Ottoman vocabulary, which contained predominantly (up to 80%) Arabic and Persian words.\nExternal links.\nLinguistic reports\nDictionaries\nCourses"}
{"id": "4151", "revid": "5862", "url": "https://en.wikipedia.org/wiki?curid=4151", "title": "Brainfuck programming language/Examples", "text": ""}
{"id": "4153", "revid": "57939", "url": "https://en.wikipedia.org/wiki?curid=4153", "title": "Bipyramid", "text": "A (symmetric) \"n\"-gonal bipyramid or dipyramid is a polyhedron formed by joining an \"n\"-gonal pyramid and its mirror image base-to-base. An \"n\"-gonal bipyramid has 2\"n\" triangle faces, 3\"n\" edges, and 2\u00a0+\u00a0\"n\" vertices.\nThe referenced \"n\"-gon in the name of a bipyramid is not a face but the internal polygon base, lying in the mirror plane that connects the two pyramid halves. (If it were a face, then each of its edges would connect three faces instead of two.)\n\"Regular\", right bipyramids.\nA \"regular\" bipyramid has a \"regular\" polygon base. It is usually implied to be also a \"right\" bipyramid.\nA \"right\" bipyramid has its two apexes \"right\" above and \"right\" below the center or the \"centroid\" of its polygon base.\nA \"regular\" right (symmetric) \"n\"-gonal bipyramid has Schl\u00e4fli symbol }.\nA right (symmetric) bipyramid has Schl\u00e4fli symbol , for polygon base P.\nThe \"regular\" right (thus face-transitive) \"n\"-gonal bipyramid with regular vertices is the dual of the \"n\"-gonal uniform (thus right) prism, and has congruent isosceles triangle faces.\nA \"regular\" right (symmetric) \"n\"-gonal bipyramid can be projected on a sphere or globe as a \"regular\" right (symmetric) \"n\"-gonal spherical bipyramid: \"n\" equally spaced lines of longitude going from pole to pole, and an equator line bisecting them.\nEquilateral triangle bipyramids.\nOnly three kinds of bipyramids can have all edges of the same length (which implies that all faces are equilateral triangles, and thus the bipyramid is a deltahedron): the \"regular\" right (symmetric) triangular, tetragonal, and pentagonal bipyramids. The tetragonal or square bipyramid with same length edges, or regular octahedron, counts among the Platonic solids; the triangular and pentagonal bipyramids with same length edges count among the Johnson solids (J12 and J13).\nKaleidoscopic symmetry.\nA \"\"regular\" right\" (symmetric) \"n\"-gonal bipyramid has dihedral symmetry group D\"n\"h, of order 4\"n\", except in the case of a regular octahedron, which has the larger octahedral symmetry group Oh, of order 48, which has three versions of D4h as subgroups. The rotation group is D\"n\", of order 2\"n\", except in the case of a regular octahedron, which has the larger rotation group O, of order 24, which has three versions of D4 as subgroups.\nThe 4\"n triangle faces of a \"regular\" right (symmetric) 2\"n-gonal bipyramid, projected as the 4\"n spherical triangle faces of a \"regular\" right (symmetric) 2\"n-gonal spherical bipyramid, represent the fundamental domains of dihedral symmetry in three dimensions: D\"n\"h, [\"n\",2], (*\"n\"22), order 4\"n\". These domains can be shown as alternately colored spherical triangles:\nAn \"n\"-gonal (symmetric) bipyramid can be seen as the Kleetope of the \"corresponding\" \"n\"-gonal dihedron.\nVolume.\nVolume of a (symmetric) bipyramid:\nwhere \"B\" is the area of the base and \"h\" the height from the base plane to an apex.\nThis works for any shape of the base, and for any location of the apex, provided that \"h\" is measured as the perpendicular distance from the plane which contains the internal polygon base. Hence:\nVolume of a (symmetric) bipyramid whose base is a \"regular\" \"n\"-sided polygon with side length \"s\" and whose height is \"h\":\nOblique bipyramids.\nNon-right bipyramids are called oblique bipyramids.\nConcave bipyramids.\nA \"concave\" bipyramid has a \"concave\" polygon base.\n(*) Its base has no obvious centroid; if its apexes are not \"right\" above/below the gravity center of its base, it is not a \"right\" bipyramid. Anyway, it is a concave octahedron.\nAsymmetric/inverted right bipyramids.\nAn asymmetric \"right\" bipyramid joins two \"right\" pyramids with congruent bases but unequal heights, base-to-base.\nAn inverted \"right\" bipyramid joins two \"right\" pyramids with congruent bases but unequal heights, base-to-base, but on the same side of their common base.\nThe dual of an asymmetric or inverted right bipyramid is a frustum.\nA \"regular\" asymmetric/inverted right \"n\"-gonal bipyramid has symmetry group C\"n\"v, of order 2\"n\".\nScalene triangle bipyramids.\nAn \"isotoxal\" \"right\" (symmetric) di-\"n\"-gonal bipyramid is a \"right\" (symmetric) 2\"n\"-gonal bipyramid with an \"isotoxal\" flat polygon base: its 2\"n\" vertices around sides are coplanar, but alternate in two radii.\nAn \"isotoxal\" right (symmetric) di-\"n\"-gonal bipyramid has \"n\" two-fold rotation axes through vertices around sides, \"n\" reflection planes through vertices and apexes, an \"n\"-fold rotation axis through apexes, a reflection plane through base, and an \"n\"-fold rotation-reflection axis through apexes, representing symmetry group D\"n\"h, [\"n\",2], (*22\"n\"), of order 4\"n\". (The reflection in base plane corresponds to the 0\u00b0 rotation-reflection. If \"n\" is even, there is a symmetry about the center, corresponding to the 180\u00b0 rotation-reflection.)\nAll its faces are congruent scalene triangles, and it is isohedral. It can be seen as another type of right \"symmetric\" di-\"n\"-gonal \"scalenohedron\".\nNote: For at most two particular apex heights, triangle faces may be isoceles.\nExample:\nIn crystallography, \"isotoxal\" right (symmetric) \"didigonal\" (*) (8-faced), ditrigonal (12-faced), ditetragonal (16-faced), and dihexagonal (24-faced) bipyramids exist.\n(*) The smallest geometric di-\"n\"-gonal bipyramids have eight faces, and are topologically identical to the regular octahedron. In this case (2\"n\"\u00a0=\u00a02\u00d72):&lt;br&gt;an \"isotoxal\" right (symmetric) \"didigonal\" bipyramid is called a \"rhombic bipyramid\", although all its faces are scalene triangles, because its flat polygon base is a rhombus.\nScalenohedra.\nA \"\"regular\" right \"symmetric\"\" di-\"n\"-gonal scalenohedron can be made with a \"regular\" zig-zag skew 2\"n\"-gon base, two \"symmetric\" apexes \"right\" above and \"right\" below the base center, and triangle faces connecting each base edge to each apex.\nIt has two apexes and 2\"n\" vertices around sides, 4\"n\" faces, and 6\"n\" edges; it is topologically identical to a 2\"n\"-gonal bipyramid, but its 2\"n\" vertices around sides alternate in two rings above and below the center.\nA \"regular\" right \"symmetric\" di-\"n\"-gonal scalenohedron has \"n\" two-fold rotation axes through mid-edges around sides, \"n\" reflection planes through vertices and apexes, an \"n\"-fold rotation axis through apexes, and an \"n\"-fold rotation-reflection axis through apexes, representing symmetry group D\"n\"v = D\"n\"d, [2+,2\"n\"], (2*\"n\"), of order 4\"n\". (If n is odd, there is a symmetry about the center, corresponding to the 180\u00b0 rotation-reflection.)\nAll its faces are congruent scalene triangles, and it is isohedral. It can be seen as another type of right \"symmetric\" 2\"n\"-gonal bipyramid, with a regular zig-zag skew polygon base.\nNote: For at most two particular apex heights, triangle faces may be isoceles.\nIn crystallography, \"regular\" right \"symmetric\" \"didigonal\" (8-faced) and ditrigonal (12-faced) scalenohedra exist.\nThe smallest geometric scalenohedra have eight faces, and are topologically identical to the regular octahedron. In this case (2\"n\"\u00a0=\u00a02\u00d72):&lt;br&gt;a \"regular\" right \"symmetric\" \"didigonal\" scalenohedron is called a \"tetragonal scalenohedron\"; its six vertices can be represented as (0,0,\u00b11), (\u00b11,0,\"z\"), (0,\u00b11,\u2212\"z\"), where \"z\" is a parameter between 0 and 1; &lt;br&gt;at \"z\"\u00a0=\u00a00, it is a regular octahedron; at \"z\"\u00a0=\u00a01, it is a disphenoid with all merged coplanar faces (four congruent isosceles triangles); for \"z\"\u00a0&gt;\u00a01, it becomes concave.\nNote: If the 2\"n\"-gon base is both isotoxal in-out and zig-zag skew, then not all triangle faces of the \"isotoxal\" right \"symmetric\" solid are congruent.\nExample: The solid with isotoxal in-out zig-zag skew 2\u00d72-gon base vertices:&lt;br&gt;U(1;0;1), U'(-1;0;1), V(0;2;-1), V'(0;-2;-1),&lt;br&gt;and with \"right\" symmetric apexes:&lt;br&gt;A(0;0;3), A'(0;0;-3),&lt;br&gt;has five different edge lengths:\nthus \"not\" all its triangle faces are congruent.\n\"Regular\" star bipyramids.\nA self-intersecting or \"star\" bipyramid has a \"star\" polygon base.\nA \"\"regular\" right symmetric\" star bipyramid can be made with a \"regular\" star polygon base, two \"symmetric\" apexes \"right\" above and \"right\" below the base center, and thus one-to-one \"symmetric\" triangle faces connecting each base edge to each apex.\nA \"regular\" right symmetric star bipyramid has congruent isosceles triangle faces, and is isohedral.\nNote: For at most one particular apex height, triangle faces may be equilateral.\nA {\"p\"/\"q\"}-bipyramid has Coxeter diagram .\nScalene triangle star bipyramids.\nAn \"\"isotoxal\" right symmetric\" 2\"p\"/\"q\"-gonal star bipyramid can be made with an \"isotoxal\" in-out star 2\"p\"/\"q\"-gon base, two \"symmetric\" apexes \"right\" above and \"right\" below the base center, and thus one-to-one \"symmetric\" triangle faces connecting each base edge to each apex.\nAn \"isotoxal\" right symmetric 2\"p\"/\"q\"-gonal star bipyramid has congruent scalene triangle faces, and is isohedral. It can be seen as another type of 2\"p\"/\"q\"-gonal right \"symmetric\" \"star scalenohedron\".\nNote: For at most two particular apex heights, triangle faces may be isoceles.\nStar scalenohedra.\nA \"\"regular\" right \"symmetric\"\" 2\"p\"/\"q\"-gonal star scalenohedron can be made with a \"regular\" zig-zag skew star 2\"p\"/\"q\"-gon base, two \"symmetric\" apexes \"right\" above and \"right\" below the base center, and triangle faces connecting each base edge to each apex.\nA \"regular\" right \"symmetric\" 2\"p\"/\"q\"-gonal star scalenohedron has congruent scalene triangle faces, and is isohedral. It can be seen as another type of right \"symmetric\" 2\"p\"/\"q\"-gonal star bipyramid, with a regular zig-zag skew star polygon base.\nNote: For at most two particular apex heights, triangle faces may be isosceles.\nNote: If the star 2\"p\"/\"q\"-gon base is both isotoxal in-out and zig-zag skew, then not all triangle faces of the \"isotoxal\" right \"symmetric\" star polyhedron are congruent.\nWith base vertices:&lt;br&gt;U0(1;0;1), U1(0;1;1), U2(-1;0;1), U3(0;-1;1),&lt;br&gt;V0(2;2;-1), V1(-2;2;-1), V2(-2;-2;-1), V3(2;-2;-1),&lt;br&gt;and with apexes:&lt;br&gt;A(0;0;3), A'(0;0;-3),&lt;br&gt;it has four different edge lengths:\nthus \"not\" all its triangle faces are congruent.\n4-polytopes with bipyramid cells.\nThe dual of the rectification of each convex regular 4-polytopes is a cell-transitive 4-polytope with bipyramidal cells. In the following, the apex vertex of the bipyramid is A and an equator vertex is E. The distance between adjacent vertices on the equator EE\u00a0=\u00a01, the apex to equator edge is AE and the distance between the apices is AA. The bipyramid 4-polytope will have \"V\"A vertices where the apices of \"N\"A bipyramids meet. It will have \"V\"E vertices where the type E vertices of \"N\"E bipyramids meet. \"N\"AE bipyramids meet along each type AE edge. \"N\"EE bipyramids meet along each type EE edge. \"C\"AE is the cosine of the dihedral angle along an AE edge. \"C\"EE is the cosine of the dihedral angle along an EE edge. As cells must fit around an edge, \nHigher dimensions.\nIn general, a \"bipyramid\" can be seen as an \"n\"-polytope constructed with a (\"n\"\u00a0\u2212\u00a01)-polytope in a hyperplane with two points in opposite directions, equal distance perpendicular from the hyperplane. If the (\"n\"\u00a0\u2212\u00a01)-polytope is a regular polytope, it will have identical pyramidal facets. An example is the 16-cell, which is an octahedral bipyramid, and more generally an \"n\"-orthoplex is an (\"n\"\u00a0\u2212\u00a01)-orthoplex bipyramid.\nA two-dimensional bipyramid is a square."}
{"id": "4154", "revid": "41186042", "url": "https://en.wikipedia.org/wiki?curid=4154", "title": "Beast of Bodmin Moor", "text": "In British folklore, the Beast of Bodmin Moor, () is a phantom wild cat purported to live in Cornwall, England, United Kingdom. Bodmin Moor became a centre of purported sightings after 1978, with occasional reports of mutilated slain livestock; the alleged panther/ leopard-like black cats of the same region came to be popularly known as the Beast of Bodmin Moor.\nIn general, scientists reject such claims because of the improbably large numbers necessary to maintain a breeding population and because climate and food supply issues would make such purported creatures' survival in reported habitats unlikely.\nInvestigation.\nA long-held hypothesis suggests the possibility that alien big cats at large in the United Kingdom could have been imported as part of private collections or zoos, then later escaped or set free. An escaped big cat would not be reported to the authorities due to the illegality of owning and importing the animals. It has been claimed that animal trainer Mary Chipperfield released three pumas into the wild following the closure of her Plymouth zoo in 1978 and that subsequent sightings of the animals gave rise to rumours of the Beast.\nThe Ministry of Agriculture, Fisheries and Food conducted an official investigation in 1995 led by investigators Simon Baker and Charles Wilson. On 19th of July 1995 the study found that there was \"no verifiable evidence\" of exotic felines loose in Britain and that the mauled farm animals could have been attacked by common indigenous species. The report stated that \"no verifiable evidence for the presence of a 'big cat' was found\u00a0... There is no significant threat to livestock from a 'big cat' in Bodmin Moor\".\nSkull.\nLess than a week after the government report, on 24 July 1995, a boy was walking by the River Fowey when he discovered a large cat skull. Measuring about long by wide, the skull was lacking its lower jaw but possessed three sharp, prominent canines that suggested that it might have been a leopard. The story hit the national press at about the same time as the official denial of alien big cat evidence on Bodmin Moor.\nThe skull was sent to the Natural History Museum in London for verification. A team of entomologists and zoologists from the Natural History Museum in London determined that it was a genuine skull from a young male leopard, but also found that the cat had not died in Britain and that the skull had been imported as part of a leopard-skin rug. The back of the skull was cleanly cut off in a way that is commonly used to mount the head on a rug. There was an egg case inside the skull that had been laid by a tropical cockroach that could not possibly be found in Britain. There were also cut marks on the skull indicating the flesh had been scraped off with a knife, and the skull had begun to decompose only after a recent immersion in water.\nSoon after the discovery, an investigation held in December 1997 was incited by bite marks on farm animals, droppings, and new photographs, one of which was taken through binoculars close to St. Austell, Cornwall, and evidently showed an adult pregnant female jaguar."}
{"id": "4157", "revid": "38612296", "url": "https://en.wikipedia.org/wiki?curid=4157", "title": "Brown University", "text": "Brown University is a private Ivy League research university in Providence, Rhode Island. Founded in 1764 as the \"College in the English Colony of Rhode Island and Providence Plantations\", it is the seventh-oldest institution of higher education in the United States and one of the nine colonial colleges chartered before the American Revolution.\nAt its foundation, Brown was the first college in the U.S. to accept students regardless of their religious affiliation. Its engineering program was established in 1847, making it the oldest in the Ivy League. The university was one of the early doctoral-granting U.S. institutions in the late 19th century, adding masters and doctoral studies in 1887. In 1969, Brown adopted a New Curriculum sometimes referred to as the Brown Curriculum after a period of student lobbying. The New Curriculum eliminated mandatory \"general education\" distribution requirements, made students \"the architects of their own syllabus\" and allowed them to take any course for a grade of satisfactory (Pass) or no-credit (Fail) which is unrecorded on external transcripts. In 1971, Brown's coordinate women's institution, Pembroke College, was fully merged into the university; the Pembroke Campus now includes dormitories and classrooms used by all of Brown.\nAdmissions is among the most selective in the United States, with an acceptance rate of about 7% for Fall 2019.\nThe university comprises the College, the Graduate School, Alpert Medical School, the School of Engineering, the School of Public Health and the School of Professional Studies (which includes the IE Brown Executive MBA program). Brown's international programs are organized through the Watson Institute for International and Public Affairs, and the university is academically affiliated with the Marine Biological Laboratory and the Rhode Island School of Design. The Brown/RISD Dual Degree Program, offered in conjunction with the Rhode Island School of Design, is a five-year course that awards degrees from both institutions.\nBrown's main campus is located in the College Hill neighborhood of Providence, Rhode Island. The university's neighborhood is a federally listed architectural district with a dense concentration of Colonial-era buildings. Benefit Street, on the western edge of the campus, contains \"one of the finest cohesive collections of restored seventeenth- and eighteenth-century architecture in the United States\".\n, 8 Nobel Prize winners have been affiliated with Brown University as alumni, faculty, or researchers, as well as five National Humanities Medalists and 10 National Medal of Science laureates. Other notable alumni include 25 Pulitzer Prize winners, twelve billionaires, one U.S. Supreme Court Chief Justice, four U.S. Secretaries of State, 99 members of the United States Congress, 57 Rhodes Scholars, 52 Gates Cambridge Scholars, 50 Marshall Scholars, and 15 MacArthur Genius Fellows.\nHistory.\nThe foundation and the charter.\nThe origin of Brown University can be dated to 1761, when three residents of Newport, Rhode Island, drafted a petition to the General Assembly of the colony:\nYour Petitioners propose to open a literary institution or School for instructing young Gentlemen in the Languages, Mathematics, Geography &amp; History, &amp; such other branches of Knowledge as shall be desired. That for this End ... it will be necessary ... to erect a public Building or Buildings for the boarding of the youth &amp; the Residence of the Professors.\nThe three petitioners were Ezra Stiles, pastor of Newport's Second Congregational Church and future president of Yale; William Ellery, Jr., future signer of the United States Declaration of Independence; and Josias Lyndon, future governor of the colony. Stiles and Ellery were co-authors of the Charter of the College two years later. The editor of Stiles's papers observes, \"This draft of a petition connects itself with other evidence of Dr. Stiles's project for a Collegiate Institution in Rhode Island, before the charter of what became Brown University.\"\nThere is further documentary evidence that Stiles was making plans for a college in 1762. On January 20, Chauncey Whittelsey, pastor of the First Church of New Haven, answered a letter from Stiles:\nThe week before last I sent you the Copy of Yale College Charter ... Should you make any Progress in the Affair of a Colledge, I should be glad to hear of it; I heartily wish you Success therein.\nThe Philadelphia Association of Baptist Churches also had an eye on Rhode Island, home of the mother church of their denomination: the First Baptist Church in America, founded in Providence in 1638 by Roger Williams. The Baptists were as yet unrepresented among colonial colleges; the Congregationalists had Harvard and Yale, the Presbyterians had the College of New Jersey (later Princeton), and the Episcopalians had the College of William and Mary and King's College (later Columbia). Isaac Backus was the historian of the New England Baptists and an inaugural Trustee of Brown, writing in 1784. He described the October 1762 resolution taken at Philadelphia:\nThe Philadelphia Association obtained such an acquaintance with our affairs, as to bring them to an apprehension that it was practicable and expedient to erect a college in the Colony of Rhode-Island, under the chief direction of the Baptists; ... Mr. James Manning, who took his first degree in New-Jersey college in September, 1762, was esteemed a suitable leader in this important work.\nManning arrived at Newport in July 1763 and was introduced to Stiles, who agreed to write the Charter for the college. Stiles's first draft was read to the General Assembly in August 1763 and rejected by Baptist members who worried that the College Board of Fellows would under-represent the Baptists. A revised Charter written by Stiles and Ellery was adopted by the Assembly on March 3, 1764.\nIn September 1764, the inaugural meeting of the College Corporation was held at Newport. Governor Stephen Hopkins was chosen chancellor, former and future governor Samuel Ward was vice chancellor, John Tillinghast treasurer, and Thomas Eyres secretary. The Charter stipulated that the Board of Trustees be composed of 22 Baptists, five Quakers, five Episcopalians, and four Congregationalists. Of the 12 Fellows, eight should be Baptists\u2014including the College president\u2014\"and the rest indifferently of any or all Denominations.\"\nThe Charter was not the grant of King George III, as is sometimes supposed, but rather an Act of the colonial General Assembly. In two particulars, the Charter may be said to be a uniquely progressive document. First, other colleges had curricular strictures against opposing doctrines, while Brown's Charter asserted, \"Sectarian differences of opinions, shall not make any Part of the Public and Classical Instruction.\" Second, according to Brown University historian Walter Bronson, \"the instrument governing Brown University recognized more broadly and fundamentally than any other the principle of denominational cooperation.\" The oft-repeated statement is inaccurate that Brown's Charter alone prohibited a religious test for College membership; other college charters were also liberal in that particular.\nJames Manning was sworn in as the college's first president in 1765 and served until 1791. In 1770, the College moved from Warren, Rhode Island, to the crest of College Hill overlooking Providence. Solomon Drowne, a freshman in the class of 1773, wrote in his diary on March 26, 1770:\nThis day the Committee for settling the spot for the College, met at the New-Brick School House, when it was determined it should be set on ye Hill opposite Mr. John Jenkes; up the Presbyterian Lane.\nPresbyterian Lane is the present College Street. The eight-acre site had been purchased in two parcels by the corporation for \u00a3219, mainly from Moses Brown and John Brown, the parcels having \"formed a part of the original home lots of their ancestor, Chad Brown, and of George Rickard, who bought them from the Indians.\" University Hall was known as \"The College Edifice\" until 1823; it was modelled on Nassau Hall at the College of New Jersey. Its construction was managed by the firm of Nicholas Brown and Company, which spent \u00a32,844 in the first year building the College Edifice and the adjacent President's House.\nThe Brown family.\nNicholas Brown, a slave trader, his son Nicholas Brown, Jr. (class of 1786), John Brown, Joseph Brown, and Moses Brown were all instrumental in moving the college to Providence and securing its endowment. Joseph became a professor of natural philosophy at the college; John served as its treasurer from 1775 to 1796; and Nicholas Junior succeeded his uncle as treasurer from 1796 to 1825.\nOn September 8, 1803, the Corporation voted, \"That the donation of $5000 Dollars, if made to this College within one Year from the late Commencement, shall entitle the donor to name the College.\" That appeal was answered by College treasurer Nicholas Brown, Junior, in a letter dated September 6, 1804, and the Corporation honored its promise. \"In gratitude to Mr. Brown, the Corporation at the same meeting voted, 'That this College be called and known in all future time by the Name of Brown University'.\" Over the years, the benefactions of Nicholas Brown, Jr., totaled nearly $160,000, an enormous sum for that period, and included the buildings Hope College (1821\u201322) and Manning Hall (1834-35).\nIt is sometimes erroneously supposed that Brown was named after John Brown, whose commercial activity included the transportation of African slaves. In fact, Brown was named for Nicholas Brown, Jr., philanthropist, founder of the Providence Athenaeum, co-founder of Butler Hospital, and an abolitionist. Nicholas Brown, Jr., became a financier of the movement under the guidance of his uncle Moses Brown, one of the leading abolitionists of his day.\nThe American Revolution.\nThe College library was moved out of Providence for safekeeping in the fall of 1776, with British vessels patrolling Narragansett Bay. On December 7, 1776, six thousand British and Hessian troops sailed into Newport harbor under the command of Sir Peter Parker. College President Manning said in a letter written after the war:\nThe royal Army landed on Rhode Island &amp; took possession of the same: This brought their Camp in plain View from the College with the naked Eye; upon which the Country flew to Arms &amp; marched for Providence, there, unprovided with Barracks they marched into the College &amp; dispossessed the Students, about 40 in Number.\n\"In the claim for damages presented by the Corporation to the United States government,\" says the university historian, \"it is stated that the American troops used it for barracks and hospital from December 10, 1776, to April 20, 1780, and that the French troops used it for a hospital from June 26, 1780, to May 27, 1782.\" The French troops were those of the Comte de Rochambeau.\nPresidents.\nBrown's current president Christina Hull Paxson took office in 2012. She had previously been dean of the Woodrow Wilson School at Princeton University and a past-chair of Princeton's economics department. In 2014 and 2015, Paxson presided over the year-long celebration of the 250th anniversary of Brown's founding. Her immediate predecessor as president was Ruth Simmons, the first African American president of an Ivy League institution.\nThe New Curriculum.\nIn 1966, the first Group Independent Study Project (GISP) at Brown was formed, involving 80 students and 15 professors. The GISP was inspired by student-initiated experimental schools, especially San Francisco State College, and sought ways to \"put students at the center of their education\" and \"teach students how to think rather than just teaching facts.\"\nMembers of the GISP, Ira Magaziner and Elliot Maxwell published a paper of their findings entitled, \"Draft of a Working Paper for Education at Brown University.\" The paper made proposals for the new curriculum, including interdisciplinary freshman-year courses that would introduce \"modes of thought,\" with instruction from faculty from different disciplines as well as for an end to letter grades. The following year Magaziner began organizing the student body to press for the reforms, organizing discussions and protests.\nIn 1969, University President Ray Heffner Special Committee on Curricular Philosophy in response to student rallies held support of curriculum reform. The committee was tasked with developing specific reforms and the resulting report was called the Maeder Report after the committee's chairman. The report was presented to the faculty, which voted the New Curriculum into existence on May 7, 1969. Its key features included:\nThe Modes of Thought course was discontinued early on, but the other elements are still in place. In 2006, the reintroduction of plus/minus grading was broached by persons concerned about grade inflation. The idea was rejected by the College Curriculum Council after canvassing alumni, faculty, and students, including the original authors of the Magaziner-Maxwell Report. However, President Christina Paxson has noted that grade inflation clearly exists at Brown, with 53.4% of grades given at Brown being As during the 2012\u20132013 academic year. Another unique feature of the grading system at Brown is that failures are erased from the student's transcript; as a result, some students have asked a professor for a failing grade, rather than having a C on his or her transcript. While erasure of failing grades from external transcripts is unique to Brown, it is important to note that grade inflation also exists at other U.S. universities.\nSlavery and Justice report.\nIn 2003, then-University president Ruth Simmons launched a steering committee to research the school's eighteenth-century ties to slavery. The committee released a report documenting the findings in October 2006. Entitled \"Slavery and Justice,\" the report details how the university benefited both directly and indirectly from the transatlantic slave trade and the labor of enslaved people.\nIn addition to documentation, the report included seven recommendations concerning how the university should address this legacy. Brown has since completed a number of these recommendations including the establishment of the Center for the Study of Slavery and Justice, the construction of a slavery memorial, and the funding of a $10 million permanent endowment for the Providence Public Schools.\nThe Slavery and Justice report marked the first major effort by an American university to address its ties to slavery, and prompted other institutions to undertake similar processes.\nCoat of arms.\nBrown's coat of arms was created in 1834. The prior year, president Francis Wayland had commissioned a committee to update the school's logo to match the name the university had adopted in 1804. Central in the coat of arms is a white escutcheon divided into four sectors by a red cross; within each sector is an open book. Above the shield is a crest consisting of the upper half of a sun in splendor among the clouds atop a red and white torse.\nThe sun and clouds represent \"learning piercing the clouds of ignorance,\" while the cross is believed to be a Saint George's Cross. The seal's four open books symbolize learning, and are rumored to represent Harvard, Yale, Cambridge and Oxford.\nCampus.\nBrown is the largest institutional landowner in Providence, with properties on College Hill and in the Jewelry District. The College Hill campus was built contemporarily with the eighteenth- and nineteenth-century precincts that surround it, so that university buildings blend with the architectural fabric of the city. Brown's central campus, built around University Hall, is defined by a brick and wrought-iron fence, which traces the block's perimeter. The character of Brown's urban campus is European organic rather than American landscaped.\nMain campus.\nBrown's main campus, comprises 235 buildings and in the East Side neighborhood of College Hill. The university's central, historic campus sits on the crest of College Hill while newer buildings extend northward, eastward, and southward. This area of campus, constructed between 1770 and 1926\u2014is defined by three greens: the Front or Quiet Green, the Middle or College Green, and the Ruth J. Simmons Quadrangle (historically known as Lincoln Field).\nAdjacent to this older campus are, to the south, academic buildings and residential quadrangles, including Wriston, Keeney, and Gregorian quadrangles; to the east, Sciences Park occupying two city blocks; to the north connected to Simmons Quadrangle by The Walk, academic and residential precincts, including the life sciences complex and the Pembroke Campus; and to the west, on the slope of College Hill, academic buildings, including List Art Center and the Hay and Rockefeller libraries. The perimeter of the old campus contains the university's four significant examples of Brutalist architecture, the John D. Rockefeller Jr. Library, the Sciences Library, the List Art Building, and the Graduate Center. \nBrown's campus is contiguous that of the Rhode Island School of Design, which sits further down College Hill.\nVan Wickle Gates.\nBuilt in 1901, the Van Wickle Gates are a set of wrought iron gates on the front of Brown's campus. The gates feature two gates flaking a much larger gate. At Convocation the central gate opens inward to admit the procession of new students; at Commencement, the gate opens outward for the procession of graduates. A Brown superstition is that students who walk through the central gate a second time prematurely will not graduate, although walking backward is said to cancel the hex. Members of the Brown University Band famously flout the superstition by walking through the gate three times too many, as they annually play their role in the Commencement parade.\nJohn Hay Library.\nThe John Hay Library is the second oldest library on campus. It was opened in 1910 and named for John Hay (class of 1858, private secretary to Abraham Lincoln and Secretary of State under two Presidents) at the request of his friend Andrew Carnegie, who contributed half of the $300,000 cost of the building. It is now the repository of the university's archives, rare books and manuscripts, and special collections. Noteworthy among the latter are the Anne S. K. Brown Military Collection (described as \"the foremost American collection of material devoted to the history and iconography of soldiers and soldiering\"), the Harris Collection of American Poetry and Plays (described as \"the largest and most comprehensive collection of its kind in any research library\"), the Lownes Collection of the History of Science (described as \"one of the three most important private collections of books of science in America\"), and the papers of H. P. Lovecraft. The Hay Library is home to one of the broadest collections of incunabula in the Americas, one of Brown's two Shakespeare First Folios, the manuscript of George Orwell's \"Nineteen Eighty-Four,\" and three books bound in human skin.\nJohn Carter Brown Library\nFounded in 1846, the John Carter Brown Library, is administered separately from the university but since 1904 has been located on Brown's campus. The John Carter Brown Library is generally regarded as the world's leading collection of primary historical sources relating to the exploration and colonization of the Americas. The library contains the best preserved of the eleven surviving copies of the Bay Psalm Book\u2014the earliest extant book printed in British North America and the most expensive printed book in the world. Other holdings include a Shakespeare First Folio and the world's largest collection of 16th century Mexican texts. \nHaffenreffer Museum.\nThe exhibition galleries of the Haffenreffer Museum of Anthropology, Brown's teaching museum, are located in Manning Hall on the campus's main green. Its one million artifacts, available for research and educational purposes, are located at its Collections Research Center in Bristol, RI. The museum's goal is to inspire creative and critical thinking about culture by fostering an interdisciplinary understanding of the material world. It provides opportunities for faculty and students to work with collections and the public, teaching through objects and programs in classrooms and exhibitions. The museum sponsors lectures and events in all areas of anthropology, and also runs an extensive program of outreach to local schools.\nAnnmary Brown Memorial.\nThe Annmary Brown Memorial was constructed from 1903 to 1907 by the politician, Civil War veteran, and book collector General Rush Hawkins, as a mausoleum for his wife, Annmary Brown, a member of the Brown family. In addition to its crypt\u2014the final repository for Brown and Hawkins\u2014the Memorial includes works of art from Hawkins's private collection, including paintings by Angelica Kauffman, Peter Paul Rubens, Gilbert Stuart, Giovanni Battista Tiepolo, Benjamin West, and Eastman Johnson, among others. His collection of over 450 incunabula was relocated to the John Hay Library in 1990. Today the Memorial is home to Brown's Medieval Studies and Renaissance Studies programs.\nThe Walk.\nThe \"Walk\" connects Pembroke Campus to the main campus. It is a succession of green spaces extending from Ruth Simmons Quadrangle (Lincoln Field) in the south to the Pembroke College monument on Meeting Street in the north. It is bordered by departmental buildings and the Granoff Center for the Creative Arts. A focal point of The Walk is Maya Lin's water-circulating topographical sculpture of Narragansett Bay, entitled \"Under the Laurentide.\" Installed in 2015, it is next to the Institute for the Study of Environment and Society.\nPembroke campus.\nThe Women's College in Brown University, known as Pembroke College, was founded in October 1891. When it merged with Brown in 1971, the Pembroke Campus was absorbed into the Brown campus. The Pembroke campus is centered on a quadrangle that fronts on Meeting Street, where a garden and monument\u2014with scale-model of the quadrangle in bronze\u2014compose the formal entry to the campus. The Pembroke campus is characterized by largely Georgian and Victorian architecture. The west side of the quadrangle comprises Pembroke Hall (1897), Smith-Buonanno Hall (1907), and Metcalf Hall (1919), while the east side comprises Alumnae Hall (1927) and Miller Hall (1910). The quadrangle culminates on the north with Andrews Hall (1947) and its terrace and garden. Pembroke Hall, originally a classroom building and library, now houses the Cogut Center for the Humanities.\nEast Campus, centered on Hope and Charlesfield streets, was originally the site of Bryant University. In 1969, as Bryant was preparing to move to Smithfield, Rhode Island, Brown purchased their Providence campus for $5 million. This expanded the Brown campus by and 26 buildings. In 1971, the area was named East Campus.\nThayer Street runs through Brown's main campus, north to south, and is College Hill's reduced-scale counterpart to Harvard Square or Berkeley's Telegraph Avenue. Restaurants, cafes, bistros, taverns, pubs, bookstores, second-hand shops, and the like abound. Tourists, people-watchers, buskers, and students from Providence's six colleges make the scene. Half a mile south of campus is Thayer Street's hipper cousin, Wickenden Street. More picturesque and with older architecture, it features galleries, pubs, specialty shops, artist-supply stores, and a regionally famous coffee shop that doubles as a film set (for Woody Allen and others).\nBrown Stadium, which was built in 1925 and is home to the football team, is located approximately a mile to the northeast of the main campus. Marston Boathouse, the home of the crew teams, lies on the Seekonk River, to the southeast of campus. Brown's Warren Alpert Medical School is situated in the historic Jewelry District of Providence, near the medical campus of Brown's teaching hospitals, Rhode Island Hospital, Women and Infants Hospital, and Hasbro Children's Hospital. Other university research facilities in the Jewelry District include the Laboratories for Molecular Medicine.\nBrown's School of Public Health occupies a landmark modernist building overlooking Memorial Park on the Providence Riverwalk. Brown also owns the Mount Hope Grant in Bristol, Rhode Island, an important Native American and King Philip's War site. Brown's Haffenreffer Museum of Anthropology Collection Research Center, particularly strong in Native American items, is located in the Mount Hope Grant.\nSustainability.\nBrown has committed to \"minimize its energy use, reduce negative environmental impacts and promote environmental stewardship.\" The Energy and Environmental Advisory Committee has developed a set of ambitious goals for the university to reduce its carbon emissions and eventually achieve carbon neutrality. The \"Brown is Green\" website collects information about Brown's progress toward greenhouse gas emissions reductions and related campus initiatives, such as student groups, courses, and research. Brown's grade of A-minus was the top one issued in the 2009 report of the Sustainable Endowments Institute (no A-grade was issued).\nBrown has a number of active environmental leadership groups on campus. These groups have begun a number of campus-wide environmental initiatives\u2014including promoting the reduction of supply and demand of bottled water and investigating a composting program.\nAccording to the A. W. Kuchler U.S. potential natural vegetation types, Brown would have a dominant vegetation type of Appalachian Oak (\"104\") with a dominant vegetation form of Eastern Hardwood Forest (\"25\").\nAcademics.\nThe College.\nFounded in 1764, the college is the oldest school of Brown. About 7,200 undergraduate students are currently enrolled in the college, and 81 concentrations (majors) are offered. Completed concentrations of undergraduates by area are social sciences 42 percent, humanities 26 percent, life sciences 17 percent, and physical sciences 14 percent. The concentrations with the greatest number of students are Biology, History, and International Relations. Brown is one of the few schools in the United States with an undergraduate concentration (major) in Egyptology. Undergraduates can also design an independent concentration if the existing programs do not align with their curricular focus.\n35 percent of undergraduates pursue graduate or professional study immediately, 60 percent within 5 years, and 80 percent within 10 years. For the Class of 1998, 75 percent of all graduates have since enrolled in a graduate or professional degree program. The degrees acquired were doctoral 22 percent, master's 35 percent, medicine 28 percent, and law 14 percent.\nThe highest fields of employment for graduates of the college are business 36 percent, education 19 percent, health/medical 6 percent, arts 6 percent, government 6 percent, and communications/media 5 percent.\nBrown/RISD Dual Degree Program.\nBrown's near neighbor on College Hill is the Rhode Island School of Design (RISD). Since 1902, Brown and RISD students have been able to cross-register at the two institutions, with Brown students permitted to take as many as four courses at RISD that count towards a Brown degree. The two institutions partner to provide various student-life services and the two student bodies compose a synergy in the College Hill cultural scene.\nAfter several years of discussion between the two institutions and several students pursuing dual degrees unofficially, Brown and RISD formally established a five-year dual degree program in 2007, with the first class matriculating in the fall of 2008. The Brown/RISD Dual Degree Program, among the most selective in the country, offered admission to 19 of the 707 applicants for the class entering in autumn 2018, an acceptance rate of 2.7 percent. It combines the complementary strengths of the two institutions, integrating studio art and design at RISD with the entire spectrum of Brown's departmental offerings. Students are admitted to the Dual Degree Program for a course lasting five years and culminating in both the Bachelor of Arts (A.B.) or Bachelor of Science (Sc.B.) degree from Brown and the Bachelor of Fine Arts (B.F.A.) degree from RISD. Prospective students must apply to the two schools separately and be accepted by separate admissions committees. Their application must then be approved by a third Brown/RISD joint committee.\nAdmitted students spend the first year in residence at RISD completing its first-year Experimental and Foundation Studies curriculum, while taking up to three Brown classes. The second year is spent in residence at Brown, during which students take mainly Brown courses while starting on their RISD major requirements. In the third, fourth, and fifth years, students can elect to live at either school or off-campus, and course distribution is determined by the requirements of each student's unique combination of Brown concentration and RISD major. Program participants are noted for their creative and original approach to cross-disciplinary opportunities, combining, for example, industrial design with engineering, or anatomical illustration with human biology, or philosophy with sculpture, or architecture with urban studies. An annual \"BRDD Exhibition\" is a well-publicized and heavily attended event, drawing interest and attendees from the wider world of industry, design, the media, and the fine arts.\nTheatre and playwriting.\nBrown's theatre and playwriting programs are among the best-regarded in the country. Six Brown graduates have received the Pulitzer Prize for Drama; Alfred Uhry '58 (1988), Lynn Nottage '86 (twice\u20142009, 2017), Ayad Akhtar '93, Nilo Cruz '94, Quiara Alegr\u00eda Hudes '04, Jackie Sibblies Drury MFA '04; In \"American Theater\" magazine's 2009 ranking of the most-produced American plays, Brown graduates occupied four of the top five places\u2014Peter Nachtrieb '97, Rachel Sheinkin '89, Sarah Ruhl '97, and Stephen Karam '02.\nThe undergraduate concentration (major) encompasses programs in theatre history, performance theory, playwriting, dramaturgy, acting, directing, dance, speech, and technical production. Applications for doctoral and master's degree programs are made through the University Graduate School. Master's degrees in acting and directing are pursued in conjunction with the Brown/Trinity Rep MFA program, which partners with the Trinity Repertory Company, a local regional theatre.\nWriting programs.\nWriting at Brown\u2014fiction, non-fiction, poetry, playwriting, screenwriting, electronic writing, mixed media, and the undergraduate writing proficiency requirement\u2014is catered for by various centers and degree programs, and a faculty that has long included nationally and internationally known authors. The undergraduate concentration (major) in literary arts offers courses in fiction, poetry, screenwriting, literary hypermedia, and translation. Graduate programs include the fiction and poetry MFA writing programs in the literary arts department, and the MFA playwriting program in the theatre arts and performance studies department. The non-fiction writing program is offered in the English department. Screenwriting and cinema narrativity courses are offered in the departments of literary arts and modern culture and media. The undergraduate writing proficiency requirement is supported by the Writing Center.\nAuthor prizewinners.\nAlumni authors take their degrees across the spectrum of degree concentrations, but a gauge of the strength of writing at Brown is the number of major national writing prizes won. To note only winners since the year 2000: Pulitzer Prize for Fiction-winners Jeffrey Eugenides '82 (2003), Marilynne Robinson '66 (2005), and Andrew Sean Greer '92 (2018); British Orange Prize-winners Marilynne Robinson '66 (2009) and Madeline Miller '00 (2012); Pulitzer Prize for Drama-winners Nilo Cruz '94 (2003), Lynn Nottage '86 (twice, 2009, 2017), Quiara Alegr\u00eda Hudes '04 (2012), and Ayad Akhtar '93 (2013); Pulitzer Prize for Biography-winners David Kertzer '69 (2015) and Benjamin Moser '98; Pulitzer Prize for Journalism-winners James Risen '77 (twice, 2002, 2006), Mark Maremont '80 (twice, 2003, 2007), Gareth Cook '91 (2005), Tony Horwitz '80 (2005), Peter Kovacs '77 (2006), Stephanie Grace '86 (2006), Mary Swerczek '98 (2006), Jane B. Spencer '99 (2006), Usha Lee McFarling '89 (2007), James Bandler '89 (2007), Amy Goldstein '75 (2009), David Rohde '90 (twice, 1996, 2009), Kathryn Schulz '96 (2016), and Alissa J. Rubin '80 (2016); Pulitzer Prize for General Nonfiction-winner James Forman Jr. '88 (2018), as well as Pulitzer Prize for Poetry-winner Peter Balakian PhD '80.\nComputer science.\nBrown began offering computer science courses through the departments of Economics and Applied Mathematics in 1956 when it acquired an IBM machine. Brown added an IBM 650 in January 1958, the only one of its type between Hartford and Boston. In 1960, Brown opened its first dedicated computer building. The building, designed by Philip Johnson and opened on George Street, received an IBM 7070 computer the next year. Brown granted computer sciences full Departmental status in 1979. In 2009, IBM and Brown announced the installation of a supercomputer (by teraflops standards), the most powerful in the southeastern New England region.\nIn the 1960s, Andries van Dam along with Ted Nelson, and Bob Wallace invented The Hypertext Editing Systems, HES and FRESS while at Brown. Nelson coined the word \"hypertext\" while Van Dam's students helped originate XML, XSLT, and related Web standards. Among the school's computer science alumni are principal architect of the Classic Mac OS, Andy Hertzfeld, principal architect of the Intel 80386 and Intel 80486 microprocessors, John Crawford, former CEO of Apple, John Sculley, and digital effects programer Masi Oka. Other alumni include former CS department head at MIT, John Guttag, Workday founder, Aneel Bhusri, and MongoDB founder Eliot Horowitz.\nThe character \"Andy\" in the animated film \"Toy Story\" purportedly an homage to Van Dam from his students employed at Pixar. \nBetween 2012 and 2018, the number of concentrators in CS tripled. In 2017, computer science overtook economics as the school's most popular undergraduate concentration.\nThe Joukowsky Institute for Archaeology and the Ancient World.\nThe Joukowsky Institute for Archaeology and the Ancient World pursues fieldwork and excavations, regional surveys, and academic study of the archaeology and art of the ancient Mediterranean, Egypt, and Western Asia from the Levant to the Caucasus. The institute has a very active fieldwork profile, with faculty-led excavations and regional surveys presently in Petra, Jordan, in West-Central Turkey, at Abydos in Egypt, and in Sudan, Italy, Mexico, Guatemala, Montserrat in the West Indies, and Providence, Rhode Island.\nThe institute's faculty includes cross-appointments from the departments of Egyptology, Assyriology, Classics, Anthropology, and History of Art and Architecture. Faculty research and publication areas include Greek and Roman art and architecture, landscape archaeology, urban and religious architecture of the Levant, Roman provincial studies, the Aegean Bronze Age, and the archaeology of the Caucasus. The institute offers visiting teaching appointments and postdoctoral fellowships which have, in recent years, included Near Eastern Archaeology and Art, Classical Archaeology and Art, Islamic Archaeology and Art, and Archaeology and Media Studies.\nEgyptology and Assyriology\nFacing the Joukowsky Institute, across the Front Green, is the Department of Egyptology and Assyriology, formed in 2006 by the merger of Brown's renowned departments of Egyptology and History of Mathematics. It is one of only a handful of such departments in the United States. The curricular focus is on three principal areas: Egyptology (the study of the ancient languages, history, and culture of Egypt), Assyriology (the study of the ancient lands of present-day Iraq, Syria, and Turkey), and the history of the ancient exact sciences (astronomy, astrology, and mathematics). Many courses in the department are open to all Brown undergraduates without prerequisite, and include archaeology, languages, history, and Egyptian and Mesopotamian religions, literature, and science. Students concentrating (majoring) in the department choose a track of either Egyptology or Assyriology. Graduate level study comprises three tracks to the doctoral degree: Egyptology, Assyriology, or the History of the Exact Sciences in Antiquity.\nThe Watson Institute for International and Public Affairs.\nThe Watson Institute for International and Public Affairs is a center for the study of global issues and public affairs and is one of the leading institutes of its type in the country. It occupies an architecturally distinctive building designed by Uruguayan architect Rafael Vi\u00f1oly. The institute was initially endowed by Thomas Watson, Jr., Brown class of 1937, former Ambassador to the Soviet Union, and longtime president of IBM. Institute faculty includes, or formerly included, Italian prime minister and European Commission president Romano Prodi, Brazilian president Fernando Henrique Cardoso, Chilean president Ricardo Lagos Escobar, Mexican novelist and statesman Carlos Fuentes, Brazilian statesman and United Nations commission head Paulo S\u00e9rgio Pinheiro, Indian foreign minister and ambassador to the United States Nirupama Rao, American diplomat and Dayton Peace Accords author Richard Holbrooke (Brown '62), and Sergei Khrushchev, editor of the papers of his father Nikita Khrushchev, leader of the Soviet Union.\nThe institute's curricular interest is organized into the principal themes of development, security, and governance\u2014with further focuses on globalization, economic uncertainty, security threats, environmental degradation, and poverty. Three Brown undergraduate concentrations (majors) are hosted by the Watson Institute\u2014Development Studies, International Relations, and Public Policy. Graduate programs offered at the Watson Institute include the Graduate Program in Development (Ph.D.) and the Public Policy Program (M.P.A). The institute also offers Post Doctoral, professional development and global outreach programming. In support of these programs, the Institute houses various centers, including the Brazil Initiative, Brown-India Initiative, China Initiative, Middle East Studies center, The Center for Latin American and Caribbean Studies (CLACS) and the Taubman Center for Public Policy. In recent years, the most internationally cited product of the Watson Institute has been its Costs of War Project, first released in 2011 and continuously updated. The Project comprises a team of economists, anthropologists, political scientists, legal experts, and physicians, and seeks to calculate the economic costs, human casualties, and impact on civil liberties of the wars in Iraq, Afghanistan, and Pakistan since 2001.\nThe School of Engineering.\nEstablished in 1847, Brown's engineering program is the oldest in the Ivy League and the third oldest civilian engineering program in the country, preceded only by Rensselaer Polytechnic Institute (1824) and Union College (1845). In 1916, the departments of electrical, mechanical, and civil engineering were merged into a Division of Engineering, and in 2010 the division was elevated to a School of Engineering.\nEngineering at Brown is especially interdisciplinary. The School is organized without the traditional departments or boundaries found at most schools, and follows a model of connectivity between disciplines\u2014including biology, medicine, physics, chemistry, computer science, the humanities and the social sciences. The School practices an innovative clustering of faculties in which engineers team with non-engineers to bring a convergence of ideas.\nIE Brown Executive MBA Dual Degree Program.\nSince 2009, Brown has developed an Executive MBA program in conjunction with one of the leading Business Schools in Europe; IE Business School in Madrid. This relationship has since strengthened resulting in both institutions offering a dual degree program. In this partnership, Brown provides its traditional coursework while IE provides most of the business-related subjects making a differentiated alternative program to other Ivy League's EMBAs. The cohort typically consists of 25-30 EMBA candidates from some 20 countries. Classes are held in Providence, Madrid, Cape Town and Online.\nThe Pembroke Center.\nThe Pembroke Center for Teaching and Research on Women was established at Brown in 1981 by Joan Wallach Scott as a research center on gender. It was named for Pembroke College, the former women's coordinate college at Brown, and is affiliated with Brown's Sarah Doyle Women's Center. It supports the undergraduate concentration in Gender and Sexuality Studies, post-doctoral research fellowships, the annual Pembroke Seminar, and other academic programs. The center also manages various collections, archives, and resources, including the Elizabeth Weed Feminist Theory Papers and the Christine Dunlap Farnham Archive.\nThe Graduate School.\nEstablished in 1887, the Graduate School has around 2,000 students studying over 50 disciplines. 20 different master's degrees are offered as well as Ph.D. degrees in over 40 subjects ranging from applied mathematics to public policy. Overall, admission to the Graduate School is most competitive with an acceptance rate of about 10 percent.\nAlpert Medical School.\nThe university's medical program started in 1811, but the school was suspended by President Wayland in 1827 after the program's faculty declined to live on campus (a new requirement under Wayland). In 1975, the first M.D. degrees from the new Program in Medicine were awarded to a graduating class of 58 students. In 1991, the school was officially renamed the Brown University School of Medicine, then renamed once more to Brown Medical School in October 2000. In January 2007, Warren Alpert donated $100 million to Brown Medical School, in recognition of which its name was changed to the Warren Alpert Medical School of Brown University.\nIn 2020, \"U.S. News &amp; World Report\" ranked Brown's medical school the 9th most selective in the country, with an acceptance rate of 2.8 percent.\n\"U.S. News\" ranks it 38th for research and 35th for primary care.\nThe medical school is known especially for its eight-year Program in Liberal Medical Education (PLME), inaugurated in 1984. One of the most selective and renowned programs of its type in the country, it offered admission to 88 of the 2,530 applicants for the class entering in autumn 2020, an acceptance rate of 3.4 percent. Since 1976, the Early Identification Program (EIP) has encouraged Rhode Island residents to pursue careers in medicine by recruiting sophomores from Providence College, Rhode Island College, the University of Rhode Island, and Tougaloo College. In 2004, the school once again began to accept applications from premedical students at other colleges and universities via AMCAS like most other medical schools. The medical school also offers combined degree programs leading to the M.D./Ph.D., M.D./M.P.H. and M.D./M.P.P. degrees.\nSchool of Public Health.\nBrown's School of Public Health grew out of the Alpert Medical School's Department of Community Health and was officially founded in 2013 as an independent school. The school issues undergraduate (A.B., Sc.B.), graduate (M.P.H., Sc.M., A.M.), doctoral (Ph.D.), and dual-degrees (M.P.H./M.P.A., M.D./M.P.H.).\nOnline Programs.\nThe Brown University School of Professional Studies currently offers blended learning Executive master's degrees in Healthcare Leadership, Cyber Security, and Science and Technology Leadership. The master's degrees are designed to help students who have a job and life outside of academia to progress in their respective fields. The students meet in Providence, RI every 6\u20137 weeks for a week seminar each trimester.\nThe university has also invested in MOOC development starting in 2013, when two courses, \"Archeology's Dirty Little Secrets\" and \"The Fiction of Relationship\", both of which received thousands of students. However, after a year of courses, the university broke its contract with Coursera and revamped its online persona and MOOC development department. By 2017, the university released new courses on edx, two of which were \"The Ethics of Memory\" and \"Artful Medicine: Art's Power to Enrich Patient Care\". In January 2018, Brown published its first \"game-ified\" course called \"Fantastic Places, Unhuman Humans: Exploring Humanity Through Literature\", which featured out of platform games to help learners understand materials, as well as a story-line that immerses users into a fictional world to help characters along their journey.\nAdmissions and financial aid.\nFor the undergraduate class of 2022 (enrolling in Fall 2018), Brown received 35,438 applications, the largest applicant pool in the university's history. 2,566 were accepted for an acceptance rate of 7.2%, the lowest in university history. Additionally, for the academic year 2015-16 there were 1,834 transfer applicants, of whom 8.9% were accepted, with an SAT range of 2180\u20132330, ACT range of 31\u201334, and average college GPA of 3.85. In 2017, the Graduate School accepted 11% of 9,215 applicants. In 2014, \"U.S. News\" ranked Brown's Warren Alpert Medical School the 5th most selective in the country, with an acceptance rate of 2.9 percent.\nBrown admission policy is stipulated need-blind for all domestic first-year applicants. In 2017, Brown announced that loans would be eliminated from all undergraduate financial aid awards starting in 2018\u20132019, as part of a new $30 million campaign called the \"Brown Promise\". In 2016\u201317, the university awarded need-based scholarships worth $120.5 million. The average need-based award for the class of 2020 was $47,940.\nRankings.\nFor their 2021 rankings, The Wall Street Journal/Times Higher Education ranked Brown 5th in the \"Best Colleges 2021\" edition.\nThe \"Forbes\" magazine annual ranking of \"America's Top Colleges 2019\"\u2014which ranked 650 research universities, liberal arts colleges and service academies\u2014ranked Brown 7th overall and 7th among universities.\n\"U.S. News &amp; World Report\" ranked Brown 14th among national universities in its 2021 edition. The 2021 edition also ranked Brown 1st for undergraduate teaching, 20th in Most Innovative Schools, and 18th in Best Value Schools.\n\"Washington Monthly\" ranked Brown 37th in 2020 among 389 national universities in the U.S. based on its contribution to the public good, as measured by social mobility, research, and promoting public service.\nFor 2020, \"U.S. News &amp; World Report\" ranks Brown 102nd globally.\nIn 2014, \"Forbes\" magazine ranked Brown 7th on its list of \"America's Most Entrepreneurial Universities\". The \"Forbes\" analysis looked at the ratio of \"alumni and students who have identified themselves as founders and business owners on LinkedIn\" and the total number of alumni and students.\nLinkedIn particularized the \"Forbes\" rankings, placing Brown third (between MIT and Princeton) among \"Best Undergraduate Universities for Software Developers at Startups.\" LinkedIn's methodology involved a career-path examination of \"millions of alumni profiles\" in its membership database.\nIn 2020, \"U.S. News\" ranked Brown's Warren Alpert Medical School the 9th most selective in the country, with an acceptance rate of 2.8 percent.\nAccording to 2020 data from the U.S. Department of Education, the median starting salary of Brown computer science graduates was the highest in the United States.\nResearch.\nBrown is member of the Association of American Universities since 1933 and is classified among \"R1: Doctoral Universities \u2013 Very High Research Activity\". In FY 2017, Brown spent $212.3 million on research and was ranked 103rd in the United States by total R&amp;D expenditure by National Science Foundation.\nStudent life.\nCampus safety.\nIn 2014, Brown tied with the University of Connecticut for the highest number of reported rapes in the nation, with its \"total of reports of rape\" on their main campus standing at 43.\nSpring weekend.\nEstablished in 1950, Spring Weekend is an annual spring music festival for students. Historical performers at the festival have included Ella Fitzgerald, Dizzy Gillespie, Ray Charles, Bob Dylan, Janis Joplin, Bruce Springsteen. More recent headliners include Kendrick Lamar, Young Thug, Daniel Caesar, Anderson .Paak, Mitski, and Mac DeMarco. Since 1960, Spring Weekend has been organized by the student\u2013run Brown Concert Agency.\nResidential and Greek societies.\nAbout 12 percent of Brown students are in fraternities and sororities. There are 11 residential Greek houses: six fraternities (Beta Rho Pi, Delta Phi, Delta Tau, Phi Kappa Psi, Sigma Chi, and Theta Delta Chi; four sororities (Alpha Chi Omega, Kappa Alpha Theta, Delta Gamma, and Kappa Delta), one co-ed house (Zeta Delta Xi), and one co-ed literary society (Alpha Delta Phi). Phi Sigma Kappa fraternity was present on campus from 1906 to 1939, but was unable to reactivate after World War II due to wartime losses. All recognized Greek-letter organizations are located on campus in Wriston Quadrangle in university-owned housing. They are overseen by the Greek Council.\nAn alternative to Greek-letter organizations are the program houses organized by themes. As with Greek houses, the residents of program houses select their new members, usually at the start of the spring semester. Examples of program houses are St. Anthony Hall (located in King House), Buxton International House, the Machado French/Hispanic/Latinx House, Technology House, Harambee (African culture) House, Social Action House and Interfaith House.\nCurrently, there are three student cooperative houses at Brown. Two of them, Watermyn and Finlandia on Waterman Street, are owned by the Brown Association for Cooperative Housing (BACH), a non-profit corporation owned by its members. The third co-op, West House, is located in a Brown-owned house on Brown Street. The three organizations run a vegetarian co-op for the larger community.\nAll students not in program housing enter a lottery for general housing. Students form groups and are assigned time slots during which they can pick among the remaining housing options.\nSocieties and clubs.\nThe earliest societies at Brown were devoted to oration and debate. The Pronouncing Society is mentioned in the diary of Solomon Drowne, class of 1773, who was voted its president in 1771. It seems to have disappeared during the American Revolutionary War. We next hear of the Misokosmian Society, founded in 1794 and renamed the Philermenian Society in 1798. This was effectively a secret society with membership limited to 45. It met fortnightly to hear speeches and debate and thrived until the Civil War; in 1821 its library held 1594 volumes. In 1799, a chapter of the Philandrian Society, also secret, was established at the college. In 1806, the United Brothers was formed as an egalitarian alternative to the Philermenian Society. \"These two great rivals,\" says the university historian, \"divided the student body between them for many years, surviving into the days of President Sears. A tincture of political controversy sharpened their rivalry, the older society inclining to the aristocratic Federals, the younger to the Republicans, the democrats of that day. ... The students continuing to increase in number, they outran the constitutional limits of both societies, and a third, the Franklin Society, was established in 1824; it never had the vitality of the other two, however, and died after ten years.\" Other nineteenth century clubs and societies, too numerous to treat here, are described in Bronson's history of the university.\nThe Cammarian Club\u2014founded in 1893 and taking its name from the Latin for lobster, its members' favorite dinner food\u2014was at first a semi-secret society which \"tapped\" 15 seniors each year. In 1915, self-perpetuating membership gave way to popular election by the student body, and thenceforward the Club served as the \"de facto\" undergraduate student government. In 1971, unaccountably, it voted the name Cammarian Club out of existence, thereby amputating its tradition and longevity. The successor and present-day organization is the generically-named Undergraduate Council of Students.\nSocietas Domi Pacificae, known colloquially as \"Pacifica House,\" is a present-day, self-described secret society, which nonetheless publishes a website and an email address. It claims a continuous line of descent from the Franklin Society of 1824, citing a supposed intermediary \"Franklin Society\" traceable in the nineteenth century. But the intermediary turns out to be, on closer inspection, the well-known Providence Franklin Society, a civic organization unconnected to Brown whose origins and activity are well-documented. It was founded in 1821 by merchants William Grinnell and Joseph Balch, Jr., and chartered by the General Assembly in January 1823. The \"Pacifica House\" account of this (conflated) Franklin Society cites published mentions of it in 1859, 1876, and 1883. But the first of these (Rhees 1859, see footnote \"infra\") is merely a sketch of the 1824 Brown organization; the second (Stockwell 1876) is a reference-book article on the Providence Franklin Society itself; and the third is the Providence Franklin Society's own publication, which the \"Pacifica House\" reference mis-ascribes to the \"Franklin Society,\" dropping the word \"Providence.\"\nStudent organizations.\nThere are over 300 registered student organizations on campus with diverse interests. The Student Activities Fair, during the orientation program, provides first-year students the opportunity to become acquainted with the wide range of organizations. A sample of organizations includes:\nResource centers.\nBrown has several resource centers on campus. The centers often act as sources of support as well as safe spaces for students to explore certain aspects of their identity. Additionally, the centers often provide physical spaces for students to study and have meetings. Although most centers are identity-focused, some provide academic support as well.\nThe Brown Center for Students of Color (BCSC) is a space that provides support for students of color. Established in 1972 at the demand of student protests, the BCSC encourages students to engage in critical dialogue, develop leadership skills, and promote social justice. The center houses various programs for students to share their knowledge and engage in discussion. Programs include the Third World Transition Program, the Minority Peer Counselor Program, the Heritage Series, and other student-led initiatives. Additionally, the BCSC hopes to foster community among the students it serves by providing spaces for students to meet and study.\nThe Sarah Doyle Women's Center aims to provide a space for members of the Brown community to examine and explore issues surrounding gender. The center was named after one of the first women to attend Brown, Sarah Doyle. The center emphasizes intersectionality in its conversations on gender, encouraging people to see gender as present and relevant in various aspects of life. The center hosts programs and workshops in order to facilitate dialogue and provide resources for students, faculty, and staff.\nOther centers include the LGBTQ+ Center, the Undocumented, First-Generation College and Low-Income Student (U-FLi) Center, and the Curricular Resource Center.\nActivism.\nThe 1968 Black Student Walkout.\nOn December 5 of 1968, several Black women from Pembroke College initiated a walkout in protest an atmosphere at the colleges described by Black students as a \u201cstifling, frustrating, [and] degrading place for Black students\u201d after feeling the colleges were non-responsive to their concerns. In total, 65 Black students participated in the walk out. Their principal demand was to increase Black student enrollment to 11% of the student populace, in an attempt to match that of the proportion in the US. This ultimately resulted in a 300% increase in Black enrollment the following year, but some demands have yet to be met.\nAthletics.\nBrown is a member of the Ivy League athletic conference, which is categorized as a Division I (top level) conference of the National Collegiate Athletic Association (NCAA). The Brown Bears has one of the largest university sports programs in the United States, sponsoring 32 varsity intercollegiate teams. Brown's athletic program is one of the \"U.S. News &amp; World Report\" top 20\u2014the \"College Sports Honor Roll\"\u2014based on breadth of program and athletes' graduation rates. Brown's newest varsity team is women's rugby, promoted from club-sport status in 2014.\nBrown women's rowing has won 7 national titles between 1999 and 2011. Brown men's rowing perennially finishes in the top 5 in the nation, most recently winning silver, bronze, and silver in the national championship races of 2012, 2013, and 2014. The men's and women's crews have also won championship trophies at the Henley Royal Regatta and the Henley Women's Regatta. Brown's men's soccer is consistently ranked in the top 20, and has won 18 Ivy League titles overall; recent soccer graduates play professionally in Major League Soccer and overseas. Brown football, under its most successful coach historically, Phil Estes, won Ivy League championships in 1999, 2005, and 2008. (Brown football's reemergence is credited to its 1976 Ivy League championship team, \"The Magnificent Andersons,\" so named for its coach, John Anderson.) High-profile alumni of the football program include Houston Texans head coach Bill O'Brien; former Penn State football coach Joe Paterno, Heisman Trophy namesake John W. Heisman, and Pollard Award namesake Fritz Pollard. The Men's Lacrosse team also has a long and storied history. Brown women's gymnastics won the Ivy League tournament in 2013 and 2014. Brown varsity equestrian has won the Ivy League championship several times. The Brown women's sailing team has won 5 national championships, most recently in 2019 while the coed sailing team won 2 national championships in 1942 and 1948. Both teams are consistency ranked in the top 10 in the nation.\nThe first intercollegiate ice hockey game in America was played between Brown and Harvard on January 19, 1898. The first university rowing regatta larger than a dual-meet was held between Brown, Harvard, and Yale at Lake Quinsigamond in Massachusetts on July 26, 1859.\nBrown also supports competitive intercollegiate club sports, including ultimate frisbee. The men's ultimate team, Brownian Motion, has won three national championships, in 2000, 2005 and 2019.\nNotable people.\nAlumni in politics include U.S. Secretary of State John Hay (1852), U.S. Secretary of State and Attorney General Richard Olney (1856), Chief Justice of the United States and U.S. Secretary of State Charles Evans Hughes (1881), Governor Bobby Jindal '92 of Louisiana, Senator Maggie Hassan '80 of New Hampshire, Governor Jack Markell '82 of Delaware, Rhode Island Representative David Cicilline '83, Minnesota Representative Dean Phillips '91, 2020 Presidential candidate and entrepreneur Andrew Yang '96, and DNC Chair Tom Perez '83.\nProminent alumni in business and finance include philanthropist John D. Rockefeller Jr. (1897), former Chair of the Federal Reserve and current Secretary of the Treasury Janet Yellen '67, World Bank President Jim Yong Kim '82, Bank of America CEO Brian Moynihan '81, CNN founder Ted Turner '60, IBM chairman and CEO Thomas Watson, Jr. '37, co-founder of Starwood Capital Group Barry Sternlicht '82, Apple Inc. CEO John Sculley '61, and Uber CEO Dara Khosrowshahi '91. Companies founded by Brown alumni include \"The Wall Street Journal,\" Searchlight Pictures, Netgear, W Hotels, Workday, Warby Parker, Casper, Figma, and Cards Against Humanity.\"\"\nAlumni in the arts and media include actors Emma Watson '14, Daveed Diggs '04, Julie Bowen '91, Tracee Ellis Ross '94, and Jessica Capshaw '98; NPR program host Ira Glass '82; singer-composer Mary Chapin Carpenter '81; humorist and Marx Brothers screenwriter S.J. Perelman '25; novelists Nathanael West '24, Jeffrey Eugenides '83, Edwidge Danticat (MFA '93), and Marilynne Robinson '66; composer and synthesizer pioneer Wendy Carlos '62; journalist James Risen '77; political pundit Mara Liasson; MSNBC host and The Nation editor-at-large Chris Hayes '01; \"New York Times, \"publisher A. G. Sulzberger '04, and magazine editor John F. Kennedy, Jr. '83.\nImportant figures in the history of education include the father of American public school education Horace Mann (1819), civil libertarian and Amherst College president Alexander Meiklejohn, first president of the University of South Carolina Jonathan Maxcy (1787), Bates College founder Oren B. Cheney (1836), University of Michigan president (1871\u20131909) James Burrill Angell (1849), University of California president (1899\u20131919) Benjamin Ide Wheeler (1875), and Morehouse College's first African-American president John Hope (1894).\nAlumni in the computer sciences and industry include architect of Intel 386, 486, and Pentium microprocessors John H. Crawford '75, inventor of the first silicon transistor Gordon Kidd Teal '31, MongoDB founder Eliot Horowitz '03, and Macintosh developer Andy Hertzfeld '75.\nOther notable alumni include \"Lafayette of the Greek Revolution\" and its historian Samuel Gridley Howe (1821) Governor of Wyoming Territory and Governor of Nebraska John Milton Thayer (1841), Governor of Rhode Island Augustus Bourn (1855), NASA head during first seven Apollo missions Thomas O. Paine '42, diplomat Richard Holbrooke '62, sportscaster Chris Berman '77, Houston Texans head coach Bill O'Brien '92, 2018 Miss America Cara Mund '16, Penn State football coach Joe Paterno '50, Heisman Trophy namesake John W. Heisman '91, \nOlympic and world champion triathlete Joanna Zeiger, royals and nobles such as Prince Rahim Aga Khan, Prince Faisal bin Al Hussein of the Hashemite Kingdom of Jordan, Princess Leila Pahlavi of Iran '92, Prince Nikolaos of Greece and Denmark, Prince Nikita Romanov, Princess Theodora of Greece and Denmark, Prince Jaime of Bourbon-Parma, Duke of San Jaime and Count of Bardi, Prince Ra'ad bin Zeid, Lady Gabriella Windsor, Prince Alexander von F\u00fcrstenberg, Countess Cosima von B\u00fclow Pavoncelli, and her half-brother Prince Alexander-Georg von Auersperg, David Shrier, American futurist and author, and Olympic gold ('98), silver ('02), and bronze ('06) medal-winning hockey player Katie King-Crowley '97.\nNobel Laureates include Craig Mello '82 and Jerry White '87, Cooley\u2013Tukey FFT algorithm co-originator John Wilder Tukey '36, biologist Stanley Falkow (PhD '59), and psychologist Aaron Beck '50.\nNotable past or current faculty have included Nobel Laureates Michael Kosterlitz, Lars Onsager, George Stigler, Vernon L. Smith, George Snell and Leon Cooper; Fields Medal winning mathematician David Mumford, Pulitzer Prize\u2013winning historian Gordon S. Wood, Sakurai Prize winning physicist Gerald Guralnik, computer scientist Andries van Dam, engineer Daniel C. Drucker, sociologist Lester Frank Ward, former Prime Minister of Italy and former EU chief Romano Prodi, former President of Brazil Fernando Cardoso, former President of Chile Ricardo Lagos, writers Carlos Fuentes, Chinua Achebe, and Robert Coover, philosopher Martha Nussbaum, developmental psychologist William Damon, linguist Hans Kurath, historian Ibram X. Kendi, political scientist James Morone, biologist Kenneth R. Miller, and Senior Fellow Sergei Khrushchev.\nIn popular culture.\nBrown's reputation as an institution with a free-spirited, iconoclastic student body is portrayed in fiction and popular culture. \"Family Guy\" character Brian Griffin is a Brown alumnus. \"The O.C.\"s main character Seth Cohen is denied acceptance to Brown while his girlfriend Summer Roberts is accepted. In \"The West Wing\", Amy Gardner is a Brown alumna. In \"Gossip Girl\", New York socialite Serena vies with her friends for a spot at Brown."}
{"id": "4158", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4158", "title": "Bill Atkinson", "text": "Bill Atkinson (born March 17, 1951) is an American computer engineer and photographer. Atkinson worked at Apple Computer from 1978 to 1990.\nAtkinson was the principal designer and developer of the graphical user interface (GUI) of the Apple Lisa and, later, one of the first thirty members of the original Apple Macintosh development team, and was the creator of the ground-breaking MacPaint application, which fulfilled the vision of using the computer as a creative tool. He also designed and implemented QuickDraw, the fundamental toolbox that the Lisa and Macintosh used for graphics. QuickDraw's performance was essential for the success of the Macintosh GUI. He also was one of the main designers of the Lisa and Macintosh user interfaces. Atkinson also conceived, designed and implemented HyperCard, the first popular hypermedia system. HyperCard put the power of computer programming and database design into the hands of nonprogrammers. In 1994, Atkinson received the EFF Pioneer Award for his contributions.\nEducation.\nHe received his undergraduate degree from the University of California, San Diego, where Apple Macintosh developer Jef Raskin was one of his professors. Atkinson continued his studies as a graduate student in neurochemistry at the University of Washington. Raskin invited Atkinson to visit him at Apple Computer; Steve Jobs persuaded him to join the company immediately as employee No. 51, and Atkinson never finished his PhD.\nCareer.\nAround 1990, General Magic's founding, with Bill Atkinson as one of the three cofounders, met the following press in \"Byte\" magazine:\nThe obstacles to General Magic's success may appear daunting, but General Magic is not your typical start-up company. Its partners include some of the biggest players in the worlds of computing, communications, and consumer electronics, and it's loaded with top-notch engineers who have been given a clean slate to reinvent traditional approaches to ubiquitous worldwide communications.\nIn 2007, Atkinson began working as an outside developer with Numenta, a startup working on computer intelligence. On his work there Atkinson said, \"what Numenta is doing is more fundamentally important to society than the personal computer and the rise of the Internet.\"\nCurrently, Atkinson has combined his passion for computer programming with his love of nature photography to create art images. He takes close-up photographs of stones that have been cut and polished. His works are highly regarded for their resemblance to miniature landscapes which are hidden within the stones. Atkinson's 2004 book \"Within the Stone\" features a collection of his close-up photographs. The highly intricate and detailed images he creates are made possible by the accuracy and creative control of the digital printing process that he helped create.\nSome of Atkinson's noteworthy contributions to the field of computing include:\nAtkinson now works as a nature photographer. Actor Nelson Franklin portrayed him in the 2013 film \"Jobs\"."}
{"id": "4160", "revid": "351274", "url": "https://en.wikipedia.org/wiki?curid=4160", "title": "Battle of Lostwithiel", "text": "The Battle of Lostwithiel took place over a 13-day period spanning 21 August \u2013 2 September near Lostwithiel and along the River Fowey valley in Cornwall during the First English Civil War in 1644. In the battle King Charles led the Royalists to a decisive victory over the Parliamentarians commanded by the Earl of Essex.\nThe battle was the worst defeat suffered by the Parliamentarians in the First English Civil War and secured South-West England for the Royalists until the end of the civil war.\nBackground.\nDuring April and May 1644, Parliamentarian commanders Sir William Waller and the Earl of Essex combined their armies and carried out a campaign against King Charles and the Royalist garrisons surrounding Oxford. Trusting Waller to deal with the King in Oxfordshire, Essex divided the Parliamentarian army on 6 June and headed southwest to relieve the Royalist siege of Lyme in Dorset. Lyme had been under siege by King Charles' nephew, Prince Maurice, and the Royalists for nearly two months.\nSouth-West England at that time was largely under the control of the Royalists. The town of Lyme, however, was a Parliamentarian stronghold and served as an important seaport for the Parliamentarian fleet of the Earl of Warwick. As Essex approached Lyme in mid-June Prince Maurice ended the siege and took his troops west to Exeter.\nEssex then proceeded further southwest toward Cornwall with the intent to relieve the siege of Plymouth. Plymouth was the only other significant Parliamentarian stronghold in the South-West and it was under siege by Richard Grenville and Cornish Royalists. Essex had been told by Lord Robartes, a wealthy politician and merchant from Cornwall, that the Parliamentarians would gain considerable military support if he moved against Grenville and freed Plymouth. Given Lord Robartes\u2019 advice, Essex advanced toward Plymouth. His action caused Grenville to end the siege. Essex then advanced further west believing that he could take full control of the South-West from the Royalists.\nMeanwhile, in Oxfordshire, King Charles battled with the Parliamentarians and defeated Sir William Waller at the Battle of Cropredy Bridge on 29 June. On 12 July after a Royalist council of war recommended that Essex be dealt with before he could be reinforced, King Charles and his Oxford army departed Evesham. King Charles accepted the council's advice, not solely because it was good strategy, but more so because his Queen was in Exeter where she had recently given birth to the Princess Henrietta and had been denied safe conduct to Bath by Essex.\nTrapped in Cornwall.\nOn 26 July, King Charles arrived in Exeter and joined his Oxford army with the Royalist forces commanded by Prince Maurice. On that same day, Essex and his Parliamentary force entered Cornwall. One week later, as Essex bivouacked with his army at Bodmin, he learned that King Charles had defeated Waller; brought his Oxford army to the South-West; and joined forces with Prince Maurice. Essex had also seen that he was not getting the military support from the people of Cornwall as Lord Robartes asserted. At that time, Essex understood that he and his army were trapped in Cornwall and his only salvation would be reinforcements or an escape through the port of Fowey by means of the Parliamentarian fleet.\nEssex immediately marched his troops eight kilometers south to the small town of Lostwithiel arriving on 2 August. He immediately deployed his men in a defensive arc with detachments on the high ground to the north at Restormel Castle and the high ground to the east at Beacon Hill. Essex also sent a small contingent of foot south to secure the port of Fowey aiming to eventually evacuate his infantry by sea. At Essex's disposal was a force of 6,500 foot and 3,000 horse.\nAided through intelligence provided by the people of Cornwell, King Charles followed westward, slowly and deliberately cutting off the potential escape routes that Essex might attempt to utilize. On 6 August King Charles communicated with Essex, calling for him to surrender. Stalling for several days, Essex considered the offer but ultimately refused.\nOn 11 August, Grenville and the Cornish Royalists entered Bodmin forcing out Essex's rear-guard cavalry. Grenville then proceeds south across Respryn Bridge to meet and join forces with King Charles and Prince Maurice. It is estimated that the Royalist forces at that time were composed of 12,000 foot and 7,000 horse. Over the next two days the Royalists deployed detachments along the east side of the River Fowey to prevent a Parliamentarian escape across country. Finally the Royalists sent 200 foot with artillery south to garrison the fort at Polruan, effectively blocking the entrance to the harbour of Fowey. At about that time, Essex learned that reinforcements under the command of Sir John Middleton were turned back by the Royalists at Bridgwater in Somerset.\nFirst battle - 21\u201330 August 1644.\nAt 07:00 hours on 21 August, King Charles launched his first attack on Essex and the Parliamentarians at Lostwithiel. From the north, Grenville and the Cornish Royalists attacked Restormel Castle and easily dislodged the Parliamentarians who fell back quickly. From the east, King Charles and the Oxford army captured Beacon Hill with little resistance from the Parliamentarians. Prince Maurice and his force occupied Druid Hill. Casualties were fairly low and by nightfall the fighting ended and the Royalists held the high ground on the north and east sides of Lostwithiel.\nFor the next couple of days the two opposing forces exchanged fire only in a number of small skirmishes. On 24 August, King Charles further tightened the noose encircling the Parliamentarians when he sent Lord Goring and Sir Thomas Bassett to secure the town of St Blazey and the area to the southwest of Lostwithiel. This reduced the foraging area for the Parliamentarians and access to the coves and inlets in the vicinity of the port of Par.\nEssex and the Parliamentarians were now totally surrounded and boxed into a three kilometer by eight kilometer area spanning from Lostwithiel in the north to the port of Fowey in the south. Knowing that he would not be able to fight his way out, Essex made his final plans for an escape. Since a sea evacuation of his cavalry would not be possible, Essex ordered his cavalry commander William Balfour to attempt a breakout to Plymouth. For the infantry, Essex planned to retreat south and meet Lord Warwick and the Parliamentarian fleet at Fowey. At 03:00 hours on 31 August, Balfour and 2,000 members of his cavalry executed the first step of Essex's plan when they successfully crossed the River Fowey and escaped intact without engaging the Royalist defenders.\nSecond battle - 31 August - 2 September 1644.\nEarly on the morning on 31 August, the Parliamentarians ransacked and looted Lostwithiel and began their withdrawal south. At 07:00 hours, the Royalists observed the actions of the Parliamentarians and immediately proceeded to attack. Grenville attacked from the north. King Charles and Prince Maurice crossed the River Fowey, joined up with Grenville, and entered Lostwithiel. Together the Royalists engaged the Parliamentarian rear-guards and quickly took possession of the town. The Royalist also sent detachments down along the east side of the River Fowey to protect against any further breakouts and to capture the town of Polruan.\nThe Royalists then began to pursue Essex and the Parliamentarian infantry down the river valley. At the outset the Royalist pushed the Parliamentarians about four kilometers south through the hedged fields, hills and valleys. At the narrow pass near St. Veep, Philip Skippon, Essex's commander of the infantry, counter-attacked the Royalists and pushed them back several fields attempting to give Essex time to set up a line of defense further south. At 11:00 hours, the Royalist cavalry mounted a charge and won back the territory lost. There was a lull in the battle at 12:00 hours as King Charles waited for his full army to come up and reform.\nThe fighting resumed and continued through the afternoon as the Parliamentarians tried to disengage and continue south. At 16:00 hours, the Parliamentarians tried again to counter-attack with their remaining cavalry only to be driven back by King Charles\u2019 Life Guard. About a kilometer north of Castle Dore, the Parliamentarians right flank began to give way. At 18:00 hours when the Parliamentarians were pushed back to Castle Dore they made their last attempt to rally only to be pushed back and surrounded.\nAbout that time the fighting ended with the Royalists satisfied in their accomplishments of the day. Exhausted and discouraged, the Parliamentarians hunkered down for the night. Later that evening under the darkness of night, Essex and his command staff stole away to the seashore where they used a fishing boat to flee to Plymouth, leaving Skippon in command.\nEarly on 1 September, Skippon met with his officers to inform them about Essex's escape and to discuss alternatives. It was decided that they would approach King Charles and seek terms. Concerned that Parliamentarian reinforcements might be on their way, the King quickly agreed on 2 September to generous terms. The battle was over. Six thousand Parliamentarians were taken as prisoners. Their weapons were taken away and they were marched to Southampton. They suffered the wrath of the Cornish people in route and as many as 3,000 died of exposure and disease along the way. Those that survived the journey were, however, eventually set free. Total casualties associated with the battle were extremely high especially when considering those who died on the march back to Southampton. To those numbers as many as 700 Parliamentarians are estimated to have been killed or wounded during the fighting in Cornwall along with an estimated 500 Royalists.\nAftermath.\nThe Battle of Lostwithiel was a great victory for King Charles and the greatest loss that the Parliamentarians would suffer in the First English Civil War. For King Charles the victory secured the South-West for the remainder of the war and mitigated criticism for a while against the Royalist war effort.\nFor the Parliamentarians, the defeat resulted in recriminations with Middleton ultimately being blamed for his failure to break-through with reinforcements. The Parliamentarian failure at Lostwithiel along with the failure to defeat King Charles at the Second Battle of Newbury ultimately led Parliament to adopt the Self-denying Ordinance and led to the implementation of the New Model Army."}
{"id": "4162", "revid": "27335766", "url": "https://en.wikipedia.org/wiki?curid=4162", "title": "Beeb", "text": "Beeb or BEEB may refer to:"}
{"id": "4163", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=4163", "title": "Bertrand Russell", "text": "Bertrand Arthur William Russell, 3rd Earl Russell (18 May 1872 \u2013 2 February 1970) was a British polymath, philosopher, logician, mathematician, historian, writer, social critic, political activist, and Nobel laureate. Throughout his life, Russell considered himself a liberal, a socialist and a pacifist, although he sometimes suggested that his sceptical nature had led him to feel that he had \"never been any of these things, in any profound sense\". Russell was born in Monmouthshire into one of the most prominent aristocratic families in the United Kingdom.\nIn the early 20th century, Russell led the British \"revolt against idealism\". He is considered one of the founders of analytic philosophy along with his predecessor Gottlob Frege, colleague G.\u00a0E. Moore and prot\u00e9g\u00e9 Ludwig Wittgenstein. He is widely held to be one of the 20th century's premier logicians. With A. N. Whitehead he wrote \"Principia Mathematica\", an attempt to create a logical basis for mathematics, the quintessential work of classical logic. His philosophical essay \"On Denoting\" has been considered a \"paradigm of philosophy\". His work has had a considerable influence on mathematics, logic, set theory, linguistics, artificial intelligence, cognitive science, computer science (see type theory and type system) and philosophy, especially the philosophy of language, epistemology and metaphysics.\nRussell was a prominent anti-war activist, championed anti-imperialism, and chaired the India League. Occasionally, he advocated preventive nuclear war, before the opportunity provided by the atomic monopoly had passed and he decided he would \"welcome with enthusiasm\" world government. He went to prison for his pacifism during World War I. Later, Russell concluded that the war against Adolf Hitler's Nazi Germany was a necessary \"lesser of two evils\" and also criticised Stalinist totalitarianism, condemned the involvement of the United States in the Vietnam War and was an outspoken proponent of nuclear disarmament. In 1950, Russell was awarded the Nobel Prize in Literature \"in recognition of his varied and significant writings in which he champions humanitarian ideals and freedom of thought\".\nBiography.\nEarly life and background.\nBertrand Arthur William Russell was born on 18 May 1872 at Ravenscroft, Trellech, Monmouthshire, Wales, into an influential and liberal family of the British aristocracy. His parents, Viscount and Viscountess Amberley, were radical for their times. Lord Amberley consented to his wife's affair with their children's tutor, the biologist Douglas Spalding. Both were early advocates of birth control at a time when this was considered scandalous. Lord Amberley was an atheist and his atheism was evident when he asked the philosopher John Stuart Mill to act as Russell's secular godfather. Mill died the year after Russell's birth, but his writings had a great effect on Russell's life.\nHis paternal grandfather, the Earl Russell, had twice been Prime Minister in the 1840s and 1860s. The Russells had been prominent in England for several centuries before this, coming to power and the peerage with the rise of the Tudor dynasty (see: Duke of Bedford). They established themselves as one of the leading British Whig families and participated in every great political event from the Dissolution of the Monasteries in 1536\u20131540 to the Glorious Revolution in 1688\u20131689 and the Great Reform Act in 1832.\nLady Amberley was the daughter of Lord and Lady Stanley of Alderley. Russell often feared the ridicule of his maternal grandmother, one of the campaigners for education of women.\nChildhood and adolescence.\nRussell had two siblings: brother Frank (nearly seven years older than Bertrand), and sister Rachel (four years older). In June 1874 Russell's mother died of diphtheria, followed shortly by Rachel's death. In January 1876, his father died of bronchitis following a long period of depression. Frank and Bertrand were placed in the care of their staunchly Victorian paternal grandparents, who lived at Pembroke Lodge in Richmond Park. His grandfather, former Prime Minister Earl Russell, died in 1878, and was remembered by Russell as a kindly old man in a wheelchair. His grandmother, the Countess Russell (n\u00e9e Lady Frances Elliot), was the dominant family figure for the rest of Russell's childhood and youth.\nThe countess was from a Scottish Presbyterian family, and successfully petitioned the Court of Chancery to set aside a provision in Amberley's will requiring the children to be raised as agnostics. Despite her religious conservatism, she held progressive views in other areas (accepting Darwinism and supporting Irish Home Rule), and her influence on Bertrand Russell's outlook on social justice and standing up for principle remained with him throughout his life. Her favourite Bible verse, \"Thou shalt not follow a multitude to do evil\" (), became his motto. The atmosphere at Pembroke Lodge was one of frequent prayer, emotional repression, and formality; Frank reacted to this with open rebellion, but the young Bertrand learned to hide his feelings.\nRussell's adolescence was very lonely, and he often contemplated suicide. He remarked in his autobiography that his keenest interests were in \"nature and books and (later) mathematics saved me from complete despondency;\" only his wish to know more mathematics kept him from suicide. He was educated at home by a series of tutors. When Russell was eleven years old, his brother Frank introduced him to the work of Euclid, which he described in his autobiography as \"one of the great events of my life, as dazzling as first love.\"\nDuring these formative years he also discovered the works of Percy Bysshe Shelley. Russell wrote: \"I spent all my spare time reading him, and learning him by heart, knowing no one to whom I could speak of what I thought or felt, I used to reflect how wonderful it would have been to know Shelley, and to wonder whether I should meet any live human being with whom I should feel so much sympathy.\" Russell claimed that beginning at age 15, he spent considerable time thinking about the validity of Christian religious dogma, which he found very unconvincing. At this age, he came to the conclusion that there is no free will and, two years later, that there is no life after death. Finally, at the age of 18, after reading Mill's \"Autobiography\", he abandoned the \"First Cause\" argument and became an atheist.\nHe travelled to the continent in 1890 with an American friend, Edward FitzGerald, and with FitzGerald's family he visited the Paris Exhibition of 1889 and was able to climb the Eiffel Tower soon after it was completed.\nUniversity and first marriage.\nRussell won a scholarship to read for the Mathematical Tripos at Trinity College, Cambridge, and commenced his studies there in 1890, taking as coach Robert Rumsey Webb. He became acquainted with the younger George Edward Moore and came under the influence of Alfred North Whitehead, who recommended him to the Cambridge Apostles. He quickly distinguished himself in mathematics and philosophy, graduating as seventh Wrangler in the former in 1893 and becoming a Fellow in the latter in 1895.\nRussell was 17\u00a0years old in the summer of 1889 when he met the family of Alys Pearsall Smith, an American Quaker five years older, who was a graduate of Bryn Mawr College near Philadelphia. He became a friend of the Pearsall Smith family\u2014they knew him primarily as \"Lord John's grandson\" and enjoyed showing him off.\nHe soon fell in love with the puritanical, high-minded Alys, and, contrary to his grandmother's wishes, married her on 13 December 1894. Their marriage began to fall apart in 1901 when it occurred to Russell, while he was cycling, that he no longer loved her. She asked him if he loved her and he replied that he did not. Russell also disliked Alys's mother, finding her controlling and cruel. It was to be a hollow shell of a marriage. A lengthy period of separation began in 1911 with Russell's affair with Lady Ottoline Morrell, and he and Alys finally divorced in 1921 to enable Russell to remarry.\nDuring his years of separation from Alys, Russell had passionate (and often simultaneous) affairs with a number of women, including Morrell and the actress Lady Constance Malleson. Some have suggested that at this point he had an affair with Vivienne Haigh-Wood, the English governess and writer, and first wife of T. S. Eliot.\nEarly career.\nRussell began his published work in 1896 with \"German Social Democracy\", a study in politics that was an early indication of a lifelong interest in political and social theory. In 1896 he taught German social democracy at the London School of Economics. He was a member of the Coefficients dining club of social reformers set up in 1902 by the Fabian campaigners Sidney and Beatrice Webb.\nHe now started an intensive study of the foundations of mathematics at Trinity. In 1897, he wrote \"An Essay on the Foundations of Geometry\" (submitted at the Fellowship Examination of Trinity College) which discussed the Cayley\u2013Klein metrics used for non-Euclidean geometry. He attended the First International Congress of Philosophy in Paris in 1900 where he met Giuseppe Peano and Alessandro Padoa. The Italians had responded to Georg Cantor, making a science of set theory; they gave Russell their literature including the \"Formulario mathematico\". Russell was impressed by the precision of Peano's arguments at the Congress, read the literature upon returning to England, and came upon Russell's paradox. In 1903 he published \"The Principles of Mathematics\", a work on foundations of mathematics. It advanced a thesis of logicism, that mathematics and logic are one and the same.\nAt the age of 29, in February 1901, Russell underwent what he called a \"sort of mystic illumination\", after witnessing Whitehead's wife's acute suffering in an angina attack. \"I found myself filled with semi-mystical feelings about beauty ... and with a desire almost as profound as that of the Buddha to find some philosophy which should make human life endurable\", Russell would later recall. \"At the end of those five minutes, I had become a completely different person.\"\nIn 1905, he wrote the essay \"On Denoting\", which was published in the philosophical journal \"Mind\". Russell was elected a Fellow of the Royal Society (FRS) in 1908. The three-volume \"Principia Mathematica\", written with Whitehead, was published between 1910 and 1913. This, along with the earlier \"The Principles of Mathematics\", soon made Russell world-famous in his field.\nIn 1910, he became a University of Cambridge lecturer at Trinity College, where he had studied. He was considered for a Fellowship, which would give him a vote in the college government and protect him from being fired for his opinions, but was passed over because he was \"anti-clerical\", essentially because he was agnostic. He was approached by the Austrian engineering student Ludwig Wittgenstein, who became his PhD student. Russell viewed Wittgenstein as a genius and a successor who would continue his work on logic. He spent hours dealing with Wittgenstein's various phobias and his frequent bouts of despair. This was often a drain on Russell's energy, but Russell continued to be fascinated by him and encouraged his academic development, including the publication of Wittgenstein's \"Tractatus Logico-Philosophicus\" in 1922. Russell delivered his lectures on logical atomism, his version of these ideas, in 1918, before the end of World War I. Wittgenstein was, at that time, serving in the Austrian Army and subsequently spent nine months in an Italian prisoner of war camp at the end of the conflict.\nFirst World War.\nDuring World War I, Russell was one of the few people to engage in active pacifist activities. In 1916, because of his lack of a Fellowship, he was dismissed from Trinity College following his conviction under the Defence of the Realm Act 1914. He later described this as an illegitimate means the state used to violate freedom of expression, in Free Thought and Official Propaganda. Russell championed the case of Eric Chappelow, a poet jailed and abused as a conscientious objector. Russell played a significant part in the \"Leeds Convention\" in June 1917, a historic event which saw well over a thousand \"anti-war socialists\" gather; many being delegates from the Independent Labour Party and the Socialist Party, united in their pacifist beliefs and advocating a peace settlement. The international press reported that Russell appeared with a number of Labour MPs, including Ramsay MacDonald and Philip Snowden, as well as former Liberal MP and anti-conscription campaigner, Professor Arnold Lupton. After the event, Russell told Lady Ottoline Morrell that, \"to my surprise, when I got up to speak, I was given the greatest ovation that was possible to give anybody\".\nHis conviction in 1916 resulted in Russell being fined \u00a3100 (), which he refused to pay in hope that he would be sent to prison, but his books were sold at auction to raise the money. The books were bought by friends; he later treasured his copy of the King James Bible that was stamped \"Confiscated by Cambridge Police\".\nA later conviction for publicly lecturing against inviting the United States to enter the war on the United Kingdom's side resulted in six months' imprisonment in Brixton Prison (see \"Bertrand Russell's political views\") in 1918. He later said of his imprisonment:\nWhile he was reading Strachey's \"Eminent Victorians\" chapter about Gordon he laughed out loud in his cell prompting the warden to intervene and reminding him that \"prison was a place of punishment\".\nRussell was reinstated to Trinity in 1919, resigned in 1920, was Tarner Lecturer 1926 and became a Fellow again in 1944 until 1949.\nIn 1924, Russell again gained press attention when attending a \"banquet\" in the House of Commons with well-known campaigners, including Arnold Lupton, who had been a Member of Parliament and had also endured imprisonment for \"passive resistance to military or naval service\".\nG. H. Hardy on the Trinity controversy.\nIn 1941, G. H. Hardy wrote a 61-page pamphlet titled \"Bertrand Russell and Trinity\"\u2014published later as a book by Cambridge University Press with a foreword by C. D. Broad\u2014in which he gave an authoritative account about Russell's 1916 dismissal from Trinity College, explaining that a reconciliation between the college and Russell had later taken place and gave details about Russell's personal life. Hardy writes that Russell's dismissal had created a scandal since the vast majority of the Fellows of the College opposed the decision. The ensuing pressure from the Fellows induced the Council to reinstate Russell. In January 1920, it was announced that Russell had accepted the reinstatement offer from Trinity and would begin lecturing from October. In July 1920, Russell applied for a one year leave of absence; this was approved. He spent the year giving lectures in China and Japan. In January 1921, it was announced by Trinity that Russell had resigned and his resignation had been accepted. This resignation, Hardy explains, was completely voluntary and was not the result of another altercation.\nThe reason for the resignation, according to Hardy, was that Russell was going through a tumultuous time in his personal life with a divorce and subsequent remarriage. Russell contemplated asking Trinity for another one-year leave of absence but decided against it, since this would have been an \"unusual application\" and the situation had the potential to snowball into another controversy. Although Russell did the right thing, in Hardy's opinion, the reputation of the College suffered due to Russell's resignation since the 'world of learning' knew about Russell's altercation with Trinity but not that the rift had healed. In 1925, Russell was asked by the Council of Trinity College to give the \"Tarner Lectures\" on the Philosophy of the Sciences; these would later be the basis for one of Russell's best-received books according to Hardy: \"The Analysis of Matter\", published in 1927. In the preface to the Trinity pamphlet, Hardy wrote:\nBetween the wars.\nIn August 1920, Russell travelled to Soviet Russia as part of an official delegation sent by the British government to investigate the effects of the Russian Revolution. He wrote a four-part series of articles, titled \"Soviet Russia1920\", for the US magazine \"The Nation\". He met Vladimir Lenin and had an hour-long conversation with him. In his autobiography, he mentions that he found Lenin disappointing, sensing an \"impish cruelty\" in him and comparing him to \"an opinionated professor\". He cruised down the Volga on a steamship. His experiences destroyed his previous tentative support for the revolution. He subsequently wrote a book, \"The Practice and Theory of Bolshevism\", about his experiences on this trip, taken with a group of 24 others from the UK, all of whom came home thinking well of the Soviet regime, despite Russell's attempts to change their minds. For example, he told them that he had heard shots fired in the middle of the night and was sure that these were clandestine executions, but the others maintained that it was only cars backfiring.\nRussell's lover Dora Black, a British author, feminist and socialist campaigner, visited Soviet Russia independently at the same time; in contrast to his reaction, she was enthusiastic about the Bolshevik revolution.\nThe following autumn, Russell, accompanied by Dora, visited Peking (as it was then known in the West) to lecture on philosophy for a year. He went with optimism and hope, seeing China as then being on a new path. Other scholars present in China at the time included John Dewey and Rabindranath Tagore, the Indian Nobel-laureate poet. Before leaving China, Russell became gravely ill with pneumonia, and incorrect reports of his death were published in the Japanese press. When the couple visited Japan on their return journey, Dora took on the role of spurning the local press by handing out notices reading \"Mr. Bertrand Russell, having died according to the Japanese press, is unable to give interviews to Japanese journalists\". Apparently they found this harsh and reacted resentfully.\nDora was six months pregnant when the couple returned to England on 26 August 1921. Russell arranged a hasty divorce from Alys, marrying Dora six days after the divorce was finalised, on 27 September 1921. Russell's children with Dora were John Conrad Russell, 4th Earl Russell, born on 16 November 1921, and Katharine Jane Russell (now Lady Katharine Tait), born on 29 December 1923. Russell supported his family during this time by writing popular books explaining matters of physics, ethics, and education to the layman.\nFrom 1922 to 1927 the Russells divided their time between London and Cornwall, spending summers in Porthcurno. In the 1922 and 1923 general elections Russell stood as a Labour Party candidate in the Chelsea constituency, but only on the basis that he knew he was extremely unlikely to be elected in such a safe Conservative seat, and he was unsuccessful on both occasions.\nOwing to the birth of his two children, he became interested in education, especially early childhood education. He was not satisfied with the old traditional education and thought that progressive education also had some flaws, as a result, together with Dora, Russell founded the experimental Beacon Hill School in 1927. The school was run from a succession of different locations, including its original premises at the Russells' residence, Telegraph House, near Harting, West Sussex. During this time, he published On Education, Especially in Early Childhood. On 8 July 1930 Dora gave birth to her third child Harriet Ruth. After he left the school in 1932, Dora continued it until 1943.\nOn a tour through the US in 1927, Russell met Barry Fox (later Barry Stevens), who became a well-known Gestalt therapist and writer in later years. Russell and Fox developed an intensive relationship. In Fox's words: \"...for three years we were very close.\" Fox sent her daughter Judith to Beacon Hill School for some time. From 1927 to 1932 Russell wrote 34 letters to Fox.\nUpon the death of his elder brother Frank, in 1931, Russell became the 3rd Earl Russell.\nRussell's marriage to Dora grew increasingly tenuous, and it reached a breaking point over her having two children with an American journalist, Griffin Barry. They separated in 1932 and finally divorced. On 18 January 1936, Russell married his third wife, an Oxford undergraduate named Patricia (\"Peter\") Spence, who had been his children's governess since 1930. Russell and Peter had one son, Conrad Sebastian Robert Russell, 5th Earl Russell, who became a prominent historian and one of the leading figures in the Liberal Democrat party.\nRussell returned to the London School of Economics to lecture on the science of power in 1937.\nDuring the 1930s, Russell became a close friend and collaborator of V. K. Krishna Menon, then President of the India League, the foremost lobby in the United Kingdom for Indian self-rule. Russel was Chair of the India League from 1932-1939.\nSecond World War.\nRussell's political views changed over time, mostly about war. He opposed rearmament against Nazi Germany. In 1937, he wrote in a personal letter: \"If the Germans succeed in sending an invading army to England we should do best to treat them as visitors, give them quarters and invite the commander and chief to dine with the prime minister.\" In 1940, he changed his appeasement view that avoiding a full-scale world war was more important than defeating Hitler. He concluded that Adolf Hitler taking over all of Europe would be a permanent threat to democracy. In 1943, he adopted a stance toward large-scale warfare called \"relative political pacifism\": \"War was always a great evil, but in some particularly extreme circumstances, it may be the lesser of two evils.\"\nBefore World War II, Russell taught at the University of Chicago, later moving on to Los Angeles to lecture at the UCLA Department of Philosophy. He was appointed professor at the City College of New York (CCNY) in 1940, but after a public outcry the appointment was annulled by a court judgment that pronounced him \"morally unfit\" to teach at the college due to his opinions, especially those relating to sexual morality, detailed in \"Marriage and Morals\" (1929). The matter was however taken to the New York Supreme Court by Jean Kay who was afraid that her daughter would be harmed by the appointment, though her daughter was not a student at CCNY. Many intellectuals, led by John Dewey, protested at his treatment. Albert Einstein's oft-quoted aphorism that \"great spirits have always encountered violent opposition from mediocre minds\" originated in his open letter, dated 19 March 1940, to Morris Raphael Cohen, a professor emeritus at CCNY, supporting Russell's appointment. Dewey and Horace M. Kallen edited a collection of articles on the CCNY affair in \"The Bertrand Russell Case\". Russell soon joined the Barnes Foundation, lecturing to a varied audience on the history of philosophy; these lectures formed the basis of \"A History of Western Philosophy\". His relationship with the eccentric Albert C. Barnes soon soured, and he returned to the UK in 1944 to rejoin the faculty of Trinity College.\nLater life.\nRussell participated in many broadcasts over the BBC, particularly \"The Brains Trust\" and the Third Programme, on various topical and philosophical subjects. By this time Russell was world-famous outside academic circles, frequently the subject or author of magazine and newspaper articles, and was called upon to offer opinions on a wide variety of subjects, even mundane ones. En route to one of his lectures in Trondheim, Russell was one of 24 survivors (among a total of 43 passengers) of an aeroplane crash in Hommelvik in October 1948. He said he owed his life to smoking since the people who drowned were in the non-smoking part of the plane. \"A History of Western Philosophy\" (1945) became a best-seller and provided Russell with a steady income for the remainder of his life.\nIn 1942, Russell argued in favour of a moderate socialism, capable of overcoming its metaphysical principles, in an inquiry on dialectical materialism, launched by the Austrian artist and philosopher Wolfgang Paalen in his journal \"DYN\", saying \"I think the metaphysics of both Hegel and Marx plain nonsense\u2014Marx's claim to be 'science' is no more justified than Mary Baker Eddy's. This does not mean that I am opposed to socialism.\"\nIn 1943, Russell expressed support for Zionism: \"I have come gradually to see that, in a dangerous and largely hostile world, it is essential to Jews to have some country which is theirs, some region where they are not suspected aliens, some state which embodies what is distinctive in their culture\".\nIn a speech in 1948, Russell said that if the USSR's aggression continued, it would be morally worse to go to war after the USSR possessed an atomic bomb than before it possessed one, because if the USSR had no bomb the West's victory would come more swiftly and with fewer casualties than if there were atom bombs on both sides. At that time, only the United States possessed an atomic bomb, and the USSR was pursuing an extremely aggressive policy towards the countries in Eastern Europe which were being absorbed into the Soviet Union's sphere of influence. Many understood Russell's comments to mean that Russell approved of a first strike in a war with the USSR, including Nigel Lawson, who was present when Russell spoke of such matters. Others, including Griffin, who obtained a transcript of the speech, have argued that he was merely explaining the usefulness of America's atomic arsenal in deterring the USSR from continuing its domination of Eastern Europe.\nHowever, just after the atomic bombs exploded over Hiroshima and Nagasaki, Russell wrote letters, and published articles in newspapers from 1945 to 1948, stating clearly that it was morally justified and better to go to war against the USSR using atomic bombs while the United States possessed them and before the USSR did. In September 1949, one week after the USSR tested its first A-bomb, but before this became known, Russell wrote that USSR would be unable to develop nuclear weapons because following Stalin's purges only science based on Marxist principles would be practised in the Soviet Union. After it became known that the USSR carried out its nuclear bomb tests, Russell declared his position advocating for the total abolition of atomic weapons.\nIn 1948, Russell was invited by the BBC to deliver the inaugural Reith Lectures\u2014what was to become an annual series of lectures, still broadcast by the BBC. His series of six broadcasts, titled \"Authority and the Individual\", explored themes such as the role of individual initiative in the development of a community and the role of state control in a progressive society. Russell continued to write about philosophy. He wrote a foreword to \"Words and Things\" by Ernest Gellner, which was highly critical of the later thought of Ludwig Wittgenstein and of ordinary language philosophy. Gilbert Ryle refused to have the book reviewed in the philosophical journal \"Mind\", which caused Russell to respond via \"The Times\". The result was a month-long correspondence in \"The Times\" between the supporters and detractors of ordinary language philosophy, which was only ended when the paper published an editorial critical of both sides but agreeing with the opponents of ordinary language philosophy.\nIn the King's Birthday Honours of 9 June 1949, Russell was awarded the Order of Merit, and the following year he was awarded the Nobel Prize in Literature. When he was given the Order of Merit, George VI was affable but slightly embarrassed at decorating a former jailbird, saying, \"You have sometimes behaved in a manner that would not do if generally adopted\". Russell merely smiled, but afterwards claimed that the reply \"That's right, just like your brother\" immediately came to mind.\nIn 1950, Russell attended the inaugural conference for the Congress for Cultural Freedom, a CIA-funded anti-communist organisation committed to the deployment of culture as a weapon during the Cold War. Russell was one of the best-known patrons of the Congress, until he resigned in 1956.\nIn 1952, Russell was divorced by Spence, with whom he had been very unhappy. Conrad, Russell's son by Spence, did not see his father between the time of the divorce and 1968 (at which time his decision to meet his father caused a permanent breach with his mother). Russell married his fourth wife, Edith Finch, soon after the divorce, on 15 December 1952. They had known each other since 1925, and Edith had taught English at Bryn Mawr College near Philadelphia, sharing a house for 20 years with Russell's old friend Lucy Donnelly. Edith remained with him until his death, and, by all accounts, their marriage was a happy, close, and loving one. Russell's eldest son John suffered from serious mental illness, which was the source of ongoing disputes between Russell and his former wife Dora.\nIn September 1961, at the age of 89, Russell was jailed for seven days in Brixton Prison for \"breach of peace\" after taking part in an anti-nuclear demonstration in London. The magistrate offered to exempt him from jail if he pledged himself to \"good behaviour\", to which Russell replied: \"No, I won't.\"\nIn 1962 Russell played a public role in the Cuban Missile Crisis: in an exchange of telegrams with Soviet leader Nikita Khrushchev, Khrushchev assured him that the Soviet government would not be reckless. Russell sent this telegram to President Kennedy:\nYOUR ACTION DESPERATE. THREAT TO HUMAN SURVIVAL. NO CONCEIVABLE JUSTIFICATION. CIVILIZED MAN CONDEMNS IT. WE WILL NOT HAVE MASS MURDER. ULTIMATUM MEANS WAR... END THIS MADNESS.\nAccording to historian Peter Knight, after JFK's assassination, Russell, \"prompted by the emerging work of the lawyer Mark Lane in the US ... rallied support from other noteworthy and left-leaning compatriots to form a Who Killed Kennedy Committee in June 1964, members of which included Michael Foot MP, Caroline Benn, the publisher Victor Gollancz, the writers John Arden and J. B. Priestley, and the Oxford history professor Hugh Trevor-Roper.\" Russell published a highly critical article weeks before the Warren Commission Report was published, setting forth \"16 Questions on the Assassination\" and equating the Oswald case with the Dreyfus affair of late 19th-century France, in which the state wrongly convicted an innocent man. Russell also criticised the American press for failing to heed any voices critical of the official version.\nPolitical causes.\nBertrand Russell was opposed to war from early on, his opposition to World War I being used as grounds for his dismissal from Trinity College at Cambridge. This incident fused two of his most controversial causes, as he had failed to be granted Fellow status, which would have protected him from firing, because he was not willing to either pretend to be a devout Christian, or at least avoid admitting he was agnostic.\nHe later described the resolution of these issues as essential to freedom of thought and expression, citing the incident in Free Thought and Official Propaganda, where he explained that the expression of any idea, even the most obviously \"bad\", must be protected not only from direct State intervention, but also economic leveraging and other means of being silenced:\nRussell spent the 1950s and 1960s engaged in political causes primarily related to nuclear disarmament and opposing the Vietnam War. The 1955 Russell\u2013Einstein Manifesto was a document calling for nuclear disarmament and was signed by eleven of the most prominent nuclear physicists and intellectuals of the time. In 1966\u20131967, Russell worked with Jean-Paul Sartre and many other intellectual figures to form the Russell Vietnam War Crimes Tribunal to investigate the conduct of the United States in Vietnam. He wrote a great many letters to world leaders during this period.\nHe was an advocate of population control:The nations which at present increase rapidly should be encouraged to adopt the methods by which, in the West, the increase of population has been checked. Educational propaganda, with government help, could achieve this result in a generation. There are, however, two powerful forces opposed to such a policy: one is religion, the other is nationalism. I think it is the duty of all to proclaim that opposition to the spread of birth is appalling depth of misery and degradation, and that within another fifty years or so. I do not pretend that birth control is the only way in which population can be kept from increasing. There are others, which, one must suppose, opponents of birth control would prefer. War, as I remarked a moment ago, has hitherto been disappointing i this respect, but perhaps bacteriological war may prove more effective. If a Black Death could be spread throughout the whole world once in every generation survivors could procreate freely without making the world too full.In 1956, immediately before and during the Suez Crisis, Russell expressed his opposition to European imperialism in the Middle East. He viewed the crisis as another reminder of the pressing need for a more effective mechanism for international governance, and to restrict national sovereignty to places such as the Suez Canal area \"where general interest is involved\". At the same time the Suez Crisis was taking place, the world was also captivated by the Hungarian Revolution and the subsequent crushing of the revolt by intervening Soviet forces. Russell attracted criticism for speaking out fervently against the Suez war while ignoring Soviet repression in Hungary, to which he responded that he did not criticise the Soviets \"because there was no need. Most of the so-called Western World was fulminating\". Although he later feigned a lack of concern, at the time he was disgusted by the brutal Soviet response, and on 16 November 1956, he expressed approval for a declaration of support for Hungarian scholars which Michael Polanyi had cabled to the Soviet embassy in London twelve days previously, shortly after Soviet troops had already entered Budapest.\nIn November 1957 Russell wrote an article addressing US President Dwight D. Eisenhower and Soviet Premier Nikita Khrushchev, urging a summit to consider \"the conditions of co-existence\". Khrushchev responded that peace could indeed be served by such a meeting. In January 1958 Russell elaborated his views in \"The Observer\", proposing a cessation of all nuclear-weapons production, with the UK taking the first step by unilaterally suspending its own nuclear-weapons program if necessary, and with Germany \"freed from all alien armed forces and pledged to neutrality in any conflict between East and West\". US Secretary of State John Foster Dulles replied for Eisenhower. The exchange of letters was published as \"The Vital Letters of Russell, Khrushchev, and Dulles\".\nRussell was asked by \"The New Republic\", a liberal American magazine, to elaborate his views on world peace. He urged that all nuclear-weapons testing and constant flights by planes armed with nuclear weapons be halted immediately, and negotiations be opened for the destruction of all hydrogen bombs, with the number of conventional nuclear devices limited to ensure a balance of power. He proposed that Germany be reunified and accept the Oder-Neisse line as its border, and that a neutral zone be established in Central Europe, consisting at the minimum of Germany, Poland, Hungary, and Czechoslovakia, with each of these countries being free of foreign troops and influence, and prohibited from forming alliances with countries outside the zone. In the Middle East, Russell suggested that the West avoid opposing Arab nationalism, and proposed the creation of a United Nations peacekeeping force to guard Israel's frontiers to ensure that Israel was prevented from committing aggression and protected from it. He also suggested Western recognition of the People's Republic of China, and that it be admitted to the UN with a permanent seat on the UN Security Council.\nHe was in contact with Lionel Rogosin while the latter was filming his anti-war film \"Good Times, Wonderful Times\" in the 1960s. He became a hero to many of the youthful members of the New Left. In early 1963, in particular, Russell became increasingly vocal in his disapproval of the Vietnam War, and felt that the US government's policies there were near-genocidal. In 1963 he became the inaugural recipient of the Jerusalem Prize, an award for writers concerned with the freedom of the individual in society. In 1964 he was one of eleven world figures who issued an appeal to Israel and the Arab countries to accept an arms embargo and international supervision of nuclear plants and rocket weaponry. In October 1965 he tore up his Labour Party card because he suspected Harold Wilson's Labour government was going to send troops to support the United States in Vietnam.\nFinal years, death and legacy.\nIn June 1955, Russell had leased Plas Penrhyn in Penrhyndeudraeth, Merionethshire, Wales and on 5 July of the following year it became his and Edith's principal residence.\nRussell published his three-volume autobiography in 1967, 1968, and 1969. Russell made a cameo appearance playing himself in the anti-war Hindi film \"Aman\", by Mohan Kumar, which was released in India in 1967. This was Russell's only appearance in a feature film.\nOn 23 November 1969. he wrote to \"The Times\" newspaper saying that the preparation for show trials in Czechoslovakia was \"highly alarming\". The same month, he appealed to Secretary General U Thant of the United Nations to support an international war crimes commission to investigate alleged torture and genocide by the United States in South Vietnam during the Vietnam War. The following month, he protested to Alexei Kosygin over the expulsion of Aleksandr Solzhenitsyn from the Soviet Union of Writers.\nOn 31 January 1970. Russell issued a statement condemning \"Israel's aggression in the Middle East\", and in particular, Israeli bombing raids being carried out deep in Egyptian territory as part of the War of Attrition. He called for an Israeli withdrawal to the pre-Six-Day War borders. This was Russell's final political statement or act. It was read out at the International Conference of Parliamentarians in Cairo on 3 February 1970, the day after his death.\nRussell died of influenza, just after 8\u00a0pm on 2 February 1970 at his home in Penrhyndeudraeth. His body was cremated in Colwyn Bay on 5 February 1970 with five people present. In accordance with his will, there was no religious ceremony but one minute's silence; his ashes were scattered over the Welsh mountains later that year. He left an estate valued at \u00a369,423 (equivalent to \u00a3\u00a0million in ).\nIn 1980, a memorial to Russell was commissioned by a committee including the philosopher A. J. Ayer. It consists of a bust of Russell in Red Lion Square in London sculpted by Marcelle Quinton.\nLady Katharine Jane Tait, Russell's daughter, founded the Bertrand Russell Society in 1974 to preserve and understand his work. It publishes the \"Bertrand Russell Society Bulletin\", holds meetings and awards prizes for scholarship. She also authored several essays about her father; as well as a book, \"My Father, Bertrand Russell\", which was published in 1975. All members receive \"Russell: The Journal of Bertrand Russell Studies\".\nTitles and honours from birth.\nRussell held throughout his life the following styles and honours:\nViews.\nPhilosophy.\nRussell is generally credited with being one of the founders of analytic philosophy. He was deeply impressed by Gottfried Leibniz (1646\u20131716), and wrote on every major area of philosophy except aesthetics. He was particularly prolific in the fields of metaphysics, logic and the philosophy of mathematics, the philosophy of language, ethics and epistemology. When Brand Blanshard asked Russell why he did not write on aesthetics, Russell replied that he did not know anything about it, though he hastened to add \"but that is not a very good excuse, for my friends tell me it has not deterred me from writing on other subjects\".\nOn ethics, Russell wrote that he was a utilitarian in his youth, yet he later distanced himself from this view.\nFor the advancement of science and protection of the right to freedom of expression, Russell advocated The Will to Doubt, the recognition that all human knowledge is at most a best guess, that one should always remember:\nReligion.\nRussell described himself in 1947 as an agnostic, saying: \"Therefore, in regard to the Olympic gods, speaking to a purely philosophical audience, I would say that I am an Agnostic. But speaking popularly, I think that all of us would say in regard to those gods that we were Atheists. In regard to the Christian God, I should, I think, take exactly the same line.\" For most of his adult life, Russell maintained religion to be little more than superstition and, despite any positive effects, largely harmful to people. He believed that religion and the religious outlook serve to impede knowledge and foster fear and dependency, and to be responsible for much of our world's wars, oppression, and misery. He was a member of the Advisory Council of the British Humanist Association and President of Cardiff Humanists until his death.\nSociety.\nPolitical and social activism occupied much of Russell's time for most of his life. Russell remained politically active almost to the end of his life, writing to and exhorting world leaders and lending his name to various causes.\nRussell argued for a \"scientific society\", where war would be abolished, population growth would be limited, and prosperity would be shared. He suggested the establishment of a \"single supreme world government\" able to enforce peace, claiming that \"the only thing that will redeem mankind is co-operation\". Russell also expressed support for guild socialism, and commented positively on several socialist thinkers and activists.\nRussell was an active supporter of the Homosexual Law Reform Society, being one of the signatories of A. E. Dyson's 1958 letter to \"The Times\" calling for a change in the law regarding male homosexual practices, which were partly legalised in 1967, when Russell was still alive.\nIn \"Reflections on My Eightieth Birthday\" (\"Postscript\" in his \"Autobiography\"), Russell wrote: \"I have lived in the pursuit of a vision, both personal and social. Personal: to care for what is noble, for what is beautiful, for what is gentle; to allow moments of insight to give wisdom at more mundane times. Social: to see in imagination the society that is to be created, where individuals grow freely, and where hate and greed and envy die because there is nothing to nourish them. These things I believe, and the world, for all its horrors, has left me unshaken\".\nWilliam T. Ross, a literary scholar at the University of South Florida, has argued that Russell held a deeply \"colonialist mindset\", citing anti-black, white supremacist comments embedded in a number of his works.\nFreedom of opinion and expression.\nLike George Orwell, Russell was a champion of freedom of opinion and an opponent of both censorship and indoctrination. In 1928, he wrote: \"The fundamental argument for freedom of opinion is the doubtfulness of all our belief... when the State intervenes to ensure the indoctrination of some doctrine, it does so because there is no conclusive evidence in favour of that doctrine .. It is clear that thought is not free if the profession of certain opinions make it impossible to make a living. In 1957, he wrote: \"'Free thought' means thinking freely ... to be worthy of the name freethinker he must be free of two things: the force of tradition and the tyranny of his own passions.\"\nSelected bibliography.\nBelow is a selected bibliography of Russell's books in English, sorted by year of first publication:\nRussell was the author of more than sixty books and over two thousand articles. Additionally, he wrote many pamphlets, introductions, and letters to the editor. One pamphlet titled, \"'I Appeal unto Caesar': The Case of the Conscientious Objectors\", ghostwritten for Margaret Hobhouse, the mother of imprisoned peace activist Stephen Hobhouse, allegedly helped secure the release from prison of hundreds of conscientious objectors.\nHis works can be found in anthologies and collections, including \"The Collected Papers of Bertrand Russell\", which McMaster University began publishing in 1983. By March 2017 this collection of his shorter and previously unpublished works included 18 volumes, and several more are in progress. A bibliography in three additional volumes catalogues his publications. The Russell Archives held by McMaster's William Ready Division of Archives and Research Collections possess over 40,000 of his letters.\nReferences.\nSources.\nPrimary sources\nSecondary sources"}
{"id": "4165", "revid": "454986", "url": "https://en.wikipedia.org/wiki?curid=4165", "title": "Boeing 767", "text": "The Boeing 767 is a wide-body airliner developed and manufactured by Boeing Commercial Airplanes.\nThe airliner was launched as the 7X7 project on July 14, 1978, the prototype first flew on September 26, 1981, and it was certified on July 30, 1982.\nThe original 767-200 entered service on September 8, 1982 with United Airlines, and the extended-range 767-200ER in 1984.\nIt was stretched into the in October 1986, followed by the 767-300ER in 1988, the most popular variant.\nThe 767-300F, a production freighter version, debuted in October 1995.\nIt was stretched again into the 767-400ER from September 2000.\nTo complement the larger 747, it has a seven-abreast cross-section, accommodating smaller LD2 ULD cargo containers.\nThe 767 is Boeing's first wide-body twinjet, powered by General Electric CF6, Rolls-Royce RB211, or Pratt &amp; Whitney JT9D turbofans. JT9D engines were eventually replaced by PW4000 engines.\nThe aircraft has a conventional tail and a supercritical wing for reduced aerodynamic drag.\nIts two-crew glass cockpit, a first for a Boeing airliner, was developed jointly for the 757 \u2212 a narrow-body aircraft, allowing a common pilot type rating.\nStudies for a higher-capacity 767 in 1986 led Boeing to develop the larger 777 twinjet, introduced in June 1995.\nThe 767-200 typically seats 216 passengers over 3,900 nmi (7,200\u00a0km), while the 767-200ER seats 181 over a 6,590 nautical miles (12,200\u00a0km) range.\nThe 767-300 typically seats 269 passengers over 3,900 nmi (7,200\u00a0km), while the 767-300ER seats 218 over 5,980 nmi (11,070\u00a0km).\nThe 767-300F can haul over 3,225 nmi (6,025\u00a0km), and the 767-400ER typically seats 245 passengers over 5,625 nmi (10,415\u00a0km).\nMilitary derivatives include the E-767 for surveillance and the KC-767 and KC-46 aerial tankers. \nAfter being initially used on U.S. transcontinental routes, that was extended with ETOPS regulations from 1985 and it is frequently used on transatlantic flights.\n, Boeing has received 1,254 orders from 74 customers, with 1,161 delivered, while the remaining orders are for cargo or tanker variants. A total of 742 of these aircraft were in service in July 2018. Delta Air Lines is the largest operator with 77 aircraft. Competitors have included the Airbus A300, A310, and A330-200.\nIts successor, the 787 Dreamliner, entered service in 2011.\nDevelopment.\nBackground.\nIn 1970, Boeing's 747 became the first wide-body jetliner to enter service. The 747 was the first passenger jet wide enough to feature a twin-aisle cabin. Two years later, the manufacturer began a development study, code-named 7X7, for a new wide-body aircraft intended to replace the 707 and other early generation narrow-body jets. The aircraft would also provide twin-aisle seating, but in a smaller fuselage than the existing 747, McDonnell Douglas DC-10, and Lockheed L-1011 TriStar wide-bodies. To defray the high cost of development, Boeing signed risk-sharing agreements with Italian corporation Aeritalia and the Civil Transport Development Corporation (CTDC), a consortium of Japanese aerospace companies. This marked the manufacturer's first major international joint venture, and both Aeritalia and the CTDC received supply contracts in return for their early participation. The initial 7X7 was conceived as a short take-off and landing airliner intended for short-distance flights, but customers were unenthusiastic about the concept, leading to its redefinition as a mid-size, transcontinental-range airliner. At this stage the proposed aircraft featured two or three engines, with possible configurations including over-wing engines and a T-tail.\nBy 1976, a twinjet layout, similar to the one which had debuted on the Airbus A300, became the baseline configuration. The decision to use two engines reflected increased industry confidence in the reliability and economics of new-generation jet powerplants. While airline requirements for new wide-body aircraft remained ambiguous, the 7X7 was generally focused on mid-size, high-density markets. As such, it was intended to transport large numbers of passengers between major cities. Advancements in civil aerospace technology, including high-bypass-ratio turbofan engines, new flight deck systems, aerodynamic improvements, and lighter construction materials were to be applied to the 7X7. Many of these features were also included in a parallel development effort for a new mid-size narrow-body airliner, code-named 7N7, which would become the 757. Work on both proposals proceeded through the airline industry upturn in the late 1970s.\nIn January 1978, Boeing announced a major extension of its Everett factory\u2014which was then dedicated to manufacturing the 747\u2014to accommodate its new wide-body family. In February 1978, the new jetliner received the 767 model designation, and three variants were planned: a with 190 seats, a with 210 seats, and a trijet 767MR/LR version with 200 seats intended for intercontinental routes. The 767MR/LR was subsequently renamed 777 for differentiation purposes. The 767 was officially launched on July 14, 1978, when United Airlines ordered 30 of the 767-200 variant, followed by 50 more 767-200 orders from American Airlines and Delta Air Lines later that year. The 767-100 was ultimately not offered for sale, as its capacity was too close to the 757's seating, while the 777 trijet was eventually dropped in favor of standardizing around the twinjet configuration.\nDesign effort.\nIn the late 1970s, operating cost replaced capacity as the primary factor in airliner purchases. As a result, the 767's design process emphasized fuel efficiency from the outset. Boeing targeted a 20 to 30\u00a0percent cost saving over earlier aircraft, mainly through new engine and wing technology. As development progressed, engineers used computer-aided design for over a third of the 767's design drawings, and performed 26,000 hours of wind tunnel tests. Design work occurred concurrently with the 757 twinjet, leading Boeing to treat both as almost one program to reduce risk and cost. Both aircraft would ultimately receive shared design features, including avionics, flight management systems, instruments, and handling characteristics. Combined development costs were estimated at $3.5 to $4\u00a0billion.\nEarly 767 customers were given the choice of Pratt &amp; Whitney JT9D or General Electric CF6 turbofans, marking the first time that Boeing had offered more than one engine option at the launch of a new airliner. Both jet engine models had a maximum output of of thrust. The engines were mounted approximately one-third the length of the wing from the fuselage, similar to previous wide-body trijets. The larger wings were designed using an aft-loaded shape which reduced aerodynamic drag and distributed lift more evenly across their surface span than any of the manufacturer's previous aircraft. The wings provided higher-altitude cruise performance, added fuel capacity, and expansion room for future stretched variants. The initial 767-200 was designed for sufficient range to fly across North America or across the northern Atlantic, and would be capable of operating routes up to .\nThe 767's fuselage width was set midway between that of the 707 and the 747 at . While it was narrower than previous wide-body designs, seven abreast seating with two aisles could be fitted, and the reduced width produced less aerodynamic drag. The fuselage was not wide enough to accommodate two standard LD3 wide-body unit load devices side-by-side, so a smaller container, the LD2, was created specifically for the 767. Using a conventional tail design also allowed the rear fuselage to be tapered over a shorter section, providing for parallel aisles along the full length of the passenger cabin, and eliminating irregular seat rows toward the rear of the aircraft.\nThe 767 was the first Boeing wide-body to be designed with a two-crew digital glass cockpit. Cathode ray tube (CRT) color displays and new electronics replaced the role of the flight engineer by enabling the pilot and co-pilot to monitor aircraft systems directly. Despite the promise of reduced crew costs, United Airlines initially demanded a conventional three-person cockpit, citing concerns about the risks associated with introducing a new aircraft. The carrier maintained this position until July 1981, when a US presidential task force determined that a crew of two was safe for operating wide-body jets. A three-crew cockpit remained as an option and was fitted to the first production models. Ansett Australia ordered 767s with three-crew cockpits due to union demands; it was the only airline to operate 767s so configured. The 767's two-crew cockpit was also applied to the 757, allowing pilots to operate both aircraft after a short conversion course, and adding incentive for airlines to purchase both types.\nProduction and testing.\nTo produce the 767, Boeing formed a network of subcontractors which included domestic suppliers and international contributions from Italy's Aeritalia and Japan's CTDC. The wings and cabin floor were produced in-house, while Aeritalia provided control surfaces, Boeing Vertol made the leading edge for the wings, and Boeing Wichita produced the forward fuselage. The CTDC provided multiple assemblies through its constituent companies, namely Fuji Heavy Industries (wing fairings and gear doors), Kawasaki Heavy Industries (center fuselage), and Mitsubishi Heavy Industries (rear fuselage, doors, and tail). Components were integrated during final assembly at the Everett factory. For expedited production of wing spars, the main structural member of aircraft wings, the Everett factory received robotic machinery to automate the process of drilling holes and inserting fasteners. This method of wing construction expanded on techniques developed for the 747. Final assembly of the first aircraft began in July 1979.\nThe prototype aircraft, registered N767BA and equipped with JT9D turbofans, rolled out on August 4, 1981. By this time, the 767 program had accumulated 173 firm orders from 17 customers, including Air Canada, All Nippon Airways, Britannia Airways, Transbrasil, and Trans World Airlines (TWA). On September 26, 1981, the prototype took its maiden flight under the command of company test pilots Tommy Edmonds, Lew Wallick, and John Brit. The maiden flight was largely uneventful, save for the inability to retract the landing gear because of a hydraulic fluid leak. The prototype was used for subsequent flight tests.\nThe 10-month 767 flight test program utilized the first six aircraft built. The first four aircraft were equipped with JT9D engines, while the fifth and sixth were fitted with CF6 engines. The test fleet was largely used to evaluate avionics, flight systems, handling, and performance, while the sixth aircraft was used for route-proving flights. During testing, pilots described the 767 as generally easy to fly, with its maneuverability unencumbered by the bulkiness associated with larger wide-body jets. Following 1,600 hours of flight tests, the JT9D-powered 767-200 received certification from the US Federal Aviation Administration (FAA) and the UK Civil Aviation Authority (CAA) in July 1982. The first delivery occurred on August 19, 1982, to United Airlines. The CF6-powered 767-200 received certification in September 1982, followed by the first delivery to Delta Air Lines on October 25, 1982.\nService entry and operations.\nThe 767 entered service with United Airlines on September 8, 1982. The aircraft's first commercial flight used a JT9D-powered on the Chicago-to-Denver route. The CF6-powered 767-200 commenced service three months later with Delta Air Lines. Upon delivery, early 767s were mainly deployed on domestic routes, including US transcontinental services. American Airlines and TWA began flying the 767-200 in late 1982, while Air Canada, China Airlines, and El Al began operating the aircraft in 1983. The aircraft's introduction was relatively smooth, with few operational glitches and greater dispatch reliability than prior jetliners. In its first year, the 767 logged a 96.1\u00a0percent dispatch rate, which exceeded the industry average for new aircraft. Operators reported generally favorable ratings for the twinjet's sound levels, interior comfort, and economic performance. Resolved issues were minor and included the recalibration of a leading edge sensor to prevent false readings, the replacement of an evacuation slide latch, and the repair of a tailplane pivot to match production specifications.\nSeeking to capitalize on its new wide-body's potential for growth, Boeing offered an extended-range model, the 767-200ER, in its first year of service. Ethiopian Airlines placed the first order for the type in December 1982. Featuring increased gross weight and greater fuel capacity, the extended-range model could carry heavier payloads at distances up to , and was targeted at overseas customers. The 767-200ER entered service with El Al Airline on March 27, 1984. The type was mainly ordered by international airlines operating medium-traffic, long-distance flights. In May 1984, an Ethiopian Airlines 767-200ER set a non-stop record for a commercial twinjet of from Washington DC to Addis Ababa.\nIn the mid-1980s, the 767 spearheaded the growth of twinjet flights across the northern Atlantic under extended-range twin-engine operational performance standards (ETOPS) regulations, the FAA's safety rules governing transoceanic flights by aircraft with two engines. Before the 767, overwater flight paths of twinjets could be no more than 90 minutes away from diversion airports. In May 1985, the FAA granted its first approval for 120-minute ETOPS flights to 767 operators, on an individual airline basis starting with TWA, provided that the operator met flight safety criteria. This allowed the aircraft to fly overseas routes at up to two hours' distance from land. The larger safety margins were permitted because of the improved reliability demonstrated by the twinjet and its turbofan engines. The FAA lengthened the ETOPS time to 180 minutes for CF6-powered 767s in 1989, making the type the first to be certified under the longer duration, and all available engines received approval by 1993. Regulatory approval spurred the expansion of transoceanic 767 flights and boosted the aircraft's sales.\nStretched derivatives.\nForecasting airline interest in larger-capacity models, Boeing announced the stretched in 1983 and the extended-range 767-300ER in 1984. Both models offered a 20\u00a0percent passenger capacity increase, while the extended-range version was capable of operating flights up to . Japan Airlines placed the first order for the -300 in September 1983. Following its first flight on January 30, 1986, the type entered service with Japan Airlines on October 20, 1986. The 767-300ER completed its first flight on December 9, 1986, but it was not until March 1987 that the first firm order, from American Airlines, was placed. The type entered service with American Airlines on March 3, 1988. The 767-300 and 767-300ER gained popularity after entering service, and came to account for approximately two-thirds of all 767s sold.\nAfter the debut of the first stretched 767s, Boeing sought to address airline requests for greater capacity by proposing larger models, including a partial double-deck version informally named the \"Hunchback of Mukilteo\" (from a town near Boeing's Everett factory) with a 757 body section mounted over the aft main fuselage. In 1986, Boeing proposed the 767-X, a revised model with extended wings and a wider cabin, but received little interest. By 1988, the 767-X had evolved into an all-new twinjet, which revived the 777 designation. Until the 777's 1995 debut, the 767-300 and 767-300ER remained Boeing's second-largest wide-bodies behind the 747.\nBuoyed by a recovering global economy and ETOPS approval, 767 sales accelerated in the mid-to-late 1980s; 1989 was the most prolific year with 132 firm orders. By the early 1990s, the wide-body twinjet had become its manufacturer's annual best-selling aircraft, despite a slight decrease due to economic recession. During this period, the 767 became the most common airliner for transatlantic flights between North America and Europe. By the end of the decade, 767s crossed the Atlantic more frequently than all other aircraft types combined. The 767 also propelled the growth of point-to-point flights which bypassed major airline hubs in favor of direct routes. Taking advantage of the aircraft's lower operating costs and smaller capacity, operators added non-stop flights to secondary population centers, thereby eliminating the need for connecting flights. The increased number of cities receiving non-stop services caused a paradigm shift in the airline industry as point-to-point travel gained prominence at the expense of the traditional hub-and-spoke model.\nIn February 1990, the first 767 equipped with Rolls-Royce RB211 turbofans, a , was delivered to British Airways. Six months later, the carrier temporarily grounded its entire 767 fleet after discovering cracks in the engine pylons of several aircraft. The cracks were related to the extra weight of the RB211 engines, which are heavier than other 767 engines. During the grounding, interim repairs were conducted to alleviate stress on engine pylon components, and a parts redesign in 1991 prevented further cracks. Boeing also performed a structural reassessment, resulting in production changes and modifications to the engine pylons of all 767s in service.\nIn January 1993, following an order from UPS Airlines, Boeing launched a freighter variant, the 767-300F, which entered service with UPS on October 16, 1995. The 767-300F featured a main deck cargo hold, upgraded landing gear, and strengthened wing structure. In November 1993, the Japanese government launched the first 767 military derivative when it placed orders for the , an Airborne Early Warning and Control (AWACS) variant based on the 767-200ER. The first two , featuring extensive modifications to accommodate surveillance radar and other monitoring equipment, were delivered in 1998 to the Japan Self-Defense Forces.\nIn November 1995, after abandoning development of a smaller version of the 777, Boeing announced that it was revisiting studies for a larger 767. The proposed 767-400X, a second stretch of the aircraft, offered a 12\u00a0percent capacity increase versus the , and featured an upgraded flight deck, enhanced interior, and greater wingspan. The variant was specifically aimed at Delta Air Lines' pending replacement of its aging Lockheed L-1011 TriStars, and faced competition from the A330-200, a shortened derivative of the Airbus A330. In March 1997, Delta Air Lines launched the 767-400ER when it ordered the type to replace its L-1011 fleet. In October 1997, Continental Airlines also ordered the 767-400ER to replace its McDonnell Douglas DC-10 fleet. The type completed its first flight on October 9, 1999, and entered service with Continental Airlines on September 14, 2000.\nDreamliner introduction.\nIn the early 2000s, cumulative 767 deliveries approached 900, but new sales declined during an airline industry downturn. In 2001, Boeing dropped plans for a longer-range model, the 767-400ERX, in favor of the proposed Sonic Cruiser, a new jetliner which aimed to fly 15\u00a0percent faster while having comparable fuel costs to the 767. The following year, Boeing announced the KC-767 Tanker Transport, a second military derivative of the 767-200ER. Launched with an order in October 2002 from the Italian Air Force, the KC-767 was intended for the dual role of refueling other aircraft and carrying cargo. The Japanese government became the second customer for the type in March 2003. In May 2003, the United States Air Force (USAF) announced its intent to lease KC-767s to replace its aging KC-135 tankers. The plan was suspended in March 2004 amid a conflict of interest scandal, resulting in multiple US government investigations and the departure of several Boeing officials, including Philip Condit, the company's chief executive officer, and chief financial officer Michael Sears. The first KC-767s were delivered in 2008 to the Japan Self-Defense Forces.\nIn late 2002, after airlines expressed reservations about its emphasis on speed over cost reduction, Boeing halted development of the Sonic Cruiser. The following year, the manufacturer announced the 7E7, a mid-size 767 successor made from composite materials which promised to be 20\u00a0percent more fuel efficient. The new jetliner was the first stage of a replacement aircraft initiative called the Boeing Yellowstone Project. Customers embraced the 7E7, later renamed 787 Dreamliner, and within two years it had become the fastest-selling airliner in the company's history. In 2005, Boeing opted to continue 767 production despite record Dreamliner sales, citing a need to provide customers waiting for the 787 with a more readily available option. Subsequently, the 767-300ER was offered to customers affected by 787 delays, including All Nippon Airways and Japan Airlines. Some aging 767s, exceeding 20 years in age, were also kept in service past planned retirement dates due to the delays. To extend the operational lives of older aircraft, airlines increased heavy maintenance procedures, including D-check teardowns and inspections for corrosion, a recurring issue on aging 767s. The first 787s entered service with All Nippon Airways in October 2011, 42 months behind schedule.\nContinued production.\nIn 2007, the 767 received a production boost when UPS and DHL Aviation placed a combined 33 orders for the 767-300F. Renewed freighter interest led Boeing to consider enhanced versions of the 767-200 and 767-300F with increased gross weights, 767-400ER wing extensions, and 777 avionics. Net orders for the 767 declined from 24 in 2008 to just three in 2010. During the same period, operators upgraded aircraft already in service; in 2008, the first 767-300ER retrofitted with blended winglets from Aviation Partners Incorporated debuted with American Airlines. The manufacturer-sanctioned winglets, at in height, improved fuel efficiency by an estimated 6.5\u00a0percent. Other carriers including All Nippon Airways and Delta Air Lines also ordered winglet kits.\nOn February 2, 2011, the 1,000th 767 rolled out, destined for All Nippon Airways. The aircraft was the 91st 767-300ER ordered by the Japanese carrier, and with its completion the 767 became the second wide-body airliner to reach the thousand-unit milestone after the 747. The 1,000th aircraft also marked the last model produced on the original 767 assembly line. Beginning with the 1,001st aircraft, production moved to another area in the Everett factory which occupied about half of the previous floor space. The new assembly line made room for 787 production and aimed to boost manufacturing efficiency by over twenty percent.\nAt the inauguration of its new assembly line, the 767's order backlog numbered approximately 50, only enough for production to last until 2013. Despite the reduced backlog, Boeing officials expressed optimism that additional orders would be forthcoming. On February 24, 2011, the USAF announced its selection of the KC-767 Advanced Tanker, an upgraded variant of the KC-767, for its KC-X fleet renewal program. The selection followed two rounds of tanker competition between Boeing and Airbus parent EADS, and came eight years after the USAF's original 2003 announcement of its plan to lease KC-767s. The tanker order encompassed 179 aircraft and was expected to sustain 767 production past 2013.\nIn December 2011, FedEx Express announced a 767-300F order for 27 aircraft to replace its DC-10 freighters, citing the USAF tanker order and Boeing's decision to continue production as contributing factors. FedEx Express agreed to buy 19 more of the \u2212300F variant in June 2012. In June 2015, FedEx said it was accelerating retirements of planes both to reflect demand and to modernize its fleet, recording charges of $276 million. On July 21, 2015 FedEx announced an order for 50 767-300F with options on another 50, the largest order for the type. With the announcement FedEx confirmed that it has firm orders for 106 of the freighters for delivery between 2018 and 2023. In February 2018, UPS announced an order for 4 more 767-300Fs to increase the total on order to 63.\nWith its successor, the Boeing New Midsize Airplane, that was planned for introduction in 2025 or later, and the 787 being much larger, Boeing could restart a passenger 767-300ER production to bridge the gap. A demand for 50 to 60 aircraft could have to be satisfied. Having to replace its 40 767s, United Airlines requested a price quote for other widebodies. In November 2017, Boeing CEO Dennis Muilenburg cited interest beyond military and freighter uses. However, in early 2018 Boeing Commercial Airplanes VP of marketing Randy Tinseth stated that the company did not intend to resume production of the passenger variant.\nIn its first quarter of 2018 earnings report, Boeing plan to increase its production from 2.5 to 3 monthly beginning in January 2020 due to increased demand in the cargo market, as FedEx had 56 on order, UPS has four, and an unidentified customer has three on order. This rate could rise to 3.5 per month in July 2020 and 4 per month in January 2021, before decreasing to 3 per month in January 2025 and then 2 per month in July 2025.\nIn 2019, unit cost was US$ 217.9 million for a -300ER, and US$ 220.3 million for a -300F.\nRe-engined 767-XF.\nBoeing is studying a re-engined 767-XF for around 2025, based on the 767-400ER with an extended landing gear to accommodate larger General Electric GEnx turbofan engines.\nThe cargo market is the main target, but a passenger version could be a cheaper alternative to the proposed New Midsize Airplane.\nDesign.\nOverview.\nThe 767 is a low-wing cantilever monoplane with a conventional tail unit featuring a single fin and rudder. The wings are swept at 31.5 degrees and optimized for a cruising speed of Mach 0.8 (). Each wing features a supercritical airfoil cross-section and is equipped with six-panel leading edge slats, single- and double-slotted flaps, inboard and outboard ailerons, and six spoilers. The airframe further incorporates Carbon-fiber-reinforced polymer composite material wing surfaces, Kevlar fairings and access panels, plus improved aluminum alloys, which together reduce overall weight by versus preceding aircraft.\nTo distribute the aircraft's weight on the ground, the 767 has a retractable tricycle landing gear with four wheels on each main gear and two for the nose gear. The original wing and gear design accommodated the stretched 767-300 without major changes. The 767-400ER features a larger, more widely spaced main gear with 777 wheels, tires, and brakes. To prevent damage if the tail section contacts the runway surface during takeoff, 767-300 and 767-400ER models are fitted with a retractable tailskid. The 767 has left-side exit doors near the front and rear of the aircraft.\nIn addition to shared avionics and computer technology, the 767 uses the same auxiliary power unit, electric power systems, and hydraulic parts as the 757. A raised cockpit floor and the same forward cockpit windows result in similar pilot viewing angles. Related design and functionality allows 767 pilots to obtain a common type rating to operate the 757 and share the same seniority roster with pilots of either aircraft.\nFlight systems.\nThe original 767 flight deck uses six Rockwell Collins CRT screens to display Electronic flight instrument system (EFIS) and engine indication and crew alerting system (EICAS) information, allowing pilots to handle monitoring tasks previously performed by the flight engineer. The CRTs replace conventional electromechanical instruments found on earlier aircraft. An enhanced flight management system, improved over versions used on early 747s, automates navigation and other functions, while an automatic landing system facilitates CAT IIIb instrument landings in low visibility situations. The 767 became the first aircraft to receive CAT IIIb certification from the FAA for landings with minimum visibility in 1984. On the 767-400ER, the cockpit layout is simplified further with six Rockwell Collins liquid crystal display (LCD) screens, and adapted for similarities with the 777 and the Next Generation 737. To retain operational commonality, the LCD screens can be programmed to display information in the same manner as earlier 767s. In 2012, Boeing and Rockwell Collins launched a further 787-based cockpit upgrade for the 767, featuring three landscape-format LCD screens that can display two windows each.\nThe 767 is equipped with three redundant hydraulic systems for operation of control surfaces, landing gear, and utility actuation systems. Each engine powers a separate hydraulic system, and the third system uses electric pumps. A ram air turbine provides power for basic controls in the event of an emergency. An early form of fly-by-wire is employed for spoiler operation, utilizing electric signaling instead of traditional control cables. The fly-by-wire system reduces weight and allows independent operation of individual spoilers.\nInterior.\nThe 767 features a twin-aisle cabin with a typical configuration of six abreast in business class and seven across in economy. The standard seven abreast, 2\u20133\u20132 economy class layout places approximately 87\u00a0percent of all seats at a window or aisle. As a result, the aircraft can be largely occupied before center seats need to be filled, and each passenger is no more than one seat from the aisle. It is possible to configure the aircraft with extra seats for up to an eight abreast configuration, but this is less common.\nThe 767 interior introduced larger overhead bins and more lavatories per passenger than previous aircraft. The bins are wider to accommodate garment bags without folding, and strengthened for heavier carry-on items. A single, large galley is installed near the aft doors, allowing for more efficient meal service and simpler ground resupply. Passenger and service doors are an overhead plug type, which retract upwards, and commonly used doors can be equipped with an electric-assist system.\nIn 2000, a 777-style interior, known as the Boeing Signature Interior, debuted on the 767-400ER. Subsequently, adopted for all new-build 767s, the Signature Interior features even larger overhead bins, indirect lighting, and sculpted, curved panels. The 767-400ER also received larger windows derived from the 777. Older 767s can be retrofitted with the Signature Interior. Some operators have adopted a simpler modification known as the Enhanced Interior, featuring curved ceiling panels and indirect lighting with minimal modification of cabin architecture, as well as aftermarket modifications such as the NuLook 767 package by Heath Tecna.\nVariants.\nThe 767 has been produced in three fuselage lengths. These debuted in progressively larger form as the , , and 767-400ER. Longer-range variants include the 767-200ER and 767-300ER, while cargo models include the 767-300F, a production freighter, and conversions of passenger 767-200 and 767-300 models.\nWhen referring to different variants, Boeing and airlines often collapse the model number (767) and the variant designator, e.g. \u2013200 or \u2013300, into a truncated form, e.g. \"762\" or \"763\". Subsequent to the capacity number, designations may append the range identifier, though -200ER and -300ER are company marketing designations and not certificated as such. The International Civil Aviation Organization (ICAO) aircraft type designator system uses a similar numbering scheme, but adds a preceding manufacturer letter; all variants based on the 767-200 and 767-300 are classified under the codes \"B762\" and \"B763\"; the 767-400ER receives the designation of \"B764\".\n767-200.\nThe 767-200 was the original model and entered service with United Airlines in 1982. The type has been used primarily by mainline U.S. carriers for domestic routes between major hub centers such as Los Angeles to Washington. The 767-200 was the first aircraft to be used on transatlantic ETOPS flights, beginning with TWA on February 1, 1985 under 90-minute diversion rules. Deliveries for the variant totaled 128 aircraft. There were 52 examples of the model in commercial service , almost entirely as freighter conversions. The type's competitors included the Airbus A300 and A310.\nThe 767-200 was produced until 1987 when production switched to the extended-range 767-200ER. Some early 767-200s were subsequently upgraded to extended-range specification. In 1998, Boeing began offering 767-200 conversions to 767-200SF (Special Freighter) specification for cargo use, and Israel Aerospace Industries has been licensed to perform cargo conversions since 2005. The conversion process entails the installation of a side cargo door, strengthened main deck floor, and added freight monitoring and safety equipment. The 767-200SF was positioned as a replacement for Douglas DC-8 freighters.\n767-2C.\nA commercial freighter version of the Boeing with wings from the -300 series and an updated flightdeck was first flown on 29 December 2014. A military tanker variant of the Boeing 767-2C is being developed for the USAF as the KC-46. Boeing is building two aircraft as commercial freighters which will be used to obtain Federal Aviation Administration certification, a further two Boeing 767-2Cs will be modified as military tankers. , Boeing does not have customers for the freighter.\n767-200ER.\nThe 767-200ER was the first extended-range model and entered service with El Al in 1984. The type's increased range is due to extra fuel capacity and higher maximum takeoff weight (MTOW) of up to . The additional fuel capacity is accomplished by using the center tank's dry dock to carry fuel. The non-ER variant's center tank is what is called \"cheek tanks\"; two interconnected halves in each wing root with a dry dock in between. The center tank is also used on the -300ER and -400ER variants.\nThis version was originally offered with the same engines as the , while more powerful Pratt &amp; Whitney PW4000 and General Electric CF6 engines later became available. The 767-200ER was the first 767 to complete a non-stop transatlantic journey, and broke the flying distance record for a twinjet airliner on April 17, 1988 with an Air Mauritius flight from Halifax, Nova Scotia to Port Louis, Mauritius, covering . The 767-200ER has been acquired by international operators seeking smaller wide-body aircraft for long-haul routes such as New York to Beijing. Deliveries of the type totaled 121 with no unfilled orders. As of July 2018, 21 examples of passenger and freighter conversion versions were in airline service. The type's main competitors of the time included the Airbus A300-600R and the A310-300.\n767-300.\nThe , the first stretched version of the aircraft, entered service with Japan Airlines in 1986. The type features a fuselage extension over the , achieved by additional sections inserted before and after the wings, for an overall length of . Reflecting the growth potential built into the original 767 design, the wings, engines, and most systems were largely unchanged on the . An optional mid-cabin exit door is positioned ahead of the wings on the left, while more powerful Pratt &amp; Whitney PW4000 and Rolls-Royce RB211 engines later became available. The 767-300's increased capacity has been used on high-density routes within Asia and Europe. The 767-300 was produced from 1986 until 2000. Deliveries for the type totaled 104 aircraft with no unfilled orders remaining. As of July 2018, 34 of the variant were in airline service. The type's main competitor was the Airbus A300.\n767-300ER.\nThe 767-300ER, the extended-range version of the , entered service with American Airlines in 1988. The type's increased range was made possible by greater fuel tankage and a higher MTOW of . Design improvements allowed the available MTOW to increase to by 1993. Power is provided by Pratt &amp; Whitney PW4000, General Electric CF6, or Rolls-Royce RB211 engines. the 767-300ER comes in three exit configurations: the baseline configuration has four main cabin doors and four over-wing window exits, the second configuration has six main cabin doors and two over-wing window exits; and the third configuration has six main cabin doors, as well as two smaller doors that are located behind the wings. Typical routes for the type include Los Angeles to Frankfurt. The combination of increased capacity and range offered by the 767-300ER has been particularly attractive to both new and existing 767 operators. It is the most successful version of the aircraft, with more orders placed than all other variants combined. , 767-300ER deliveries stand at 583 with no unfilled orders. There were 376 examples in service . The type's main competitor is the Airbus A330-200.\nAt its 1990s peak, a new 767-300ER was valued at $85 million, dipping to around $12 million in 2018 for a 1996 build.\n767-300F.\nThe 767-300F, the production freighter version of the 767-300ER, entered service with UPS Airlines in 1995. The 767-300F can hold up to 24 standard pallets on its main deck and up to 30 LD2 unit load devices on the lower deck, with a total cargo volume of . The freighter has a main deck cargo door and crew exit, while the lower deck features two starboard-side cargo doors and one port-side cargo door. A general market version with onboard freight-handling systems, refrigeration capability, and crew facilities was delivered to Asiana Airlines on August 23, 1996. , 767-300F deliveries stand at 161 with 61 unfilled orders. Airlines operated 222 examples of the freighter variant and freighter conversions in July 2018.\nIn June 2008, All Nippon Airways took delivery of the first 767-300BCF (Boeing Converted Freighter), a modified passenger-to-freighter model. The conversion work was performed in Singapore by ST Aerospace Services, the first supplier to offer a 767-300BCF program, and involved the addition of a main deck cargo door, strengthened main deck floor, and additional freight monitoring and safety equipment. Since then, Boeing, Israel Aerospace Industries, and Wagner Aeronautical have also offered passenger-to-freighter conversion programs for series aircraft.\n767-400ER.\nThe 767-400ER, the first Boeing wide-body jet resulting from two fuselage stretches, entered service with Continental Airlines in 2000. The type features a stretch over the , for a total length of . The wingspan is also increased by through the addition of raked wingtips. The exit configuration uses six main cabin doors and two smaller exit doors behind the wings, similar to certain 767-300ERs. Other differences include an updated cockpit, redesigned landing gear, and 777-style Signature Interior. Power is provided by uprated General Electric CF6 engines.\nThe FAA granted approval for the 767-400ER to operate 180-minute ETOPS flights before it entered service. Because its fuel capacity was not increased over preceding models, the 767-400ER has a range of , less than previous extended-range 767s. No 767-400 version was developed.\nThe longer-range 767-400ERX was offered in July 2000 before being cancelled a year later, leaving the 767-400ER as the sole version of the largest 767. Boeing dropped the 767-400ER and the -200ER from its pricing list in 2014.\nA total of 37 767-400ERs were delivered to the variant's two airline customers, Continental Airlines (now merged with United Airlines) and Delta Air Lines, with no unfilled orders. All 37 examples of the -400ER were in service in July 2018. One additional example was produced as a military testbed, and later sold as a VIP transport. The type's closest competitor is the Airbus A330-200.\nMilitary and government.\nVersions of the 767 serve in a number of military and government applications, with responsibilities ranging from airborne surveillance and refueling to cargo and VIP transport. Several military 767s have been derived from the 767-200ER, the longest-range version of the aircraft.\nUndeveloped variants.\n767-X.\nIn 1986, Boeing announced plans for a partial double-deck Boeing 767 design. The aircraft would have combined the Boeing with a Boeing 757 cross section mounted over the rear fuselage. The Boeing 767-X would have also featured extended wings and a wider cabin. The 767-X did not get enough interest from airlines to launch and the model was shelved in 1988 in favor of the Boeing 777.\n767-400ERX.\nIn March 2000, Boeing was to launch the 259-seat 767-400ERX with an initial order for three from Kenya Airways with deliveries planned for 2004, as it was proposed to Lauda Air.\nIncreased gross weight and a tailplane fuel tank would have boosted its range by , and GE could offer its CF6-80C2/G2. \nRolls-Royce offered its Trent 600 for the 767-400ERX and the Boeing 747X.\nOffered in July, the longer-range -400ERX would have a strengthened wing, fuselage and landing gear for a 15,000\u00a0lb (6.8 t) higher MTOW, up to 465,000\u00a0lb (210.92 t).\nThrust would rise to for better takeoff performance, with the Trent 600 or the General Electric/Pratt &amp; Whitney Engine Alliance GP7172, also offered on the 747X.\nRange would increase by 525 nmi (950\u00a0km) to 6,150 nmi (11,390\u00a0km), with an additional fuel tank of 2,145 gallons (8,120 l) in the horizontal tail.\nThe 767-400ERX would offer the capacity of the Airbus A330-200 with 3% lower fuel burn and costs.\nBoeing cancelled the variant development in 2001.\nKenya Airways then switched its order to the 777-200ER.\nOperators.\n \nIn July 2018, 742 aircraft were in airline service: 73 -200s, 632 -300 and 37 -400 with 65 -300F on order; the largest operators are Delta Air Lines (77), FedEx (60; largest cargo operator), UPS Airlines (59), United Airlines (), Japan Airlines (35), All Nippon Airways (34). The type's competitors included the Airbus A300 and A310.\nThe largest 767 customers by orders have been Delta Air Lines with 117, FedEx Express (130), All Nippon Airways (96), American Airlines (88), and United Airlines (82). Delta and United are the only customers of all -200, -300 and -400 passenger variants. In July 2015, FedEx placed a firm order for 50 Boeing 767 freighters with deliveries from 2018 to 2023.\nOrders and deliveries.\nBoeing 767 orders and deliveries (cumulative, by year): \nAccidents and notable incidents.\n, the Boeing 767 has been in 60 aviation occurrences, including 19 hull-loss accidents. Seven fatal crashes, including three hijackings, have resulted in a total of 854 occupant fatalities.\nThe 767's first incident was Air Canada Flight 143, a , on July 23, 1983. The airplane ran out of fuel in-flight and had to glide with both engines out for almost to an emergency landing at Gimli, Manitoba, Canada. The pilots used the aircraft's ram air turbine to power the hydraulic systems for aerodynamic control. There were no fatalities and only minor injuries. This aircraft was nicknamed \"Gimli Glider\" after its landing site. The aircraft, registered C-GAUN, continued flying for Air Canada until its retirement in January 2008.\nThe airliner's first fatal crash, Lauda Air Flight 004, occurred near Bangkok on May 26, 1991, following the in-flight deployment of the left engine thrust reverser on a 767-300ER; none of the 223 aboard survived; as a result of this accident all 767 thrust reversers were deactivated until a redesign was implemented. Investigators determined that an electronically controlled valve, common to late-model Boeing aircraft, was to blame. A new locking device was installed on all affected jetliners, including 767s.\nOn October 31, 1999, EgyptAir Flight 990, a 767-300ER, crashed off Nantucket, Massachusetts, in international waters killing all 217 people on board. The United States National Transportation Safety Board (NTSB) concluded \"not determined\", but determined the probable cause to be due to a deliberate action by the first officer; Egypt disputed this conclusion.\nOn April 15, 2002, Air China Flight 129, a 767-200ER, crashed into a hill amid inclement weather while trying to land at Gimhae International Airport in Busan, South Korea. The crash resulted in the death of 129 of the 166 people on board, and the cause was attributed to pilot error.\nThe 767 has been involved in six hijackings, three resulting in loss of life, for a combined total of 282 occupant fatalities. On November 23, 1996, Ethiopian Airlines Flight 961, a 767-200ER, was hijacked and crash-landed in the Indian Ocean near the Comoro Islands after running out of fuel, killing 125 out of the 175 persons on board; survivors have been rare among instances of land-based aircraft ditching on water. Two 767s were involved in the September 11 attacks on the World Trade Center in 2001, resulting in the collapse of its two main towers. American Airlines Flight 11, a 767-200ER, crashed into the North Tower, killing all 92 people on board, and United Airlines Flight 175, a , crashed into the South Tower, with the death of all 65 on board. In addition, more than 2,600 people were killed in the towers or on the ground. A foiled 2001 shoe bomb attempt that December involved an American Airlines 767-300ER.\nOn November 1, 2011, LOT Polish Airlines Flight 16, a 767-300ER, safely landed at Warsaw Chopin Airport in Warsaw, Poland after a mechanical failure of the landing gear forced an emergency landing with the landing gear retracted. There were no injuries, but the aircraft involved was damaged and subsequently written off. At the time of the incident, aviation analysts speculated that it may have been the first instance of a complete landing gear failure in the 767's service history. Subsequent investigation determined that while a damaged hose had disabled the aircraft's primary landing gear extension system, an otherwise functional backup system was inoperative due to an accidentally deactivated circuit breaker.\nIn January 2014, the U.S. Federal Aviation Administration issued a directive that ordered inspections of the elevators on more than 400 767s beginning in March 2014; the focus was on fasteners and other parts that can fail and cause the elevators to jam. The issue was first identified in 2000 and has been the subject of several Boeing service bulletins. The inspections and repairs are required to be completed within six years. The aircraft has also had multiple occurrences of \"uncommanded escape slide inflation\" during maintenance or operations, and during flight. In late 2015, the FAA issued a preliminary directive to address the issue.\nOn October 28, 2016, American Airlines Flight 383, a 767-300ER with 161 passengers and 9 crew members, aborted takeoff at Chicago O'Hare Airport following an uncontained failure of the right GE CF6-80C2 engine. The engine failure, which hurled fragments over a considerable distance, caused a fuel leak, resulting in a fire under the right wing. Fire and smoke entered the cabin. All passengers and crew evacuated the aircraft, with 20 passengers and one flight attendant sustaining minor injuries using the evacuation slides.\nOn February 23, 2019, Atlas Air Flight 3591, a Boeing 767-300ERF air freighter operating for Amazon Air, crashed into Trinity Bay near Houston, Texas, while on descent into George Bush Intercontinental Airport; both pilots and the single passenger were killed. The cause was attributed to pilot error and spatial disorientation.\nRetirement and display.\nAs new 767s roll off the assembly line, older models have been retired and stored or scrapped. One complete aircraft, N102DAthe first to operate for Delta Air Lines and the twelfth example built, is currently on display. It was withdrawn from use and stored at Hartsfield\u2013Jackson Atlanta International Airport in 2006. The exhibition aircraft, named \"The Spirit of Delta\" by the employees who helped purchase it in 1982, underwent restoration at the Delta Flight Museum in Atlanta, Georgia. The restoration was completed in 2010. As of 2020 \"The Spirit of Delta\" N102DA (Ship 102.) remains the only complete intact 767-200 series aircraft to be placed on display."}
{"id": "4166", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=4166", "title": "Bill Walsh (American football coach)", "text": "William Ernest Walsh (November 30, 1931 \u2013 July 30, 2007) was an American professional and college football coach. He served as head coach of the San Francisco 49ers and the Stanford Cardinal, during which time he popularized the West Coast offense. After retiring from the 49ers, Walsh worked as a sports broadcaster for several years and then returned as head coach at Stanford for three seasons.\nWalsh went 102\u201363\u20131 (wins-losses-ties) with the 49ers, winning 10 of his 14 postseason games along with six division titles, three NFC Championship titles, and three Super Bowls. He was named NFL Coach of the Year in 1981 and 1984. In 1993, he was elected to the Pro Football Hall of Fame.\nEarly life.\nBorn in Los Angeles, Walsh played running back in the San Francisco Bay Area for Hayward High School in Hayward.\nWalsh played quarterback at the College of San Mateo for two seasons. (Both John Madden and Walsh played and coached at the College of San Mateo early in their careers.) After playing at the College of San Mateo, Walsh transferred to San Jos\u00e9 State University, where he played tight end and defensive end. He also participated in intercollegiate boxing, winning the golden glove.\nWalsh graduated from San Jose State with a bachelor's degree in physical education in 1955. After two years in the U.S. Army participating on their boxing team, Walsh built a championship team at Washington High School in Fremont before becoming an assistant coach at Cal, Stanford and then the Oakland Raiders in 1966.\nEarly coaching career.\nHe served under Bob Bronzan as a graduate assistant coach on the Spartans football coaching staff and graduated with a master's degree in physical education from San Jose State in 1959. His master's thesis was entitled \"Flank Formation Football -- Stress:: Defense\". Thesis 796.W228f.\nFollowing graduation, Walsh coached the football and swim teams at Washington High School in Fremont, California.\nWalsh was coaching in Fremont when he interviewed for an assistant coaching position with Marv Levy, who had just been hired as the head coach at the University of California, Berkeley.\n\"I was very impressed, individually, by his knowledge, by his intelligence, by his personality, and hired him,\" Levy said. Levy and Walsh, two future NFL Hall of Famers, would never produce a winning season at Cal.\nAfter coaching at Cal, Walsh did a stint at Stanford as an assistant coach, before beginning his pro coaching career.\nEarly professional coaching career.\nWalsh began his pro coaching career in 1966 as an assistant with the AFL's Oakland Raiders. As a Raider assistant, Walsh was trained in the vertical passing offense favored by Al Davis, putting Walsh in Davis' mentor Sid Gillman's coaching tree.\nIn 1967 Walsh was the head coach and general manager of the San Jose Apaches of the Continental Football League (CFL). Walsh led the Apaches to 2nd place in the Pacific Division. Prior to the start of the 1968 CFL season the Apaches ceased all football operations.\nIn 1968, Walsh moved to the AFL expansion Cincinnati Bengals, joining the staff of legendary coach Paul Brown. It was there that Walsh developed the philosophy now known as the \"West Coast Offense\", as a matter of necessity. Cincinnati's new quarterback, Virgil Carter, was known for his great mobility and accuracy but lacked a strong arm necessary to throw deep passes. Thus, Walsh modified the vertical passing scheme he had learned during his time with the Raiders, designing a horizontal passing system that relied on quick, short throws, often spreading the ball across the entire width of the field. The new offense was much better suited to Carter's physical abilities; he led the league in pass completion percentage in 1971.\nWalsh spent eight seasons as an assistant with the Bengals. Ken Anderson eventually replaced Carter as starting quarterback, and together with star wide receiver Isaac Curtis, produced a consistent, effective offensive attack. Initially, Walsh started out as the wide receivers coach from 1968 to 1970 before also coaching the quarterbacks from 1971 to 1975.\nWhen Brown retired as head coach following the 1975 season and appointed Bill \"Tiger\" Johnson as his successor, Walsh resigned and served as an assistant coach for Tommy Prothro with the San Diego Chargers in 1976. In a 2006 interview, Walsh claimed that during his tenure with the Bengals, Brown \"worked against my candidacy\" to be a head coach anywhere in the league. \"All the way through I had opportunities, and I never knew about them,\" Walsh said. \"And then when I left him, he called whoever he thought was necessary to keep me out of the NFL.\"\nIn 1977, Walsh was hired as the head coach at Stanford where he stayed for two seasons. His two Stanford teams were successful, posting a 9\u20133 record in 1977 with a win in the Sun Bowl, and 8\u20134 in 1978 with a win in the Bluebonnet Bowl. His notable players at Stanford included quarterbacks Guy Benjamin and Steve Dils, wide receivers James Lofton and Ken Margerum, linebacker Gordy Ceresino, in addition to running back Darrin Nelson. Walsh was the Pac-8 Conference Coach of the Year in 1977.\n49ers head coach.\nHe was appointed head coach of the San Francisco 49ers on January 9, 1979, one day after both his resignation from Stanford and team owner Edward J. DeBartolo, Jr.'s dismissal of Walsh's predecessor Fred O'Connor and general manager Joe Thomas. The long-suffering 49ers went 2\u201314 in 1978, the season before Walsh's arrival and repeated the same dismal record in his first season. But Walsh got the entire organization to buy into his philosophy and vowed to turn around a miserable situation. He also drafted quarterback Joe Montana from Notre Dame in the third round. Despite their second consecutive 2\u201314 record, the 49ers were playing more competitive football.\nIn 1980, Steve DeBerg was the starting quarterback who got San Francisco off to a 3\u20130 start, but after a 59\u201314 blowout loss to Dallas in week 6, Walsh promoted Montana to starting QB. In a Sunday game, December 7 vs. the New Orleans Saints, Montana brought the 49ers back from a 35\u20137 halftime deficit to win 38\u201335 in overtime. The 49ers improved to 6\u201310, but more importantly, Walsh had them making great strides and they were getting better every week.\n1981 championship.\nIn 1981, key victories were two wins each over the Los Angeles Rams and the Dallas Cowboys. The Rams were only two seasons removed from a Super Bowl appearance, and had dominated the series with the 49ers since 1967, winning 23, losing 3 and tying 1. San Francisco's two wins over the Rams in 1981 marked the shift of dominance in favor of the 49ers that lasted until 1998 with 30 wins (including 17 consecutively) against only 6 defeats. The 49ers blew out the Cowboys in week 6 of the regular season. On \"Monday Night Football\" that week, the win was not included in the halftime highlights. Walsh felt that this was because the Cowboys were scheduled to play the Rams the next week in a Sunday night game and that showing the highlights of the 49ers' win would potentially hurt the game's ratings. However, Walsh used this as a motivating factor for his team, who felt they were disrespected. The 49ers finished the regular season with a 13\u20133 record.\nThe 49ers faced the Cowboys again that same season in the . The game was very close, and in the fourth quarter Walsh called a series of running plays as the 49ers marched down the field against the Cowboys' prevent defense, which had been expecting the 49ers to mainly pass. The 49ers came from behind to win the game on Dwight Clark's touchdown reception, known as The Catch, propelling Walsh to his first Super Bowl. Walsh would later write that the 49ers' two wins over the Rams showed a shift of power in their division, while the wins over the Cowboys showed a shift of power in the conference.\nSan Francisco won its first championship a year removed from back-to-back two-win seasons. The 49ers won Super Bowl XVI, defeating the Cincinnati Bengals 26\u201321 in Pontiac, Michigan. Under Walsh the team rose from the cellar to the top of the NFL in just two seasons.\nThe 49ers won Super Bowl championships in the 1981, 1984 and 1988 seasons. Walsh served as 49ers head coach for 10 years, and during his tenure he and his coaching staff perfected the style of play known popularly as the West Coast offense. Walsh was nicknamed \"The Genius\" for both his innovative play calling and design. Walsh would regularly script the first 10\u201315 offensive plays before the start of each game. In the ten years during which Walsh was the 49ers' head coach, San Francisco scored 3,714 points (24.4 per game), the most of any team in the league during that span.\nIn addition to Joe Montana, Walsh drafted Ronnie Lott, Charles Haley, and Jerry Rice. He also traded a 2nd and 4th round pick in the 1987 draft for Steve Young. His success with the 49ers was rewarded with his election to the Pro Football Hall of Fame in 1993. Montana, Lott, Haley, Rice and Young were also elected to the Hall of Fame.\nCoaching tree.\nProminent assistant coaches.\nMany of Bill Walsh's assistant coaches went on to be head coaches themselves, including George Seifert, Mike Holmgren, Ray Rhodes, and Dennis Green. After Walsh's retirement from the 49ers, Seifert succeeded him as 49ers head coach, and guided San Francisco to victories in Super Bowl XXIV and Super Bowl XXIX. Holmgren won a Super Bowl with the Green Bay Packers, and made 3 Super Bowl appearances as a head coach: 2 with the Packers, and another with the Seattle Seahawks. These coaches in turn have their own disciples who have used Walsh's West Coast system, such as former Denver Broncos head coach Mike Shanahan and former Houston Texans head coach Gary Kubiak. Mike Shanahan was an offensive coordinator under George Seifert and went on to win Super Bowl XXXII and Super Bowl XXXIII during his time as head coach of the Denver Broncos. Kubiak was first a quarterback coach with the 49ers, and then offensive coordinator for Shanahan with the Broncos. In 2015, he became the Broncos' head coach and led Denver to victory in Super Bowl 50. Dennis Green trained Tony Dungy, who won a Super Bowl with the Indianapolis Colts, and Brian Billick with his brother-in law and linebackers coach Mike Smith. Billick won a Super Bowl as head coach of the Baltimore Ravens.\nMike Holmgren trained many of his assistants to become head coaches, including Jon Gruden and Andy Reid. Gruden won a Super Bowl with the Tampa Bay Buccaneers. Reid served as head coach of the Philadelphia Eagles from 1999\u20132012, and guided the Eagles to multiple winning seasons and numerous playoff appearances. Ever since 2013, Reid has served as head coach of the Kansas City Chiefs. He was finally able to win a Super Bowl, when his Chiefs defeated the San Francisco 49ers in Super Bowl LIV. In addition to this, Marc Trestman, former head coach of the Chicago Bears, served as Offensive Coordinator under Seifert in the 90's. Gruden himself would train Mike Tomlin, who led the Pittsburgh Steelers to their sixth Super Bowl championship, and Jim Harbaugh, whose 49ers would face his brother, John Harbaugh, whom Reid himself trained, and the Baltimore Ravens at Super Bowl XLVII, which marked the Ravens' second World Championship.\nBill Walsh was viewed as a strong advocate for African-American head coaches in the NFL and NCAA. Thus, the impact of Walsh also changed the NFL into an equal opportunity for African-American coaches. Along with Ray Rhodes and Dennis Green, Tyrone Willingham became the head coach at Stanford, then later Notre Dame and Washington. One of Mike Shanahan's assistants, Karl Dorrell, went on to be the head coach at UCLA. Walsh directly helped propel Dennis Green into the NFL head coaching ranks by offering to take on the head coaching job at Stanford.\nBill Walsh coaching tree.\nMany former and current NFL head coaches trace their lineage back to Bill Walsh on his coaching tree, shown below. Walsh, in turn, belonged to the coaching tree of American Football League great and Hall of Fame coach Sid Gillman of the AFL's Los Angeles/San Diego Chargers and Hall of Fame coach Paul Brown.\nTree updated through December 9, 2015.\nLater career.\nAfter leaving the coaching ranks immediately following his team's victory in Super Bowl XXIII, Walsh went to work as a broadcaster for NBC, teaming with Dick Enberg to form the lead broadcasting team, replacing Merlin Olsen.\nDuring his time with NBC, rumors began to surface that Walsh would coach again in the NFL. There were at least two known instances.\nFirst, according to a February 2015 article by Mike Florio of NBC Sports, after a 5\u201311 season in 1989, the Patriots fired Raymond Berry and unsuccessfully attempted to lure Walsh to Foxborough to become head coach and general manager. When that failed, New England promoted defensive coordinator Rod Rust; the team split its first two games and then lost 14 straight in 1990.\nSecond, late in the 1990 season, Walsh was rumored to become Tampa Bay's next head coach and general manager after the team fired Ray Perkins and promoted Richard Williamson on an interim basis. Part of the speculation was fueled by the fact that Walsh's contract with NBC, which ran for 1989 and 1990, would soon be up for renewal, to say nothing of the pressure Hugh Culverhouse faced to increase fan support and to fill the seats at Tampa Stadium. However, less than a week after Super Bowl XXV, Walsh not only declined Tampa Bay's offer, but he and NBC agreed on a contract extension. Walsh would continue in his role with NBC for 1991. Meanwhile, after unsuccessfully courting then-recently fired Eagles coach Buddy Ryan or Giants then-defensive coordinator Bill Belichick to man the sidelines for Tampa Bay in 1991, the Bucs stuck with Williamson. Under Williamson's leadership, Tampa Bay won only three games in 1991.\nWalsh did return to Stanford as head coach in 1992, leading the Cardinal to a 10\u20133 record and a Pacific-10 Conference co-championship. Stanford finished the season with an upset victory over Penn State in the Blockbuster Bowl on January 1, 1993 and a #9 ranking in the final AP Poll. In 1994, after consecutive losing seasons, Walsh left Stanford and retired from coaching.\nIn 1996 Walsh returned to the 49ers as an administrative aide Walsh was the Vice President and General Manager for the 49ers from 1999 to 2001 and was a special consultant to the team for three years afterwards.\nIn 2004, Walsh was appointed as special assistant to the athletic director at Stanford. In 2005, after then-athletic director Ted Leland stepped down, Walsh was named interim athletic director. He also acted as a consultant for his alma mater San Jose State University in their search for an Athletic Director and Head Football Coach in 2005.\nWalsh was also the author of three books, a motivational speaker, and taught classes at the Stanford Graduate School of Business.\nWalsh was a Board Member for the Lott IMPACT Trophy, which is named after Pro Football Hall of Fame defensive back Ronnie Lott, and is awarded annually to college football's Defensive IMPACT Player of the Year. Walsh served as a keynote speaker at the award's banquet.\nDeath.\nBill Walsh died of leukemia on July 30, 2007 at his home in Woodside, California.\nFollowing Walsh's death, the playing field at the former Candlestick Park was renamed \"Bill Walsh Field\". Additionally, the regular San Jose State versus Stanford football game was renamed the \"Bill Walsh Legacy Game\".\nFamily.\nWalsh is survived by his wife Geri, his son Craig and his daughter Elizabeth. Walsh had another son, Steve, who died in 2002.\nExternal links.\n "}
{"id": "4167", "revid": "33145", "url": "https://en.wikipedia.org/wiki?curid=4167", "title": "Box-cutter knives", "text": ""}
{"id": "4168", "revid": "35454987", "url": "https://en.wikipedia.org/wiki?curid=4168", "title": "Utility knife", "text": "A utility knife is any of various types of knives used for general or utility purposes. The utility knife was originally a fixed blade knife with a cutting edge suitable for general work such as cutting hides and cordage, scraping hides, butchering animals, cleaning fish, and other tasks. Craft knives are tools mostly used for crafts. Today, the term \"utility knife\" also includes small folding or retractable-and-replaceable-razor-blade knives suited for use in the general workplace or in the construction industry. The latter type is sometimes generically called a Stanley knife, after a prominent brand. \nThere is also a utility knife for kitchen use, which is between a chef's knife and paring knife in size.\nHistory.\nThe fixed-blade utility knife was developed some 5,000 years ago, when human ancestors began to make stone knives. These knives were general-purpose tools, designed for cutting and shaping wooden implements, scraping hides, preparing food, and for other utilitarian purposes.\nBy the 19th century the fixed-blade utility knife had evolved into a steel-bladed outdoors field knife capable of butchering game, cutting wood, and preparing campfires and meals. With the invention of the backspring, pocket-size utility knives were introduced with folding blades and other folding tools designed to increase the utility of the overall design. The folding pocketknife and utility tool is typified by the \"Camper\" or \"Boy Scout\" pocketknife, the U.S. folding utility knife, the Swiss Army Knife, and by multi-tools fitted with knife blades. The development of stronger locking blade mechanisms for folding knives\u2014as with the Spanish navaja, the Opinel, and the Buck 110 Folding Hunter\u2014significantly increased the utility of such knives when employed for heavy-duty tasks such as preparing game or cutting through dense or tough materials.\nContemporary utility knives.\nThe fixed or folding blade utility knife is popular for both indoor and outdoor use. One of the most popular types of workplace utility knife is the retractable or folding utility knife (also known as a \"Stanley knife\", \"box cutter\", \"X-Acto knife\", or by various other names). These types of utility knives are designed as multi-purpose cutting tools for use in a variety of trades and crafts. Designed to be lightweight and easy to carry and use, utility knives are commonly used in factories, warehouses, construction projects, and other situations where a tool is routinely needed to mark cut lines, trim plastic or wood materials, or to cut tape, cord, strapping, cardboard, or other packaging material.\nNames.\nIn British, Australian and New Zealand English, along with Dutch and Austrian German, a utility knife frequently used in the construction industry is known as a \"Stanley knife\". This name is a generic trademark named after Stanley Works, a manufacturer of such knives. In Israel and Switzerland, these knives are known as \"Japanese knives\". In Brazil they are known as \"estiletes\" or \"cortadores Olfa\" (the latter, being another genericised trademark). In Portugal and Canada they are also known as \"X-Acto\" (yet another genericised trademark ). In India, the Philippines, France, Iraq, Italy, Egypt, and Germany, they are simply called \"cutter\". In the Flemish region of Belgium it is called \"cuttermes(je)\" (cutter knife). In general Spanish, they are known as \"cortaplumas\" (penknife, when it comes to folding blades); in Spain, Mexico, and Costa Rica, they are colloquially known as \"cutters\"; in Argentina and Uruguay the segmented fixed-blade knives are known as \"Trinchetas\". In Turkey, they are known as \"maket b\u0131\u00e7a\u011f\u0131\" (which literally translates as \"model knife\").\nOther names for the tool are \"box cutter\" or \"boxcutter\", \"razor blade knife\", \"razor knife\", \"carpet knife\", \"pen knife\", \"stationery knife\", \"sheetrock knife\", or \"drywall knife\".\nDesign.\nUtility knives may use fixed, folding, or retractable or replaceable blades, and come in a wide variety of lengths and styles suited to the particular set of tasks they are designed to perform. Thus, an outdoors utility knife suited for camping or hunting might use a broad fixed blade, while a utility knife designed for the construction industry might feature a replaceable utility or razor blade for cutting packaging, cutting shingles, marking cut lines, or scraping paint.\nFixed blade utility knife.\nLarge fixed-blade utility knives are most often employed in an outdoors context, such as fishing, camping, or hunting. Outdoor utility knives typically feature sturdy blades from in length, with edge geometry designed to resist chipping and breakage.\nThe term \"utility knife\" may also refer to small fixed-blade knives used for crafts, model-making and other artisanal projects. These small knives feature light-duty blades best suited for cutting thin, lightweight materials. The small, thin blade and specialized handle permit cuts requiring a high degree of precision and control.\nWorkplace utility knives.\nThe largest construction or workplace utility knives typically feature retractable and replaceable blades, made of either die-cast metal or molded plastic. Some use standard razor blades, others specialized double-ended utility blades. The user can adjust how far the blade extends from the handle, so that, for example, the knife can be used to cut the tape sealing a package without damaging the contents of the package. When the blade becomes dull, it can be quickly reversed or switched for a new one. Spare or used blades are stored in the hollow handle of some models, and can be accessed by removing a screw and opening the handle. Other models feature a quick-change mechanism that allows replacing the blade without tools, as well as a flip-out blade storage tray. The blades for this type of utility knife come in both double- and single-ended versions, and are interchangeable with many, but not all, of the later copies. Specialized blades also exist for cutting string, linoleum, and other materials.\nAnother style is a snap-off utility knife that contains a long, segmented blade that slides out from it. As the endmost edge becomes dull, it can be broken off the remaining blade, exposing the next section, which is sharp and ready for use. The snapping is best accomplished with a blade snapper that is often built-in, or a pair of pliers, and the break occurs at the score lines, where the metal is thinnest. When all of the individual segments are used, the knife may be thrown away, or, more often, refilled with a replacement blade. This design was introduced by Japanese manufacturer Olfa Corporation in 1956 as the world's first snap-off blade and was inspired from analyzing the sharp cutting edge produced when glass is broken and how pieces of a chocolate bar break into segments. The sharp cutting edge on these knives is not on the edge where the blade is snapped off; rather one long edge of the whole blade is sharpened, and there are scored diagonal breakoff lines at intervals down the blade. Thus each snapped-off piece is roughly a parallelogram, with each long edge being a breaking edge, and one or both of the short ends being a sharpened edge.\nAnother utility knife often used for cutting open boxes consists of a simple sleeve around a rectangular handle into which single-edge utility blades can be inserted. The sleeve slides up and down on the handle, holding the blade in place during use and covering the blade when not in use. The blade holder may either retract or fold into the handle, much like a folding-blade pocketknife. The blade holder is designed to expose just enough edge to cut through one layer of corrugated fibreboard, to minimize chances of damaging contents of cardboard boxes.\nUse as weapon.\nMost utility knives are not well suited to use as offensive weapons, with the exception of some outdoor-type utility knives employing longer blades. However, even small razor-blade type utility knives may sometimes find use as slashing weapons. The 9-11 commission report stated passengers in cell phone calls reported knives or \"box-cutters\" were used as weapons (also Mace or a bomb) in hi-jacking airplanes in the September 11, 2001 terrorist attacks against the United States, though the exact design of the knives used is unknown. Two of the hijackers were known to have purchased Leatherman knives, which feature a 4\" slip-joint blade which were not prohibited on U.S. flights at the time. Those knives were not found in the possessions the two hijackers left behind. Similar cutters, including paper cutters, have also been known to be used as a lethal weapon.\nSmall work-type utility knives have also been used to commit robbery and other crimes. In June 2004, a Japanese student was slashed to death with a segmented-type utility knife.\nIn the United Kingdom, the law was changed (effective 1 October 2007) to raise the age limit for purchasing knives, including utility knives, from 16 to 18, and to make it illegal to carry a utility knife in public without a good reason."}
{"id": "4169", "revid": "1650719", "url": "https://en.wikipedia.org/wiki?curid=4169", "title": "Bronze", "text": "Bronze is an alloy consisting primarily of copper, commonly with about 12\u201312.5% tin and often with the addition of other metals (such as aluminium, manganese, nickel or zinc) and sometimes non-metals or metalloids such as arsenic, phosphorus or silicon. These additions produce a range of alloys that may be harder than copper alone, or have other useful properties, such as strength, ductility, or machinability.\nThe archeological period in which bronze was the hardest metal in widespread use is known as the Bronze Age. The beginning of the Bronze Age in India and western Eurasia is conventionally dated to the mid-4th millennium BC, and to the early 2nd millennium BC in China; elsewhere it gradually spread across regions. The Bronze Age was followed by the Iron Age starting from about 1300 BC and reaching most of Eurasia by about 500 BC, although bronze continued to be much more widely used than it is in modern times.\nBecause historical pieces were often made of brasses (copper and zinc) and bronzes with different compositions, modern museum and scholarly descriptions of older objects increasingly use the generalized term \"copper alloy\" instead.\nEtymology.\nThe word \"bronze\" (1730\u201340) is borrowed from Middle French (1511), itself borrowed from Italian 'bell metal, brass' (13th century, transcribed in Medieval Latin as ) from either:\nHistory.\nThe discovery of bronze enabled people to create metal objects that were harder and more durable than previously possible. Bronze tools, weapons, armor, and building materials such as decorative tiles were harder and more durable than their stone and copper (\"Chalcolithic\") predecessors. Initially, bronze was made out of copper and arsenic, forming arsenic bronze, or from naturally or artificially mixed ores of copper and arsenic, with the earliest artifacts so far known coming from the Iranian plateau in the 5th millennium BC. It was only later that tin was used, becoming the major non-copper ingredient of bronze in the late 3rd millennium BC.\nTin bronze was superior to arsenic bronze in that the alloying process could be more easily controlled, and the resulting alloy was stronger and easier to cast. Also, unlike arsenic, metallic tin and fumes from tin refining are not toxic. The earliest tin-alloy bronze dates to 4500 BC in a Vin\u010da culture site in Plo\u010dnik (Serbia). Other early examples date to the late 4th millennium BC in Egypt, Susa (Iran) and some ancient sites in China, Luristan (Iran) and Mesopotamia (Iraq).\nOres of copper and the far rarer tin are not often found together (exceptions include Cornwall in Britain, one ancient site in Thailand and one in Iran), so serious bronze work has always involved trade. Tin sources and trade in ancient times had a major influence on the development of cultures. In Europe, a major source of tin was the British deposits of ore in Cornwall, which were traded as far as Phoenicia in the eastern Mediterranean.\nIn many parts of the world, large hoards of bronze artifacts are found, suggesting that bronze also represented a store of value and an indicator of social status. In Europe, large hoards of bronze tools, typically socketed axes (illustrated above), are found, which mostly show no signs of wear. With Chinese ritual bronzes, which are documented in the inscriptions they carry and from other sources, the case is clear. These were made in enormous quantities for elite burials, and also used by the living for ritual offerings.\nTransition to iron.\nThough bronze is generally harder than wrought iron, with Vickers hardness of 60\u2013258 vs. 30\u201380, the Bronze Age gave way to the Iron Age after a serious disruption of the tin trade: the population migrations of around 1200\u20131100 BC reduced the shipping of tin around the Mediterranean and from Britain, limiting supplies and raising prices. As the art of working in iron improved, iron became cheaper and improved in quality. As cultures advanced from hand-wrought iron to machine-forged iron (typically made with trip hammers powered by water), blacksmiths learned how to make steel. Steel is stronger than bronze and holds a sharper edge longer.\nBronze was still used during the Iron Age, and has continued in use for many purposes to the modern day.\nComposition.\nThere are many different bronze alloys, but typically modern bronze is 88% copper and 12% tin. Alpha bronze consists of the alpha solid solution of tin in copper. Alpha bronze alloys of 4\u20135% tin are used to make coins, springs, turbines and blades. Historical \"bronzes\" are highly variable in composition, as most metalworkers probably used whatever scrap was on hand; the metal of the 12th-century English Gloucester Candlestick is bronze containing a mixture of copper, zinc, tin, lead, nickel, iron, antimony, arsenic with an unusually large amount of silver \u2013 between 22.5% in the base and 5.76% in the pan below the candle. The proportions of this mixture suggest that the candlestick was made from a hoard of old coins. The Benin Bronzes are in fact brass, and the Romanesque Baptismal font at St Bartholomew's Church, Li\u00e8ge is described as both bronze and brass.\nIn the Bronze Age, two forms of bronze were commonly used: \"classic bronze\", about 10% tin, was used in casting; and \"mild bronze\", about 6% tin, was hammered from ingots to make sheets. Bladed weapons were mostly cast from classic bronze, while helmets and armor were hammered from mild bronze.\nCommercial bronze (90% copper and 10% zinc) and architectural bronze (57% copper, 3% lead, 40% zinc) are more properly regarded as brass alloys because they contain zinc as the main alloying ingredient. They are commonly used in architectural applications.\nPlastic bronze contains a significant quantity of lead, which makes for improved plasticity possibly used by the ancient Greeks in their ship construction.\n has a composition of Si: 2.80\u20133.80%, Mn: 0.50\u20131.30%, Fe: 0.80% max., Zn: 1.50% max., Pb: 0.05% max., Cu: balance.\nOther bronze alloys include aluminium bronze, phosphor bronze, manganese bronze, bell metal, arsenical bronze, speculum metal and cymbal alloys.\nProperties.\nBronzes are typically ductile alloys, considerably less brittle than cast iron. Typically bronze oxidizes only superficially; once a copper oxide (eventually becoming copper carbonate) layer is formed, the underlying metal is protected from further corrosion. This can be seen on statues from the Hellenistic period. However, if copper chlorides are formed, a corrosion-mode called \"bronze disease\" will eventually completely destroy it. Copper-based alloys have lower melting points than steel or iron and are more readily produced from their constituent metals. They are generally about 10 percent denser than steel, although alloys using aluminium or silicon may be slightly less dense. Bronze is a better conductor of heat and electricity than most steels. The cost of copper-base alloys is generally higher than that of steels but lower than that of nickel-base alloys.\nCopper and its alloys have a huge variety of uses that reflect their versatile physical, mechanical, and chemical properties. Some common examples are the high electrical conductivity of pure copper, low-friction properties of bearing bronze (bronze that has a high lead content\u2014 6\u20138%), resonant qualities of bell bronze (20% tin, 80% copper), and resistance to corrosion by seawater of several bronze alloys.\nThe melting point of bronze varies depending on the ratio of the alloy components and is about . Bronze is usually nonmagnetic, but certain alloys containing iron or nickel may have magnetic properties.\nUses.\nBronze, or bronze-like alloys and mixtures, were used for coins over a longer period. Bronze was especially suitable for use in boat and ship fittings prior to the wide employment of stainless steel owing to its combination of toughness and resistance to salt water corrosion. Bronze is still commonly used in ship propellers and submerged bearings.\nIn the 20th century, silicon was introduced as the primary alloying element, creating an alloy with wide application in industry and the major form used in contemporary statuary. Sculptors may prefer silicon bronze because of the ready availability of silicon bronze brazing rod, which allows colour-matched repair of defects in castings. Aluminium is also used for the structural metal aluminium bronze.\nBronze parts are tough and typically used for bearings, clips, electrical connectors and springs.\nBronze also has low friction against dissimilar metals, making it important for cannons prior to modern tolerancing, where iron cannonballs would otherwise stick in the barrel. It is still widely used today for springs, bearings, bushings, automobile transmission pilot bearings, and similar fittings, and is particularly common in the bearings of small electric motors. Phosphor bronze is particularly suited to precision-grade bearings and springs. It is also used in guitar and piano strings.\nUnlike steel, bronze struck against a hard surface will not generate sparks, so it (along with beryllium copper) is used to make hammers, mallets, wrenches and other durable tools to be used in explosive atmospheres or in the presence of flammable vapors. Bronze is used to make bronze wool for woodworking applications where steel wool would discolour oak.\nPhosphor bronze is used for ships' propellers, musical instruments, and electrical contacts. Bearings are often made of bronze for its friction properties. It can be impregnated with oil to make the proprietary Oilite and similar material for bearings. Aluminium bronze is hard and wear-resistant, and is used for bearings and machine tool ways.\nSculptures.\nBronze is widely used for casting bronze sculptures. Common bronze alloys have the unusual and desirable property of expanding slightly just before they set, thus filling the finest details of a mould. Then, as the bronze cools, it shrinks a little, making it easier to separate from the mould.\nThe Assyrian king Sennacherib (704\u2013681 BC) claims to have been the first to cast monumental bronze statues (of up to 30 tonnes) using two-part moulds instead of the lost-wax method.\nBronze statues were regarded as the highest form of sculpture in Ancient Greek art, though survivals are few, as bronze was a valuable material in short supply in the Late Antique and medieval periods. Many of the most famous Greek bronze sculptures are known through Roman copies in marble, which were more likely to survive.\nIn India, bronze sculptures from the Kushana (Chausa hoard) and Gupta periods (Brahma from Mirpur-Khas, Akota Hoard, Sultanganj Buddha) and later periods (Hansi Hoard) have been found. Indian Hindu artisans from the period of the Chola empire in Tamil Nadu used bronze to create intricate statues via the lost-wax casting method with ornate detailing depicting the deities of Hinduism. The art form survives to this day, with many silpis, craftsmen, working in the areas of Swamimalai and Chennai.\nIn antiquity other cultures also produced works of high art using bronze. For example: in Africa, the bronze heads of the Kingdom of Benin; in Europe, Grecian bronzes typically of figures from Greek mythology; in east Asia, Chinese ritual bronzes of the Shang and Zhou dynasty\u2014more often ceremonial vessels but including some figurine examples. Bronze sculptures, although known for their longevity, still undergo microbial degradation; such as from certain species of yeasts.\nBronze continues into modern times as one of the materials of choice for monumental statuary.\nMirrors.\nBefore it became possible to produce glass with acceptably flat surfaces, bronze was a standard material for mirrors. The reflecting surface was typically made slightly convex so that the whole face could be seen in a small mirror. Bronze was used for this purpose in many parts of the world, probably based on independent discoveries.\nBronze mirrors survive from the Egyptian Middle Kingdom (2040\u20131750 BC). In Europe, the Etruscans were making bronze mirrors in the sixth century BC, and Greek and Roman mirrors followed the same pattern. Although other materials such as speculum metal had come into use, bronze mirrors were still being made in Japan in the eighteenth century AD.\nMusical instruments.\nBronze is the preferred metal for bells in the form of a high tin bronze alloy known colloquially as bell metal, which is about 23% tin.\nNearly all professional cymbals are made from bronze, which gives a desirable balance of durability and timbre. Several types of bronze are used, commonly B20 bronze, which is roughly 20% tin, 80% copper, with traces of silver, or the tougher B8 bronze made from 8% tin and 92% copper. As the tin content in a bell or cymbal rises, the timbre drops.\nBronze is also used for the windings of steel and nylon strings of various stringed instruments such as the double bass, piano, harpsichord, and guitar. Bronze strings are commonly reserved on pianoforte for the lower pitch tones, as they possess a superior sustain quality to that of high-tensile steel.\nBronzes of various metallurgical properties are widely used in struck idiophones around the world, notably bells, singing bowls, gongs, cymbals, and other idiophones from Asia. Examples include Tibetan singing bowls, temple bells of many sizes and shapes, gongs, Javanese gamelan, and other bronze musical instruments. The earliest bronze archeological finds in Indonesia date from 1\u20132 BC, including flat plates probably suspended and struck by a wooden or bone mallet. Ancient bronze drums from Thailand and Vietnam date back 2,000 years. Bronze bells from Thailand and Cambodia date back to 3,600 BC.\nSome companies are now making saxophones from phosphor bronze (3.5 to 10% tin and up to 1% phosphorus content). Bell bronze/B20 is used to make the tone rings of many professional model banjos. The tone ring is a heavy (usually 3\u00a0lbs.) folded or arched metal ring attached to a thick wood rim, over which a skin, or most often, a plastic membrane (or head) is stretched \u2013 it is the bell bronze that gives the banjo a crisp powerful lower register and clear bell-like treble register.\nBiblical references.\nThere are 139 references to bronze in the Bible, such as the account of Moses holding up a bronze snake on a pole in the Book of Numbers, chapter 21. \nCoins and medals.\nBronze has also been used in coins; most \u201ccopper\u201d coins are actually bronze, with about 4 percent tin and 1 percent zinc.\nAs with coins, bronze has been used in the manufacture of various types of medals for centuries, and are known in contemporary times for being awarded for third place in sporting competitions and other events. The later usage was in part attributed to the choices of gold, silver and bronze to represent the first three Ages of Man in Greek mythology: the Golden Age, when men lived among the gods; the Silver age, where youth lasted a hundred years; and the Bronze Age, the era of heroes, and was first adopted at the 1904 Summer Olympics. At the 1896 event, silver was awarded to winners and bronze to runners-up, while at 1900 other prizes were given rather than medals."}
{"id": "4170", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=4170", "title": "Benelux", "text": "The Benelux Union (; ; ), also known as simply Benelux, is a politico-economic union and formal international intergovernmental cooperation of three neighbouring states in western Europe: Belgium, the Netherlands, and Luxembourg. The name \"Benelux\" is a portmanteau formed from joining the first few letters of each country's name\u2014Belgium, Netherlands, Luxembourg\u2014and was first used to name the customs agreement that initiated the union (signed in 1944). It is now used more generally to refer to the geographic, economic, and cultural grouping of the three countries.\nThe Benelux is an economically dynamic and densely populated region, with 5.6% of the European population (29.2 million residents) and 7.9% of the joint EU GDP (\u20ac36,000/resident) on no more than 1.7% of the whole surface of the EU.\nSome examples of results of Benelux cooperation: automatic level recognition of all diplomas and degrees within the Benelux, a new Benelux Treaty on Police cooperation, common road inspections and a Benelux pilot with digital consignment notes.\nThe main institutions of the Union are the Committee of Ministers, the Council of the Union, the General Secretariat, the Interparliamentary Consultative Council and the Benelux Court of Justice while the Benelux Office for Intellectual Property cover the same land but are not part of the Benelux Union.\nThe Benelux General Secretariat is located in Brussels. It is the central platform of the Benelux Union cooperation. It handles the secretariat of the Committee of Ministers, the Council of Benelux Union and the sundry committees and working parties. The General Secretariat provides day-to-day support for the Benelux cooperation on the substantive, procedural, diplomatic and logistical levels. The Secretary-General is Alain de Muyser from Luxembourg and there are two deputies: Deputy Secretary-General Frank Weekers from the Netherlands and Deputy Secretary-General Rudolf Huygelen from Belgium.\nThe presidency of the Benelux is held in turn by the three countries for a period of one year. Belgium holds the presidency for 2021.\nOverview.\nCooperation among the governments of Belgium, the Netherlands, and Luxembourg has been a firmly established practice since the introduction of a customs union in 1944 which became operative in 1948 as the Benelux Customs Union. The initial form of economic cooperation expanded steadily over time, leading in 1958 to the signing of the treaty establishing the Benelux Economic Union. Initially, the purpose of cooperation among the three partners was to put an end to customs barriers at their borders and ensure free movement of persons, goods and services among the three countries. It was the first example of international economic integration in Europe since the Second World War. The three countries therefore foreshadowed and provided the model for future European integration, such as the European Coal and Steel Community, the European Economic Community (EEC), and the European Community\u2013European Union (EC\u2013EU). The three partners continue to play this pioneering role. They also launched the Schengen process, which came into operation in 1985, promoting it from the outset. Benelux cooperation has been constantly adapted and now goes much further than mere economic cooperation, extending to new and topical policy areas connected with security, sustainable development, and the economy. Benelux models its cooperation on that of the European Union and is able to take up and pursue original ideas. The Benelux countries also work together in the so-called Pentalateral Energy Forum, a regional cooperation group formed of five members\u2014the Benelux states, France, Germany, Austria, and Switzerland. Formed ten years the ministers for energy from the various countries represent a total of 200 million residents and 40% of the European electricity network. As of November 2019, the Benelux Union has a population of more than 29.55 million.\nOn 17 June 2008, Belgium (in all its component parts), the Netherlands, and Luxembourg signed a new Benelux treaty in The Hague. The purpose of the Benelux Union is to deepen and expand cooperation among the three countries so that it can continue its role as precursor within the European Union and strengthen and improve cross-border cooperation at every level. Through better cooperation between the countries the Benelux strives to promote the prosperity and welfare of the citizens of Belgium, the Netherlands and Luxembourg.\nBenelux works together on the basis of an annual plan embedded in a four-year joint work programme.\nBenelux seeks region-to-region cooperation, be it with France and Germany (North Rhine-Westphalia) or beyond with the Baltic States, the Nordic Council, the Visegrad countries, or even further. In 2018 a renewed political declaration was adopted between Benelux and North Rhine-Westphalia to give cooperation a further impetus.\nSome examples of recent results of Benelux cooperation: automatic level recognition of all diplomas and degrees within the Benelux, a new Benelux Treaty on Police cooperation, common road inspections and a Benelux pilot with digital consignment notes. The Benelux is also committed to working together on adaptation to climate change. On 5 June 2018 the Benelux Treaty celebrated its 60 years of existence. In 2018, a Benelux Youth Parliament was created.\nIn addition to cooperation based on a Treaty, there is also political cooperation in the Benelux context, including summits of the Benelux government leaders. In 2019 a Benelux summit was held in Luxembourg. In 2020 a virtual Benelux Summit was held under Dutch Presidency on 7 October between the prime ministers.\nPolitics.\nA Benelux Parliament (officially referred to as an \"Interparliamentary Consultative Council\") was created in 1955. This parliamentary assembly is composed of 21 members of the Dutch parliament, 21 members of the Belgian national and regional parliaments, and 7 members of the Luxembourg parliament. On 20 January 2015, the governments of the three countries, including, as far as Belgium is concerned, the community and regional governments, signed in Brussels the Treaty of the Benelux Interparliamentary Assembly. This treaty entered into force on 1 August 2019. This made the 1955 Convention on the Consultative Interparliamentary Council for the Benelux to expire. Moreover, the current official name has been largely obsolete in daily practice for a number of years: both internally in the Benelux and in external references, the name Benelux Parliament has been used \"de facto\" for a number of years now.\nIn 1944, exiled representatives of the three countries signed the London Customs Convention, the treaty that established the Benelux Customs Union. Ratified in 1947, the treaty was in force from 1948 until it was superseded by the Benelux Economic Union. The treaty establishing the Benelux Economic Union (\"Benelux Economische Unie\", \"Union \u00c9conomique Benelux\") was signed on 3 February 1958 in The Hague and came into force on 1 November 1960 to promote the free movement of workers, capital, services, and goods in the region. Under the Treaty the Union implies the co-operation of economic, financial and social policies.\nCooperation with other Geopolitic-regions.\nIn 2017 the members of the Benelux, the Baltic Assembly, three members of the Nordic Council (Sweden, Denmark and Finland), and all the other countries EU member states, sought to increase cooperation in the Digital Single Market, as well as discussing social matters, the Economic and Monetary Union of the European Union, the European migrant crisis and defence cooperation. Relations with Russia, Turkey and the United Kingdom were also on the agenda.\nSince 2008 the Benelux Union works together with the German Land (state) North Rhine-Westphalia.\nIn 2018 Benelux Union signed a declaration with France to strengthen cross-border cooperation.\nBenelux legal instruments.\nThe Benelux Union involves intergovernmental cooperation.\nThe Treaty establishing the Benelux Union explicitly provides that the Benelux Committee of Ministers can resort to four legal instruments (art. 6, paragraph 2, under a), f), g) and h)):\n1. Decisions\nDecisions are legally binding regulations for implementing the Treaty establishing the Benelux Union or other Benelux treaties.\nTheir legally binding force concerns the Benelux states (and their sub-state entities), which have to implement them. However, they have no direct effect towards individual citizens or companies (notwithstanding any indirect protection of their rights based on such decisions as a source of international law). Only national provisions implementing a decision can directly create rights and obligations for citizens or companies.\n2. Agreements\nThe Committee of Ministers can draw up agreements, which are then submitted to the Benelux states (and/or their sub-state entities) for signature and subsequent parliamentary ratification. These agreements can deal with any subject matter, also in policy areas that are not yet covered by cooperation in the framework of the Benelux Union.\nThese are in fact traditional treaties, with the same direct legally binding force towards both authorities and citizens or companies. The negotiations do however take place in the established context of the Benelux working groups and institutions, rather than on an ad hoc basis.\n3. Recommendations\nRecommendations are non-binding orientations, adopted at ministerial level, which underpin the functioning of the Benelux Union. These (policy) orientations may not be legally binding, but given their adoption at the highest political level and their legal basis vested directly in the Treaty, they do entail a strong moral obligation for any authority concerned in the Benelux countries.\n4. Directives\nDirectives of the Committee of Ministers are mere inter-institutional instructions towards the Benelux Council and/or the Secretariat-General, for which they are binding. This instrument has so far only been used occasionally, basically in order to organise certain activities within a Benelux working group or to give them impetus.\nAll four instruments require the unanimous approval of the members of the Committee of Ministers (and, in the case of agreements, subsequent signature and ratification at national level).\nIn 1965, the treaty establishing a Benelux Court of Justice was signed. It entered into force in 1974. The Court, composed of judges from the highest courts of the three States, has to guarantee the uniform interpretation of common legal rules. This international judicial institution is located in Luxembourg.\nThe Benelux is particularly active in the field of intellectual property. The three countries established a Benelux Trademarks Office and a Benelux Designs Office, both situated in The Hague. In 2005, they concluded a treaty establishing a \"Benelux Organisation for Intellectual Property\" which replaced both offices upon its entry into force on 1 September 2006. This Organisation is the official body for the registration of trademarks and designs in the Benelux. In addition, it offers the possibility to formally record the existence of ideas, concepts, designs, prototypes and the like.\nAll higher education degrees recognised throughout Benelux.\nIn 2018 the education ministers from Belgium's three communities as well as those from the Netherlands and Luxembourg signed an agreement to recognise the level of all higher education diplomas between the three countries, a unique development in the EU. To continue studies or get a job in another country, applicants must have their locally earned degree recognised by the other country, which entails a lot of paperwork, fees and sometimes a months-long wait. In 2015, the Benelux countries agreed to recognise each other's bachelor's and master's diplomas without such hindrances. Now, recognition is extended to PhDs and to so-called graduate degrees, which are earned from adult educational institutions. This means that a graduate of any of the three countries can continue their education or seek a job in the other countries without having to have their degree officially recognised.\nNew Benelux Treaty on police cooperation.\nThe Belgian Minister of Security and Home Affairs, Jan Jambon, the Belgian Minister of Justice, Koen Geens, the Dutch Minister of Justice and Security, Ferdinand Grapperhaus, the Luxembourg Minister of Homeland Security, Etienne Schneider and the Luxembourg Minister of Justice, F\u00e9lix Braz, signed in 2018 a new Benelux police treaty, which will improve the exchange of information, create more opportunities for cross-border action and facilitate police investigations in the neighbouring country. In 2004, a Treaty on cross-border cooperation between the Benelux police forces was concluded. This has been completely revised and expanded. The Benelux countries are at the forefront of the European Union in this respect.\nThis new Treaty will allow direct access to each other's police databases on the basis of hit/no hit. In addition, direct consultation of police databases will be possible during joint operations and in common police stations. It will also be possible to consult population registers within the limits of national legislation. In the future, ANPR (Automatic Number Plate Recognition) camera data, which play an increasingly important role in the fight against crime, can be exchanged between the Benelux countries in accordance with their own applicable law. Police and judicial authorities will also work more closely with local authorities to exchange information on organised crime in a more targeted way (administrative approach) in accordance with national law.\nThe Treaty makes cross-border pursuit a lot easier and broadens the investigative powers of Benelux police officers. For example, it will be possible to continue a lawful hot pursuit in one's own country across the border, without the thresholds for criminal offences that characterise the current regulation. Another new feature of the Treaty is that a police officer can, under certain conditions, carry out cross-border investigations.\nThe existing intensive cooperation in the field of police liaison officers, joint patrols and checks as well as the provision of assistance at major events will be maintained. In addition, the possibilities for cross-border escort and surveillance missions and for operating on international trains will be considerably extended.\nIn the event of a crisis situation, special police units will now be able to act across borders; this can also be used to support important events with a high security risk, such as a NATO Summit.\nAfter approval by the parliaments, and the elaboration of implementation agreements, the new Benelux Police Treaty will enter into force.\nBenelux Treaty of Li\u00e8ge: joint Benelux road transport inspections.\nThe Treaty of Li\u00e8ge entered into force in 2017. As a result, Dutch, Belgian and Luxembourg inspectors may carry out joint inspections of trucks and buses in the three countries. This treaty was signed in 2014 in Li\u00e8ge (Belgium) by the three countries. In the meantime, on the basis of a transitional regime and pending the entry into force of the Treaty, several major Benelux road transport inspections have taken place. Under this transition regime, inspectors from neighbouring countries could only act as observers. Now they can exercise all of their skills.\nCo-operation on the basis of this Benelux Treaty leads to a more uniform control of road transport, cost reductions, more honest competition between transport companies and better working conditions for drivers. In addition, this cooperation strengthens general road safety in the three countries.\nThe Benelux Treaty seeks to intensify cooperation by improving the existing situation through intensive harmonisation of controls, exchange of equipment and training of personnel in order to reduce costs and by allowing inspectors of a country to participate in Inspections in another Benelux country by exercising all their powers, which in particular enables the expertise of the specialists in each country to be obtained. In so doing, they are fully committed to road safety for citizens and create a level playing field, so that entrepreneurs inside and outside the Benelux must comply with the same rules of control.\nThe application of the Treaty of Li\u00e8ge allows the three Benelux countries to play the role of forerunners in Europe. In addition, the treaty expressly provides for the possibility of accession of other countries.\nBy June 2019 already a total of 922 vehicles were subject to common Benelux inspections.\nBenelux pilot project with digital consignment notes.\nA Benelux-wide pilot project was launched in 2017 to enable the use of digital consignment notes (e-CMR) for national and intra-Benelux transport. The switch to e-CMR in the Benelux offers possible savings of \u20ac4.50 per consignment. With an annual figure of around 65 million consignment notes used, this represents overall savings of close to \u20ac300 million per year. With this operation, the Benelux countries are testing the operation of the digital consignment note ( from a control perspective). They will share findings with the European Union.\nBenelux enhances cross-border mobility.\nCurrently 37% of the total number of EU frontier workers work in the Benelux and surrounding areas. 35,000 Belgian citizens work in Luxembourg, while 37,000 Belgian citizens cross the border to work in the Netherlands each day. In addition, 12,000 Dutch and close to a thousand Luxembourg residents work in Belgium. As regards cross-border professional mobility the Benelux is undoubtedly a hot spot in Europe.\nAs the European pioneer, the Benelux cooperation takes varied action to remove the existing obstacles and efforts are made to provide frontier workers in the entire Benelux border area with information about their rights and obligations in a reliable and accessible manner through a specially designed website http://startpuntgrensarbeid.benelux.int\nThe Benelux countries aim at making the labour and services market 'border neutral' so as to help towards smoother searches for suitable jobs in a neighbouring country. The Benelux cooperation seeks to achieve this through improved access to information about social rights and fiscal issues for employees and employers on either side of the borders.\nBenelux countries take the lead in stimulating European cycling policy.\nIn a joint political declaration (July 2020), the mobility ministers of the Benelux countries called on the European Commission to prioritise cycling in European climate policy and Sustainable Transport strategies. They call on the commission to co-finance the construction of cycling infrastructure and to provide funds to stimulate cycling policy as part of the European Green Deal.\nThe COVID-19 crisis has had a massive impact on the state of mobility in Europe. During the lockdown period, cycle use increased in almost every European country. The (increased) use of this sustainable form of transport is not just essential if the EU is to achieve its climate objectives by 2050, but also has a positive impact on public health and the economy in the EU. Cycling in Europe brings \u20ac150 billion in benefits, of which \u20ac90 billion are linked to the environment, health and the mobility system. The cycle industry already provides hundreds of thousands of jobs and annual revenue from cycle tourism in the EU is estimated at \u20ac44 billion.\nIn their statement the ministers stress that the provision of safe, high quality cycling infrastructure and secureycle parking is essential to further stimulate cycle use. Further European research is also needed to map out the potential for cycling post COVID-19.\nWith this declaration, the mobility ministers of the Benelux are also calling on other EU Member States to provide the European Commission with up-to-date data on active mobility, which is not currently collected at EU level. They also call on them to make adequate funding available for cycling projects in their COVID-19 recovery plans and to take cycling into account in tourism and road policy. They ask regional and local authorities to expand networks of cyclepaths, to promote cycling campaigns and to arrange cycle sharing schemes during the summer months.\nRenewal of the agreement.\nThe Treaty between the Benelux countries establishing the Benelux Economic Union was limited to a period of 50 years. During the following years, and even more so after the creation of the European Union, the Benelux cooperation focused on developing other fields of activity within a constantly changing international context.\nAt the end of the 50 years, the governments of the three Benelux countries decided to renew the agreement, taking into account the new aspects of the Benelux-cooperation \u2013 such as security \u2013 and the new federal government structure of Belgium. The original establishing treaty, set to expire in 2010, was replaced by a new legal framework (called the Treaty revising the Treaty establishing the Benelux Economic Union), which was signed on 17 June 2008.\nThe new treaty has no set time limit and the name of the \"Benelux Economic Union\" changed to \"Benelux Union\" to reflect the broad scope on the union. The main objectives of the treaty are the continuation and enlargement of the cooperation between the three member states within a larger European context. The renewed treaty explicitly foresees the possibility that the Benelux countries will cooperate with other European member States or with regional cooperation structures. The new Benelux cooperation focuses on three main topics: internal market and economic union, sustainability, justice and internal affairs. The number of structures in the renewed Treaty has been reduced and thus simplified. Five Benelux institutions remain: the Benelux Committee of Ministers, the Benelux Council, the Benelux Parliament, the Benelux Court of Justice, the Benelux Secretariat General. Beside these five institutions, the Benelux Organisation for Intellectual Property is also present in this Treaty as an independent organisation.\nBenelux Committee of Ministers:\nThe Committee of Ministers is the supreme decision-making body of the Benelux. It includes at least one representative at ministerial level from the three countries. Its composition varies according to its agenda. The ministers determine the orientations and priorities of Benelux cooperation. The presidency of the Committee rotates between the three countries on an annual basis.\nBenelux Council:\nThe council is composed of senior officials from the relevant ministries. Its composition varies according to its agenda. The council's main task is to prepare the dossiers for the ministers.\nBenelux InterParliamentary Consultative Council:\nThe Benelux Parliament comprises 49 representatives from the parliaments of Belgium, the Netherlands and Luxembourg. Its members inform and advise their respective governments on all Benelux matters.\nBenelux Court of Justice:\nThe Benelux Court of Justice is an international court. Its mission is to promote uniformity in the application of Benelux legislation. When faced with difficulty interpreting a common Benelux legal rule, national courts must seek an interpretive ruling from the Benelux Court, which subsequently renders a binding decision. The members of the Court are appointed from among the judges of the 'Cour de cassation' of Belgium, the 'Hoge Raad of the Netherlands' and the 'Cour de cassation' of Luxembourg.\nBenelux General Secretariat:\nThe General Secretariat, which is based in Brussels, forms the cooperation platform of the Benelux Union. It acts as the secretariat of the Committee of Ministers, the council and various commissions and working groups. Because the General Secretariat operates under strict neutrality, it is perfectly placed to build bridges between the various partners and stakeholders. The General Secretariat has years of expertise in the area of Benelux cooperation and is familiar with the policy agreements and differences between the three countries. Building on what already been achieved, the General Secretariat puts its knowledge, network and experience at the service of partners and stakeholders who endorse its mission. It initiates, supports and monitors cooperation results in the areas of economy, sustainability and security. In a greatly enlarged European Union, Benelux cooperation is a source of inspiration for Europe."}
{"id": "4171", "revid": "450380", "url": "https://en.wikipedia.org/wiki?curid=4171", "title": "Boston Herald", "text": "The Boston Herald is an American daily newspaper whose primary market is Boston, Massachusetts and its surrounding area. It was founded in 1846 and is one of the oldest daily newspapers in the United States. It has been awarded eight Pulitzer Prizes in its history, including four for editorial writing and three for photography before it was converted to tabloid format in 1981. The \"Herald\" was named one of the \"10 Newspapers That 'Do It Right' in 2012 by \"Editor &amp; Publisher\".\nIn December 2017, the \"Herald\" filed for bankruptcy. On February 14, 2018, Digital First Media successfully bid $11.9 million to purchase the company in a bankruptcy auction; the acquisition was completed on March 19, 2018. As of August 2018, the paper had approximately 110 total employees, compared to about 225 before the sale.\nHistory.\nThe \"Herald\" history can be traced back through two lineages, the \"Daily Advertiser\" and the old \"Boston Herald\", and two media moguls, William Randolph Hearst and Rupert Murdoch.\nThe original \"Boston Herald\".\nThe original \"Boston Herald\" was founded in 1846 by a group of Boston printers jointly under the name of John A. French &amp; Company. The paper was published as a single two-sided sheet, selling for one cent. Its first editor, William O. Eaton, just 22 years old, said \"The \"Herald\" will be independent in politics and religion; liberal, industrious, enterprising, critically concerned with literacy and dramatic matters, and diligent in its mission to report and analyze the news, local and global.\"\nIn 1847, the \"Boston Herald\" absorbed the Boston \"American Eagle\" and the Boston \"Daily Times\".\n\"The Boston Herald and Boston Journal\".\nIn October 1917, John H. Higgins, the publisher and treasurer of the Boston Herald bought out its next door neighbor \"The Boston Journal\" and created \"The Boston Herald and Boston Journal\"\n\"The American Traveler\".\nEven earlier than the \"Herald\", the weekly \"American Traveler\" was founded in 1825 as a bulletin for stagecoach listings.\nThe \"Boston Evening Traveller\".\nThe \"Boston Evening Traveler\" was founded in 1845. The \" Boston Evening Traveler\" was the successor to the weekly \"American Traveler\" and the semi-weekly \"Boston Traveler\". In 1912, the \"Herald\" acquired the \"Traveler\", continuing to publish both under their own names. For many years, the newspaper was controlled by many of the investors in United Shoe Machinery Co. After a newspaper strike in 1967, Herald-Traveler Corp. suspended the afternoon \"Traveler\" and absorbed the evening edition into the Herald to create the \"Boston Herald Traveler.\"\n\"The Boston Daily Advertiser\".\nThe \"Boston Daily Advertiser\" was established in 1813 in Boston by Nathan Hale. The paper grew to prominence throughout the 19th century, taking over other Boston area papers. In 1832 The Advertiser took over control of \"The Boston Patriot\", and then in 1840 it took over and absorbed \"The Boston Gazette\". The paper was purchased by William Randolph Hearst in 1917. In 1920 the \"Advertiser\" was merged with \"The Boston Record\", initially the combined newspaper was called the \"Boston Advertiser\" however when the combined newspaper became an illustrated tabloid in 1921 it was renamed \"The Boston American\". Hearst Corp. continued using the name \"Advertiser\" for its Sunday paper until the early 1970s.\n\"The Boston Record\".\nOn September 3, 1884, \"The Boston Evening Record\" was started by the \"Boston Advertiser\" as a campaign newspaper. The \"Record\" was so popular that it was made a permanent publication.\n\"The Boston American\".\nIn 1904, William Randolph Hearst began publishing his own newspaper in Boston called \"The American\". Hearst ultimately ended up purchasing the \"Daily Advertiser\" in 1917. By 1938, the \"Daily Advertiser\" had changed to the \"Daily Record\", and \"The American\" had become the \"Sunday Advertiser\". A third paper owned by Hearst, called the \"Afternoon Record\", which had been renamed the \"Evening American\", merged in 1961 with the \"Daily Record\" to form the \"Record American\". The \"Sunday Advertiser\" and \"Record American\" would ultimately be merged in 1972 into \"The Boston Herald Traveler\" a line of newspapers that stretched back to the old \"Boston Herald\".\n\"The Boston Herald Traveler\".\nIn 1946, Herald-Traveler Corporation acquired Boston radio station WHDH. Two years later, WHDH-FM was licensed, and on November 26, 1957, WHDH-TV made its d\u00e9but as an ABC affiliate on channel 5. In 1961, WHDH-TV's affiliation switched to CBS. Herald-Traveler Corp. operated for years beginning some time after under temporary authority from the Federal Communications Commission stemming from controversy over luncheon meetings the newspaper's chief executive purportedly had with John C. Doerfer, chairman of the FCC between 1957 and 1960, who served as a commissioner during the original licensing process. (Some Boston broadcast historians accuse \"The Boston Globe\" of being covertly behind the proceeding as a sort of vendetta for not getting a license\u2014The \"Herald Traveler\" was Republican in sympathies, and the \"Globe\" then had a firm policy of not endorsing political candidates, although Doerfer's history at the FCC also lent suspicions) The FCC ordered comparative hearings, and in 1969 a competing applicant, Boston Broadcasters, Inc., was granted a construction permit to replace WHDH-TV on channel 5. Herald-Traveler Corp. fought the decision in court\u2014by this time, revenues from channel 5 were all but keeping the newspaper afloat\u2014but its final appeal ran out in 1972, and on March 19 WHDH-TV was forced to surrender channel 5 to the new WCVB-TV.\n\"The Boston Herald Traveler and Record American\".\nWithout a television station to subsidize the newspaper, the \"Herald Traveler\" was no longer able to remain in business, and the newspaper was sold to Hearst Corporation, which published the rival all-day newspaper, the \"Record American\". The two papers were merged to become an all-day paper called the \"Boston Herald Traveler and Record American\" in the morning and \"Record-American and Boston Herald Traveler\" in the afternoon. The first editions published under the new combined name were those of June 19, 1972. The afternoon edition was soon dropped and the unwieldy name shortened to \"Boston Herald American\", with the Sunday edition called the \"Sunday Herald Advertiser\". The \"Herald American\" was printed in broadsheet format, and failed to target a particular readership; where the \"Record American\" had been a typical city tabloid, the \"Herald Traveler\" was a Republican paper.\nMurdoch purchases \"The Herald American\".\nThe \"Herald American\" converted to tabloid format in September 1981, but Hearst faced steep declines in circulation and advertising. The company announced it would close the \"Herald American\"\u2014making Boston a one-newspaper town\u2014on December 3, 1982. When the deadline came, Australian media baron Rupert Murdoch was negotiating to buy the paper and save it. He closed on the deal after 30 hours of talks with Hearst and newspaper unions\u2014and five hours after Hearst had sent out notices to newsroom employees telling them they were terminated. The newspaper announced its own survival the next day with a full-page headline: \"You Bet We're Alive!\"\nThe \"Boston Herald\" once again.\nMurdoch changed the paper's name back to the \"Boston Herald\". The \"Herald\" continued to grow, expanding its coverage and increasing its circulation until 2001, when nearly all newspapers fell victim to declining circulations and revenue.\nIndependent ownership.\nIn February 1994, Murdoch's News Corporation was forced to sell the paper, in order that its subsidiary Fox Television Stations could legally consummate its purchase of Fox affiliate WFXT (Channel 25) because Massachusetts Senator Ted Kennedy included language in an appropriations barring one company from owning a newspaper and television station in the same market. Patrick J. Purcell, who was the publisher of the \"Boston Herald\" and a former News Corporation executive, purchased the \"Herald\" and established it as an independent newspaper. Several years later, Purcell would give the \"Herald\" a suburban presence it never had by purchasing the money-losing Community Newspaper Company from Fidelity Investments. Although the companies merged under the banner of Herald Media, Inc., the suburban papers maintained their distinct editorial and marketing identity.\nAfter years of operating profits at Community Newspaper and losses at the \"Herald\", Purcell in 2006 sold the suburban chain to newspaper conglomerate Liberty Group Publishing of Illinois, which soon after changed its name to GateHouse Media. The deal, which also saw GateHouse acquiring \"The Patriot Ledger\" and \"The Enterprise\" respectively in south suburban Quincy and Brockton, netted $225 million for Purcell, who vowed to use the funds to clear the \"Herald\"'s debt and reinvest in the Paper.\nBoston Herald Radio.\nOn August 5, 2013, the \"Herald\" launched an internet radio station named Boston Herald Radio which includes radio shows by much of the Herald staff. The station's morning lineup is simulcast on 830 AM WCRN from 10 AM Eastern time to 12 noon Eastern time.\nBankruptcy.\nIn December 2017, the \"Herald\" announced plans to sell itself to GateHouse Media after filing for chapter 11 bankruptcy protection. The deal was scheduled to be completed by February 2018, with the new company streamlining and having layoffs in coming months. However, in early January 2018, another potential buyer, Revolution Capital Group of Los Angeles, filed a bid with the federal bankruptcy court; the \"Herald\" reported in a press release that \"the court requires BHI [Boston Herald, Inc.] to hold an auction to allow all potential buyers an opportunity to submit competing offers.\"\nDigital First Media acquisition.\nIn February 2018, acquisition of the \"Herald\" by Digital First Media for almost $12 million was approved by the bankruptcy court judge in Delaware. The new owner, DFM, said they would be keeping 175 of the approximately 240 employees the \"Herald\" had when it sought bankruptcy protection in December 2017. The acquisition was completed on March 19, 2018.\nThe Herald and parent DFM were criticized for ending the ten-year printing contract with competitor \"The Boston Globe\", moving printing from Taunton, Massachusetts, to Rhode Island and its \"dehumanizing cost-cutting efforts\" in personnel. In June, some design and advertising layoffs were expected, with work moving to a sister paper, \"The Denver Post\". The \"consolidation\" took effect in August, with nine jobs eliminated.\nIn late August 2018, it was announced that the \"Herald\" would move its offices from Boston's Seaport District to Braintree, Massachusetts, in late November or early December.\n\"Boston Herald\" in Education Program.\nThe Boston Herald Newspapers in Education (NIE) program provides teachers with classroom newspapers and educational materials designed to help students of all ages and abilities excel. This is made possible through donations from Herald readers and other sponsors. The \"Boston Herald\" is available in two formats: the print edition and the online e-Edition. The website can be found at http://bostonheraldnie.com/\nReferences.\n Boston Herald July 29, 1998"}
{"id": "4173", "revid": "39374154", "url": "https://en.wikipedia.org/wiki?curid=4173", "title": "Babe Ruth", "text": "George Herman \"Babe\" Ruth Jr. (February 6, 1895\u2013August 16, 1948) was an American professional baseball player whose career in Major League Baseball (MLB) spanned 22\u00a0seasons, from 1914 through 1935. Nicknamed \"The Bambino\" and \"The Sultan of Swat\", he began his MLB career as a star left-handed pitcher for the Boston Red Sox, but achieved his greatest fame as a slugging outfielder for the New York Yankees. Ruth established many MLB batting (and some pitching) records, including career home runs (714), runs batted in (RBIs) (2,213), bases on balls (2,062), slugging percentage (.690), and on-base plus slugging (OPS) (1.164); the last two still stand . Ruth is regarded as one of the greatest sports heroes in American culture and is considered by many to be the greatest baseball player of all time. In 1936, Ruth was elected into the Baseball Hall of Fame as one of its \"first five\" inaugural members.\nAt age seven, Ruth was sent to St. Mary's Industrial School for Boys, a reformatory where he was mentored by Brother Matthias Boutlier of the Xaverian Brothers, the school's disciplinarian and a capable baseball player. In 1914, Ruth was signed to play minor-league baseball for the Baltimore Orioles but was soon sold to the Red Sox. By 1916, he had built a reputation as an outstanding pitcher who sometimes hit long home runs, a feat unusual for any player in the pre-1920 dead-ball era. Although Ruth twice won 23 games in a season as a pitcher and was a member of three World Series championship teams with the Red Sox, he wanted to play every day and was allowed to convert to an outfielder. With regular playing time, he broke the MLB single-season home run record in 1919.\nAfter that season, Red Sox owner Harry Frazee sold Ruth to the Yankees amid controversy. The trade fueled Boston's subsequent 86-year championship drought and popularized the \"Curse of the Bambino\" superstition. In his 15 years with the Yankees, Ruth helped the team win seven American League (AL) pennants and four World Series championships. His big swing led to escalating home run totals that not only drew fans to the ballpark and boosted the sport's popularity but also helped usher in baseball's live-ball era, which evolved from a low-scoring game of strategy to a sport where the home run was a major factor. As part of the Yankees' vaunted \"Murderers' Row\" lineup of 1927, Ruth hit 60 home runs, which extended his MLB single-season record by a single home run. Ruth's last season with the Yankees was 1934; he retired from the game the following year, after a short stint with the Boston Braves. During his career, Ruth led the AL in home runs during a season 12 times.\nDuring Ruth's career, he was the target of intense press and public attention for his baseball exploits and off-field penchants for drinking and womanizing. After his retirement as a player, he was denied the opportunity to manage a major league club, most likely due to poor behavior during parts of his playing career. In his final years, Ruth made many public appearances, especially in support of American efforts in World War II. In 1946, he became ill with nasopharyngeal cancer and died from the disease two years later. Ruth remains a part of American culture, and in 2018 President Donald Trump posthumously awarded him the Presidential Medal of Freedom.\nEarly years.\nGeorge Herman Ruth Jr. was born in 1895 at 216 Emory Street in the Pigtown section of Baltimore, Maryland. Ruth's parents, Katherine (n\u00e9e Schamberger) and George Herman Ruth Sr., were both of German ancestry. According to the 1880 census, his parents were born in Maryland. His paternal grandparents were from Prussia and Hanover. Ruth Sr. worked a series of jobs that included lightning rod salesman and streetcar operator. The elder Ruth then became a counterman in a family-owned combination grocery and saloon business on Frederick Street. George Ruth Jr. was born in the house of his maternal grandfather, Pius Schamberger, a German immigrant and trade unionist. Only one of young Ruth's seven siblings, his younger sister Mamie, survived infancy.\nMany details of Ruth's childhood are unknown, including the date of his parents' marriage. As a child, Ruth spoke German. When Ruth was a toddler, the family moved to 339 South Woodyear Street, not far from the rail yards; by the time he was six years old, his father had a saloon with an upstairs apartment at 426 West Camden Street. Details are equally scanty about why Ruth was sent at the age of seven to St. Mary's Industrial School for Boys, a reformatory and orphanage. However, according to Julia Ruth Stevens' recount in 1999, because George Sr. was a saloon owner in Baltimore and had given Ruth little supervision growing up, he became a delinquent. Ruth was sent to St. Mary's because George Sr. ran out of ideas to discipline and mentor his son. As an adult, Ruth admitted that as a youth he ran the streets, rarely attended school, and drank beer when his father was not looking. Some accounts say that following a violent incident at his father's saloon, the city authorities decided that this environment was unsuitable for a small child. Ruth entered St. Mary's on June 13, 1902. He was recorded as \"incorrigible\" and spent much of the next 12 years there.\nAlthough St. Mary's boys received an education, students were also expected to learn work skills and help operate the school, particularly once the boys turned 12. Ruth became a shirtmaker and was also proficient as a carpenter. He would adjust his own shirt collars, rather than having a tailor do so, even during his well-paid baseball career. The boys, aged 5 to 21, did most of the work around the facility, from cooking to shoemaking, and renovated St. Mary's in 1912. The food was simple, and the Xaverian Brothers who ran the school insisted on strict discipline; corporal punishment was common. Ruth's nickname there was \"Niggerlips\", as he had large facial features and was darker than most boys at the all-white reformatory.\nRuth was sometimes allowed to rejoin his family or was placed at St. James's Home, a supervised residence with work in the community, but he was always returned to St. Mary's. He was rarely visited by his family; his mother died when he was 12 and, by some accounts, he was permitted to leave St. Mary's only to attend the funeral. How Ruth came to play baseball there is uncertain: according to one account, his placement at St. Mary's was due in part to repeatedly breaking Baltimore's windows with long hits while playing street ball; by another, he was told to join a team on his first day at St. Mary's by the school's athletic director, Brother Herman, becoming a catcher even though left-handers rarely play that position. During his time there he also played third base and shortstop, again unusual for a left-hander, and was forced to wear mitts and gloves made for right-handers. He was encouraged in his pursuits by the school's Prefect of Discipline, Brother Matthias Boutlier, a native of Nova Scotia. A large man, Brother Matthias was greatly respected by the boys both for his strength and for his fairness. For the rest of his life, Ruth would praise Brother Matthias, and his running and hitting styles closely resembled his teacher's. Ruth stated, \"I think I was born as a hitter the first day I ever saw him hit a baseball.\" The older man became a mentor and role model to Ruth; biographer Robert W. Creamer commented on the closeness between the two:\nThe school's influence remained with Ruth in other ways. He was a lifelong Catholic who would sometimes attend Mass after carousing all night, and he became a well-known member of the Knights of Columbus. He would visit orphanages, schools, and hospitals throughout his life, often avoiding publicity. He was generous to St. Mary's as he became famous and rich, donating money and his presence at fundraisers, and spending $5,000 to buy Brother Matthias a Cadillac in 1926\u2014subsequently replacing it when it was destroyed in an accident. Nevertheless, his biographer Leigh Montville suggests that many of the off-the-field excesses of Ruth's career were driven by the deprivations of his time at St. Mary's.\nMost of the boys at St. Mary's played baseball in organized leagues at different levels of proficiency. Ruth later estimated that he played 200 games a year as he steadily climbed the ladder of success. Although he played all positions at one time or another, he gained stardom as a pitcher. According to Brother Matthias, Ruth was standing to one side laughing at the bumbling pitching efforts of fellow students, and Matthias told him to go in and see if he could do better. Ruth had become the best pitcher at St. Mary's, and when he was 18 in 1913, he was allowed to leave the premises to play weekend games on teams that were drawn from the community. He was mentioned in several newspaper articles, for both his pitching prowess and ability to hit long home runs.\nProfessional baseball.\nMinor league, Baltimore Orioles.\nIn early 1914, Ruth signed a professional baseball contract with Jack Dunn, who owned and managed the minor-league Baltimore Orioles, an International League team. The circumstances of Ruth's signing are not known with certainty; historical fact is obscured by stories that cannot all be true. By some accounts, Dunn was urged to attend a game between an all-star team from St. Mary's and one from another Xaverian facility, Mount St. Mary's College. Some versions have Ruth running away before the eagerly awaited game, to return in time to be punished, and then pitching St. Mary's to victory as Dunn watched. Others have Washington Senators pitcher Joe Engel, a Mount St. Mary's graduate, pitching in an alumni game after watching a preliminary contest between the college's freshmen and a team from St. Mary's, including Ruth. Engel watched Ruth play, then told Dunn about him at a chance meeting in Washington. Ruth, in his autobiography, stated only that he worked out for Dunn for a half hour, and was signed. According to biographer Kal Wagenheim, there were legal difficulties to be straightened out as Ruth was supposed to remain at the school until he turned 21, though SportsCentury stated in a documentary that Ruth had already been discharged from St. Mary's when he turned 19, and earned a monthly salary of $100.\nThe train journey to spring training in Fayetteville, North Carolina, in early March was likely Ruth's first outside the Baltimore area. The rookie ballplayer was the subject of various pranks by the veterans, who were probably also the source of his famous nickname. There are various accounts of how Ruth came to be called \"Babe\", but most center on his being referred to as \"Dunnie's babe\" or a variant. SportsCentury reported that his nickname was gained because he was the new \"darling\" or \"project\" of Dunn, not only due to Ruth's raw talent, but also because of his lack of knowledge of the proper etiquette of eating out in a restaurant, being in a hotel, or being on a train. \"Babe\" was, at that time, a common nickname in baseball, with perhaps the most famous to that point being Pittsburgh Pirates pitcher and 1909 World Series hero Babe Adams, who appeared younger than his actual age.\nRuth made his first appearance as a professional ballplayer in an inter-squad game on March 7, 1914. He played shortstop and pitched the last two innings of a 15\u20139 victory. In his second at-bat, Ruth hit a long home run to right field; the blast was locally reported to be longer than a legendary shot hit by Jim Thorpe in Fayetteville. Ruth made his first appearance against a team in organized baseball in an exhibition game versus the major-league Philadelphia Phillies. Ruth pitched the middle three innings and gave up two runs in the fourth, but then settled down and pitched a scoreless fifth and sixth innings. In a game against the Phillies the following afternoon, Ruth entered during the sixth inning and did not allow a run the rest of the way. The Orioles scored seven runs in the bottom of the eighth inning to overcome a 6\u20130 deficit, and Ruth was the winning pitcher.\nOnce the regular season began, Ruth was a star pitcher who was also dangerous at the plate. The team performed well, yet received almost no attention from the Baltimore press. A third major league, the Federal League, had begun play, and the local franchise, the Baltimore Terrapins, restored that city to the major leagues for the first time since 1902. Few fans visited Oriole Park, where Ruth and his teammates labored in relative obscurity. Ruth may have been offered a bonus and a larger salary to jump to the Terrapins; when rumors to that effect swept Baltimore, giving Ruth the most publicity he had experienced to date, a Terrapins official denied it, stating it was their policy not to sign players under contract to Dunn.\nThe competition from the Terrapins caused Dunn to sustain large losses. Although by late June the Orioles were in first place, having won over two-thirds of their games, the paid attendance dropped as low as 150. Dunn explored a possible move by the Orioles to Richmond, Virginia, as well as the sale of a minority interest in the club. These possibilities fell through, leaving Dunn with little choice other than to sell his best players to major league teams to raise money. He offered Ruth to the reigning World Series champions, Connie Mack's Philadelphia Athletics, but Mack had his own financial problems. The Cincinnati Reds and New York Giants expressed interest in Ruth, but Dunn sold his contract, along with those of pitchers Ernie Shore and Ben Egan, to the Boston Red Sox of the American League (AL) on July 4. The sale price was announced as $25,000 but other reports lower the amount to half that, or possibly $8,500 plus the cancellation of a $3,000 loan. Ruth remained with the Orioles for several days while the Red Sox completed a road trip, and reported to the team in Boston on July 11.\nBoston Red Sox (1914\u20131919).\nDeveloping star.\nOn July 11, 1914, Ruth arrived in Boston with Egan and Shore. Ruth later told the story of how that morning he had met Helen Woodford, the girl who would become his first wife. She was a 16-year-old waitress at Landers Coffee Shop, and Ruth related that she served him when he had breakfast there. Other stories, though, suggested that the meeting occurred on another day, and perhaps under other circumstances. Regardless of when he began to woo his first wife, he won his first game as a pitcher for the Red Sox that afternoon, 4\u20133, over the Cleveland Naps. His catcher was Bill Carrigan, who was also the Red Sox manager. Shore was given a start by Carrigan the next day; he won that and his second start and thereafter was pitched regularly. Ruth lost his second start, and was thereafter little used. In his major league debut as a batter, Ruth went 0-for-2 against left-hander Willie Mitchell, striking out in his first at bat before being removed for a pinch hitter in the seventh inning. Ruth was not much noticed by the fans, as Bostonians watched the Red Sox's crosstown rivals, the Braves, begin a legendary comeback that would take them from last place on the Fourth of July to the 1914 World Series championship.\nEgan was traded to Cleveland after two weeks on the Boston roster. During his time with the Red Sox, he kept an eye on the inexperienced Ruth, much as Dunn had in Baltimore. When he was traded, no one took his place as supervisor. Ruth's new teammates considered him brash, and would have preferred him, as a rookie, to remain quiet and inconspicuous. When Ruth insisted on taking batting practice despite being both a rookie who did not play regularly, and a pitcher, he arrived to find his bats sawn in half. His teammates nicknamed him \"the Big Baboon\", a name the swarthy Ruth, who had disliked the nickname \"Niggerlips\" at St. Mary's, detested. Ruth had received a raise on promotion to the major leagues, and quickly acquired tastes for fine food, liquor, and women, among other temptations.\nManager Carrigan allowed Ruth to pitch two exhibition games in mid-August. Although Ruth won both against minor-league competition, he was not restored to the pitching rotation. It is uncertain why Carrigan did not give Ruth additional opportunities to pitch. There are legends\u2014filmed for the screen in \"The Babe Ruth Story\" (1948)\u2014that the young pitcher had a habit of signaling his intent to throw a curveball by sticking out his tongue slightly, and that he was easy to hit until this changed. Creamer pointed out that it is common for inexperienced pitchers to display such habits, and the need to break Ruth of his would not constitute a reason to not use him at all. The biographer suggested that Carrigan was unwilling to use Ruth due to poor behavior by the rookie.\nOn July 30, 1914, Boston owner Joseph Lannin had purchased the minor-league Providence Grays, members of the International League. The Providence team had been owned by several people associated with the Detroit Tigers, including star hitter Ty Cobb, and as part of the transaction, a Providence pitcher was sent to the Tigers. To soothe Providence fans upset at losing a star, Lannin announced that the Red Sox would soon send a replacement to the Grays. This was intended to be Ruth, but his departure for Providence was delayed when Cincinnati Reds owner Garry Herrmann claimed him off waivers. After Lannin wrote to Herrmann explaining that the Red Sox wanted Ruth in Providence so he could develop as a player, and would not release him to a major league club, Herrmann allowed Ruth to be sent to the minors. Carrigan later stated that Ruth was not sent down to Providence to make him a better player, but to help the Grays win the International League pennant (league championship).\nRuth joined the Grays on August 18, 1914. After Dunn's deals, the Baltimore Orioles managed to hold on to first place until August 15, after which they continued to fade, leaving the pennant race between Providence and Rochester. Ruth was deeply impressed by Providence manager \"Wild Bill\" Donovan, previously a star pitcher with a 25\u20134 win\u2013loss record for Detroit in 1907; in later years, he credited Donovan with teaching him much about pitching. Ruth was often called upon to pitch, in one stretch starting (and winning) four games in eight days. On September 5 at Maple Leaf Park in Toronto, Ruth pitched a one-hit 9\u20130 victory, and hit his first professional home run, his only one as a minor leaguer, off Ellis Johnson. Recalled to Boston after Providence finished the season in first place, he pitched and won a game for the Red Sox against the New York Yankees on October 2, getting his first major league hit, a double. Ruth finished the season with a record of 2\u20131 as a major leaguer and 23\u20138 in the International League (for Baltimore and Providence). Once the season concluded, Ruth married Helen in Ellicott City, Maryland. Creamer speculated that they did not marry in Baltimore, where the newlyweds boarded with George Ruth Sr., to avoid possible interference from those at St. Mary's\u2014both bride and groom were not yet of age and Ruth remained on parole from that institution until his 21st birthday.\nIn March 1915, Ruth reported to Hot Springs, Arkansas, for his first major league spring training. Despite a relatively successful first season, he was not slated to start regularly for the Red Sox, who already had two \"superb\" left-handed pitchers, according to Creamer: the established stars Dutch Leonard, who had broken the record for the lowest earned run average (ERA) in a single season; and Ray Collins, a 20-game winner in both 1913 and 1914. Ruth was ineffective in his first start, taking the loss in the third game of the season. Injuries and ineffective pitching by other Boston pitchers gave Ruth another chance, and after some good relief appearances, Carrigan allowed Ruth another start, and he won a rain-shortened seven inning game. Ten days later, the manager had him start against the New York Yankees at the Polo Grounds. Ruth took a 3\u20132 lead into the ninth, but lost the game 4\u20133 in 13 innings. Ruth, hitting ninth as was customary for pitchers, hit a massive home run into the upper deck in right field off of Jack Warhop. At the time, home runs were rare in baseball, and Ruth's majestic shot awed the crowd. The winning pitcher, Warhop, would in August 1915 conclude a major league career of eight seasons, undistinguished but for being the first major league pitcher to give up a home run to Babe Ruth.\nCarrigan was sufficiently impressed by Ruth's pitching to give him a spot in the starting rotation. Ruth finished the 1915 season 18\u20138 as a pitcher; as a hitter, he batted .315 and had four home runs. The Red Sox won the AL pennant, but with the pitching staff healthy, Ruth was not called upon to pitch in the 1915 World Series against the Philadelphia Phillies. Boston won in five games; Ruth was used as a pinch hitter in Game Five, but grounded out against Phillies ace Grover Cleveland Alexander. Despite his success as a pitcher, Ruth was acquiring a reputation for long home runs; at Sportsman's Park against the St. Louis Browns, a Ruth hit soared over Grand Avenue, breaking the window of a Chevrolet dealership.\nIn 1916, there was attention focused on Ruth for his pitching, as he engaged in repeated pitching duels with the ace of the Washington Senators, Walter Johnson. The two met five times during the season, with Ruth winning four and Johnson one (Ruth had a no decision in Johnson's victory). Two of Ruth's victories were by the score of 1\u20130, one in a 13-inning game. Of the 1\u20130 shutout decided without extra innings, AL President Ban Johnson stated, \"That was one of the best ball games I have ever seen.\" For the season, Ruth went 23\u201312, with a 1.75 ERA and nine shutouts, both of which led the league. Ruth's nine shutouts in 1916 set a league record for left-handers that would remain unmatched until Ron Guidry tied it in 1978. The Red Sox won the pennant and World Series again, this time defeating the Brooklyn Robins (as the Dodgers were then known) in five games. Ruth started and won Game 2, 2\u20131, in 14 innings. Until another game of that length was played in 2005, this was the longest World Series game, and Ruth's pitching performance is still the longest postseason complete game victory.\nCarrigan retired as player and manager after 1916, returning to his native Maine to be a businessman. Ruth, who played under four managers who are in the National Baseball Hall of Fame, always maintained that Carrigan, who is not enshrined there, was the best skipper he ever played for. There were other changes in the Red Sox organization that offseason, as Lannin sold the team to a three-man group headed by New York theatrical promoter Harry Frazee. Jack Barry was hired by Frazee as manager.\nEmergence as a hitter.\nRuth went 24\u201313 with a 2.01 ERA and six shutouts in 1917, but the Sox finished in second place in the league, nine games behind the Chicago White Sox in the standings. On June 23 at Washington, when home plate umpire 'Brick' Owens called the first four pitches as balls, Ruth threw a punch at him, and was ejected from the game and later suspended for ten days and fined $100. Ernie Shore was called in to relieve Ruth, and was allowed eight warm-up pitches. The runner who had reached base on the walk was caught stealing, and Shore retired all 26 batters he faced to win the game. Shore's feat was listed as a perfect game for many years. In 1991, Major League Baseball's (MLB) Committee on Statistical Accuracy amended it to be listed as a combined no-hitter. In 1917, Ruth was used little as a batter, other than for his plate appearances while pitching, and hit .325 with two home runs.\nThe United States' entry into World War I occurred at the start of the season and overshadowed the sport. Conscription was introduced in September 1917, and most baseball players in the big leagues were of draft age. This included Barry, who was a player-manager, and who joined the Naval Reserve in an attempt to avoid the draft, only to be called up after the 1917 season. Frazee hired International League President Ed Barrow as Red Sox manager. Barrow had spent the previous 30 years in a variety of baseball jobs, though he never played the game professionally. With the major leagues shorthanded due to the war, Barrow had many holes in the Red Sox lineup to fill.\nRuth also noticed these vacancies in the lineup. He was dissatisfied in the role of a pitcher who appeared every four or five days and wanted to play every day at another position. Barrow used Ruth at first base and in the outfield during the exhibition season, but he restricted him to pitching as the team moved toward Boston and the season opener. At the time, Ruth was possibly the best left-handed pitcher in baseball, and allowing him to play another position was an experiment that could have backfired.\nInexperienced as a manager, Barrow had player Harry Hooper advise him on baseball game strategy. Hooper urged his manager to allow Ruth to play another position when he was not pitching, arguing to Barrow, who had invested in the club, that the crowds were larger on days when Ruth played, as they were attracted by his hitting. In early May, Barrow gave in; Ruth promptly hit home runs in four consecutive games (one an exhibition), the last off of Walter Johnson. For the first time in his career (disregarding pinch-hitting appearances), Ruth was assigned a place in the batting order higher than ninth.\nAlthough Barrow predicted that Ruth would beg to return to pitching the first time he experienced a batting slump, that did not occur. Barrow used Ruth primarily as an outfielder in the war-shortened 1918 season. Ruth hit .300, with 11 home runs, enough to secure him a share of the major league home run title with Tilly Walker of the Philadelphia Athletics. He was still occasionally used as a pitcher, and had a 13\u20137 record with a 2.22 ERA.\nIn 1918, the Red Sox won their third pennant in four years and faced the Chicago Cubs in the World Series, which began on September 5, the earliest date in history. The season had been shortened because the government had ruled that baseball players who were eligible for the military would have to be inducted or work in critical war industries, such as armaments plants. Ruth pitched and won Game One for the Red Sox, a 1\u20130 shutout. Before Game Four, Ruth injured his left hand in a fight but pitched anyway. He gave up seven hits and six walks, but was helped by outstanding fielding behind him and by his own batting efforts, as a fourth-inning triple by Ruth gave his team a 2\u20130 lead. The Cubs tied the game in the eighth inning, but the Red Sox scored to take a 3\u20132 lead again in the bottom of that inning. After Ruth gave up a hit and a walk to start the ninth inning, he was relieved on the mound by Joe Bush. To keep Ruth and his bat in the game, he was sent to play left field. Bush retired the side to give Ruth his second win of the Series, and the third and last World Series pitching victory of his career, against no defeats, in three pitching appearances. Ruth's effort gave his team a three-games-to-one lead, and two days later the Red Sox won their third Series in four years, four-games-to-two. Before allowing the Cubs to score in Game Four, Ruth pitched consecutive scoreless innings, a record for the World Series that stood for more than 40 years until 1961, broken by Whitey Ford after Ruth's death. Ruth was prouder of that record than he was of any of his batting feats.\nWith the World Series over, Ruth gained exemption from the war draft by accepting a nominal position with a Pennsylvania steel mill. Many industrial establishments took pride in their baseball teams and sought to hire major leaguers. The end of the war in November set Ruth free to play baseball without such contrivances.\nDuring the 1919 season, Ruth was used as a pitcher in only 17 of his 130 games and compiled an 8\u20135 record. Barrow used him as a pitcher mostly in the early part of the season, when the Red Sox manager still had hopes of a second consecutive pennant. By late June, the Red Sox were clearly out of the race, and Barrow had no objection to Ruth concentrating on his hitting, if only because it drew people to the ballpark. Ruth had hit a home run against the Yankees on Opening Day, and another during a month-long batting slump that soon followed. Relieved of his pitching duties, Ruth began an unprecedented spell of slugging home runs, which gave him widespread public and press attention. Even his failures were seen as majestic\u2014one sportswriter said, \"When Ruth misses a swipe at the ball, the stands quiver.\"\nTwo home runs by Ruth on July 5, and one in each of two consecutive games a week later, raised his season total to 11, tying his career best from 1918. The first record to fall was the AL single-season mark of 16, set by Ralph \"Socks\" Seybold in 1902. Ruth matched that on July 29, then pulled ahead toward the major league record of 25, set by Buck Freeman in 1899. By the time Ruth reached this in early September, writers had discovered that Ned Williamson of the 1884 Chicago White Stockings had hit 27\u2014though in a ballpark where the distance to right field was only . On September 20, \"Babe Ruth Day\" at Fenway Park, Ruth won the game with a home run in the bottom of the ninth inning, tying Williamson. He broke the record four days later against the Yankees at the Polo Grounds, and hit one more against the Senators to finish with 29. The home run at Washington made Ruth the first major league player to hit a home run at all eight ballparks in his league. In spite of Ruth's hitting heroics, the Red Sox finished sixth, games behind the league champion White Sox. In his six seasons with Boston, he won 89 games and recorded a 2.19 ERA. He had a four-year stretch where he was second in the AL in wins and ERA behind Walter Johnson, and Ruth had a winning record against Johnson in head-to-head matchups.\nSale to New York.\nAs an out-of-towner from New York City, Frazee had been regarded with suspicion by Boston's sportswriters and baseball fans when he bought the team. He won them over with success on the field and a willingness to build the Red Sox by purchasing or trading for players. He offered the Senators $60,000 for Walter Johnson, but Washington owner Clark Griffith was unwilling. Even so, Frazee was successful in bringing other players to Boston, especially as replacements for players in the military. This willingness to spend for players helped the Red Sox secure the 1918 title. The 1919 season saw record-breaking attendance, and Ruth's home runs for Boston made him a national sensation. In March 1919 Ruth was reported as having accepted a three-year contract for a total of $27,000, after protracted negotiations. Nevertheless, on December 26, 1919, Frazee sold Ruth's contract to the New York Yankees.\nNot all the circumstances concerning the sale are known, but brewer and former congressman Jacob Ruppert, the New York team's principal owner, reportedly asked Yankee manager Miller Huggins what the team needed to be successful. \"Get Ruth from Boston\", Huggins supposedly replied, noting that Frazee was perennially in need of money to finance his theatrical productions. In any event, there was precedent for the Ruth transaction: when Boston pitcher Carl Mays left the Red Sox in a 1919 dispute, Frazee had settled the matter by selling Mays to the Yankees, though over the opposition of AL President Johnson.\nAccording to one of Ruth's biographers, Jim Reisler, \"why Frazee needed cash in 1919\u2014and large infusions of it quickly\u2014is still, more than 80 years later, a bit of a mystery\". The often-told story is that Frazee needed money to finance the musical \"No, No, Nanette\", which was a Broadway hit and brought Frazee financial security. That play did not open until 1925, however, by which time Frazee had sold the Red Sox. Still, the story may be true in essence: \"No, No, Nanette\" was based on a Frazee-produced play, \"My Lady Friends\", which opened in 1919.\nThere were other financial pressures on Frazee, despite his team's success. Ruth, fully aware of baseball's popularity and his role in it, wanted to renegotiate his contract, signed before the 1919 season for $10,000 per year through 1921. He demanded that his salary be doubled, or he would sit out the season and cash in on his popularity through other ventures. Ruth's salary demands were causing other players to ask for more money. Additionally, Frazee still owed Lannin as much as $125,000 from the purchase of the club.\nAlthough Ruppert and his co-owner, Colonel Tillinghast Huston, were both wealthy, and had aggressively purchased and traded for players in 1918 and 1919 to build a winning team, Ruppert faced losses in his brewing interests as Prohibition was implemented, and if their team left the Polo Grounds, where the Yankees were the tenants of the New York Giants, building a stadium in New York would be expensive. Nevertheless, when Frazee, who moved in the same social circles as Huston, hinted to the colonel that Ruth was available for the right price, the Yankees owners quickly pursued the purchase.\nFrazee sold the rights to Babe Ruth for $100,000, the largest sum ever paid for a baseball player. The deal also involved a $350,000 loan from Ruppert to Frazee, secured by a mortgage on Fenway Park. Once it was agreed, Frazee informed Barrow, who, stunned, told the owner that he was getting the worse end of the bargain. Cynics have suggested that Barrow may have played a larger role in the Ruth sale, as less than a year after, he became the Yankee general manager, and in the following years made a number of purchases of Red Sox players from Frazee. The $100,000 price included $25,000 in cash, and notes for the same amount due November 1 in 1920, 1921, and 1922; Ruppert and Huston assisted Frazee in selling the notes to banks for immediate cash.\nThe transaction was contingent on Ruth signing a new contract, which was quickly accomplished\u2014Ruth agreed to fulfill the remaining two years on his contract, but was given a $20,000 bonus, payable over two seasons. The deal was announced on January 6, 1920. Reaction in Boston was mixed: some fans were embittered at the loss of Ruth; others conceded that Ruth had become difficult to deal with. \"The New York Times\" suggested that \"The short right field wall at the Polo Grounds should prove an easy target for Ruth next season and, playing seventy-seven games at home, it would not be surprising if Ruth surpassed his home run record of twenty-nine circuit clouts next Summer.\" According to Reisler, \"The Yankees had pulled off the sports steal of the century.\"\nAccording to Marty Appel in his history of the Yankees, the transaction, \"changed the fortunes of two high-profile franchises for decades\". The Red Sox, winners of five of the first 16 World Series, those played between 1903 and 1919, would not win another pennant until 1946, or another World Series until 2004, a drought attributed in baseball superstition to Frazee's sale of Ruth and sometimes dubbed the \"Curse of the Bambino\". The Yankees, on the other hand, had not won the AL championship prior to their acquisition of Ruth. They won seven AL pennants and four World Series with Ruth, and led baseball with 40 pennants and 27 World Series titles in their history.\nNew York Yankees (1920\u20131934).\nInitial success (1920\u20131923).\nWhen Ruth signed with the Yankees, he completed his transition from a pitcher to a power-hitting outfielder. His fifteen-season Yankee career consisted of over 2,000 games, and Ruth broke many batting records while making only five widely scattered appearances on the mound, winning all of them.\nAt the end of April 1920, the Yankees were 4\u20137, with the Red Sox leading the league with a 10\u20132 mark. Ruth had done little, having injured himself swinging the bat. Both situations began to change on May 1, when Ruth hit a tape measure home run that sent the ball completely out of the Polo Grounds, a feat believed to have been previously accomplished only by Shoeless Joe Jackson. The Yankees won, 6\u20130, taking three out of four from the Red Sox. Ruth hit his second home run on May 2, and by the end of the month had set a major league record for home runs in a month with 11, and promptly broke it with 13 in June. Fans responded with record attendance figures. On May 16, Ruth and the Yankees drew 38,600 to the Polo Grounds, a record for the ballpark, and 15,000 fans were turned away. Large crowds jammed stadiums to see Ruth play when the Yankees were on the road.\nThe home runs kept on coming. Ruth tied his own record of 29 on July 15 and broke it with home runs in both games of a doubleheader four days later. By the end of July, he had 37, but his pace slackened somewhat after that. Nevertheless, on September 4, he both tied and broke the organized baseball record for home runs in a season, snapping Perry Werden's 1895 mark of 44 in the minor Western League. The Yankees played well as a team, battling for the league lead early in the summer, but slumped in August in the AL pennant battle with Chicago and Cleveland. The pennant and the World Series were won by Cleveland, who surged ahead after the Black Sox Scandal broke on September 28 and led to the suspension of many of Chicago's top players, including Shoeless Joe Jackson. The Yankees finished third, but drew 1.2\u00a0million fans to the Polo Grounds, the first time a team had drawn a seven-figure attendance. The rest of the league sold 600,000 more tickets, many fans there to see Ruth, who led the league with 54 home runs, 158 runs, and 137 runs batted in (RBIs).\nIn 1920 and afterwards, Ruth was aided in his power hitting by the fact that A.J. Reach Company\u2014the maker of baseballs used in the major leagues\u2014was using a more efficient machine to wind the yarn found within the baseball. The new baseballs went into play in 1920 and ushered the start of the live-ball era; the number of home runs across the major leagues increased by 184 over the previous year. Baseball statistician Bill James pointed out that while Ruth was likely aided by the change in the baseball, there were other factors at work, including the gradual abolition of the spitball (accelerated after the death of Ray Chapman, struck by a pitched ball thrown by Mays in August 1920) and the more frequent use of new baseballs (also a response to Chapman's death). Nevertheless, James theorized that Ruth's 1920 explosion might have happened in 1919, had a full season of 154 games been played rather than 140, had Ruth refrained from pitching 133 innings that season, and if he were playing at any other home field but Fenway Park, where he hit only 9 of 29 home runs.\nYankees business manager Harry Sparrow had died early in the 1920 season. Ruppert and Huston hired Barrow to replace him. The two men quickly made a deal with Frazee for New York to acquire some of the players who would be mainstays of the early Yankee pennant-winning teams, including catcher Wally Schang and pitcher Waite Hoyt. The 21-year-old Hoyt became close to Ruth:\nIn the offseason, Ruth spent some time in Havana, Cuba, where he was said to have lost $35,000 () betting on horse races.\nRuth hit home runs early and often in the 1921 season, during which he broke Roger Connor's mark for home runs in a career, 138. Each of the almost 600 home runs Ruth hit in his career after that extended his own record. After a slow start, the Yankees were soon locked in a tight pennant race with Cleveland, winners of the 1920 World Series. On September 15, Ruth hit his 55th home run, shattering his year-old single season record. In late September, the Yankees visited Cleveland and won three out of four games, giving them the upper hand in the race, and clinched their first pennant a few days later. Ruth finished the regular season with 59 home runs, batting .378 and with a slugging percentage of .846.\nThe Yankees had high expectations when they met the New York Giants in the 1921 World Series, every game of which was played in the Polo Grounds. The Yankees won the first two games with Ruth in the lineup. However, Ruth badly scraped his elbow during Game 2 when he slid into third base (he had walked and stolen both second and third bases). After the game, he was told by the team physician not to play the rest of the series. Despite this advice, he did play in the next three games, and pinch-hit in Game Eight of the best-of-nine series, but the Yankees lost, five games to three. Ruth hit .316, drove in five runs and hit his first World Series home run.\nAfter the Series, Ruth and teammates Bob Meusel and Bill Piercy participated in a barnstorming tour in the Northeast. A rule then in force prohibited World Series participants from playing in exhibition games during the offseason, the purpose being to prevent Series participants from replicating the Series and undermining its value. Baseball Commissioner Kenesaw Mountain Landis suspended the trio until May 20, 1922, and fined them their 1921 World Series checks. In August 1922, the rule was changed to allow limited barnstorming for World Series participants, with Landis's permission required.\nOn March 6, 1922, Ruth signed a new contract for three years at $52,000 a year (). This was more than two times the largest sum ever paid to a ballplayer up to that point and it represented 40% of the team's player payroll.\nDespite his suspension, Ruth was named the Yankees' new on-field captain prior to the 1922 season. During the suspension, he worked out with the team in the morning and played exhibition games with the Yankees on their off days. He and Meusel returned on May 20 to a sellout crowd at the Polo Grounds, but Ruth batted 0-for-4 and was booed. On May 25, he was thrown out of the game for throwing dust in umpire George Hildebrand's face, then climbed into the stands to confront a heckler. Ban Johnson ordered him fined, suspended, and stripped of position as team captain. In his shortened season, Ruth appeared in 110 games, batted .315, with 35 home runs, and drove in 99 runs, but the 1922 season was a disappointment in comparison to his two previous dominating years. Despite Ruth's off-year, the Yankees managed to win the pennant and faced the New York Giants in the World Series for the second consecutive year. In the Series, Giants manager John McGraw instructed his pitchers to throw him nothing but curveballs, and Ruth never adjusted. Ruth had just two hits in 17 at bats, and the Yankees lost to the Giants for the second straight year, by 4\u20130 (with one tie game). Sportswriter Joe Vila called him, \"an exploded phenomenon\".\nAfter the season, Ruth was a guest at an Elks Club banquet, set up by Ruth's agent with Yankee team support. There, each speaker, concluding with future New York mayor Jimmy Walker, censured him for his poor behavior. An emotional Ruth promised reform, and, to the surprise of many, followed through. When he reported to spring training, he was in his best shape as a Yankee, weighing only .\nThe Yankees' status as tenants of the Giants at the Polo Grounds had become increasingly uneasy, and in 1922, Giants owner Charles Stoneham said the Yankees' lease, expiring after that season, would not be renewed. Ruppert and Huston had long contemplated a new stadium, and had taken an option on property at 161st Street and River Avenue in the Bronx. Yankee Stadium was completed in time for the home opener on April 18, 1923, at which Ruth hit the first home run in what was quickly dubbed \"the House that Ruth Built\". The ballpark was designed with Ruth in mind: although the venue's left-field fence was further from home plate than at the Polo Grounds, Yankee Stadium's right-field fence was closer, making home runs easier to hit for left-handed batters. To spare Ruth's eyes, right field\u2014his defensive position\u2014was not pointed into the afternoon sun, as was traditional; left fielder Meusel was soon suffering headaches from squinting toward home plate.\nDuring the 1923 season, The Yankees were never seriously challenged and won the AL pennant by 17 games. Ruth finished the season with a career-high .393 batting average and 41 home runs, which tied Cy Williams for the most in the major-leagues that year. Ruth hit a career-high 45 doubles in 1923, and he reached base 379 times, then a major league record. For the third straight year, the Yankees faced the Giants in the World Series, which Ruth dominated. He batted .368, walked eight times, scored eight runs, hit three home runs and slugged 1.000 during the series, as the Yankees christened their new stadium with their first World Series championship, four games to two.\nBatting title and \"bellyache\" (1924\u20131925).\nIn 1924, the Yankees were favored to become the first team to win four consecutive pennants. Plagued by injuries, they found themselves in a battle with the Senators. Although the Yankees won 18 of 22 at one point in September, the Senators beat out the Yankees by two games. Ruth hit .378, winning his only AL batting title, with a league-leading 46 home runs.\nRuth did not look like an athlete; he was described as \"toothpicks attached to a piano\", with a big upper body but thin wrists and legs. Ruth had kept up his efforts to stay in shape in 1923 and 1924, but by early 1925 weighed nearly . His annual visit to Hot Springs, Arkansas, where he exercised and took saunas early in the year, did him no good as he spent much of the time carousing in the resort town. He became ill while there, and suffered relapses during spring training. Ruth collapsed in Asheville, North Carolina, as the team journeyed north. He was put on a train for New York, where he was briefly hospitalized. A rumor circulated that he had died, prompting British newspapers to print a premature obituary. In New York, Ruth collapsed again and was found unconscious in his hotel bathroom. He was taken to a hospital where he suffered multiple convulsions. After sportswriter W. O. McGeehan wrote that Ruth's illness was due to binging on hot dogs and soda pop before a game, it became known as \"the bellyache heard 'round the world\". However, the exact cause of his ailment has never been confirmed and remains a mystery. Glenn Stout, in his history of the Yankees, writes that the Ruth legend is \"still one of the most sheltered in sports\"; he suggests that alcohol was at the root of Ruth's illness, pointing to the fact that Ruth remained six weeks at St. Vincent's Hospital but was allowed to leave, under supervision, for workouts with the team for part of that time. He concludes that the hospitalization was behavior-related. Playing just 98 games, Ruth had his worst season as a Yankee; he finished with a .290 average and 25 home runs. The Yankees finished next to last in the AL with a 69\u201385 record, their last season with a losing record until 1965.\nMurderers' Row (1926\u20131928).\nRuth spent part of the offseason of 1925\u201326 working out at Artie McGovern's gym, where he got back into shape. Barrow and Huggins had rebuilt the team and surrounded the veteran core with good young players like Tony Lazzeri and Lou Gehrig, but the Yankees were not expected to win the pennant.\nRuth returned to his normal production during 1926, when he batted .372 with 47 home runs and 146 RBIs. The Yankees built a 10-game lead by mid-June and coasted to win the pennant by three games. The St. Louis Cardinals had won the National League with the lowest winning percentage for a pennant winner to that point (.578) and the Yankees were expected to win the World Series easily. Although the Yankees won the opener in New York, St. Louis took Games Two and Three. In Game Four, Ruth hit three home runs\u2014the first time this had been done in a World Series game\u2014to lead the Yankees to victory. In the fifth game, Ruth caught a ball as he crashed into the fence. The play was described by baseball writers as a defensive gem. New York took that game, but Grover Cleveland Alexander won Game Six for St. Louis to tie the Series at three games each, then got very drunk. He was nevertheless inserted into Game Seven in the seventh inning and shut down the Yankees to win the game, 3\u20132, and win the Series. Ruth had hit his fourth home run of the Series earlier in the game and was the only Yankee to reach base off Alexander; he walked in the ninth inning before being thrown out to end the game when he attempted to steal second base. Although Ruth's attempt to steal second is often deemed a baserunning blunder, Creamer pointed out that the Yankees' chances of tying the game would have been greatly improved with a runner in scoring position.\nThe 1926 World Series was also known for Ruth's promise to Johnny Sylvester, a hospitalized 11-year-old boy. Ruth promised the child that he would hit a home run on his behalf. Sylvester had been injured in a fall from a horse, and a friend of Sylvester's father gave the boy two autographed baseballs signed by Yankees and Cardinals. The friend relayed a promise from Ruth (who did not know the boy) that he would hit a home run for him. After the Series, Ruth visited the boy in the hospital. When the matter became public, the press greatly inflated it, and by some accounts, Ruth allegedly saved the boy's life by visiting him, emotionally promising to hit a home run, and doing so. Ruth's 1926 salary of $52,000 was far more than any other baseball player, but he made at least twice as much in other income, including $100,000 from 12 weeks of vaudeville.\nThe 1927 New York Yankees team is considered one of the greatest squads to ever take the field. Known as Murderers' Row because of the power of its lineup, the team clinched first place on Labor Day, won a then-AL-record 110 games and took the AL pennant by 19 games. There was no suspense in the pennant race, and the nation turned its attention to Ruth's pursuit of his own single-season home run record of 59 round trippers. Ruth was not alone in this chase. Teammate Lou Gehrig proved to be a slugger who was capable of challenging Ruth for his home run crown; he tied Ruth with 24 home runs late in June. Through July and August, the dynamic duo was never separated by more than two home runs. Gehrig took the lead, 45\u201344, in the first game of a doubleheader at Fenway Park early in September; Ruth responded with two blasts of his own to take the lead, as it proved permanently\u2014Gehrig finished with 47. Even so, as of September 6, Ruth was still several games off his 1921 pace, and going into the final series against the Senators, had only 57. He hit two in the first game of the series, including one off of Paul Hopkins, facing his first major league batter, to tie the record. The following day, September 30, he broke it with his 60th homer, in the eighth inning off Tom Zachary to break a 2\u20132 tie. \"Sixty! Let's see some son of a bitch try to top that one\", Ruth exulted after the game. In addition to his career-high 60 home runs, Ruth batted .356, drove in 164 runs and slugged .772. In the 1927 World Series, the Yankees swept the Pittsburgh Pirates in four games; the National Leaguers were disheartened after watching the Yankees take batting practice before Game One, with ball after ball leaving Forbes Field. According to Appel, \"The 1927 New York Yankees. Even today, the words inspire awe... all baseball success is measured against the '27 team.\"\nThe following season started off well for the Yankees, who led the league in the early going. But the Yankees were plagued by injuries, erratic pitching and inconsistent play. The Philadelphia Athletics, rebuilding after some lean years, erased the Yankees' big lead and even took over first place briefly in early September. The Yankees, however, regained first place when they beat the Athletics three out of four games in a pivotal series at Yankee Stadium later that month, and clinched the pennant in the final weekend of the season. Ruth's play in 1928 mirrored his team's performance. He got off to a hot start and on August 1, he had 42 home runs. This put him ahead of his 60 home run pace from the previous season. He then slumped for the latter part of the season, and he hit just twelve home runs in the last two months. Ruth's batting average also fell to .323, well below his career average. Nevertheless, he ended the season with 54 home runs. The Yankees swept the favored Cardinals in four games in the World Series, with Ruth batting .625 and hitting three home runs in Game Four, including one off Alexander.\n\"Called shot\" and final Yankee years (1929\u20131934).\nBefore the 1929 season, Ruppert (who had bought out Huston in 1923) announced that the Yankees would wear uniform numbers to allow fans at cavernous Yankee Stadium to easily identify the players. The Cardinals and Indians had each experimented with uniform numbers; the Yankees were the first to use them on both home and away uniforms. Ruth batted third and was given number 3. According to a long-standing baseball legend, the Yankees adopted their now-iconic pinstriped uniforms in hopes of making Ruth look slimmer. In truth, though, they had been wearing pinstripes since 1915.\nAlthough the Yankees started well, the Athletics soon proved they were the better team in 1929, splitting two series with the Yankees in the first month of the season, then taking advantage of a Yankee losing streak in mid-May to gain first place. Although Ruth performed well, the Yankees were not able to catch the Athletics\u2014Connie Mack had built another great team. Tragedy struck the Yankees late in the year as manager Huggins died at 51 of erysipelas, a bacterial skin infection, on September 25, only ten days after he had last directed the team. Despite their past differences, Ruth praised Huggins and described him as a \"great guy\". The Yankees finished second, 18 games behind the Athletics. Ruth hit .345 during the season, with 46 home runs and 154 RBIs.\nOn October 17, the Yankees hired Bob Shawkey as manager; he was their fourth choice. Ruth had politicked for the job of player-manager, but Ruppert and Barrow never seriously considered him for the position. Stout deemed this the first hint Ruth would have no future with the Yankees once he retired as a player. Shawkey, a former Yankees player and teammate of Ruth, would prove unable to command Ruth's respect.\nOn January 7, 1930, salary negotiations between the Yankees and Ruth quickly broke down. Having just concluded a three-year contract at an annual salary of $70,000, Ruth promptly rejected both the Yankees' initial proposal of $70,000 for one year and their 'final' offer of two years at seventy-five\u2014the latter figure equalling the annual salary of then US President Herbert Hoover; instead, Ruth demanded at least $85,000 and three years. When asked why he thought he was \"worth more than the President of the United States,\" Ruth responded: \"Say, if I hadn't been sick last summer, I'd have broken hell out of that home run record! Besides, the President gets a four-year contract. I'm only asking for three.\" Exactly two months later, a compromise was reached, with Ruth settling for two years at an unprecedented $80,000 per year. Ruth's salary was more than 2.4 times greater than the next-highest salary that season, a record margin .\nIn 1930, Ruth hit .359 with 49 home runs (his best in his years after 1928) and 153 RBIs, and pitched his first game in nine years, a complete game victory. Nevertheless, the Athletics won their second consecutive pennant and World Series, as the Yankees finished in third place, sixteen games back. At the end of the season, Shawkey was fired and replaced with Cubs manager Joe McCarthy, though Ruth again unsuccessfully sought the job.\nMcCarthy was a disciplinarian, but chose not to interfere with Ruth, who did not seek conflict with the manager. The team improved in 1931, but was no match for the Athletics, who won 107 games, games in front of the Yankees. Ruth, for his part, hit .373, with 46 home runs and 163 RBIs. He had 31 doubles, his most since 1924. In the 1932 season, the Yankees went 107\u201347 and won the pennant. Ruth's effectiveness had decreased somewhat, but he still hit .341 with 41 home runs and 137 RBIs. Nevertheless, he was sidelined twice due to injuries during the season.\nThe Yankees faced the Cubs, McCarthy's former team, in the 1932 World Series. There was bad blood between the two teams as the Yankees resented the Cubs only awarding half a World Series share to Mark Koenig, a former Yankee. The games at Yankee Stadium had not been sellouts; both were won by the home team, with Ruth collecting two singles, but scoring four runs as he was walked four times by the Cubs pitchers. In Chicago, Ruth was resentful at the hostile crowds that met the Yankees' train and jeered them at the hotel. The crowd for Game Three included New York Governor Franklin D. Roosevelt, the Democratic candidate for president, who sat with Chicago Mayor Anton Cermak. Many in the crowd threw lemons at Ruth, a sign of derision, and others (as well as the Cubs themselves) shouted abuse at Ruth and other Yankees. They were briefly silenced when Ruth hit a three-run home run off Charlie Root in the first inning, but soon revived, and the Cubs tied the score at 4\u20134 in the fourth inning, partly due to Ruth's fielding error in the outfield. When Ruth came to the plate in the top of the fifth, the Chicago crowd and players, led by pitcher Guy Bush, were screaming insults at Ruth. With the count at two balls and one strike, Ruth gestured, possibly in the direction of center field, and after the next pitch (a strike), may have pointed there with one hand. Ruth hit the fifth pitch over the center field fence; estimates were that it traveled nearly . Whether or not Ruth intended to indicate where he planned to (and did) hit the ball (Charlie Devens, who, in 1999, was interviewed as Ruth's surviving teammate in that game, did not think so), the incident has gone down in legend as Babe Ruth's called shot. The Yankees won Game Three, and the following day clinched the Series with another victory. During that game, Bush hit Ruth on the arm with a pitch, causing words to be exchanged and provoking a game-winning Yankee rally.\nRuth remained productive in 1933. He batted .301, with 34 home runs, 103 RBIs, and a league-leading 114 walks, as the Yankees finished in second place, seven games behind the Senators. Athletics manager Connie Mack selected him to play right field in the first Major League Baseball All-Star Game, held on July 6, 1933, at Comiskey Park in Chicago. He hit the first home run in the All-Star Game's history, a two-run blast against Bill Hallahan during the third inning, which helped the AL win the game 4\u20132. During the final game of the 1933 season, as a publicity stunt organized by his team, Ruth was called upon and pitched a complete game victory against the Red Sox, his final appearance as a pitcher. Despite unremarkable pitching numbers, Ruth had a 5\u20130 record in five games for the Yankees, raising his career totals to 94\u201346.\nIn 1934, Ruth played in his last full season with the Yankees. By this time, years of high living were starting to catch up with him. His conditioning had deteriorated to the point that he could no longer field or run. He accepted a pay cut to $35,000 from Ruppert, but he was still the highest-paid player in the major leagues. He could still handle a bat and recorded a .288 batting average with 22 home runs; these were statistics that Reisler described as \"merely mortal\". Ruth was selected to the AL All-Star team for the second consecutive year, even though he was in the twilight of his career. During the game, New York Giants pitcher Carl Hubbell struck out Ruth and four other future Hall-of-Famers consecutively. The Yankees finished second again, seven games behind the Tigers.\nBoston Braves (1935).\nAlthough Ruth knew he was nearly finished as a player, he desired to remain in baseball as a manager. He was often spoken of as a possible candidate as managerial jobs opened up, but in 1932, when he was mentioned as a contender for the Red Sox position, Ruth stated that he was not yet ready to leave the field. There were rumors that Ruth was a likely candidate each time when the Cleveland Indians, Cincinnati Reds, and Detroit Tigers were looking for a manager, but nothing came of them.\nJust before the 1934 season, Ruppert offered to make Ruth the manager of the Yankees' top minor-league team, the Newark Bears, but he was talked out of it by his wife, Claire, and his business manager, Christy Walsh. Early in the 1934 season, Ruth openly campaigned to become the Yankees manager. However, the Yankee job was never a serious possibility. Ruppert always supported McCarthy, who would remain in his position for another 12 seasons. The relationship between Ruth and McCarthy had been lukewarm at best and Ruth's managerial ambitions further chilled their interpersonal relations. By the end of the season, Ruth hinted that he would retire unless Ruppert named him manager of the Yankees. When the time came, Ruppert wanted Ruth to leave the team without drama or hard feelings.\nDuring the 1934\u201335 offseason, Ruth circled the world with his wife; the trip included a barnstorming tour of the Far East. At his final stop in the United Kingdom before returning home, Ruth was introduced to cricket by Australian player Alan Fairfax, and after having little luck in a cricketer's stance, he stood as a baseball batter and launched some massive shots around the field, destroying the bat in the process. Although Fairfax regretted that he could not have the time to make Ruth a cricket player, Ruth had lost any interest in such a career upon learning that the best batsmen made only about $40 per week.\nAlso during the offseason, Ruppert had been sounding out the other clubs in hopes of finding one that would be willing to take Ruth as a manager and/or a player. However, the only serious offer came from Athletics owner-manager Connie Mack, who gave some thought to stepping down as manager in favor of Ruth. However, Mack later dropped the idea, saying that Ruth's wife would be running the team in a month if Ruth ever took over.\nWhile the barnstorming tour was underway, Ruppert began negotiating with Boston Braves owner Judge Emil Fuchs, who wanted Ruth as a gate attraction. The Braves had enjoyed modest recent success, finishing fourth in the National League in both 1933 and 1934, but the team drew poorly at the box office. Unable to afford the rent at Braves Field, Fuchs had considered holding dog races there when the Braves were not at home, only to be turned down by Landis. After a series of phone calls, letters, and meetings, the Yankees traded Ruth to the Braves on February 26, 1935. Ruppert had stated that he would not release Ruth to go to another team as a full-time player. For this reason, it was announced that Ruth would become a team vice president and would be consulted on all club transactions, in addition to playing. He was also made assistant manager to Braves skipper Bill McKechnie. In a long letter to Ruth a few days before the press conference, Fuchs promised Ruth a share in the Braves' profits, with the possibility of becoming co-owner of the team. Fuchs also raised the possibility of Ruth succeeding McKechnie as manager, perhaps as early as 1936. Ruppert called the deal \"the greatest opportunity Ruth ever had\".\nThere was considerable attention as Ruth reported for spring training. He did not hit his first home run of the spring until after the team had left Florida, and was beginning the road north in Savannah. He hit two in an exhibition game against the Bears. Amid much press attention, Ruth played his first home game in Boston in over 16 years. Before an opening-day crowd of over 25,000, including five of New England's six state governors, Ruth accounted for all the Braves' runs in a 4\u20132 defeat of the New York Giants, hitting a two-run home run, singling to drive in a third run and later in the inning scoring the fourth. Although age and weight had slowed him, he made a running catch in left field that sportswriters deemed the defensive highlight of the game.\nRuth had two hits in the second game of the season, but it quickly went downhill both for him and the Braves from there. The season soon settled down to a routine of Ruth performing poorly on the few occasions he even played at all. As April passed into May, Ruth's physical deterioration became even more pronounced. While he remained productive at the plate early on, he could do little else. His conditioning had become so poor that he could barely trot around the bases. He made so many errors that three Braves pitchers told McKechnie they would not take the mound if he was in the lineup. Before long, Ruth stopped hitting as well. He grew increasingly annoyed that McKechnie ignored most of his advice. McKechnie later said that Ruth's presence made enforcing discipline nearly impossible.\nRuth soon realized that Fuchs had deceived him, and had no intention of making him manager or giving him any significant off-field duties. He later said his only duties as vice president consisted of making public appearances and autographing tickets. Ruth also found out that far from giving him a share of the profits, Fuchs wanted him to invest some of \"his\" money in the team in a last-ditch effort to improve its balance sheet. As it turned out, Fuchs and Ruppert had both known all along that Ruth's non-playing positions were meaningless.\nBy the end of the first month of the season, Ruth concluded he was finished even as a part-time player. As early as May 12, he asked Fuchs to let him retire. Ultimately, Fuchs persuaded Ruth to remain at least until after the Memorial Day doubleheader in Philadelphia. In the interim was a western road trip, at which the rival teams had scheduled days to honor him. In Chicago and St. Louis, Ruth performed poorly, and his batting average sank to .155, with only two additional home runs for a total of three on the season so far. In the first two games in Pittsburgh, Ruth had only one hit, though a long fly caught by Paul Waner probably would have been a home run in any other ballpark besides Forbes Field.\nRuth played in the third game of the Pittsburgh series on May 25, 1935, and added one more tale to his playing legend. Ruth went 4-for-4, including three home runs, though the Braves lost the game 11\u20137. The last two were off Ruth's old Cubs nemesis, Guy Bush. The final home run, both of the game and of Ruth's career, sailed out of the park over the right field upper deck\u2013the first time anyone had hit a fair ball completely out of Forbes Field. Ruth was urged to make this his last game, but he had given his word to Fuchs and played in Cincinnati and Philadelphia. The first game of the doubleheader in Philadelphia\u2014the Braves lost both\u2014was his final major league appearance. Ruth retired on June 2 after an argument with Fuchs. He finished 1935 with a .181 average\u2014easily his worst as a full-time position player\u2014and the final six of his 714 home runs. The Braves, 10\u201327 when Ruth left, finished 38\u2013115, at .248 the worst winning percentage in modern National League history. Insolvent like his team, Fuchs gave up control of the Braves before the end of the season; the National League took over the franchise at the end of the year.\nOf the 5 members in the inaugural class of Baseball Hall of Fame in 1936 (Ty Cobb, Honus Wagner, Christy Mathewson, Walter Johnson and Ruth himself), only Ruth was not given an offer to manage a baseball team.\nRetirement.\nAlthough Fuchs had given Ruth his unconditional release, no major league team expressed an interest in hiring him in any capacity. Ruth still hoped to be hired as a manager if he could not play anymore, but only one managerial position, Cleveland, became available between Ruth's retirement and the end of the 1937 season. Asked if he had considered Ruth for the job, Indians owner Alva Bradley replied negatively. Team owners and general managers assessed Ruth's flamboyant personal habits as a reason to exclude him from a managerial job; Barrow said of him, \"How can he manage other men when he can't even manage himself?\" Creamer believed Ruth was unfairly treated in never being given an opportunity to manage a major league club. The author believed there was not necessarily a relationship between personal conduct and managerial success, noting that McGraw, Billy Martin, and Bobby Valentine were winners despite character flaws.\nRuth played much golf and in a few exhibition baseball games, where he demonstrated a continuing ability to draw large crowds. This appeal contributed to the Dodgers hiring him as first base coach in 1938. When Ruth was hired, Brooklyn general manager Larry MacPhail made it clear that Ruth would not be considered for the manager's job if, as expected, Burleigh Grimes retired at the end of the season. Although much was said about what Ruth could teach the younger players, in practice, his duties were to appear on the field in uniform and encourage base runners\u2014he was not called upon to relay signs. Ruth got along well with everyone except team captain Leo Durocher, who was hired as Grimes' replacement at season's end. Ruth then left his job as a first base coach and would never again work in any capacity in the game of baseball.\nOn July 4, 1939, Ruth spoke on Lou Gehrig Appreciation Day at Yankee Stadium as members of the 1927 Yankees and a sellout crowd turned out to honor the first baseman, who was forced into premature retirement by ALS, which would kill him two years later. The next week, Ruth went to Cooperstown, New York, for the formal opening of the Baseball Hall of Fame. Three years earlier, he was one of the first five players elected to the hall. As radio broadcasts of baseball games became popular, Ruth sought a job in that field, arguing that his celebrity and knowledge of baseball would assure large audiences, but he received no offers. During World War II, he made many personal appearances to advance the war effort, including his last appearance as a player at Yankee Stadium, in a 1943 exhibition for the Army-Navy Relief Fund. He hit a long fly ball off Walter Johnson; the blast left the field, curving foul, but Ruth circled the bases anyway. In 1946, he made a final effort to gain a job in baseball when he contacted new Yankees boss MacPhail, but he was sent a rejection letter. In 1999, Ruth's granddaughter, Linda Tosetti, and his stepdaughter, Julia Ruth Stevens, said that Babe's inability to land a managerial role with the Yankees caused him to feel hurt and slump into a severe depression.\nPersonal life.\nRuth met Helen Woodford (1897\u20131929), by some accounts, in a coffee shop in Boston where she was a waitress, and they were married as teenagers on October 17, 1914. Although Ruth later claimed to have been married in Elkton, Maryland, records show that they were married at St. Paul's Catholic Church in Ellicott City. They adopted a daughter, Dorothy (1921\u20131989), in 1921. Ruth and Helen separated around 1925, reportedly due to his repeated infidelities and neglect. They appeared in public as a couple for the last time during the 1926 World Series. Helen died in January 1929 at age 31 in a house fire in Watertown, Massachusetts, in a house owned by Edward Kinder, a dentist with whom she had been living as \"Mrs. Kinder\". In her book, \"My Dad, the Babe\", Dorothy claimed that she was Ruth's biological child by a mistress named Juanita Jennings. Juanita admitted to this fact to Dorothy and Julia Ruth Stevens, Dorothy's stepsister, in 1980, who was at the time already very ill.\nOn April 17, 1929 (only three months after the death of his first wife) Ruth married actress and model Claire Merritt Hodgson (1897\u20131976) and adopted her daughter Julia (1916\u20132019). It was the second and final marriage for both parties. Claire, much unlike Helen, was well-travelled and educated, and went on to put structure into Ruth's life, like Miller Huggins did with him on the field.\nBy one account, Julia and Dorothy were, through no fault of their own, the reason for the seven-year rift in Ruth's relationship with teammate Lou Gehrig. Sometime in 1932, during a conversation that she assumed was private, Gehrig's mother remarked, \"It's a shame [Claire] doesn't dress Dorothy as nicely as she dresses her own daughter.\" When the comment inevitably got back to Ruth, he angrily told Gehrig to tell his mother to mind her own business. Gehrig, in turn, took offense at what he perceived as Ruth's comment about his mother. The two men reportedly never spoke off the field until they reconciled at Yankee Stadium on Lou Gehrig Appreciation Day, July 4, 1939, which was shortly after Gehrig's retirement from baseball.\nAlthough Ruth was married throughout most of his baseball career, when team co-owner Tillinghast 'Cap' Huston asked him to tone down his lifestyle, the player said, \"I'll promise to go easier on drinking and to get to bed earlier, but not for you, fifty thousand dollars, or two-hundred and fifty thousand dollars will I give up women. They're too much fun\". A detective that the Yankees hired to follow him one night in Chicago reported that Ruth had been with six women. Ping Bodie said that he was not Ruth's roommate while traveling; \"I room with his suitcase\". Before the start of the 1922 season, Ruth had signed a three-year contact at $52,000 per year with an option to renew for two additional years. His performance during the 1922 season had been disappointing, attributed in part to his drinking and late-night hours. After the end of the 1922 season, he was asked to sign a contract addendum with a morals clause. Ruth and Ruppert signed it on November 11, 1922. It called for Ruth to abstain entirely from the use of intoxicating liquors, and to not stay up later than 1:00\u00a0a.m. during the training and playing season without permission of the manager. Ruth was also enjoined from any action or misbehavior that would compromise his ability to play baseball.\nCancer and death (1946\u20131948).\nAs early as the war years, doctors had cautioned Ruth to take better care of his health, and he grudgingly followed their advice, limiting his drinking and not going on a proposed trip to support the troops in the South Pacific. In 1946, Ruth began experiencing severe pain over his left eye and had difficulty swallowing. In November 1946, Ruth entered French Hospital in New York for tests, which revealed that he had an inoperable malignant tumor at the base of his skull and in his neck. The malady was a lesion known as nasopharyngeal carcinoma, or \"lymphoepithelioma.\" His name and fame gave him access to experimental treatments, and he was one of the first cancer patients to receive both drugs and radiation treatment simultaneously. Having lost , he was discharged from the hospital in February and went to Florida to recuperate. He returned to New York and Yankee Stadium after the season started. The new commissioner, Happy Chandler (Judge Landis had died in 1944), proclaimed April 27, 1947, Babe Ruth Day around the major leagues, with the most significant observance to be at Yankee Stadium. A number of teammates and others spoke in honor of Ruth, who briefly addressed the crowd of almost 60,000. By then, his voice was a soft whisper with a very low, raspy tone.\nAround this time, developments in chemotherapy offered some hope for Ruth. The doctors had not told Ruth he had cancer because of his family's fear that he might do himself harm. They treated him with pterolyl triglutamate (Teropterin), a folic acid derivative; he may have been the first human subject. Ruth showed dramatic improvement during the summer of 1947, so much so that his case was presented by his doctors at a scientific meeting, without using his name. He was able to travel around the country, doing promotional work for the Ford Motor Company on American Legion Baseball. He appeared again at another day in his honor at Yankee Stadium in September, but was not well enough to pitch in an old-timers game as he had hoped.\nThe improvement was only a temporary remission, and by late 1947, Ruth was unable to help with the writing of his autobiography, \"The Babe Ruth Story\", which was almost entirely ghostwritten. In and out of the hospital in Manhattan, he left for Florida in February 1948, doing what activities he could. After six weeks he returned to New York to appear at a book-signing party. He also traveled to California to witness the filming of the movie based on the book.\nOn June 5, 1948, a \"gaunt and hollowed out\" Ruth visited Yale University to donate a manuscript of \"The Babe Ruth Story\" to its library. At Yale, he met with future president George H. W. Bush, who was the captain of the Yale baseball team. On June 13, Ruth visited Yankee Stadium for the final time in his life, appearing at the 25th-anniversary celebrations of \"The House that Ruth Built\". By this time he had lost much weight and had difficulty walking. Introduced along with his surviving teammates from 1923, Ruth used a bat as a cane. Nat Fein's photo of Ruth taken from behind, standing near home plate and facing \"Ruthville\" (right field) became one of baseball's most famous and widely circulated photographs, and won the Pulitzer Prize.\nRuth made one final trip on behalf of American Legion Baseball, then entered Memorial Hospital, where he would die. He was never told he had cancer, but before his death, had surmised it. He was able to leave the hospital for a few short trips, including a final visit to Baltimore. On July 26, 1948, Ruth left the hospital to attend the premiere of the film \"The Babe Ruth Story\". Shortly thereafter, Ruth returned to the hospital for the final time. He was barely able to speak. Ruth's condition gradually grew worse; only a few visitors were allowed to see him, one of whom was National League president and future Commissioner of Baseball Ford Frick. \"Ruth was so thin it was unbelievable. He had been such a big man and his arms were just skinny little bones, and his face was so haggard\", Frick said years later.\nThousands of New Yorkers, including many children, stood vigil outside the hospital during Ruth's final days. On August 16, 1948, at 8:01\u00a0p.m., Ruth died in his sleep at the age of 53. His open casket was placed on display in the rotunda of Yankee Stadium, where it remained for two days; 77,000 people filed past to pay him tribute. His funeral Mass took place at St. Patrick's Cathedral; a crowd estimated at 75,000 waited outside. Ruth rests with his second wife, Claire, on a hillside in Section 25 at the Gate of Heaven Cemetery in Hawthorne, New York.\nMemorial and museum.\nOn April 19, 1949, the Yankees unveiled a granite monument in Ruth's honor in center field of Yankee Stadium. The monument was located in the field of play next to a flagpole and similar tributes to Huggins and Gehrig until the stadium was remodeled from 1974 to 1975, which resulted in the outfield fences moving inward and enclosing the monuments from the playing field. This area was known thereafter as Monument Park. Yankee Stadium, \"the House that Ruth Built\", was replaced after the 2008 season with a new Yankee Stadium across the street from the old one; Monument Park was subsequently moved to the new venue behind the center field fence. Ruth's uniform number 3 has been retired by the Yankees, and he is one of five Yankees players or managers to have a granite monument within the stadium.\nThe Babe Ruth Birthplace Museum is located at 216 Emory Street, a Baltimore row house where Ruth was born, and three blocks west of Oriole Park at Camden Yards, where the AL's Baltimore Orioles play. The property was restored and opened to the public in 1973 by the non-profit Babe Ruth Birthplace Foundation, Inc. Ruth's widow, Claire, his two daughters, Dorothy and Julia, and his sister, Mamie, helped select and install exhibits for the museum.\nContemporary impact.\nRuth was the first baseball star to be the subject of overwhelming public adulation. Baseball had been known for star players such as Ty Cobb and \"Shoeless Joe\" Jackson, but both men had uneasy relations with fans. In Cobb's case, the incidents were sometimes marked by violence. Ruth's biographers agreed that he benefited from the timing of his ascension to \"Home Run King\". The country had been hit hard by both the war and the 1918 flu pandemic and longed for something to help put these traumas behind it. Ruth also resonated in a country which felt, in the aftermath of the war, that it took second place to no one. Montville argued that Ruth was a larger-than-life figure who was capable of unprecedented athletic feats in the nation's largest city. Ruth became an icon of the social changes that marked the early 1920s. In his history of the Yankees, Glenn Stout writes that \"Ruth was New York incarnate\u2014uncouth and raw, flamboyant and flashy, oversized, out of scale, and absolutely unstoppable\".\nDuring his lifetime, Ruth became a symbol of the United States. During World War II Japanese soldiers yelled in English, \"To hell with Babe Ruth\", to anger American soldiers. Ruth replied that he hoped \"every Jap that mention[ed] my name gets shot\". Creamer recorded that \"Babe Ruth transcended sport and moved far beyond the artificial limits of baselines and outfield fences and sports pages\". Wagenheim stated, \"He appealed to a deeply rooted American yearning for the definitive climax: clean, quick, unarguable.\" According to Glenn Stout, \"Ruth's home runs were exalted, uplifting experience that meant more to fans than any runs they were responsible for. A Babe Ruth home run was an event unto itself, one that meant anything was possible.\"\nAlthough Ruth was not just a power hitter\u2014he was the Yankees' best bunter, and an excellent outfielder\u2014Ruth's penchant for hitting home runs altered how baseball is played. Prior to 1920, home runs were unusual, and managers tried to win games by getting a runner on base and bringing him around to score through such means as the stolen base, the bunt, and the hit and run. Advocates of what was dubbed \"inside baseball\", such as Giants manager McGraw, disliked the home run, considering it a blot on the purity of the game. According to sportswriter W. A. Phelon, after the 1920 season, Ruth's breakout performance that season and the response in excitement and attendance, \"settled, for all time to come, that the American public is nuttier over the Home Run than the Clever Fielding or the Hitless Pitching. Viva el Home Run and two times viva Babe Ruth, exponent of the home run, and overshadowing star.\" Bill James states, \"When the owners discovered that the fans \"liked\" to see home runs, and when the foundations of the games were simultaneously imperiled by disgrace [in the Black Sox Scandal], then there was no turning back.\" While a few, such as McGraw and Cobb, decried the passing of the old-style play, teams quickly began to seek and develop sluggers.\nAccording to contemporary sportswriter Grantland Rice, only two sports figures of the 1920s approached Ruth in popularity\u2014boxer Jack Dempsey and racehorse Man o' War. One of the factors that contributed to Ruth's broad appeal was the uncertainty about his family and early life. Ruth appeared to exemplify the American success story, that even an uneducated, unsophisticated youth, without any family wealth or connections, can do something better than anyone else in the world. Montville writes that \"the fog [surrounding his childhood] will make him forever accessible, universal. He will be the patron saint of American possibility.\" Similarly, the fact that Ruth played in the pre-television era, when a relatively small portion of his fans had the opportunity to see him play allowed his legend to grow through word of mouth and the hyperbole of sports reporters. Reisler states that recent sluggers who surpassed Ruth's 60-home run mark, such as Mark McGwire and Barry Bonds, generated much less excitement than when Ruth repeatedly broke the single-season home run record in the 1920s. Ruth dominated a relatively small sports world, while Americans of the present era have many sports available to watch.\nLegacy.\nCreamer describes Ruth as \"a unique figure in the social history of the United States\". Thomas Barthel describes him as one of the first celebrity athletes; numerous biographies have portrayed him as \"larger than life\". He entered the language: a dominant figure in a field, whether within or outside sports, is often referred to as \"the Babe Ruth\" of that field. Similarly, \"Ruthian\" has come to mean in sports, \"colossal, dramatic, prodigious, magnificent; with great power\". He was the first athlete to make more money from endorsements and other off-the-field activities than from his sport.\nIn 2006, Montville stated that more books have been written about Ruth than any other member of the Baseball Hall of Fame. At least five of these books (including Creamer's and Wagenheim's) were written in 1973 and 1974. The books were timed to capitalize on the increase in public interest in Ruth as Hank Aaron approached his career home run mark, which he broke on April 8, 1974. As he approached Ruth's record, Aaron stated, \"I can't remember a day this year or last when I did not hear the name of Babe Ruth.\"\nMontville suggested that Ruth is probably even more popular today than he was when his career home run record was broken by Aaron. The long ball era that Ruth started continues in baseball, to the delight of the fans. Owners build ballparks to encourage home runs, which are featured on \"SportsCenter\" and \"Baseball Tonight\" each evening during the season. The questions of performance-enhancing drug use, which dogged later home run hitters such as McGwire and Bonds, do nothing to diminish Ruth's reputation; his overindulgences with beer and hot dogs seem part of a simpler time.\nIn various surveys and rankings, Ruth has been named the greatest baseball player of all time. In 1998, \"The Sporting News\" ranked him number one on the list of \"Baseball's 100 Greatest Players\". In 1999, baseball fans named Ruth to the Major League Baseball All-Century Team. He was named baseball's Greatest Player Ever in a ballot commemorating the 100th anniversary of professional baseball in 1969. The Associated Press reported in 1993 that Muhammad Ali was tied with Babe Ruth as the most recognized athlete in America. In a 1999 ESPN poll, he was ranked as the second-greatest U.S. athlete of the century, behind Michael Jordan. In 1983, the United States Postal Service honored Ruth with the issuance of a twenty-cent stamp.\nSeveral of the most expensive items of sports memorabilia and baseball memorabilia ever sold at auction are associated with Ruth. , the most expensive piece of sports memorabilia ever sold is Ruth's 1920 Yankees jersey, which sold for $4,415,658 in 2012 (equivalent to $ million in ). The bat with which he hit the first home run at Yankee Stadium is in \"The Guinness Book of World Records\" as the most expensive baseball bat sold at auction, having fetched $1.265\u00a0million on December 2, 2004 (equivalent to $ million in ). A hat of Ruth's from the 1934 season set a record for a baseball cap when David Wells sold it at auction for $537,278 in 2012. In 2017, Charlie Sheen sold Ruth's 1927 World Series ring for $2,093,927 at auction. It easily broke the record for a championship ring previously set when Julius Erving's 1974 ABA championship ring sold for $460,741 in 2011.\nOne long-term survivor of the craze over Ruth may be the Baby Ruth candy bar. The original company to market the confectionery, the Curtis Candy Company, maintained that the bar was named after Ruth Cleveland, daughter of former president Grover Cleveland. She died in 1904 and the bar was first marketed in 1921, at the height of the craze over Ruth. He later sought to market candy bearing his name; he was refused a trademark because of the Baby Ruth bar. Corporate files from 1921 are no longer extant; the brand has changed hands several times and is now owned by Ferrero. The Ruth estate licensed his likeness for use in an advertising campaign for Baby Ruth in 1995. Due to a marketing arrangement, in 2005, the Baby Ruth bar became the official candy bar of Major League Baseball.\nIn 2018, President Donald Trump announced that Ruth, along with Elvis Presley and Antonin Scalia, would posthumously receive the Presidential Medal of Freedom. Montville describes the continuing relevance of Babe Ruth in American culture, more than three-quarters of a century after he last swung a bat in a major league game:\nExternal links.\n \n \n \n "}
{"id": "4174", "revid": "15250126", "url": "https://en.wikipedia.org/wiki?curid=4174", "title": "Bacon number", "text": ""}
{"id": "4177", "revid": "1010273580", "url": "https://en.wikipedia.org/wiki?curid=4177", "title": "Barge", "text": "A barge is a shoal-draft flat-bottomed boat, built mainly for river and canal transport of bulk goods. Originally barges were towed by draft horses on an adjacent towpath. Barges have changed throughout time. From 1967-1983 barges were considered a flat bottom boat that was nineteen feet in length or larger. Today, barges may be self-propelled, usually with a slow-revving diesel engine and a large-diameter fixed-pitch propeller. Otherwise, \"dumb barges\" must be towed by tugs, or pushed by pusher boats. Compared to a towed barge, a pusher system has improved handling and is more efficient, as the pushing tug becomes \"part of the unit\" and it contributes to the momentum of the whole.\nHistory.\nIn Great Britain, during the Industrial Revolution, a substantial network of narrow barges was developed from 1750 onwards; but from 1825 competition from the railways eventually took over from canal traffic due to the higher speed, falling costs and route flexibility of rail transport. Barges carrying bulk and heavy cargoes continue to be viable. Originally, British canals had locks only wide, so narrowboats could be no more than 6'10\" wide if they were to be able to navigate the system. It was soon realised that narrow locks were too limiting, and later locks were doubled in width to . Accordingly, on the British canal system the term 'barge' is used to describe a \"Thames [sailing barge], Duch [barge], or other styles of barge\" (the people who move barges are often known as lightermen), and does not include Narrowboats and Widebeams (see also canal craft).\nIn the United States, deckhands perform the labor and are supervised by a leadman or the mate. The captain and pilot steer the towboat, which pushes one or more barges held together with rigging, collectively called 'the tow'. The crew live aboard the towboat as it travels along the inland river system or the intracoastal waterways. These towboats travel between ports and are also called line-haul boats.\nPoles were not used on barges to fend off the barge as it nears other vessels or a wharf. These are often called 'pike poles'. (The long pole used to maneuver or propel a barge has given rise to the saying \"I wouldn't touch that with a barge pole\".)\nEtymology.\n\"Barge\" is attested from 1300, from Old French \"barge\", from Vulgar Latin \"barga\". The word originally could refer to any small boat; the modern meaning arose around 1480. \"Bark\" \"small ship\" is attested from 1420, from Old French \"barque\", from Vulgar Latin \"barca\" (400 AD). The more precise meaning \"three-masted ship\" arose in the 17th century, and often takes the French spelling for disambiguation. Both are probably derived from the Latin \"barica\", from Greek \"baris\" \"Egyptian boat\", from Coptic \"bari\" \"small boat\", hieroglyphic Egyptian and similar \"ba-y-r\" for \"basket-shaped boat\". By extension, the term \"embark\" literally means to board the kind of boat called a \"barque\".\nThe long pole used to maneuver or propel a barge have given rise to the saying \"I wouldn't touch that [subject/thing] with a barge pole.\"\nModern use.\nBarges are used today for low-value bulk items, as the cost of hauling goods by barge is very low. Barges are also used for very heavy or bulky items; a typical American barge measures , and can carry up to about of cargo. The most common European barge measures and can carry up to about .\nAs an example, on June 26, 2006, a catalytic cracking unit reactor was shipped by barge from the Tulsa Port of Catoosa in Oklahoma to a refinery in Pascagoula, Mississippi. Extremely large objects are normally shipped in sections and assembled onsite, but shipping an assembled unit reduced costs and avoided reliance on construction labor at the delivery site (which in this case was still recovering from Hurricane Katrina). Of the reactor's journey, only about were traveled overland, from the final port to the refinery.\nSelf-propelled barges may be used as such when traveling downstream or upstream in placid waters; they are operated as an unpowered barge, with the assistance of a tugboat, when traveling upstream in faster waters. Canal barges are usually made for the particular canal in which they will operate.\nMany barges, primarily Dutch barges, which were originally designed for carrying cargo along the canals of Europe, are no longer large enough to compete in this industry with larger newer vessels. Many of these barges have been renovated and are now used as luxury hotel barges carrying holidaymakers along the same canals on which they once carried grain or coal.\nBarges in the United States.\nIn times before industrial development, railways, and highways: barges were the predominant and most efficient means of inland transportation in many regions. This holds true today, for many areas of the world.\nIn such pre-industrialized, or poorly developed infrastructure regions, many barges are purpose-designed to be powered on waterways by long slender poles \u2013 thereby becoming known on American waterways as poleboats as the extensive west of North America was settled using the vast tributary river systems of the Mississippi drainage basin. Poleboats use muscle power of \"walkers\" along the sides of the craft pushing a pole against the streambed, canal or lake bottom to move the vessel where desired. In settling the American west it was generally faster to navigate downriver from Brownsville, Pennsylvania, to the Ohio River confluence with the Mississippi and then pole upriver against the current to St. Louis than to travel overland on the rare primitive dirt roads for many decades after the American Revolution.\nOnce the New York Central and Pennsylvania Railroads reached Chicago, that time dynamic changed, and American poleboats became less common, relegated to smaller rivers and more remote streams. On the Mississippi riverine system today, including that of other sheltered waterways, industrial barge trafficking in bulk raw materials such as coal, coke, timber, iron ore and other minerals is extremely common; in the developed world using huge cargo barges that connect in groups and trains-of-barges in ways that allow cargo volumes and weights considerably greater than those used by pioneers of modern barge systems and methods in the Victorian era.\nSuch barges need to be towed by tugboats or pushed by towboats. Canal barges, towed by draft animals on a waterway adjacent towpath were of fundamental importance in the early Industrial Revolution, whose major early engineering projects were efforts to build viaducts, aqueducts and especially canals to fuel and feed raw materials to nascent factories in the early industrial takeoff (18th century) and take their goods to ports and cities for distribution.\nThe barge and canal system contended favourably with the railways in the early Industrial Revolution before around the 1850s\u20131860s; for example, the Erie Canal in New York state is credited by economic historians with giving the growth boost needed for New York City to eclipse Philadelphia as America's largest port and city \u2013 but such canal systems with their locks, need for maintenance and dredging, pumps and sanitary issues were eventually outcompeted in the carriage of high-value items by the railways due to the higher speed, falling costs and route flexibility of rail transport. Barge and canal systems were nonetheless of great, perhaps even primary, economic importance until after the First World War in Europe, particularly in the more developed nations of the Low Countries, France, Germany and especially Great Britain which more or less made the system characteristically its own.\nNowadays, custom built special purpose equipment called modular barges are extensively used in surveying, mapping, laying and burial of subsea optic fibre cables worldwide and other support services."}
{"id": "4178", "revid": "39569616", "url": "https://en.wikipedia.org/wiki?curid=4178", "title": "Bill Schelter", "text": "William Frederick Schelter (1947 \u2013 July 30, 2001) was a professor of mathematics at The University of Texas at Austin and a Lisp developer and programmer. Schelter is credited with the development of the GNU Common Lisp (GCL) implementation of Common Lisp and the GPL'd version of the computer algebra system Macsyma called Maxima. Schelter authored Austin Kyoto Common Lisp (AKCL) under contract with IBM. AKCL formed the foundation for Axiom, another computer algebra system. AKCL eventually became GNU Common Lisp. He is also credited with the first port of the GNU C compiler to the Intel 386 architecture, used in the original implementation of the Linux kernel.\nSchelter obtained his Ph.D. at McGill University in 1972. His mathematical specialties were noncommutative ring theory and computational algebra and its applications, including automated theorem proving in geometry.\nIn the summer of 2001, age 54, he died suddenly of a heart attack while traveling in Russia."}
{"id": "4179", "revid": "37261445", "url": "https://en.wikipedia.org/wiki?curid=4179", "title": "British English", "text": "British English (BrE) is the standard dialect of the English language as spoken and written in the United Kingdom. Variations exist in formal, written English in the United Kingdom. For example, the adjective \"wee\" is almost exclusively used in parts of Scotland, North East England, Ireland, and occasionally Yorkshire, whereas the adjective \"little\" is predominant elsewhere. Nevertheless, there is a meaningful degree of uniformity in written English within the United Kingdom, and this could be described by the term \"British English\". The forms of spoken English, however, vary considerably more than in most other areas of the world where English is spoken, and so a uniform concept of British English is more difficult to apply to the spoken language. According to Tom McArthur in the \"Oxford Guide to World English\", British English shares \"all the ambiguities and tensions in the word 'British' and as a result can be used and interpreted in two ways, more broadly or more narrowly, within a range of blurring and ambiguity\".\nColloquial portmanteau words for British English include: \"Bringlish\" (recorded from 1967), \"Britglish\" (1973), \"Britlish\" (1976), \"Brenglish\" (1993) and \"Brilish\" (2011).\nHistory.\nEnglish is a West Germanic language that originated from the Anglo-Frisian dialects brought to Britain by Germanic settlers from various parts of what is now northwest Germany and the northern Netherlands. The resident population at this time was generally speaking Common Brittonic\u2014the insular variety of continental Celtic, which was influenced by the Roman occupation. This group of languages (Welsh, Cornish, Cumbric) cohabited alongside English into the modern period, but due to their remoteness from the Germanic languages, influence on English was notably limited. However, the degree of influence remains debated, and it has recently been argued that its grammatical influence accounts for the substantial innovations noted between English and the other West Germanic languages.\nInitially, Old English was a diverse group of dialects, reflecting the varied origins of the Anglo-Saxon Kingdoms of England. One of these dialects, Late West Saxon, eventually came to dominate. The original Old English language was then influenced by two waves of invasion: the first was by speakers of the Scandinavian branch of the Germanic family, who settled in parts of Britain in the 8th and 9th centuries; the second was the Normans in the 11th century, who spoke Old Norman and ultimately developed an English variety of this called Anglo-Norman. These two invasions caused English to become \"mixed\" to some degree (though it was never a truly mixed language in the strictest sense of the word; mixed languages arise from the cohabitation of speakers of different languages, who develop a hybrid tongue for basic communication).\nThe more idiomatic, concrete and descriptive English is, the more it is from Anglo-Saxon origins. The more intellectual and abstract English is, the more it contains Latin and French influences e.g. swine (like the Germanic schwein) is the animal in the field bred by the occupied Anglo-Saxons and pork (like the French porc) is the animal at the table eaten by the occupying Normans.Another example is the Anglo-Saxon \u2018cu\u2019 meaning cow, and the French \u2018b\u0153f\u2019 meaning beef.\nCohabitation with the Scandinavians resulted in a significant grammatical simplification and lexical enrichment of the Anglo-Frisian core of English; the later Norman occupation led to the grafting onto that Germanic core of a more elaborate layer of words from the Romance branch of the European languages. This Norman influence entered English largely through the courts and government. Thus, English developed into a \"borrowing\" language of great flexibility and with a huge vocabulary.\nDialects.\nDialects and accents vary amongst the four countries of the United Kingdom, as well as within the countries themselves.\nThe major divisions are normally classified as English English (or English as spoken in England, which encompasses Southern English dialects, West Country dialects, East and West Midlands English dialects and Northern English dialects), Ulster English in Northern Ireland, Welsh English (not to be confused with the Welsh language), and Scottish English (not to be confused with the Scots language or Scottish Gaelic language). The various British dialects also differ in the words that they have borrowed from other languages. Around the middle of the 15th century, there were points where within the 5 major dialects there were almost 500 ways to spell the word \"though\".\nFollowing its last major survey of English Dialects (1949\u20131950), the University of Leeds has started work on a new project. In May 2007 the Arts and Humanities Research Council awarded a grant to Leeds to study British regional dialects.\nThe team are sifting through a large collection of examples of regional slang words and phrases turned up by the \"Voices project\" run by the BBC, in which they invited the public to send in examples of English still spoken throughout the country. The BBC Voices project also collected hundreds of news articles about how the British speak English from swearing through to items on language schools. This information will also be collated and analysed by Johnson's team both for content and for where it was reported. \"Perhaps the most remarkable finding in the Voices study is that the English language is as diverse as ever, despite our increased mobility and constant exposure to other accents and dialects through TV and radio\". When discussing the award of the grant in 2007, Leeds University stated:\nRegional.\nMost people in Britain speak with a regional accent or dialect. However, about 2% of Britons speak with an accent called Received Pronunciation (also called \"the Queen's English\", \"Oxford English\" and \"BBC English\"), that is essentially region-less. It derives from a mixture of the Midlands and Southern dialects spoken in London in the early modern period. It is frequently used as a model for teaching English to foreign learners.\nIn the South East there are significantly different accents; the Cockney accent spoken by some East Londoners is strikingly different from Received Pronunciation (RP). The Cockney rhyming slang can be (and was initially intended to be) difficult for outsiders to understand, although the extent of its use is often somewhat exaggerated.\nEstuary English has been gaining prominence in recent decades: it has some features of RP and some of Cockney. In London itself, the broad local accent is still changing, partly influenced by Caribbean speech. Immigrants to the UK in recent decades have brought many more languages to the country. Surveys started in 1979 by the Inner London Education Authority discovered over 100 languages being spoken domestically by the families of the inner city's schoolchildren. As a result, Londoners speak with a mixture of accents, depending on ethnicity, neighbourhood, class, age, upbringing, and sundry other factors.\nSince the mass internal migration to Northamptonshire in the 1940s and its position between several major accent regions, it has become a source of various accent developments. In Northampton the older accent has been influenced by overspill Londoners. There is an accent known locally as the Kettering accent, which is a transitional accent between the East Midlands and East Anglian. It is the last southern Midlands accent to use the broad \"a\" in words like \"bath\"/\"grass\" (i.e. barth/grarss). Conversely \"crass\"/\"plastic\" use a slender \"a\". A few miles northwest in Leicestershire the slender \"a\" becomes more widespread generally. In the town of Corby, north, one can find Corbyite, which unlike the Kettering accent, is largely influenced by the West Scottish accent.\nIn addition, many British people can to some degree temporarily \"swing\" their accent towards a more neutral form of English at will, to reduce difficulty where very different accents are involved, or when speaking to foreigners.\nFeatures.\nPhonological features characteristic of British English revolve around the pronunciation of the letter R, as well as the dental plosive T and some diphthongs specific to this dialect.\nT-stopping.\nOnce regarded as a Cockney feature, a number of forms of spoken British English, has became commonly realised as a glottal stop when it is in the intervocalic position, in a process called T-glottalisation. National media being based in London has seen a glottal-stop spreading wider than it once was in word final, \"not\" being heard a no. It is still stigmatised when used at the beginning and central positions, such as \"later\", \"often\" has all but regained . Other consonants subject to this usage in Cockney English are \"p\", as in paer and \"k\" as in baer.\nR-dropping.\nIn most areas of England, outside the West Country and near other countries of the UK, the consonant R is not pronounced if not followed by a vowel, lengthening the preceding vowel instead. This phenomenon is known as non-rhoticity.\nIn these same areas, a tendency exists to insert an R between a word ending in a vowel and a next word beginning with a vowel. This is called the intrusive R. It could be understood as a merger, in that words that once ended in an R and words that did not are no longer treated differently. This is also due to London-centric influences. Examples of R-dropping are \"car\" and \"sugar\", where the R is not pronounced.\nDiphthongisation.\nBritish dialects differ on the extent of diphthongisation of long vowels, with southern varieties extensively turning them into diphthongs, and with northern dialects normally preserving many of them. As a comparison, North American varieties could be said to be in-between.\nNorth.\nLong vowels /i\u02d0/ and /u\u02d0/ are usually preserved, and in several areas also /o\u02d0/ and /e\u02d0/, as in go and say (unlike other varieties of English, that change them to [o\u028a] and [e\u026a] respectively). Some areas go as far as not diphthongising medieval /i\u02d0/ and /u\u02d0/, that give rise to modern /a\u026a/ and /a\u028a/; that is, for example, in the traditional accent of Newcastle upon Tyne, 'out' will sound as 'oot', and in parts of Scotland and North-West England, 'my' will be pronounced as 'me'.\nSouth.\nLong vowels /i\u02d0/ and /u\u02d0/ are diphthongised to [\u026ai] and [\u028au] respectively (or, more technically, [\u028f\u0289], with a raised tongue), so that ee and oo in feed and food are pronounced with a movement. The diphthong [o\u028a] is also pronounced with a greater movement, normally [\u0259\u028a], [\u0259\u0289] or [\u0259\u0268].\nPeople in groups.\nDropping a morphological grammatical number, in collective nouns, is stronger in British English than North American English. This is to treat them as plural when once grammatically singular, a perceived natural number prevails, especially when applying to institutional nouns and groups of people.\nThe noun 'police', for example, undergoes this treatment:\nA football team can be treated likewise:\nThis tendency can be observed in texts produced already in the 19th century. For example, Jane Austen, a British author, writes in Chapter 4 of Pride and Prejudice, published in 1813:All the world are good and agreeable in your eyes.However, in Chapter 16, the grammatical number is used. The world is blinded by his fortune and consequence.\nNegatives.\nSome dialects of British English use negative concords, also known as double negatives. Rather than changing a word or using a positive, words like nobody, not, nothing, and never would be used in the same sentence. While this does not occur in Standard English, it does occur in non-standard dialects. The double negation follows the idea of two different morphemes, one that causes the double negation, and one that is used for the point or the verb.\nStandardisation.\nAs with English around the world, the English language as used in the United Kingdom is governed by convention rather than formal code: there is no body equivalent to the Acad\u00e9mie Fran\u00e7aise or the Real Academia Espa\u00f1ola. Dictionaries (for example, the Oxford English Dictionary, the Longman Dictionary of Contemporary English, the Chambers Dictionary, and the Collins Dictionary) record usage rather than attempting to prescribe it. In addition, vocabulary and usage change with time: words are freely borrowed from other languages and other strains of English, and neologisms are frequent.\nFor historical reasons dating back to the rise of London in the 9th century, the form of language spoken in London and the East Midlands became standard English within the Court, and ultimately became the basis for generally accepted use in the law, government, literature and education in Britain. The standardisation of British English is thought to be from both dialect levelling and a thought of social superiority. Speaking in the Standard dialect created class distinctions; those who did not speak the standard English would be considered of a lesser class or social status and often discounted or considered of a low intelligence. Another contribution to the standardisation of British English was the introduction of the printing press to England in the mid-15th century. In doing so, William Caxton enabled a common language and spelling to be dispersed among the entirety of England at a much faster rate.\nSamuel Johnson's A Dictionary of the English Language (1755) was a large step in the English-language spelling reform, where the purification of language focused on standardising both speech and spelling. By the early 20th century, British authors had produced numerous books intended as guides to English grammar and usage, a few of which achieved sufficient acclaim to have remained in print for long periods and to have been reissued in new editions after some decades. These include, most notably of all, Fowler's Modern English Usage and The Complete Plain Words by Sir Ernest Gowers.\nDetailed guidance on many aspects of writing British English for publication is included in style guides issued by various publishers including The Times newspaper, the Oxford University Press and the Cambridge University Press. The Oxford University Press guidelines were originally drafted as a single broadsheet page by Horace Henry Hart, and were at the time (1893) the first guide of their type in English; they were gradually expanded and eventually published, first as Hart's Rules, and in 2002 as part of \"The Oxford Manual of Style\". Comparable in authority and stature to The Chicago Manual of Style for published American English, the Oxford Manual is a fairly exhaustive standard for published British English that writers can turn to in the absence of specific guidance from their publishing house."}
{"id": "4181", "revid": "26161718", "url": "https://en.wikipedia.org/wiki?curid=4181", "title": "Battle", "text": "A battle is a combat in warfare between two or more armed forces. A war usually consists of multiple battles. In general, a battle is a military engagement that is well defined in duration, area, and force commitment. An engagement with only limited commitment between the forces and without decisive results is sometimes called a skirmish.\nWars and military campaigns are guided by strategy, whereas battles take place on a level of planning and execution known as operational mobility. German strategist Carl von Clausewitz stated that \"the employment of battles ... to achieve the object of war\" was the essence of strategy.\nEtymology.\nBattle is a loanword from the Old French , first attested in 1297, from Late Latin , meaning \"exercise of soldiers and gladiators in fighting and fencing\", from Late Latin (taken from Germanic) \"beat\", from which the English word battery is also derived via Middle English .\nCharacteristics.\nThe defining characteristic of the fight as a concept in Military science has changed with the variations in the organisation, employment and technology of military forces. The English military historian John Keegan suggested an ideal definition of battle as \"something which happens between two armies leading to the moral then physical disintegration of one or the other of them\" but the origins and outcomes of battles can rarely be summarized so neatly. Battle in the 20th and 21st centuries is defined as the combat between large components of the forces in a military campaign, used to achieve military objectives. Where the duration of the battle is longer than a week, it is often for reasons of planning called an operation. Battles can be planned, encountered or forced by one side when the other is unable to withdraw from combat.\nA battle always has as its purpose the reaching of a mission goal by use of military force. A victory in the battle is achieved when one of the opposing sides forces the other to abandon its mission and surrender its forces, routs the other (i.e., forces it to retreat or renders it militarily ineffective for further combat operations) or annihilates the latter, resulting in their deaths or capture. A battle may end in a Pyrrhic victory, which ultimately favors the defeated party. If no resolution is reached in a battle, it can result in a stalemate. A conflict in which one side is unwilling to reach a decision by a direct battle using conventional warfare often becomes an insurgency.\nUntil the 19th century the majority of battles were of short duration, many lasting a part of a day. (The Battle of Preston (1648), the Battle of Nations (1813) and the Battle of Gettysburg (1863) were exceptional in lasting three days.) This was mainly due to the difficulty of supplying armies in the field or conducting night operations. The means of prolonging a battle was typically with siege warfare. Improvements in transport and the sudden evolving of trench warfare, with its siege-like nature during the First World War in the 20th century, lengthened the duration of battles to days and weeks. This created the requirement for unit rotation to prevent combat fatigue, with troops preferably not remaining in a combat area of operations for more than a month.\nThe use of the term \"battle\" in military history has led to its misuse when referring to almost any scale of combat, notably by strategic forces involving hundreds of thousands of troops that may be engaged in either one battle at a time (Battle of Leipzig) or operations (Battle of Kursk). The space a battle occupies depends on the range of the weapons of the combatants. A \"battle\" in this broader sense may be of long duration and take place over a large area, as in the case of the Battle of Britain or the Battle of the Atlantic. Until the advent of artillery and aircraft, battles were fought with the two sides within sight, if not reach, of each other. The depth of the battlefield has also increased in modern warfare with inclusion of the supporting units in the rear areas; supply, artillery, medical personnel etc. often outnumber the front-line combat troops.\nBattles are made up of a multitude of individual combats, skirmishes and small engagements and the combatants will usually only experience a small part of the battle. To the infantryman, there may be little to distinguish between combat as part of a minor raid or a big offensive, nor is it likely that he anticipates the future course of the battle; few of the British infantry who went over the top on the first day on the Somme, 1 July 1916, would have anticipated that the battle would last five months. some of the Allied infantry who had just dealt a crushing defeat to the French at the Battle of Waterloo fully expected to have to fight again the next day (at the Battle of Wavre).\nBattlespace.\nBattlespace is a unified strategic concept to integrate and combine armed forces for the military theatre of operations, including air, information, land, sea and space. It includes the environment, factors and conditions that must be understood to apply combat power, protect the force or complete the mission, comprising enemy and friendly armed forces; facilities; weather; terrain; and the electromagnetic spectrum.\nFactors.\nBattles are decided by various factors, the number and quality of combatants and equipment, the skill of commanders and terrain are among the most prominent. Weapons and armour can be decisive; on many occasions armies have achieved victory through more advanced weapons than those of their opponents. An extreme example was in the Battle of Omdurman, in which a large army of Sudanese Mahdists armed in a traditional manner were destroyed by an Anglo-Egyptian force equipped with Maxim machine guns and artillery.\nOn some occasions, simple weapons employed in an unorthodox fashion have proven advantageous; Swiss pikemen gained many victories through their ability to transform a traditionally defensive weapon into an offensive one. Zulus in the early 19th century were victorious in battles against their rivals in part because they adopted a new kind of spear, the iklwa. Forces with inferior weapons have still emerged victorious at times, for example in the Wars of Scottish Independence and in the First Italo\u2013Ethiopian War. Disciplined troops are often of greater importance; at the Battle of Alesia, the Romans were greatly outnumbered but won because of superior training.\nBattles can also be determined by terrain. Capturing high ground has been the main tactic in innumerable battles. An army that holds the high ground forces the enemy to climb and thus wear themselves down. Areas of jungle and forest, with dense vegetation act as force-multipliers, of benefit to inferior armies. Terrain may have lost importance in modern warfare, due to the advent of aircraft, though the terrain is still vital for camouflage, especially for guerrilla warfare.\nGenerals and commanders also play an important role, Hannibal, Julius Caesar, Khalid ibn Walid, Subutai and Napoleon Bonaparte were all skilled generals and their armies were extremely successful at times. An army that can trust the commands of their leaders with conviction in its success invariably has a higher morale than an army that doubts its every move. The British in the naval Battle of Trafalgar owed its success to the reputation of Admiral Lord Nelson.\nTypes.\nBattles can be fought on land, at sea, and in the air. Naval battles have occurred since before the 5th century BC. Air battles have been far less common, due to their late conception, the most prominent being the Battle of Britain in 1940. Since the Second World War, land or sea battles have come to rely on air support. During the Battle of Midway, five aircraft carriers were sunk without either fleet coming into direct contact.\nBattles and are usually hybrids of different types listed above.\nA \"decisive battle\" is one with political effects, determining the course of the war such as the Battle of Smolensk or bringing hostilities to an end, such as the Battle of Hastings or the Battle of Hattin. A decisive battle can change the balance of power or boundaries between countries. The concept of the \"decisive battle\" became popular with the publication in 1851 of Edward Creasy's \"The Fifteen Decisive Battles of the World\". British military historians J.F.C. Fuller (\"The Decisive Battles of the Western World\") and B.H. Liddell Hart (\"Decisive Wars of History\"), among many others, have written books in the style of Creasy's work.\nLand.\nThere is an obvious difference in the way battles have been fought. Early battles were probably fought between rival hunting bands as unorganized crowds. During the Battle of Megiddo, the first reliably documented battle in the fifteenth century BC, both armies were organised and disciplined; during the many wars of the Roman Empire, barbarians continued to use mob tactics.\nAs the Age of Enlightenment dawned, armies began to fight in highly disciplined lines. Each would follow the orders from their officers and fight as a unit instead of individuals. Armies were divided into regiments, battalions, companies and platoons. These armies would march, line up and fire in divisions.\nNative Americans, on the other hand, did not fight in lines, using guerrilla tactics. American colonists and European forces continued using disciplined lines into the American Civil War.\nA new style arose from the 1850s to the First World War, known as trench warfare, which also led to tactical radio. Chemical warfare also began in 1915.\nBy the Second World War, the use of the smaller divisions, platoons and companies became much more important as precise operations became vital. Instead of the trench stalemate of 1915\u20131917, in the Second World War, battles developed where small groups encountered other platoons. As a result, elite squads became much more recognized and distinguishable. Maneuver warfare also returned with an astonishing pace with the advent of the tank, replacing the cannon of the Enlightenment Age. Artillery has since gradually replaced the use of frontal troops. Modern battles resemble those of the Second World War, along with indirect combat through the use of aircraft and missiles which has come to constitute a large portion of wars in place of battles, where battles are now mostly reserved for capturing cities.\nNaval.\nOne significant difference of modern naval battles, as opposed to earlier forms of combat is the use of marines, which introduced amphibious warfare. Today, a marine is actually an infantry regiment that sometimes fights solely on land and is no longer tied to the navy. A good example of an old naval battle is the Battle of Salamis. Most ancient naval battles were fought by fast ships using the battering ram to sink opposing fleets or steer close enough for boarding in hand-to-hand combat. Troops were often used to storm enemy ships as used by Romans and pirates. This tactic was usually used by civilizations that could not beat the enemy with ranged weaponry. Another invention in the late Middle Ages was the use of Greek fire by the Byzantines, which was used to set enemy fleets on fire. Empty demolition ships utilized the tactic to crash into opposing ships and set it afire with an explosion. After the invention of cannons, naval warfare became useful as support units for land warfare. During the 19th century, the development of mines led to a new type of naval warfare. The ironclad, first used in the American Civil War, resistant to cannons, soon made the wooden ship obsolete. The invention of military submarines, during World War I, brought naval warfare to both above and below the surface. With the development of military aircraft during World War II, battles were fought in the sky as well as below the ocean. Aircraft carriers have since become the central unit in naval warfare, acting as a mobile base for lethal aircraft.\nAerial.\nAlthough the use of aircraft has for the most part always been used as a supplement to land or naval engagements, since their first major military use in World War I aircraft have increasingly taken on larger roles in warfare. During World War I, the primary use was for reconnaissance, and small-scale bombardment. Aircraft began becoming much more prominent in the Spanish Civil War and especially World War II. Aircraft design began specializing, primarily into two types: bombers, which carried explosive payloads to bomb land targets or ships; and fighter-interceptors, which were used to either intercept incoming aircraft or to escort and protect bombers (engagements between fighter aircraft were known as dog fights). Some of the more notable aerial battles in this period include the Battle of Britain and the Battle of Midway. Another important use of aircraft came with the development of the helicopter, which first became heavily used during the Vietnam War, and still continues to be widely used today to transport and augment ground forces. Today, direct engagements between aircraft are rare \u2013 the most modern fighter-interceptors carry much more extensive bombing payloads, and are used to bomb precision land targets, rather than to fight other aircraft. Anti-aircraft batteries are used much more extensively to defend against incoming aircraft than interceptors. Despite this, aircraft today are much more extensively used as the primary tools for both army and navy, as evidenced by the prominent use of helicopters to transport and support troops, the use of aerial bombardment as the \"first strike\" in many engagements, and the replacement of the battleship with the aircraft carrier as the center of most modern navies.\nNaming.\nBattles are usually named after some feature of the battlefield geography, such as a town, forest or river, commonly prefixed \"Battle of...\". Occasionally battles are named after the date on which they took place, such as The Glorious First of June. In the Middle Ages it was considered important to settle on a suitable name for a battle which could be used by the chroniclers. After Henry V of England defeated a French army on October 25, 1415, he met with the senior French herald and they agreed to name the battle after the nearby castle and so it was called the Battle of Agincourt. In other cases, the sides adopted different names for the same battle, such as the Battle of Gallipoli which is known in Turkey as the Battle of \u00c7anakkale. During the American Civil War, the Union tended to name the battles after the nearest watercourse, such as the Battle of Wilsons Creek and the Battle of Stones River, whereas the Confederates favoured the nearby towns, as in the Battles of Chancellorsville and Murfreesboro. Occasionally both names for the same battle entered the popular culture, such as the First Battle of Bull Run and the Second Battle of Bull Run, which are also referred to as the First and Second Battles of Manassas.\nSometimes in desert warfare, there is no nearby town name to use; map coordinates gave the name to the Battle of 73 Easting in the First Gulf War. Some place names have become synonymous with battles, such as the Passchendaele, Pearl Harbor, the Alamo, Thermopylae and Waterloo. Military operations, many of which result in battle, are given codenames, which are not necessarily meaningful or indicative of the type or the location of the battle. Operation Market Garden and Operation Rolling Thunder are examples of battles known by their military codenames. When a battleground is the site of more than one battle in the same conflict, the instances are distinguished by ordinal number, such as the First and Second Battles of Bull Run. An extreme case are the twelve Battles of the Isonzo\u2014First to Twelfth\u2014between Italy and Austria-Hungary during the First World War.\nSome battles are named for the convenience of military historians so that periods of combat can be neatly distinguished from one another. Following the First World War, the British Battles Nomenclature Committee was formed to decide on standard names for all battles and subsidiary actions. To the soldiers who did the fighting, the distinction was usually academic; a soldier fighting at Beaumont Hamel on November 13, 1916 was probably unaware he was taking part in what the committee named the Battle of the Ancre. Many combats are too small to be battles; terms such as \"action\", \"affair\" \"skirmish\", \"firefight\" \"raid\" or \"offensive patrol\" are used to describe small military encounters. These combats often take place within the time and space of a battle and while they may have an objective, they are not necessarily \"decisive\". Sometimes the soldiers are unable to immediately gauge the significance of the combat; in the aftermath of the Battle of Waterloo, some British officers were in doubt as to whether the day's events merited the title of \"battle\" or would be called an \"action\".\nEffects.\nBattles affect the individuals who take part, as well as the political actors. Personal effects of battle range from mild psychological issues to permanent and crippling injuries. Some battle-survivors have nightmares about the conditions they encountered or abnormal reactions to certain sights or sounds and some suffer flashbacks. Physical effects of battle can include scars, amputations, lesions, loss of bodily functions, blindness, paralysis and death. Battles affect politics; a decisive battle can cause the losing side to surrender, while a Pyrrhic victory such as the Battle of Asculum can cause the winning side to reconsider its goals. Battles in civil wars have often decided the fate of monarchs or political factions. Famous examples include the Wars of the Roses, as well as the Jacobite risings. Battles affect the commitment of one side or the other to the continuance of a war, for example the Battle of Inchon and the Battle of Hu\u1ebf during the Tet Offensive."}
{"id": "4182", "revid": "204692", "url": "https://en.wikipedia.org/wiki?curid=4182", "title": "Berry Berenson", "text": "Berinthia \"Berry\" Berenson-Perkins (April 14, 1948 \u2013 September 11, 2001) was an American photographer, actress, and model. Berenson, who was married to actor Anthony Perkins from 1973 until his death in 1992, died in the September 11 attacks as a passenger on American Airlines Flight 11.\nEarly life.\nBerinthia \"Berry\" Berenson was born in Murray Hill, Manhattan. Her mother was born Maria-Luisa Yvonne Radha de Wendt de Kerlor, better known as Gogo Schiaparelli, a socialite of Italian, Swiss, French, and Egyptian ancestry. Her father, Robert Lawrence Berenson, was an American career diplomat turned shipping executive; he was of Lithuanian-Jewish descent, and his family's original surname was \"Valvrojenski\". \nBerenson's maternal grandmother was the Italian-born fashion designer Elsa Schiaparelli, and her maternal grandfather was Wilhelm de Wendt de Kerlor, a Theosophist and psychic medium. Her elder sister, Marisa Berenson, became a well-known model and actress. She also was a great-grandniece of Giovanni Schiaparelli, an Italian astronomer who believed he had discovered the supposed canals of Mars, and a second cousin, once removed, of art expert Bernard Berenson (1865\u20131959) and his sister Senda Berenson (1868\u20131954), an athlete and educator who was one of the first two women elected to the Basketball Hall of Fame.\nCareer.\nFollowing a brief modeling career in the late 1960s, Berenson became a freelance photographer. By 1973, her photographs had been published in \"Life\", \"Glamour\", \"Vogue\" and \"Newsweek\".\nBerenson studied acting at New York's The American Place Theatre with Wynn Handman along with Richard Gere, Philip Anglim, Penelope Milford, Robert Ozn, Ingrid Boulting and her sister Marisa.\nBerenson also appeared in several motion pictures. She starred opposite Anthony Perkins in the 1978 Alan Rudolph film \"Remember My Name\", and appeared with Jeff Bridges in the 1979 film \"Winter Kills\" and Malcolm McDowell in \"Cat People\" (1982).\nPersonal life and death.\nOn August 9, 1973, in Cape Cod, Massachusetts, Berenson, three months pregnant, married her future \"Remember My Name\" costar Anthony Perkins. The couple raised two sons: actor-musician Oz Perkins (born February 2, 1974) and folk/rock recording artist Elvis Perkins (born February 9, 1976). They remained married until Perkins's death from AIDS-related complications on September 12, 1992.\nBerenson died at age 53 in the September 11 attacks aboard American Airlines Flight 11, one day before the ninth anniversary of Perkins's death. She was returning to her California home following a holiday on Cape Cod.\nAt the National September 11 Memorial &amp; Museum, Berenson is memorialized at the North Pool, on Panel N-76."}
{"id": "4183", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=4183", "title": "Botany", "text": "Botany, also called , plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. The term \"botany\" comes from the Ancient Greek word (\"botan\u0113\") meaning \"pasture\", \"herbs\" \"grass\", or \"fodder\"; is in turn derived from (), \"to feed\" or \"to graze\". Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists (in the strict sense) study approximately 410,000 species of land plants of which some 391,000 species are vascular plants (including approximately 369,000 species of flowering plants), and approximately 20,000 are bryophytes.\nBotany originated in prehistory as herbalism with the efforts of early humans to identify \u2013 and later cultivate \u2013 edible, medicinal and poisonous plants, making it one of the oldest branches of science. Medieval physic gardens, often attached to monasteries, contained plants of medical importance. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of nomenclature of Carl Linnaeus that remains in use to this day for the naming of all biological species.\nIn the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.\nModern botany is a broad, multidisciplinary subject with inputs from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which study the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.\nHistory.\nEarly botany.\nThere is evidence humans used plants as far back as 10,000 years ago in the Little Tennessee River Valley, generally as firewood or food. Botany originated as herbalism, the study and use of plants for their medicinal properties. The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient texts from India dating back to before 1100 BCE, Ancient Egypt, in archaic Avestan writings, and in works from China purportedly from before 221 BCE.\nModern botany traces its roots back to Ancient Greece specifically to Theophrastus (c. 371\u2013287 BCE), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the \"Father of Botany\". His major works, \"Enquiry into Plants\" and \"On the Causes of Plants\", constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries later.\nAnother work from Ancient Greece that made an early impact on botany is \"De Materia Medica\", a five-volume encyclopedia about herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. \"De Materia Medica\" was widely read for more than 1,500 years. Important contributions from the medieval Muslim world include Ibn Wahshiyya's \"Nabatean Agriculture\", Ab\u016b \u1e24an\u012bfa D\u012bnawar\u012b's (828\u2013896) the \"Book of Plants\", and Ibn Bassal's \"The Classification of Soils\". In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.\nIn the mid-16th century, botanical gardens were founded in a number of Italian universities. The Padua botanical garden in 1545 is usually considered to be the first which is still in its original location. These gardens continued the practical value of earlier \"physic gardens\", often associated with monasteries, in which plants were cultivated for medical use. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens and their medical uses demonstrated. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621. Throughout this period, botany remained firmly subordinate to medicine.\nGerman physician Leonhart Fuchs (1501\u20131566) was one of \"the three German fathers of botany\", along with theologian Otto Brunfels (1489\u20131534) and physician Hieronymus Bock (1498\u20131554) (also called Hieronymus Tragus). Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.\nPhysician Valerius Cordus (1515\u20131544) authored a botanically and pharmacologically important herbal \"Historia Plantarum\" in 1544 and a pharmacopoeia of lasting importance, the \"Dispensatorium\" in 1546. Naturalist Conrad von Gesner (1516\u20131565) and herbalist John Gerard (1545\u2013c. 1611) published herbals covering the medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522\u20131605) was considered the \"father of natural history\", which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells, a term he coined, in cork, and a short time later in living plant tissue.\nEarly modern botany.\nDuring the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys. By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753, Carl von Linn\u00e9 (Carl Linnaeus) published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus. For the purposes of identification, Linnaeus's \"Systema Sexuale\" classified plants into 24 groups according to the number of their male sexual organs. The 24th group, \"Cryptogamia\", included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.\nIncreasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later Bentham &amp; Hooker system, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the \"Origin of Species\" in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.\nBotany was greatly stimulated by the appearance of the first \"modern\" textbook, Matthias Schleiden's \"\", published in English in 1849 as \"Principles of Scientific Botany\". Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831.\nIn 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.\nLate modern botany.\nBuilding upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822\u20131884), August Weismann (1834\u20131914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters. The work of Katherine Esau (1898\u20131997) on plant anatomy is still a major foundation of modern botany. Her books \"Plant Anatomy\" and \"Anatomy of Seed Plants\" have been key plant structural biology texts for more than half a century.\nThe discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunki\u00e6r whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology. Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887\u20131943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.\nParticularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues), the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled precise description of the rates of gas exchange between plants and the atmosphere. Innovations in statistical analysis by Ronald Fisher, Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research. The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones. The synthetic auxin 2,4-Dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.\n20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown \"in vitro\" ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.\nModern morphology recognises a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome. Furthermore, it emphasises structural dynamics. Modern systematics aims to reflect and discover phylogenetic relationships between plants. Modern Molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species. The theoretical possibility of a practical method for identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.\nScope and importance.\nThe study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells. As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion. Plants are crucial to the future of human society as they provide food, oxygen, medicine, and products for people, as well as creating and preserving soil.\nHistorically, all living things were classified as either animals or plants and botany covered the study of all organisms not considered animals. Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.\nThe strictest definition of \"plant\" includes only the \"land plants\" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life, even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte. Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) \u2013 including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.\nPalaeobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.\nAmong the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.\nHuman nutrition.\nVirtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them. Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level. The modern forms of the major staple foods, such as hemp, teff, maize, rice, wheat and other cereal grasses, pulses, bananas and plantains, as well as hemp, flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral plants with the most desirable characteristics.\nBotanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to humanity's ability to feed the world and provide food security for future generations. Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems. Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant\u2013people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany. Some of the earliest plant-people relationships arose between the indigenous people of Canada in identifying edible plants from inedible plants. This relationship the indigenous people had with plants was recorded by ethnobotanists.\nPlant biochemistry.\nPlant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism. Others make specialised materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.\n box-shadow: 1px 1px 3px rgba(0,0,0,0.2);\"&gt;\nPlants make various photosynthetic pigments, some of which can be seen here through paper chromatography\nXanthophylls\nChlorophyll \"a\"\nChlorophyll \"b\"\nPlants and various other groups of photosynthetic eukaryotes collectively known as \"algae\" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll \"a\". Chlorophyll \"a\" (as well as its plant and green algal-specific cousin chlorophyll \"b\") absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O2) as a by-product.\nThe light energy captured by chlorophyll \"a\" is initially in the form of electrons (and later a proton gradient) that's used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesised. Some of the glucose is converted to starch which is stored in the chloroplast. Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.\nUnlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesising all their fatty acids, and most amino acids. The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out. \nPlants synthesise a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan from which the land plant cell wall is constructed.\nVascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibres that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.\nThe concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved pathways like Crassulacean acid metabolism and the carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common carbon fixation pathway. These biochemical strategies are unique to land plants.\nMedicine and materials.\nPhytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine). Native Americans have used various plants as ways of treating illness or disease for thousands of years. This knowledge Native Americans have on plants has been recorded by enthnobotanists and then in turn has been used by pharmaceutical companies as a way of drug discovery.\nPlants can synthesise useful coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.\nSugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer, can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel. Sweetgrass was used by Native Americans to ward off bugs like mosquitoes. These bug repelling properties of sweetgrass were later found by the American Chemical Society in the molecules phytol and coumarin.\nPlant ecology.\nPlant ecology is the science of the functional relationships between plants and their habitats\u00a0\u2013 the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species. Some ecologists even rely on empirical data from indigenous people that is gathered by ethnobotanists. This information can relay a great deal of information on how the land once was thousands of years ago and how it has changed over that time. The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.\nPlants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilise mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources. They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.\nHerbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example, mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection, honey bees, bats and other animals pollinate flowers and humans and other animals act as dispersal vectors to spread spores and seeds.\nPlants, climate and environmental change.\nPlant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.\nGenetics.\nInheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in \"Pisum sativum\" (peas). What Mendel learned from studying plants has had far-reaching benefits outside of botany. Similarly, \"jumping genes\" were discovered by Barbara McClintock while she was studying maize. Nevertheless, there are some distinctive genetic differences between plants and other organisms.\nSpecies boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, \"Mentha\" \u00d7 \"piperita\", a sterile hybrid between \"Mentha aquatica\" and spearmint, \"Mentha spicata\". The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids. Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes. This is one of several methods used by plants to promote outcrossing. In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.\nUnlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.\nMost sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species. Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals. Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.\nAs in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.\nMolecular genetics.\nA considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, \"Arabidopsis thaliana\", a weedy species in the mustard family (Brassicaceae). The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. \"Arabidopsis\" was the first plant to have its genome sequenced, in 2000. The sequencing of some other relatively small genomes, of rice (\"Oryza sativa\") and \"Brachypodium distachyon\", has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.\nModel plants such as \"Arabidopsis thaliana\" are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in plants. The single celled green alga \"Chlamydomonas reinhardtii\", while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study. A red alga \"Cyanidioschyzon merolae\" has also been used to study some basic chloroplast functions. Spinach, peas, soybeans and a moss \"Physcomitrella patens\" are commonly used to study plant cell biology.\n\"Agrobacterium tumefaciens\", a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species. Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.\nEpigenetics.\nEpigenetics is the study of heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence but cause the organism's genes to behave (or \"express themselves\") differently. One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable, while others are reset in the germ cells.\nEpigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilised egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.\nUnlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodelling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.\nEpigenetic changes can lead to paramutations, which do not follow the Mendelian heritage rules. These epigenetic marks are carried from one generation to the next, with one allele inducing a change on the other.\nPlant evolution.\nThe chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as \"blue-green algae\") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a cyanobacterial resident.\nThe algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants. The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.\nNonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved \"megaspory\" \u2013 their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage. Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms. Gymnosperms produce \"naked seeds\" not fully enclosed in an ovary; modern representatives include conifers, cycads, \"Ginkgo\", and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary. Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.\nPlant physiology.\nPlant physiology encompasses all the internal chemical and physical activities of plants associated with life. Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues. Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.\nMolecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur. Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulfur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals. Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.\nPlant hormones.\nPlants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of \"Mimosa pudica\", the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.\nThe hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light and gravity, and concluded \"It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements\". About the same time, the role of auxins (from the Greek , to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went. The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later. This compound mediates the tropic responses of shoots and roots towards light and gravity. The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.\nCytokinins are a class of plant hormones named for their control of cell division (especially cytokinesis). The natural cytokinin zeatin was discovered in corn, \"Zea mays\", and is a derivative of the purine adenine. Zeatin is produced in roots and transported to shoots in the xylem where it promotes cell division, bud development, and the greening of chloroplasts. The gibberelins, such as Gibberelic acid are diterpenes synthesised from acetyl CoA via the mevalonate pathway. They are involved in the promotion of germination and dormancy-breaking in seeds, in regulation of plant height by controlling stem elongation and the control of flowering. Abscisic acid (ABA) occurs in all land plants except liverworts, and is synthesised from carotenoids in the chloroplasts and other plastids. It inhibits cell division, promotes seed maturation, and dormancy, and promotes stomatal closure. It was so named because it was originally thought to control abscission. Ethylene is a gaseous hormone that is produced in all higher plant tissues from methionine. It is now known to be the hormone that stimulates or regulates fruit ripening and abscission, and it, or the synthetic growth regulator ethephon which is rapidly metabolised to produce ethylene, are used on industrial scale to promote ripening of cotton, pineapples and other climacteric crops.\nAnother class of phytohormones is the jasmonates, first isolated from the oil of \"Jasminum grandiflorum\" which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.\nIn addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.\nPlant anatomy and morphology.\nPlant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form.\nAll plants are multicellular eukaryotes, their DNA stored in nuclei. The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin, larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales divide by construction of a phragmoplast as a template for building a cell plate late in cell division.\nThe bodies of vascular plants including clubmosses, ferns and seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll. Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis. The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.\nThe root system and the shoot system are interdependent \u2013 the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system. Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots. Stolons and tubers are examples of shoots that can grow roots. Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants. In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with plants in \"Streptocarpus\" sect. \"Saintpaulia\", or even a single cell \u2013 which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.\nIn vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch, as in sugar beets and carrots.\nStems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering. Leaves gather sunlight and carry out photosynthesis. Large, flat, flexible, green leaves are called foliage leaves. Gymnosperms, such as conifers, cycads, \"Ginkgo\", and gnetophytes are seed-producing plants with open seeds. Angiosperms are seed-producing plants that produce flowers and have enclosed seeds. Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants. Some plants reproduce sexually, some asexually, and some via both means.\nAlthough reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results. Furthermore, structures can be seen as processes, that is, process combinations.\nSystematic botany.\nSystematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history. It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent \u2013 grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.\nKingdom Plantae belongs to Domain Eukarya and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural \"genera\"); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single worldwide name for each organism. For example, the tiger lily is \"Lilium columbianum\". \"Lilium\" is the genus, and \"columbianum\" the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).\nThe evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships. As an example, species of \"Pereskia\" are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an \"Echinocactus\". However, both \"Pereskia\" and \"Echinocactus\" have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.\nJudging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history \u2013 such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) \u2013 and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent. \nFrom the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having \"direct access to the genetic basis of evolution.\" As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below \u2013 fungi are more closely related to animals than to plants.\nIn 1998, the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered. Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants. Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa. Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed."}
{"id": "4184", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=4184", "title": "Bacillus thuringiensis", "text": "Bacillus thuringiensis (or Bt) is a Gram-positive, soil-dwelling bacterium, commonly used as a biological pesticide. \"B. thuringiensis\" also occurs naturally in the gut of caterpillars of various types of moths and butterflies, as well on leaf surfaces, aquatic environments, animal feces, insect-rich environments, and flour mills and grain-storage facilities. It has also been observed to parasitize other moths such as \"Cadra calidella\"\u2014in laboratory experiments working with \"C. calidella\", many of the moths were diseased due to this parasite.\nDuring sporulation, many Bt strains produce crystal proteins (proteinaceous inclusions), called \u03b4-endotoxins, that have insecticidal action. This has led to their use as insecticides, and more recently to genetically modified crops using Bt genes, such as Bt corn. Many crystal-producing Bt strains, though, do not have insecticidal properties. The subspecies \"israelensis\" is commonly used for control of mosquitoes and of fungus gnats.\nTaxonomy and discovery.\nIn 1902, \"B. thuringiensis\" was first discovered in silkworms by Japanese sericultural engineer . He named it \"B. sotto\", using the Japanese word , here referring to bacillary paralysis. In 1911, German microbiologist Ernst Berliner rediscovered it when he isolated it as the cause of a disease called \"\" in flour moth caterpillars in Thuringia (hence the specific name \"thuringiensis\", \"Thuringian\"). \"B. sotto\" would later be reassigned as \"B. thuringiensis\" var. \"sotto\".\nIn 1976, Robert A. Zakharyan reported the presence of a plasmid in a strain of \"B. thuringiensis\" and suggested the plasmid's involvement in endospore and crystal formation. \"B. thuringiensis\" is closely related to \"B. cereus\", a soil bacterium, and \"B. anthracis\", the cause of anthrax; the three organisms differ mainly in their plasmids. Like other members of the genus, all three are anaerobes capable of producing endospores.\nTubulin was long thought to be specific to eukaryotes. More recently, however, several prokaryotic proteins have been shown to be related to tubulin.\nSubspecies.\nThere are several dozen recognized subspecies of \"B. thuringiensis\". Subspecies commonly used as insecticides include \"B. thuringiensis\" subspecies \"kurstaki\" (Btk), subspecies \"israelensis\" (Bti) and subspecies \"aizawa\".\nMechanism of insecticidal action.\nUpon sporulation, \"B. thuringiensis\" forms crystals of proteinaceous insecticidal \u03b4-endotoxins (called crystal proteins or Cry proteins), which are encoded by \"cry\" genes. In most strains of \"B. thuringiensis\", the \"cry\" genes are located on a plasmid (\"cry\" is not a chromosomal gene in most strains).\nCry toxins have specific activities against insect species of the orders Lepidoptera (moths and butterflies), Diptera (flies and mosquitoes), Coleoptera (beetles) and Hymenoptera (wasps, bees, ants and sawflies), as well as against nematodes. Thus, \"B. thuringiensis\" serves as an important reservoir of Cry toxins for production of biological insecticides and insect-resistant genetically modified crops. When insects ingest toxin crystals, their alkaline digestive tracts denature the insoluble crystals, making them soluble and thus amenable to being cut with proteases found in the insect gut, which liberate the toxin from the crystal. The Cry toxin is then inserted into the insect gut cell membrane, paralyzing the digestive tract and forming a pore. The insect stops eating and starves to death; live Bt bacteria may also colonize the insect, which can contribute to death. Death occurs within a few hours or weeks. The midgut bacteria of susceptible larvae may be required for \"B. thuringiensis\" insecticidal activity.\nIt has been shown that a \"B. thuringiensis\" small RNA called BtsR1 can silence the Cry5Ba toxin expression when outside the host by binding to the RBS site of the Cry5Ba toxin transcript to avoid nematode behavioral defenses. The silencing results in an increased of the bacteria ingestion by \"C. elegans\".The expression of BtsR1 is then reduced after ingestion, resulting in Cry5Ba toxin production and host death.\nIn 1996 another class of insecticidal proteins in Bt was discovered: the vegetative insecticidal proteins (Vip; ). Vip proteins do not share sequence homology with Cry proteins, in general do not compete for the same receptors, and some kill different insects than do Cry proteins.\nIn 2000, a novel subgroup of Cry protein, designated parasporin, was discovered from non-insecticidal \"B. thuringiensis\" isolates. The proteins of parasporin group are defined as \"B. thuringiensis\" and related bacterial parasporal proteins that are not hemolytic, but capable of preferentially killing cancer cells. As of January 2013, parasporins comprise six subfamilies: PS1 to PS6.\nUse of spores and proteins in pest control.\nSpores and crystalline insecticidal proteins produced by \"B. thuringiensis\" have been used to control insect pests since the 1920s and are often applied as liquid sprays. They are now used as specific insecticides under trade names such as DiPel and Thuricide. Because of their specificity, these pesticides are regarded as environmentally friendly, with little or no effect on humans, wildlife, pollinators, and most other beneficial insects, and are used in organic farming; however, the manuals for these products do contain many environmental and human health warnings, and a 2012 European regulatory peer review of five approved strains found, while data exist to support some claims of low toxicity to humans and the environment, the data are insufficient to justify many of these claims.\nNew strains of Bt are developed and introduced over time as insects develop resistance to Bt, or the desire occurs to force mutations to modify organism characteristics, or to use homologous recombinant genetic engineering to improve crystal size and increase pesticidal activity, or broaden the host range of Bt and obtain more effective formulations. Each new strain is given a unique number and registered with the U.S. EPA and allowances may be given for genetic modification depending on \"its parental strains, the proposed pesticide use pattern, and the manner and extent to which the organism has been genetically modified\". Formulations of Bt that are approved for organic farming in the US are listed at the website of the Organic Materials Review Institute (OMRI) and several university extension websites offer advice on how to use Bt spore or protein preparations in organic farming.\nUse of Bt genes in genetic engineering of plants for pest control.\nThe Belgian company Plant Genetic Systems (now part of Bayer CropScience) was the first company (in 1985) to develop genetically modified crops (tobacco) with insect tolerance by expressing \"cry\" genes from \"B. thuringiensis\"; the resulting crops contain delta endotoxin. The Bt tobacco was never commercialized; tobacco plants are used to test genetic modifications since they are easy to manipulate genetically and are not part of the food supply.\nUsage.\nIn 1985, potato plants producing CRY 3A Bt toxin were approved safe by the Environmental Protection Agency, making it the first human-modified pesticide-producing crop to be approved in the US, though many plants produce pesticides naturally, including tobacco, coffee plants, cocoa, and black walnut. This was the 'New Leaf' potato, and it was removed from the market in 2001 due to lack of interest. \nIn 1996, genetically modified maize producing Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.\nThe Bt genes engineered into crops and approved for release include, singly and stacked: Cry1A.105, CryIAb, CryIF, Cry2Ab, Cry3Bb1, Cry34Ab1, Cry35Ab1, mCry3A, and VIP, and the engineered crops include corn and cotton.\nCorn genetically modified to produce VIP was first approved in the US in 2010.\nIn India, by 2014, more than seven million cotton farmers, occupying twenty-six million acres, had adopted Bt cotton.\nMonsanto developed a soybean expressing Cry1Ac and the glyphosate-resistance gene for the Brazilian market, which completed the Brazilian regulatory process in 2010.\nSafety studies.\nThe use of Bt toxins as plant-incorporated protectants prompted the need for extensive evaluation of their safety for use in foods and potential unintended impacts on the environment.\nDietary risk assessment.\nConcerns over the safety of consumption of genetically-modified plant materials that contain Cry proteins have been addressed in extensive dietary risk assessment studies. While the target pests are exposed to the toxins primarily through leaf and stalk material, Cry proteins are also expressed in other parts of the plant, including trace amounts in maize kernels which are ultimately consumed by both humans and animals.\nToxicology studies.\nAnimal models have been used to assess human health risk from consumption of products containing Cry proteins. The United States Environmental Protection Agency recognizes mouse acute oral feeding studies where doses as high as 5,000\u00a0mg/kg body weight resulted in no observed adverse effects. Research on other known toxic proteins suggests that , further suggesting that Bt toxins are not toxic to mammals. The results of toxicology studies are further strengthened by the lack of observed toxicity from decades of use of \"B. thuringiensis\" and its crystalline proteins as an insecticidal spray.\nAllergenicity studies.\nIntroduction of a new protein raised concerns regarding the potential for allergic responses in sensitive individuals. Bioinformatic analysis of known allergens has indicated there is no concern of allergic reactions as a result of consumption of Bt toxins. Additionally, skin prick testing using purified Bt protein resulted in no detectable production of toxin-specific IgE antibodies, even in atopic patients.\nDigestibility studies.\nStudies have been conducted to evaluate the fate of Bt toxins that are ingested in foods. Bt toxin proteins have been shown to digest within minutes of exposure to simulated gastric fluids. The instability of the proteins in digestive fluids is an additional indication that Cry proteins are unlikely to be allergenic, since most known food allergens resist degradation and are ultimately absorbed in the small intestine.\nEcological risk assessment.\nEcological risk assessment aims to ensure there is no unintended impact on non-target organisms and no contamination of natural resources as a result of the use of a new substance, such as the use of Bt in genetically-modified crops. The impact of Bt toxins on the environments where transgenic plants are grown has been evaluated to ensure no adverse effects outside of targeted crop pests.\nPersistence in environment.\nConcerns over possible environmental impact from accumulation of Bt toxins from plant tissues, pollen dispersal, and direct secretion from roots have been investigated. Bt toxins may persist in soil for over 200 days, with half-lives between 1.6 and 22 days. Much of the toxin is initially degraded rapidly by microorganisms in the environment, while some is adsorbed by organic matter and persists longer. Some studies, in contrast, claim that the toxins do not persist in the soil. Bt toxins are less likely to accumulate in bodies of water, but pollen shed or soil runoff may deposit them in an aquatic ecosystem. Fish species are not susceptible to Bt toxins if exposed.\nImpact on non-target organisms.\nThe toxic nature of Bt proteins has an adverse impact on many major crop pests, but ecological risk assessments have been conducted to ensure safety of beneficial non-target organisms that may come into contact with the toxins. Widespread concerns over toxicity in non-target lepidopterans, such as the monarch butterfly, have been disproved through proper exposure characterization, where it was determined that non-target organisms are not exposed to high enough amounts of the Bt toxins to have an adverse effect on the population. Soil-dwelling organisms, potentially exposed to Bt toxins through root exudates, are not impacted by the growth of Bt crops.\nInsect resistance.\nMultiple insects have developed a resistance to \"B. thuringiensis\". In November 2009, Monsanto scientists found the pink bollworm had become resistant to the first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, \"Cry1Ac\". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Monsanto responded by introducing a second-generation cotton with multiple Bt proteins, which was rapidly adopted. Bollworm resistance to first-generation Bt cotton was also identified in Australia, China, Spain, and the United States. Additionally, resistance to Bt was documented in field population of diamondback moth in Hawaii, the continental US, and Asia. Studies in the cabbage looper have suggested that a mutation in the membrane transporter ABCC2 can confer resistance to Bt \"Cry1Ac\".\nSecondary pests.\nSeveral studies have documented surges in \"sucking pests\" (which are not affected by Bt toxins) within a few years of adoption of Bt cotton. In China, the main problem has been with mirids, which have in some cases \"completely eroded all benefits from Bt cotton cultivation\". The increase in sucking pests depended on local temperature and rainfall conditions and increased in half the villages studied. The increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. Another study in five provinces in China found the reduction in pesticide use in Bt cotton cultivars is significantly lower than that reported in research elsewhere, consistent with the hypothesis suggested by recent studies that more pesticide sprayings are needed over time to control emerging secondary pests, such as aphids, spider mites, and lygus bugs.\nSimilar problems have been reported in India, with both mealy bugs and aphids although a survey of small Indian farms between 2002 and 2008 concluded Bt cotton adoption has led to higher yields and lower pesticide use, decreasing over time.\nControversies.\nThe controversies surrounding Bt use are among the many genetically modified food controversies more widely. \nLepidopteran toxicity.\nThe most publicised problem associated with Bt crops is the claim that pollen from Bt maize could kill the monarch butterfly. The paper produced a public uproar and demonstrations against Bt maize; however by 2001 several follow-up studies coordinated by the USDA had asserted that \"the most common types of Bt maize pollen are not toxic to monarch larvae in concentrations the insects would encounter in the fields.\" Similarly, \"B. thuringiensis\" has been widely used for controlling \"Spodoptera littoralis\" larvae growth due to their detrimental pest activities in Africa and Southern Europe. However, \"S. littoralis\" showed resistance to many strains of \"B. thuriginesis\" and were only effectively controlled by a few strains.\nWild maize genetic mixing.\nA study published in \"Nature\" in 2001 reported Bt-containing maize genes were found in maize in its center of origin, Oaxaca, Mexico. In 2002, paper concluded, \"the evidence available is not sufficient to justify the publication of the original paper.\" A significant controversy happened over the paper and \"Nature\"s unprecedented notice.\nA subsequent large-scale study in 2005 failed to find any evidence of genetic mixing in Oaxaca. A 2007 study found the \"transgenic proteins expressed in maize were found in two (0.96%) of 208 samples from farmers' fields, located in two (8%) of 25 sampled communities.\" Mexico imports a substantial amount of maize from the U.S., and due to formal and informal seed networks among rural farmers, many potential routes are available for transgenic maize to enter into food and feed webs. One study found small-scale (about 1%) introduction of transgenic sequences in sampled fields in Mexico; it did not find evidence for or against this introduced genetic material being inherited by the next generation of plants. That study was immediately criticized, with the reviewer writing, \"Genetically, any given plant should be either non-transgenic or transgenic, therefore for leaf tissue of a single transgenic plant, a GMO level close to 100% is expected. In their study, the authors chose to classify leaf samples as transgenic despite GMO levels of about 0.1%. We contend that results such as these are incorrectly interpreted as positive and are more likely to be indicative of contamination in the laboratory.\"\nColony collapse disorder.\nAs of 2007, a new phenomenon called colony collapse disorder (CCD) began affecting bee hives all over North America. Initial speculation on possible causes included new parasites, pesticide use, and the use of Bt transgenic crops. The Mid-Atlantic Apiculture Research and Extension Consortium found no evidence that pollen from Bt crops is adversely affecting bees. According to the USDA, \"Genetically modified (GM) crops, most commonly Bt corn, have been offered up as the cause of CCD. But there is no correlation between where GM crops are planted and the pattern of CCD incidents. Also, GM crops have been widely planted since the late 1990s, but CCD did not appear until 2006. In addition, CCD has been reported in countries that do not allow GM crops to be planted, such as Switzerland. German researchers have noted in one study a possible correlation between exposure to Bt pollen and compromised immunity to \"Nosema\".\" The actual cause of CCD was unknown in 2007, and scientists believe it may have multiple exacerbating causes.\nBeta-exotoxins.\nSome isolates of \"B. thuringiensis\" produce a class of insecticidal small molecules called beta-exotoxin, the common name for which is thuringiensin. A consensus document produced by the OECD says: \"Beta-exotoxins are known to be toxic to humans and almost all other forms of life and its presence is prohibited in \"B. thuringiensis\" microbial products\". Thuringiensins are nucleoside analogues. They inhibit RNA polymerase activity, a process common to all forms of life, in rats and bacteria alike."}
{"id": "4185", "revid": "93143", "url": "https://en.wikipedia.org/wiki?curid=4185", "title": "Bacteriophage", "text": "A bacteriophage (), also known informally as a phage (), is a virus that infects and replicates within bacteria and archaea. The term was derived from \"bacteria\" and the Greek \u03c6\u03b1\u03b3\u03b5\u1fd6\u03bd (\"\"), meaning \"to devour\". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have structures that are either simple or elaborate. Their genomes may encode as few as four genes (e.g. MS2) and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm.\nBacteriophages are among the most common and diverse entities in the biosphere. Bacteriophages are ubiquitous viruses, found wherever bacteria exist. It is estimated there are more than 1031 bacteriophages on the planet, more than every other organism on Earth, including bacteria, combined. Viruses are the most abundant biological entity in the water column of the world's oceans, and the second largest component of biomass after prokaryotes, where up to 9x108 virions per millilitre have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.\nPhages have been used since the late 20th century as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy). On the other hand, phages of \"Inoviridae\" have been shown to complicate biofilms involved in pneumonia and cystic fibrosis and to shelter the bacteria from drugs meant to eradicate disease, thus promoting persistent infection.\nClassification.\nBacteriophages occur abundantly in the biosphere, with different genomes, and lifestyles. Phages are classified by the International Committee on Taxonomy of Viruses (ICTV) according to morphology and nucleic acid.\nIt has been suggested that members of \"Picobirnaviridae\" infect bacteria, but not mammals.\nAnother proposed family is \"Autolykiviridae\" (dsDNA).\nHistory.\nIn 1896, Ernest Hanbury Hankin reported that something in the waters of the Ganges and Yamuna rivers in India had a marked antibacterial action against cholera and it could pass through a very fine porcelain filter. In 1915, British bacteriologist Frederick Twort, superintendent of the Brown Institution of London, discovered a small agent that infected and killed bacteria. He believed the agent must be one of the following:\nTwort's research was interrupted by the onset of World War I, as well as a shortage of funding and the discoveries of antibiotics.\nIndependently, French-Canadian microbiologist F\u00e9lix d'H\u00e9relle, working at the Pasteur Institute in Paris, announced on 3 September 1917, that he had discovered \"an invisible, antagonistic microbe of the dysentery bacillus\". For d\u2019H\u00e9relle, there was no question as to the nature of his discovery: \"In a flash I had understood: what caused my clear spots was in fact an invisible microbe\u2026 a virus parasitic on bacteria.\" D'H\u00e9relle called the virus a bacteriophage, a bacteria-eater (from the Greek \"\" meaning \"to devour\"). He also recorded a dramatic account of a man suffering from dysentery who was restored to good health by the bacteriophages. It was D'Herelle who conducted much research into bacteriophages and introduced the concept of phage therapy.\nMore than a half a century later, in 1969, Max Delbr\u00fcck, Alfred Hershey, and Salvador Luria were awarded the Nobel Prize in Physiology or Medicine for their discoveries of the replication of viruses and their genetic structure.\nUses.\nPhage therapy.\nPhages were discovered to be antibacterial agents and were used in the former Soviet Republic of Georgia (pioneered there by Giorgi Eliava with help from the co-discoverer of bacteriophages, F\u00e9lix d'Herelle) during the 1920s and 1930s for treating bacterial infections. They had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reasons:\nThe use of phages has continued since the end of the Cold War in Russia, Georgia and elsewhere in Central and Eastern Europe. The first regulated, randomized, double-blind clinical trial was reported in the \"Journal of Wound Care\" in June 2009, which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous ulcers of the leg in human patients. The FDA approved the study as a Phase I clinical trial. The study's results demonstrated the safety of therapeutic application of bacteriophages, but did not show efficacy. The authors explained that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin or silver) may have interfered with bacteriophage viability. Shortly after that, another controlled clinical trial in Western Europe (treatment of ear infections caused by \"Pseudomonas aeruginosa\") was reported in the journal \"Clinical Otolaryngology\" in August 2009. The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis associated lung infections, among others.\nMeanwhile, bacteriophage researchers have been developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes that degrade the biofilm matrix, phage structural proteins, and the enzymes responsible for lysis of the bacterial cell wall. There have been results showing that T4 phages that are small in size and short-tailed, can be helpful in detecting \"E.coli\" in the human body.\nTherapeutic efficacy of a phage cocktail was evaluated in a mice model with nasal infection of multidrug-resistant (MDR) \"A. baumannii\". Mice treated with the phage cocktail showed a 2.3-fold higher survival rate than those untreated in seven days post infection. In 2017 a patient with a pancreas compromised by MDR \"A. baumannii\" was put on several antibiotics, despite this the patient's health continued to deteriorate during a four-month period. Without effective antibiotics the patient was subjected to phage therapy using a phage cocktail containing nine different phages that had been demonstrated to be effective against MDR \"A. baumannii\". Once on this therapy the patient's downward clinical trajectory reversed, and returned to health.\nD'Herelle \"quickly learned that bacteriophages are found wherever bacteria thrive: in sewers, in rivers that catch waste runoff from pipes, and in the stools of convalescent patients.\" This includes rivers traditionally thought to have healing powers, including India's Ganges River.\nOther.\nFood industry \u2013 Since 2006, the United States Food and Drug Administration (FDA) and United States Department of Agriculture (USDA) have approved several bacteriophage products. LMP-102 (Intralytix) was approved for treating ready-to-eat (RTE) poultry and meat products. In that same year, the FDA approved LISTEX (developed and produced by Micreos) using bacteriophages on cheese to kill \"Listeria monocytogenes\" bacteria, in order to give them generally recognized as safe (GRAS) status. In July 2007, the same bacteriophage were approved for use on all food products. In 2011 USDA confirmed that LISTEX is a clean label processing aid and is included in USDA. Research in the field of food safety is continuing to see if lytic phages are a viable option to control other food-borne pathogens in various food products.\nDairy industry \u2013 Bacteriophages present in the environment can cause fermentation failures of cheese starter cultures. In order to avoid this, mixed-strain starter cultures and culture rotation regimes can be used.\nDiagnostics \u2013 In 2011, the FDA cleared the first bacteriophage-based product for in vitro diagnostic use. The KeyPath MRSA/MSSA Blood Culture Test uses a cocktail of bacteriophage to detect \"Staphylococcus aureus\" in positive blood cultures and determine methicillin resistance or susceptibility. The test returns results in about five hours, compared to two to three days for standard microbial identification and susceptibility test methods. It was the first accelerated antibiotic-susceptibility test approved by the FDA.\nCounteracting bioweapons and toxins \u2013 Government agencies in the West have for several years been looking to Georgia and the former Soviet Union for help with exploiting phages for counteracting bioweapons and toxins, such as anthrax and botulism. Developments are continuing among research groups in the U.S. Other uses include spray application in horticulture for protecting plants and vegetable produce from decay and the spread of bacterial disease. Other applications for bacteriophages are as biocides for environmental surfaces, e.g., in hospitals, and as preventative treatments for catheters and medical devices before use in clinical settings. The technology for phages to be applied to dry surfaces, e.g., uniforms, curtains, or even sutures for surgery now exists. Clinical trials reported in \"Clinical Otolaryngology\" show success in veterinary treatment of pet dogs with otitis.\nThe SEPTIC bacterium sensing and identification method uses the ion emission and its dynamics during phage infection and offers high specificity and speed for detection.\nPhage display is a different use of phages involving a library of phages with a variable peptide linked to a surface protein. Each phage genome encodes the variant of the protein displayed on its surface (hence the name), providing a link between the peptide variant and its encoding gene. Variant phages from the library may be selected through their binding affinity to an immobilized molecule (e.g., botulism toxin) to neutralize it. The bound, selected phages can be multiplied by reinfecting a susceptible bacterial strain, thus allowing them to retrieve the peptides encoded in them for further study.\nAntimicrobial drug discovery \u2013 Phage proteins often have antimicrobial activity and may serve as leads for peptidomimetics, i.e. drugs that mimic peptides. Phage-ligand technology makes use of phage proteins for various applications, such as binding of bacteria and bacterial components (e.g. endotoxin) and lysis of bacteria.\nBasic research \u2013 Bacteriophages are important model organisms for studying principles of evolution and ecology.\nReplication.\nBacteriophages may have a lytic cycle or a lysogenic cycle. With \"lytic phages\" such as the T4 phage, bacterial cells are broken open (lysed) and destroyed after immediate replication of the virion. As soon as the cell is destroyed, the phage progeny can find new hosts to infect. Lytic phages are more suitable for phage therapy. Some lytic phages undergo a phenomenon known as lysis inhibition, where completed phage progeny will not immediately lyse out of the cell if extracellular phage concentrations are high. This mechanism is not identical to that of temperate phage going dormant and usually, is temporary.\nIn contrast, the \"lysogenic cycle\" does not result in immediate lysing of the host cell. Those phages able to undergo lysogeny are known as temperate phages. Their viral genome will integrate with host DNA and replicate along with it, relatively harmlessly, or may even become established as a plasmid. The virus remains dormant until host conditions deteriorate, perhaps due to depletion of nutrients, then, the endogenous phages (known as prophages) become active. At this point they initiate the reproductive cycle, resulting in lysis of the host cell. As the lysogenic cycle allows the host cell to continue to survive and reproduce, the virus is replicated in all offspring of the cell. An example of a bacteriophage known to follow the lysogenic cycle and the lytic cycle is the phage lambda of \"E. coli.\"\nSometimes prophages may provide benefits to the host bacterium while they are dormant by adding new functions to the bacterial genome, in a phenomenon called lysogenic conversion. Examples are the conversion of harmless strains of \"Corynebacterium diphtheriae\" or \"Vibrio cholerae\" by bacteriophages, to highly virulent ones that cause diphtheria or cholera, respectively. Strategies to combat certain bacterial infections by targeting these toxin-encoding prophages have been proposed.\nAttachment and penetration.\nBacterial cells are protected by a cell wall of polysaccharides, which are important virulence factors protecting bacterial cells against both immune host defenses and antibiotics. To enter a host cell, bacteriophages bind to specific receptors on the surface of bacteria, including lipopolysaccharides, teichoic acids, proteins, or even flagella. This specificity means a bacteriophage can infect only certain bacteria bearing receptors to which they can bind, which in turn, determines the phage's host range. Polysaccharide-degrading enzymes, like endolysins are virion-associated proteins to enzymatically degrade the capsular outer layer of their hosts, at the initial step of a tightly programmed phage infection process.\nHost growth conditions also influence the ability of the phage to attach and invade them. As phage virions do not move independently, they must rely on random encounters with the correct receptors when in solution, such as blood, lymphatic circulation, irrigation, soil water, etc.\nMyovirus bacteriophages use a hypodermic syringe-like motion to inject their genetic material into the cell. After contacting the appropriate receptor, the tail fibers flex to bring the base plate closer to the surface of the cell. This is known as reversible binding. Once attached completely, irreversible binding is initiated and the tail contracts, possibly with the help of ATP, present in the tail, injecting genetic material through the bacterial membrane. The injection is accomplished through a sort of bending motion in the shaft by going to the side, contracting closer to the cell and pushing back up. Podoviruses lack an elongated tail sheath like that of a myovirus, so instead, they use their small, tooth-like tail fibers enzymatically to degrade a portion of the cell membrane before inserting their genetic material.\nSynthesis of proteins and nucleic acid.\nWithin minutes, bacterial ribosomes start translating viral mRNA into protein. For RNA-based phages, RNA replicase is synthesized early in the process. Proteins modify the bacterial RNA polymerase so it preferentially transcribes viral mRNA. The host's normal synthesis of proteins and nucleic acids is disrupted, and it is forced to manufacture viral products instead. These products go on to become part of new virions within the cell, helper proteins that contribute to the assemblage of new virions, or proteins involved in cell lysis. In 1972, Walter Fiers (University of Ghent, Belgium) was the first to establish the complete nucleotide sequence of a gene and in 1976, of the viral genome of bacteriophage MS2. Some dsDNA bacteriophages encode ribosomal proteins, which are thought to modulate protein translation during phage infection.\nVirion assembly.\nIn the case of the T4 phage, the construction of new virus particles involves the assistance of helper proteins that act catalytically during phage morphogenesis. The base plates are assembled first, with the tails being built upon them afterward. The head capsids, constructed separately, will spontaneously assemble with the tails. During assembly of the phage T4 virion, the morphogenetic proteins encoded by the phage genes interact with each other in a characteristic sequence. Maintaining an appropriate balance in the amounts of each of these proteins produced during viral infection appears to be critical for normal phage T4 morphogenesis. The DNA is packed efficiently within the heads. The whole process takes about 15 minutes.\nRelease of virions.\nPhages may be released via cell lysis, by extrusion, or, in a few cases, by budding. Lysis, by tailed phages, is achieved by an enzyme called endolysin, which attacks and breaks down the cell wall peptidoglycan. An altogether different phage type, the filamentous phage, make the host cell continually secrete new virus particles. Released virions are described as free, and, unless defective, are capable of infecting a new bacterium. Budding is associated with certain \"Mycoplasma\" phages. In contrast to virion release, phages displaying a lysogenic cycle do not kill the host but, rather, become long-term residents as prophage.\nCommunication.\nResearch in 2017 revealed that the bacteriophage \u03a63T makes a short viral protein that signals other bacteriophages to lie dormant instead of killing the host bacterium. Arbitrium is the name given to this protein by the researchers who discovered it.\nGenome structure.\nGiven the millions of different phages in the environment, phage genomes come in a variety of forms and sizes. RNA phage such as MS2 have the smallest genomes, of only a few kilobases. However, some DNA phage such as T4 may have large genomes with hundreds of genes; the size and shape of the capsid varies along with the size of the genome. The largest bacteriophage genomes reach a size of 735 kb.\nBacteriophage genomes can be highly mosaic, i.e. the genome of many phage species appear to be composed of numerous individual modules. These modules may be found in other phage species in different arrangements. Mycobacteriophages, bacteriophages with mycobacterial hosts, have provided excellent examples of this mosaicism. In these mycobacteriophages, genetic assortment may be the result of repeated instances of site-specific recombination and illegitimate recombination (the result of phage genome acquisition of bacterial host genetic sequences). Evolutionary mechanisms shaping the genomes of bacterial viruses vary between different families and depend upon the type of the nucleic acid, characteristics of the virion structure, as well as the mode of the viral life cycle.\nSystems biology.\nThe field of systems biology investigates the complex networks of interactions within an organism, usually using computational tools and modeling. For example, a phage genome that enters into a bacterial host cell may express hundreds of phage proteins which will affect the expression of numerous host gene or the host's metabolism. All of these complex interactions can be described and simulated in computer models.\nFor instance, infection of \"Pseudomonas aeruginosa\" by the temperate phage PaP3 changed the expression of 38% (2160/5633) of its host's genes. Many of these effects are probably indirect, hence the challenge becomes to identify the direct interactions among bacteria and phage.\nSeveral attempts have been made to map protein\u2013protein interactions among phage and their host. For instance, bacteriophage lambda was found to interact with its host, \"E. coli\", by dozens of interactions. Again, the significance of many of these interactions remains unclear, but these studies suggest that there most likely are several key interactions and many indirect interactions whose role remains uncharacterized.\nIn the environment.\nMetagenomics has allowed the in-water detection of bacteriophages that was not possible previously.\nAlso, bacteriophages have been used in hydrological tracing and modelling in river systems, especially where surface water and groundwater interactions occur. The use of phages is preferred to the more conventional dye marker because they are significantly less absorbed when passing through ground waters and they are readily detected at very low concentrations. Non-polluted water may contain approximately 2\u00d7108 bacteriophages per ml.\nBacteriophages are thought to contribute extensively to horizontal gene transfer in natural environments, principally via transduction, but also via transformation. Metagenomics-based studies also have revealed that viromes from a variety of environments harbor antibiotic-resistance genes, including those that could confer multidrug resistance.\nModel bacteriophages.\nThe following bacteriophages are extensively studied:"}
{"id": "4186", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4186", "title": "Bacteriostat", "text": ""}
{"id": "4187", "revid": "991938445", "url": "https://en.wikipedia.org/wiki?curid=4187", "title": "Bactericide", "text": "A bactericide or bacteriocide, sometimes abbreviated Bcidal, is a substance which kills bacteria. Bactericides are disinfectants, antiseptics, or antibiotics.\nHowever, material surfaces can also have bactericidal properties based solely on their physical surface structure, as for example biomaterials like insect wings.\nDefinition.\nA bactericide is a substance which kills bacteria. Bactericides are chemical substances like disinfectants, antiseptics, or antibiotics.\nDisinfectants.\nThe most used disinfectants are those applying\nAntiseptics.\nAs antiseptics (i.e., germicide agents that can be used on human or animal body, skin, mucoses, wounds and the like), few of the above-mentioned disinfectants can be used, under proper conditions (mainly concentration, pH, temperature and toxicity toward humans and animals). Among them, some important are\nOthers are generally not applicable as safe antiseptics, either because of their corrosive or toxic nature.\nAntibiotics.\nBactericidal antibiotics kill bacteria; bacteriostatic antibiotics slow their growth or reproduction.\nBactericidal antibiotics that inhibit cell wall synthesis: the beta-lactam antibiotics (penicillin derivatives (penams), cephalosporins (cephems), monobactams, and carbapenems) and vancomycin.\nAlso bactericidal are daptomycin, fluoroquinolones, metronidazole, nitrofurantoin, co-trimoxazole, telithromycin.\nAminoglycosidic antibiotics are usually considered bactericidal, although they may be bacteriostatic with some organisms.\nAs of 2004, the distinction between bactericidal and bacteriostatic agents appeared to be clear according to the basic/clinical definition, but this only applies under strict laboratory conditions and it is important to distinguish microbiological and clinical definitions. The distinction is more arbitrary when agents are categorized in clinical situations. The supposed superiority of bactericidal agents over bacteriostatic agents is of little relevance when treating the vast majority of infections with gram-positive bacteria, particularly in patients with uncomplicated infections and noncompromised immune systems. Bacteriostatic agents have been effectively used for treatment that are considered to require bactericidal activity. Furthermore, some broad classes of antibacterial agents considered bacteriostatic can exhibit bactericidal activity against some bacteria on the basis of in vitro determination of MBC/MIC values. At high concentrations, bacteriostatic agents are often bactericidal against some susceptible organisms. The ultimate guide to treatment of any infection must be clinical outcome.\nSurfaces.\nMaterial surfaces can exhibit bactericidal properties because of their crystallographic surface structure.\nSomewhere in the mid 2000s it was shown that metallic nanoparticles can kill bacteria. The effect of a silver nanoparticle for example depends on its size with a preferential diameter of about 1-10\u00a0nm to interact with bacteria.\nIn 2013, cicada wings were found to have a selective anti-Gram-negative bactericidal effect based on their physical surface structure. Mechanical deformation of the more or less rigid nanopillars found on the wing releases energy, striking and killing bacteria within minutes, hence called a mechano-bactericidal effect.\nIn 2020 researchers have combined cationic polymer adsorption and femtosecond laser surface structuring to generate a bactericidal effect against both Gram-positive \"Staphylococcus aureus\" and Gram-negative \"Escherichia coli\" bacteria on borosilicate glass surfaces, providing a practical platform for the study of the bacteria-surface interaction."}
{"id": "4188", "revid": "38890092", "url": "https://en.wikipedia.org/wiki?curid=4188", "title": "Brion Gysin", "text": "Brion Gysin (19 January 1916 \u2013 13 July 1986) was a painter, writer, sound poet, performance artist and inventor of experimental devices born in Taplow, Buckinghamshire.\nHe is best known for his use of the cut-up technique, alongside his close friend, the novelist William S. Burroughs. With the engineer Ian Sommerville he also invented the Dreamachine, a flicker device designed as an art object to be viewed with the eyes closed. It was in painting and drawing, however, that Gysin devoted his greatest efforts, creating calligraphic works inspired by cursive Japanese \"grass\" script and Arabic script. Burroughs later stated that \"Brion Gysin was the only man I ever respected.\"\nBiography.\nEarly years.\nJohn Clifford Brian Gysin was born at the Canadian military hospital in the grounds of Cliveden, Taplow, England. His mother, Stella Margaret Martin, was a Canadian from Deseronto, Ontario. His father, Leonard Gysin, a captain with the Canadian Expeditionary Force, was killed in action eight months after his son's birth. Stella returned to Canada and settled in Edmonton, Alberta where her son became \"the only Catholic day-boy at an Anglican boarding school\". Graduating at fifteen, Gysin was sent to Downside School in Stratton-on-the-Fosse, near Bath, Somerset in England, a prestigious college run by the Benedictines and known as \"the Eton of Catholic public schools\". Despite, or because of, attending a Catholic school, Gysin became an atheist.\nSurrealism.\nIn 1934, he moved to Paris to study \"La Civilisation Fran\u00e7aise\", an open course given at the Sorbonne where he made literary and artistic contacts through Marie Berthe Aurenche, Max Ernst's second wife. He joined the Surrealist Group and began frequenting Valentine Hugo, Leonor Fini, Salvador Dal\u00ed, Picasso and Dora Maar. A year later, he had his first exhibition at the \"Galerie Quatre Chemins\" in Paris with Ernst, Picasso, Hans Arp, Hans Bellmer, Victor Brauner, Giorgio de Chirico, Dal\u00ed, Marcel Duchamp, Ren\u00e9 Magritte, Man Ray and Yves Tanguy. On the day of the preview, however, he was expelled from the Surrealist Group by Andr\u00e9 Breton, who ordered the poet Paul \u00c9luard to take down his pictures. Gysin was 19 years old. His biographer, John Geiger, suggests the arbitrary expulsion \"had the effect of a curse. Years later, he blamed other failures on the Breton incident. It gave rise to conspiracy theories about the powerful interests who seek control of the art world. He gave various explanations for the expulsion, the more elaborate involving 'insubordination' or \"l\u00e8se majest\u00e9\" towards Breton\".\nAfter World War II.\nAfter serving in the U.S. army during World War II, Gysin published a biography of Josiah \"Uncle Tom\" Henson titled, \"To Master, a Long Goodnight: The History of Slavery in Canada\" (1946). A gifted draughtsman, he took an 18-month course learning the Japanese language (including calligraphy) that would greatly influence his artwork. In 1949, he was among the first Fulbright Fellows. His goal was to research, at the University of Bordeaux and in the Archivo de Indias in Seville, Spain, the history of slavery, a project that he later abandoned. He moved to Tangier, Morocco, after visiting the city with novelist and composer Paul Bowles in 1950. In 1952/3 he met the travel writer and sexual adventurer Anne Cumming and they remained friends until his death.\nMorocco and the Beat Hotel.\nIn 1954 in Tangier, Gysin opened a restaurant called The 1001 Nights, with his friend Mohamed Hamri, who was the cook. Gysin hired the Master Musicians of Jajouka from the village of Jajouka to perform alongside entertainment that included acrobats, a dancing boy and fire eaters. The musicians performed there for an international clientele that included William S. Burroughs. Gysin lost the business in 1958, and the restaurant closed permanently. That same year, Gysin returned to Paris, taking lodgings in a flophouse located at 9 rue G\u00eet-le-Coeur that would become famous as the Beat Hotel. Working on a drawing, he discovered a Dada technique by accident:\nWilliam Burroughs and I first went into techniques of writing, together, back in room No. 15 of the Beat Hotel during the cold Paris spring of 1958... Burroughs was more intent on Scotch-taping his photos together into one great continuum on the wall, where scenes faded and slipped into one another, than occupied with editing the monster manuscript... \"Naked Lunch\" appeared and Burroughs disappeared. He kicked his habit with Apomorphine and flew off to London to see Dr Dent, who had first turned him on to the cure. While cutting a mount for a drawing in room No. 15, I sliced through a pile of newspapers with my Stanley blade and thought of what I had said to Burroughs some six months earlier about the necessity for turning painters' techniques directly into writing. I picked up the raw words and began to piece together texts that later appeared as \"First Cut-Ups\" in \"Minutes to Go\" (Two Cities, Paris 1960).\nWhen Burroughs returned from London in September 1959, Gysin not only shared his discovery with his friend but the new techniques he had developed for it. Burroughs then put the techniques to use while completing \"Naked Lunch\" and the experiment dramatically changed the landscape of American literature. Gysin helped Burroughs with the editing of several of his novels including \"Interzone\", and wrote a script for a film version of \"Naked Lunch\", which was never produced. The pair collaborated on a large manuscript for Grove Press titled \"The Third Mind\" but it was determined that it would be impractical to publish it as originally envisioned. The book later published under that title incorporates little of this material. Interviewed for \"The Guardian\" in 1997, Burroughs explained that Gysin was \"the only man that I've ever respected in my life. I've admired people, I've liked them, but he's the only man I've ever respected.\" In 1969, Gysin completed his finest novel, \"The Process\", a work judged by critic Robert Palmer as \"a classic of 20th century modernism\".\nA consummate innovator, Gysin altered the cut-up technique to produce what he called permutation poems in which a single phrase was repeated several times with the words rearranged in a different order with each reiteration. An example of this is \"I don't dig work, man/Man, work I don't dig.\" Many of these permutations were derived using a random sequence generator in an early computer program written by Ian Sommerville. Commissioned by the BBC in 1960 to produce material for broadcast, Gysin's results included \"Pistol Poem\", which was created by recording a gun firing at different distances and then splicing the sounds. That year, the piece was subsequently used as a theme for the Paris performance of Le Domaine Poetique, a showcase for experimental works by people like Gysin, Fran\u00e7ois Dufr\u00eane, Bernard Heidsieck, and Henri Chopin.\nWith Sommerville, he built the Dreamachine in 1961. Described as \"the first art object to be seen with the eyes closed\", the flicker device uses alpha waves in the 8\u201316 Hz range to produce a change of consciousness in receptive viewers.\nLater years.\nIn April 1974, while sitting at a social engagement, Gysin had a very noticeable rectal bleeding. In May he wrote to Burroughs complaining he was not feeling well. A short time later he was diagnosed with colon cancer and began to receive cobalt treatment. Between December 1974 and April 1975, Gysin had to undergo several surgeries, among them a very traumatic colostomy, that drove him to extreme depression and to a suicide attempt. Later, in \"Fire: Words by Day \u2013 Images by Night\" (1975), a crudely lucid text, he would describe the horrendous ordeal he went through.\nIn 1985 Gysin was made an American Commander of the French Ordre des Arts et des Lettres. He'd begun to work extensively with noted jazz soprano saxophonist Steve Lacy. They recorded an album in 1986 with French musician Ramuntcho Matta, featuring Gysin singing/rapping his own texts, with performances by Lacy, Don Cherry, Elli Medeiros, Lizzy Mercier Descloux and more. The album was reissued on CD in 1993 by Crammed Discs, under the title \"Self-Portrait Jumping\".\nDeath.\nOn 13 July 1986 Brion Gysin died of lung cancer. Anne Cumming arranged his funeral and for his ashes to be scattered at the Caves of Hercules in Morocco. An obituary by Robert Palmer published in \"The New York Times\" described him as a man who \"threw off the sort of ideas that ordinary artists would parlay into a lifetime career, great clumps of ideas, as casually as a locomotive throws off sparks\". Later that year a heavily edited version of his novel, \"The Last Museum\", was published posthumously by Faber &amp; Faber (London) and by Grove Press (New York).\nAs a joke, Gysin had contributed a recipe for marijuana fudge to a cookbook by Alice B. Toklas; it was included for publication, becoming famous under the name Alice B. Toklas brownies.\nBurroughs on the Gysin cut-up.\nIn a 1966 interview by Conrad Knickerbocker for \"The Paris Review\", William S. Burroughs explained that Brion Gysin was, to his knowledge, \"the first to create cut-ups\":\nA friend, Brion Gysin, an American poet and painter, who has lived in Europe for thirty years, was, as far as I know, the first to create cut-ups. His cut-up poem, \"Minutes to Go\", was broadcast by the BBC and later published in a pamphlet. I was in Paris in the summer of 1960; this was after the publication there of \"Naked Lunch\". I became interested in the possibilities of this technique, and I began experimenting myself. Of course, when you think of it, \"The Waste Land\" was the first great cut-up collage, and Tristan Tzara had done a bit along the same lines. Dos Passos used the same idea in 'The Camera Eye' sequences in \"USA\". I felt I had been working toward the same goal; thus it was a major revelation to me when I actually saw it being done.\nInfluence.\nAccording to Jos\u00e9 F\u00e9rez Kuri, author of \"Brion Gysin: Tuning in to the Multimedia Age\" (2003) and co-curator of a major retrospective of the artist's work at The Edmonton Art Gallery in 1998, Gysin's wide range of \"radical ideas would become a source of inspiration for artists of the Beat Generation, as well as for their successors (among them David Bowie, Mick Jagger, Keith Haring, and Laurie Anderson)\". Other artists include Genesis P-Orridge, John Zorn (as displayed on the 2013's Dreamachines album) and Brian Jones.\nSelected bibliography.\nGysin is the subject of John Geiger's biography, \"Nothing Is True Everything Is Permitted: The Life of Brion Gysin\", and features in \"Chapel of Extreme Experience: A Short History of Stroboscopic Light and the Dream Machine\", also by Geiger. \"Man From Nowhere: Storming the Citadels of Enlightenment with William Burroughs and Brion Gysin\", a biographical study of Burroughs and Gysin with a collection of homages to Gysin, was authored by Joe Ambrose, Frank Rynne, and Terry Wilson with contributions by Marianne Faithfull, John Cale, William S. Burroughs, John Giorno, Stanley Booth, Bill Laswell, Mohamed Hamri, Keith Haring and Paul Bowles. A monograph on Gysin was published in 2003 by Thames and Hudson.\nWorks.\nProse\nRadio\nCinema\nMusic\nPainting"}
{"id": "4190", "revid": "4141940", "url": "https://en.wikipedia.org/wiki?curid=4190", "title": "Bulgarian", "text": "Bulgarian may refer to:"}
{"id": "4191", "revid": "2739012", "url": "https://en.wikipedia.org/wiki?curid=4191", "title": "BCG vaccine", "text": "Bacillus Calmette\u2013Gu\u00e9rin (BCG) vaccine is a vaccine primarily used against tuberculosis (TB). It is partly named after its inventors Albert Calmette and Camille Gu\u00e9rin. In countries where tuberculosis or leprosy is common, one dose is recommended in healthy babies as close to the time of birth as possible. In areas where tuberculosis is not common, only children at high risk are typically immunized, while suspected cases of tuberculosis are individually tested for and treated. Adults who do not have tuberculosis and have not been previously immunized but are frequently exposed may be immunized as well. BCG also has some effectiveness against Buruli ulcer infection and other nontuberculous mycobacteria infections. Additionally it is sometimes used as part of the treatment of bladder cancer.\nRates of protection against tuberculosis infection vary widely and protection lasts up to twenty years. Among children it prevents about 20% from getting infected and among those who do get infected it protects half from developing disease. The vaccine is given by injection into the skin. There is no evidence that additional doses are beneficial.\nSerious side effects are rare. Often there is redness, swelling, and mild pain at the site of injection. A small ulcer may also form with some scarring after healing. Side effects are more common and potentially more severe in those with poor immune function. It is not safe for use during pregnancy. The vaccine was originally developed from \"Mycobacterium bovis\", which is commonly found in cows. While it has been weakened, it is still live.\nThe BCG vaccine was first used medically in 1921. It is on the World Health Organization's List of Essential Medicines. , the vaccine is given to about 100 million children per year globally.\nMedical uses.\nTuberculosis.\nThe main use of BCG is for vaccination against tuberculosis. BCG vaccine can be administered after birth intradermally. BCG vaccination can cause a false positive Mantoux test, although a very high-grade reading is usually due to active disease.\nThe most controversial aspect of BCG is the variable efficacy found in different clinical trials, which appears to depend on geography. Trials conducted in the UK have consistently shown a protective effect of 60 to 80%, but those conducted elsewhere have shown no protective effect, and efficacy appears to fall the closer one gets to the equator.\nA 1994 systematic review found that BCG reduces the risk of getting TB by about 50%. There are differences in effectiveness, depending on region, due to factors such as genetic differences in the populations, changes in environment, exposure to other bacterial infections, and conditions in the lab where the vaccine is grown, including genetic differences between the strains being cultured and the choice of growth medium.\nA systematic review and meta analysis conducted in 2014, demonstrated that the BCG vaccine reduced infections by 19\u201327% and reduced progression to active TB by 71%. The studies included in this review were limited to those that used interferon gamma release assay.\nThe duration of protection of BCG is not clearly known. In those studies showing a protective effect, the data are inconsistent. The MRC study showed protection waned to 59% after 15 years and to zero after 20 years; however, a study looking at Native Americans immunized in the 1930s found evidence of protection even 60 years after immunization, with only a slight waning in efficacy.\nBCG seems to have its greatest effect in preventing miliary TB or TB meningitis, so it is still extensively used even in countries where efficacy against pulmonary tuberculosis is negligible.\nEfficacy.\nA number of possible reasons for the variable efficacy of BCG in different countries have been proposed. None have been proven, some have been disproved, and none can explain the lack of efficacy in both low-TB burden countries (US) and high-TB burden countries (India). The reasons for variable efficacy have been discussed at length in a World Health Organization (WHO) document on BCG.\nMycobacteria.\nBCG has protective effects against some non-tuberculosis mycobacteria.\nCancer.\nBCG has been one of the most successful immunotherapies. BCG vaccine has been the \"standard of care for patients with bladder cancer (NMIBC)\" since 1977. By 2014 there were more than eight different considered biosimilar agents or strains used for the treatment of non\u2013muscle-invasive bladder cancer (NMIBC).\nMethod of administration.\nA tuberculin skin test is usually carried out before administering BCG. A reactive tuberculin skin test is a contraindication to BCG due to the risk of severe local inflammation and scarring; it does not indicate any immunity. BCG is also contraindicated in certain people who have IL-12 receptor pathway defects.\nBCG is given as a single intradermal injection at the insertion of the deltoid. If BCG is accidentally given subcutaneously, then a local abscess may form (a \"BCG-oma\") that can sometimes ulcerate, and may require treatment with antibiotics immediately, otherwise without treatment it could spread the infection causing severe damage to vital organs. An abscess is not always associated with incorrect administration, and it is one of the more common complications that can occur with the vaccination. Numerous medical studies on treatment of these abscesses with antibiotics have been done with varying results, but the consensus is once pus is aspirated and analysed, provided no unusual bacilli are present, the abscess will generally heal on its own in a matter of weeks.\nThe characteristic raised scar that BCG immunization leaves is often used as proof of prior immunization. This scar must be distinguished from that of smallpox vaccination, which it may resemble.\nWhen given for bladder cancer, the vaccine is not injected through the skin, but is instilled into the bladder through the urethra using a soft catheter.\nAdverse effects.\nBCG immunization generally causes some pain and scarring at the site of injection. The main adverse effects are keloids\u2014large, raised scars. The insertion to the deltoid muscle is most frequently used because the local complication rate is smallest when that site is used. Nonetheless, the buttock is an alternative site of administration because it provides better cosmetic outcomes.\nBCG vaccine should be given intradermally. If given subcutaneously, it may induce local infection and spread to the regional lymph nodes, causing either suppurative (production of pus) and non-suppurative lymphadenitis. Conservative management is usually adequate for non-suppurative lymphadenitis. If suppuration occurs, it may need needle aspiration. For non-resolving suppuration, surgical excision may be required. Evidence for the treatment of these complications is scarce.\nUncommonly, breast and gluteal abscesses can occur due to haematogenous (carried by the blood) and lymphangiomatous spread. Regional bone infection (BCG osteomyelitis or osteitis) and disseminated BCG infection are rare complications of BCG vaccination, but potentially life-threatening. Systemic anti-tuberculous therapy may be helpful in severe complications.\nIf BCG is accidentally given to an immuno-compromised patient (e.g., an infant with SCID), it can cause disseminated or life-threatening infection. The documented incidence of this happening is less than one per million immunizations given. In 2007, The World Health Organization (WHO) stopped recommending BCG for infants with HIV, even if there is a high risk of exposure to TB, because of the risk of disseminated BCG infection (which is approximately 400 per 100,000 in that higher risk context).\nUsage.\nThe age of the person and the frequency with which BCG is given has always varied from country to country. The World Health Organization (WHO) currently recommends childhood BCG for all countries with a high incidence of TB and/or high leprosy burden. This is a partial list of historic and current BCG practice around the globe. A complete atlas of past and present practice has been generated.\nManufacture.\nBCG is prepared from a strain of the attenuated (virulence-reduced) live bovine tuberculosis bacillus, \"Mycobacterium bovis\", that has lost its ability to cause disease in humans. Because the living bacilli evolve to make the best use of available nutrients, they become less well-adapted to human blood and can no longer induce disease when introduced into a human host. Still, they are similar enough to their wild ancestors to provide some degree of immunity against human tuberculosis. The BCG vaccine can be anywhere from 0 to 80% effective in preventing tuberculosis for a duration of 15 years; however, its protective effect appears to vary according to geography and the lab in which the vaccine strain was grown.\nA number of different companies make BCG, sometimes using different genetic strains of the bacterium. This may result in different product characteristics. OncoTICE, used for bladder instillation for bladder cancer, was developed by Organon Laboratories (since acquired by Schering-Plough, and in turn acquired by Merck &amp; Co.). A similar application is the product of Onko BCG of the Polish company Biomed-Lublin, which owns the Brazilian substrain M. bovis BCG Moreau which is less reactogenic than vaccines including other BCG strains. Pacis BCG, made from the Montr\u00e9al (Institut Armand-Frappier) strain, was first marketed by Urocor in about 2002. Urocor was since acquired by Dianon Systems. Evans Vaccines (a subsidiary of PowderJect Pharmaceuticals). Statens Serum Institut in Denmark markets BCG vaccine prepared using Danish strain 1331. Japan BCG Laboratory markets its vaccine, based on the Tokyo 172 substrain of Pasteur BCG, in 50 countries worldwide.\nAccording to a UNICEF report published in December 2015, on BCG vaccine supply security, global demand increased in 2015 from 123 to 152.2\u00a0million doses. To improve security and to [diversify] sources of affordable and flexible supply,\" UNICEF awarded seven new manufacturers contracts to produce BCG. Along with supply availability from existing manufacturers, and a \"new WHO prequalified vaccine\" the total supply will be \"sufficient to meet both suppressed 2015 demand carried over to 2016, as well as total forecast demand through 2016\u20132018.\"\nSupply shortage.\nIn 2011, the Sanofi Pasteur plant flooded, causing problems with mold. The facility, located in Toronto, Ontario, Canada, produced BCG vaccine products, made with substrain Connaught, such as a tuberculosis vaccine ImmuCYST, a BCG Immunotherapeutic \u2013 a bladder cancer drug. By April 2012 the FDA had found dozens of documented problems with sterility at the plant including mold, nesting birds and rusted electrical conduits. The resulting closure of the plant for over two years caused shortages of bladder cancer and tuberculosis vaccines. On 29 October 2014 Health Canada gave the permission for Sanofi to resume production of BCG. An 2018 analysis of the global supply concluded that the supplies are adequate to meet forecast BCG vaccine demand, but that risks of shortages remain, mainly due to dependence of 75 percent of WHO pre-qualified supply on just two suppliers.\nPreparation.\nA weakened strain of bovine tuberculosis bacillus, \"Mycobacterium bovis\" is specially subcultured in a culture medium, usually Middlebrook 7H9.\nDried.\nSome BCG vaccines are freeze dried and become fine powder. Sometimes the powder is sealed with vacuum in a glass ampoule. Such a glass ampoule has to be opened slowly to prevent the airflow from blowing out the powder. Then the powder has to be diluted with saline water before injecting.\nHistory.\nThe history of BCG is tied to that of smallpox. By 1865 Jean Antoine Villemin had demonstrated that rabbits could be infected with tuberculosis from humans; by 1868 he had found that rabbits could be infected with tuberculosis from cows, and that rabbits could be infected with tuberculosis from other rabbits. Thus, he concluded that tuberculosis was transmitted via some unidentified microorganism (or \"virus\", as he called it). In 1882 Robert Koch regarded human and bovine tuberculosis as identical. But in 1895, Theobald Smith presented differences between human and bovine tuberculosis, which he reported to Koch. By 1901 Koch distinguished \"Mycobacterium bovis\" from \"Mycobacterium tuberculosis\". Following the success of vaccination in preventing smallpox, established during the 18th century, scientists thought to find a corollary in tuberculosis by drawing a parallel between bovine tuberculosis and cowpox: it was hypothesized that infection with bovine tuberculosis might protect against infection with human tuberculosis. In the late 19th century, clinical trials using \"M. bovis\" were conducted in Italy with disastrous results, because \"M. bovis\" was found to be just as virulent as \"M. tuberculosis\".\nAlbert Calmette, a French physician and bacteriologist, and his assistant and later colleague, Camille Gu\u00e9rin, a veterinarian, were working at the Institut Pasteur de Lille (Lille, France) in 1908. Their work included subculturing virulent strains of the tuberculosis bacillus and testing different culture media. They noted a glycerin-bile-potato mixture grew bacilli that seemed less virulent, and changed the course of their research to see if repeated subculturing would produce a strain that was attenuated enough to be considered for use as a vaccine. The BCG strain was isolated after subculturing 239 times during 13 years from virulent strain on glycerine potato medium. The research continued throughout World War I until 1919, when the now avirulent bacilli were unable to cause tuberculosis disease in research animals. Calmette and Guerin transferred to the Paris Pasteur Institute in 1919. The BCG vaccine was first used in humans in 1921.\nPublic acceptance was slow, and one disaster, in particular, did much to harm public acceptance of the vaccine. In the summer of 1930 in L\u00fcbeck, 240 infants were vaccinated in the first 10 days of life; almost all developed tuberculosis and 72 infants died. It was subsequently discovered that the BCG administered there had been contaminated with a virulent strain that was being stored in the same incubator, which led to legal action against the manufacturers of the vaccine.\nDr. R. G. Ferguson, working at the Fort Qu'Appelle Sanatorium in Saskatchewan, was among the pioneers in developing the practice of vaccination against tuberculosis. In 1928, BCG was adopted by the Health Committee of the League of Nations (predecessor to the World Health Organization (WHO)). Because of opposition, however, it only became widely used after World War II. From 1945 to 1948, relief organizations (International Tuberculosis Campaign or Joint Enterprises) vaccinated over eight million babies in eastern Europe and prevented the predicted typical increase of TB after a major war.\nBCG is very efficacious against tuberculous meningitis in the pediatric age group, but its efficacy against pulmonary tuberculosis appears to be variable. As of 2006, only a few countries do not use BCG for routine vaccination. Two countries that have never used it routinely are the United States and the Netherlands (in both countries, it is felt that having a reliable Mantoux test and therefore being able to accurately detect active disease is more beneficial to society than vaccinating against a condition that is now relatively rare there).\nOther names include \"Vaccin Bili\u00e9 de Calmette et Gu\u00e9rin vaccine\" and \"Bacille de Calmette et Gu\u00e9rin vaccine\".\nResearch.\nTentative evidence exists for a beneficial non-specific effect of BCG vaccination on overall mortality in low income countries, or for its reducing other health problems including sepsis and respiratory infections when given early, with greater benefit the earlier it is used.\nIn rhesus macaques, BCG shows improved rates of protection when given intravenously. Some risks must be evaluated before it can be translated to human.\nType 1 diabetes.\n, BCG vaccine is in the early stages of being studied in type 1 diabetes.\nCOVID-19.\nUse of the BCG vaccine may provide protection against COVID\u201119. However, epidemiologic observations in this respect are ambiguous. The WHO does not recommend its use for prevention .\n, twenty BCG trials are in various clinical stages."}
{"id": "4192", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=4192", "title": "Bunsen", "text": "Bunsen may refer to:"}
{"id": "4193", "revid": "1003724657", "url": "https://en.wikipedia.org/wiki?curid=4193", "title": "Common buzzard", "text": "The common buzzard (\"Buteo buteo\") is a medium-to-large bird of prey which has a large range. A member of the genus \"Buteo\", it is a member of the family Accipitridae. The species lives in most of Europe and extends its breeding range across much of the Palearctic as far as the northwestern China (Tien Shan), far western Siberia and northwestern Mongolia. Over much of its range, it is a year-round resident. However, buzzards from the colder parts of the Northern Hemisphere as well as those that breed in the eastern part of their range typically migrate south for the northern winter, many culminating their journey as far as South Africa. The common buzzard is an opportunistic predator that can take a wide variety of prey, but it feeds mostly on small mammals, especially rodents such as voles. It typically hunts from a perch. Like most accipitrid birds of prey, it builds a nest, typically in trees in this species, and is a devoted parent to a relatively small brood of young. The common buzzard appears to be the most common diurnal raptor in Europe, as estimates of its total global population run well into the millions.\nTaxonomy.\nThe first formal description of the common buzzard was by the Swedish naturalist Carl Linnaeus in 1758 in the tenth edition of his \"Systema Naturae\" under the binomial name \"Falco buteo\". The genus \"Buteo\" was introduced by the French naturalist Bernard Germain de Lac\u00e9p\u00e8de in 1799 by tautonymy with the specific name of this species. The word \"buteo\" is Latin for a buzzard. It should not be confused with the Turkey vulture, which is sometimes called a buzzard in American English.\nThe Buteoninae subfamily originated from and is most diversified in the Americas, with occasional broader radiations that led to common buzzards and other Eurasian and African buzzards. The common buzzard is a member of the genus \"Buteo\", a group of medium-sized raptors with robust bodies and broad wings. The \"Buteo\" species of Eurasia and Africa are usually commonly referred to as \"buzzards\" while those in the Americas are called hawks. Under current classification, the genus includes approximately 28 species, the second most diverse of all extant accipitrid genera behind only \"Accipiter\". DNA testing shows that the common buzzard is fairly closely related to the red-tailed hawk (\"Buteo jamaicensis\") of North America, which occupies a similar ecological niche to the buzzard in that continent. The two species may belong to the same species complex. Two buzzards in Africa are likely closely related to the common buzzard based on genetic materials, the mountain (\"Buteo oreophilus\") and forest buzzards (\"Buteo trizonatus\"), to the point where it has been questioned whether they are sufficiently distinct to qualify as full species. However, the distinctiveness of these African buzzards has generally been supported. Genetic studies have further indicated that the modern buzzards of Eurasia and Africa are a relatively young group, showing that they diverged at about 300,000 years ago. Nonetheless, fossils dating earlier than 5 million year old (the late Miocene period) showed \"Buteo\" species were present in Europe much earlier than that would imply, although it cannot be stated to a certainty that these would\u2019ve been related to the extant buzzards.\nSubspecies and species splits.\nSome 16 subspecies have been described in the past and up to 11 are often considered valid, although some authorities accept as few as seven. Common buzzard subspecies fall into two groups.\nThe western \"buteo\" group is mainly resident or short-distance migrants and includes:\nThe eastern \"vulpinus\" group includes:\nAt one time, races of the common buzzard were thought to range as far in Asia as a breeding bird well into the Himalayas and as far east as northeastern China, Russia to the Sea of Okhotsk, and all the islands of the Kurile Islands and of Japan, despite both the Himalayan and eastern birds showing a natural gap in distribution from the next nearest breeding common buzzard. However, DNA testing has revealed that the buzzards of these populations probably belong to different species. Most authorities now accept these buzzards as full species: the eastern buzzard (\"Buteo japonicus\"; with three subspecies of its own) and the Himalayan buzzard (\"Buteo refectus\"). Buzzards found on the islands of Cape Verde off of the coast of western Africa, once referred to as the subspecies \"B. b. bannermani\", and Socotra Island off of the northern peninsula of Arabia, once referred to as the rarely recognized subspecies \"B. b. socotrae\", are now generally thought not to belong to the common buzzard. DNA testing has indicated that these insular buzzards are actually more closely related to the long-legged buzzard (\"Buteo rufinus\") than to the common buzzard. Subsequently, some researchers have advocated full species status for the Cape Verde population, but the placement of these buzzards is generally deemed unclear.\nDescription.\nThe common buzzard is a medium-sized raptor that is highly variable in plumage. Most buzzards are distinctly round headed with a somewhat slender bill, relatively long wings that either reach or fall slightly short of the tail tip when perched, a fairly short tail, and somewhat short and mainly bare tarsi. They can appear fairly compact in overall appearance but may also appear large relative to other commoner raptorial birds such as kestrels and sparrowhawks. The common buzzard measures between in length with a wingspan. Females average about 2\u20137% larger than males linearly and weigh about 15% more. Body mass can show considerable variation. Buzzards from Great Britain alone can vary from in males, while females there can range from .\nIn Europe, most typical buzzards are dark brown above and on the upperside of the head and mantle, but can become paler and warmer brown with worn plumage. The flight feathers on perched European buzzards are always brown in the nominate subspecies (\"B. b. buteo\"). Usually the tail will usually be narrowly barred grey-brown and dark brown with a pale tip and a broad dark subterminal band but the tail in palest birds can show a varying amount a white and reduced subterminal band or even appear almost all white. In European buzzards, the underside coloring can be variable but most typically show a brown-streaked white throat with a somewhat darker chest. A pale U across breast is often present; followed by a pale line running down the belly which separates the dark areas on breast-side and flanks. These pale areas tend to have highly variable markings that tend to form irregular bars. Juvenile buzzards are quite similar to adult in the nominate race, being best told apart by having a paler eye, a narrower subterminal band on the tail and underside markings that appear as streaks rather than bars. Furthermore, juveniles may show variable creamy to rufous fringes to upperwing coverts but these also may not be present. Seen from below in flight, buzzards in Europe typically have a dark trailing edge to the wings. If seen from above, one of the best marks is their broad dark subterminal tail band. Flight feathers of typical European buzzards are largely greyish, the aforementioned dark wing linings at front with contrasting paler band along the median coverts. In flight, paler individuals tend to show dark carpal patches that can appears as blackish arches or commas but these may be indistinct in darker individuals or can appear light brownish or faded in paler individuals. Juvenile nominate buzzards are best told apart from adults in flight by the lack of a distinct subterminal band (instead showing fairly even barring throughout) and below by having less sharp and brownish rather than blackish trailing wing edge. Juvenile buzzards show streaking paler parts of under wing and body showing rather than barring as do adults. Beyond the typical mid-range brownish buzzard, birds in Europe can range from almost uniform black-brown above to mainly white. Extreme dark individuals may range from chocolate brown to blackish with almost no pale showing but a variable, faded U on the breast and with or without faint lighter brown throat streaks. Extreme pale birds are largely whitish with variable widely spaced streaks or arrowheads of light brown about the mid-chest and flanks and may or may not show dark feather-centres on the head, wing-coverts and sometimes all but part of mantle. Individuals can show nearly endless variation of colours and hues in between these extremes and the common buzzard is counted among the most variably plumage diurnal raptors for this reason. One study showed that this variation may actually be the result of diminished single-locus genetic diversity.\nBeyond the nominate form (\"B. b. buteo\") that occupies most of the common buzzard's European range, a second main, widely distributed subspecies is known as the steppe buzzard (\"B. b. vulpinus\"). The steppe buzzard race shows three main colour morphs, each of which can be predominant in a region of breeding range. It is more distinctly polymorphic rather than just individually very variable like the nominate race. This may be because, unlike the nominate buzzard, the steppe buzzard is highly migratory. Polymorphism has been linked with migratory behaviour. The most common type of steppe buzzard is the rufous morph which gives this subspecies its scientific name (\"vulpes\" is Latin for \"fox\"). This morph comprises a majority of birds seen in passage east of the Mediterranean. Rufous morph buzzards are a paler grey-brown above than most nominate \"B. b. buteo\". Compared to the nominate race, rufous \"vulpinus\" show a patterning not dissimilar but generally far more rufous-toned on head, the fringes to mantle wing coverts and, especially, on the tail and the underside. The head is grey-brown with rufous tinges usually while the tail is rufous and can vary from almost unmarked to thinly dark-barred with a subterminal band. The underside can be uniformly pale to dark rufous, barred heavily or lightly with rufous or with dusky barring, usually with darker individuals showing the U as in nominate but with a rufous hue. The pale morph of the steppe buzzard is commonest in the west of its subspecies range, predominantly seen in winter and migration at the various land bridge of the Mediterranean. As in the rufous morph, the pale morph \"vulpinus\" is grey-brown above but the tail is generally marked with thin dark bars and a subterminal band, only showing rufous near the tip. The underside in the pale morph is greyish-white with dark grey-brown or somewhat streaked head to chest and barred belly and chest, occasionally showing darker flanks that can be somewhat rufous. Dark morph \"vulpinus\" tend to be found in the east and southeast of the subspecies range and are easily outnumbered by rufous morph while largely using similar migration points. Dark morph individuals vary from grey-brown to much darker blackish-brown, and have a tail that is dark grey or somewhat mixed grey and rufous, is distinctly marked with dark barring and has a broad, black subterminal band. Dark morph \"vulpinus\" have a head and underside that is mostly uniform dark, from dark brown to blackish-brown to almost pure black. Rufous morph juveniles are often distinctly paler in ground colour (ranging even to creamy-grey) than adults with distinct barring below actually increased in pale morph type juvenile. Pale and rufous morph juveniles can only be distinguished from each other in extreme cases. Dark morph juveniles are more similar to adult dark morph \"vulpinus\" but often show a little whitish streaking below, and like all other races have lighter coloured eyes and more evenly barred tails than adults. Steppe buzzards tend to appear smaller and more agile in flight than nominate whose wing beats can look slower and clumsier. In flight, rufous morph \"vulpinus\" have their whole body and underwing varying from uniform to patterned rufous (if patterning present, it is variable, but can be on chest and often thighs, sometimes flanks, pale band across median coverts), while the under-tail usually paler rufous than above. Whitish flight feathers are more prominent than in nominate and more marked contrast with the bold dark brown band along the trailing edges. Markings of pale \"vulpinus\" as seen in flight are similar to rufous morph (such as paler wing markings) but more greyish both on wings and body. In dark morph \"vulpinus\" the broad black trailing edges and colour of body make whitish areas of inner wing stand out further with an often bolder and blacker carpal patch than in other morphs. As in nominate, juvenile \"vulpinus\" (rufous/pale) tend to have much less distinct trailing edges, general streaking on body and along median underwing coverts. Dark morph \"vulpinus\" resemble adult in flight more so than other morphs.\nSimilar species.\nThe common buzzard is often confused with other raptors especially in flight or at a distance. Inexperienced and over-enthusiastic observers have even mistaken darker birds for the far larger and differently proportioned golden eagle (\"Aquila chrysaetos\") and also dark birds for western marsh harrier (\"Circus aeruginosus\") which also flies in a dihedral but is obviously relatively much longer and slenderer winged and tailed and with far different flying methods. Also buzzards may possibly be confused with dark or light morph booted eagles (\"Hieraeetus pennatus\"), which are similar in size, but the eagle flies on level, parallel-edged wings which usually appear broader, has a longer squarer tail, with no carpal patch in pale birds and all dark flight feathers but for whitish wedge on inner primaries in dark morph ones. Pale individuals are sometimes also mistaken with pale morph short-toed eagles (\"Circaetus gallicus\") which are much larger with a considerably bigger head, longer wings (which are usually held evenly in flight rather than in a dihedral) and paler underwing lacking any carpal patch or dark wing lining. More serious identification concerns lie in other \"Buteo\" species and in flight with honey buzzards, which are quite different looking when seen perched at close range. The European honey buzzard (\"Pernis apivorus\") is thought in engage in mimicry of more powerful raptors, in particular, juveniles may mimic the plumage of the more powerful common buzzard. While less individually variable in Europe, the honey buzzard is more extensive polymorphic on underparts than even the common buzzard. The most common morph of the adult European honey buzzard is heavily and rufous barred on the underside, quite different from the common buzzard, however the brownish juvenile much more resembles an intermediate common buzzard. Honey buzzards flap with distinctively slower and more even wing beats than common buzzard. The wings are also lifted higher on each upstroke, creating a more regular and mechanical effect, furthermore their wings are held slightly arched when soaring but not in a V. On the honey buzzard, the head appears smaller, the body thinner, the tail longer and the wings narrower and more parallel edged. The steppe buzzard race is particularly often mistaken for juvenile European honey buzzards, to the point where early observers of raptor migration in Israel considered distant individuals indistinguishable. However, when compared to a steppe buzzard, the honey buzzard has distinctly darker secondaries on the underwing with fewer and broader bars and more extensive black wing-tips (whole fingers) contrasting with a less extensively pale hand. Found in the same range as the steppe buzzard in some parts of southern Siberia as well as (with wintering steppes) in southwestern India, the Oriental honey buzzard (\"Pernis ptilorhynchus\") is larger than both the European honey buzzard and the common buzzard. The oriental species is with more similar in body plan to common buzzards, being relatively broader winged, shorter tailed and more amply-headed (though the head is still relatively small) relative to the European honey buzzard, but all plumages lack carpal patches.\nIn much of Europe, the common buzzard is the only type of buzzard. However, the subarctic breeding rough-legged buzzard (\"Buteo lagopus\") comes down to occupy much of the northern part of the continent during winter in the same haunts as the common buzzard. However, the rough-legged buzzard is typically larger and distinctly longer-winged with feathered legs, as well as having a white based tail with a broad subterminal band. Rough-legged buzzards have slower wing beats and hover far more frequently than do common buzzards. The carpal patch marking on the under-wing are also bolder and blacker on all paler forms of rough-legged hawk. Many pale morph rough-legged buzzards have a bold, blackish band across the belly against contrasting paler feathers, a feature which rarely appears in individual common buzzard. Usually the face also appears somewhat whitish in most pale morphs of rough-legged buzzards, which is true of only extremely pale common buzzards. Dark morph rough-legged buzzards are usually distinctly darker (ranging to almost blackish) than even extreme dark individuals of common buzzards in Europe and still have the distinct white-based tail and broad subterminal band of other roughlegs. In eastern Europe and much of the Asian range of common buzzards, the long-legged buzzard (\"Buteo rufinus\") may live alongside the common species. As in the steppe buzzard race, the long-legged buzzard has three main colour morphs that are more or less similar in hue. In both the steppe buzzard race and long-legged buzzard, the main colour is overall fairly rufous. More so than steppe buzzards, long-legged buzzards tend to have a distinctly paler head and neck compared to other feathers, and, more distinctly, a normally unbarred tail. Furthermore, the long-legged buzzard is usually a rather larger bird, often considered fairly eagle-like in appearance (although it does appear gracile and small-billed even compared to smaller true eagles), an effect enhanced by its longer tarsi, somewhat longer neck and relatively elongated wings. The flight style of the latter species is deeper, slower and more aquiline, with much more frequent hovering, showing a more protruding head and a slightly higher V held in a soar. The smaller North African and Arabian race of long-legged buzzard (\"B. r. cirtensis\") is more similar in size and nearly all colour characteristics to steppe buzzard, extending to the heavily streaked juvenile plumage, in some cases such birds can be distinguished only by their proportions and flight patterns which remain unchanged. Hybridization with the latter race (\"B. r. cirtensis\") and nominate common buzzards has been observed in the Strait of Gibraltar, a few such birds have been reported potentially in the southern Mediterranean due to mutually encroaching ranges, which are blurring possibly due to climate change.\nWintering steppe buzzards may live alongside mountain buzzards and especially with forest buzzard while wintering in Africa. The juveniles of steppe and forest buzzards are more or less indistinguishable and only told apart by proportions and flight style, the latter species being smaller, more compact, having a smaller bill, shorter legs and shorter and thinner wings than a steppe buzzard. However, size is not diagnostic unless side by side as the two buzzards overlap in this regard. Most reliable are the species wing proportions and their flight actions. Forest buzzard have more flexible wing beats interspersed with glides, additionally soaring on flatter wings and apparently never engage in hovering. Adult forest buzzards compared to the typical adult steppe buzzard (rufous morph) are also similar, but the forest typically has a whiter underside, sometimes mostly plain white, usually with heavy blotches or drop-shaped marks on abdomen, with barring on thighs, more narrow tear-shaped on chest and more spotted on leading edges of underwing, usually lacking marking on the white U across chest (which is otherwise similar but usually broader than that of \"vulpinus\"). In comparison, the mountain buzzard, which is more similar in size to the steppe buzzard and slightly larger than the forest buzzard, is usually duller brown above than a steppe buzzard and is more whitish below with distinctive heavy brown blotches from breasts to the belly, flanks and wing linings while juvenile mountain buzzard is buffy below with smaller and streakier markings. The steppe buzzard when compared to another African species, the red-necked buzzard (\"Buteo auguralis\"), which has red tail similar to \"vulpinus\", is distinct in all other plumage aspects despite their similar size. The latter buzzard has a streaky rufous head and is white below with a contrasting bold dark chest in adult plumage and, in juvenile plumage, has heavy, dark blotches on the chest and flanks with pale wing-linings. Jackal and augur buzzards (\"Buteo rufofuscus\" &amp; \"augur\"), also both rufous on the tail, are larger and bulkier than steppe buzzards and have several distinctive plumage characteristics, most notably both having their own striking, contrasting patterns of black-brown, rufous and cream.\nDistribution and habitat.\nThe common buzzard is found throughout several islands in the eastern Atlantic islands, including the Canary Islands and Azores and almost throughout Europe. It is today found in Ireland and in nearly every part of Scotland, Wales and England. In mainland Europe, remarkably, there are no substantial gaps without breeding common buzzards from Portugal and Spain to Greece, Estonia, Belarus and the Ukraine, though are present mainly only in the breeding season in much of the eastern half of the latter three countries. They are also present in all larger Mediterranean islands such as Corsica, Sardinia, Sicily and Crete. Further north in Scandinavia, they are found mainly in southeastern Norway (though also some points in southwestern Norway close to the coast and one section north of Trondheim), just over the southern half of Sweden and hugging over the Gulf of Bothnia to Finland where they live as a breeding species over nearly two-thirds of the land. The common buzzard reaches its northern limits as a breeder in far eastern Finland and over the border to European Russia, continuing as a breeder over to the narrowest straits of the White Sea and nearly to the Kola Peninsula. In these northern quarters, the common buzzard is present typically only in summer but is a year-around resident of a hearty bit of southern Sweden and some of southern Norway. Outside of Europe, it is a resident of northern Turkey (largely close to the Black Sea) otherwise occurring mainly as a passage migrant or winter visitor in the remainder of Turkey, Georgia, sporadically but not rarely in Azerbaijan and Armenia, northern Iran (largely hugging the Caspian Sea) to northern Turkmenistan. Further north though its absent from either side of the northern Caspian Sea, the common buzzard is found in much of western Russia (though exclusively as a breeder) including all of the Central Federal District and the Volga Federal District, all but the northernmost parts of the Northwestern and Ural Federal Districts and nearly the southern half of the Siberian Federal District, its farthest easterly occurrence as a breeder. It also found in northern Kazakhstan, Kyrgyzstan, far northwestern China (Tien Shan) and northwestern Mongolia. Non-breeding populations occur, either as migrants or wintering birds, in southwestern India, Israel, Lebanon, Syria, Egypt (northeastern), northern Tunisia (and far northwestern Algeria), northern Morocco, near the coasts of The Gambia, Senegal and far southwestern Mauritania and Ivory Coast (and bordering Burkina Faso). In eastern and central Africa, it is found in winter from southeastern Sudan, Eritrea, about two-thirds of Ethiopia, much of Kenya (though apparently absent from the northeast and northwest), Uganda, southern and eastern Democratic Republic of the Congo, and more or less the entirety of southern Africa from Angola across to Tanzania down the remainder of the continent (but for an apparent gap along the coast from southwestern Angola to northwestern South Africa).\nHabitat.\nThe common buzzard generally inhabits the interface of woodlands and open grounds; most typically the species lives in forest edge, small woods or shelterbelts with adjacent grassland, arables or other farmland. It acquits to open moorland as long as there is some trees for perch hunting and nesting use. The woods they inhabit may be coniferous, temperate broadleaf and mixed forests and temperate deciduous forest with occasional preferences for the local dominant tree. It is absent from treeless tundra, as well as the Subarctic where the species almost entirely gives way to the rough-legged buzzard. The common buzzard is sporadic or rare in treeless steppe but can occasionally migrate through it (despite its name, the steppe buzzard subspecies breeds primarily in the wooded fringes of the steppe). The species may be found to some extent in both in mountainous or flat country. Although adaptable to and sometimes seen in wetlands and in coastal areas, buzzards are often considered more of an upland species and neither appear to be regularly attracted to or to strongly avoid bodies of waters in non-migratory times. Buzzards in well-wooded areas of eastern Poland largely used large, mature stands of trees that were more humid, richer and denser than prevalent in surrounding area, but showed preference for those within of openings. Mostly resident buzzards live in lowlands and foothills, but they can live in timbered ridges and uplands as well as rocky coasts, sometimes nesting on cliff ledges rather than trees. Buzzards may live from sea level to elevations of , breeding mostly below but they can winter to an elevation of and migrates easily to . In the mountainous Italian Apennines, buzzard nests were at a mean elevation of and were, relative to the surrounding area, further from human developed areas (i.e. roads) and nearer to valley bottoms in rugged, irregularly topographed places, especially ones that faced northeast. Common buzzards are fairly adaptable to agricultural lands but will show can show regional declines in apparent response to agriculture. Changes to more extensive agricultural practices were shown to reduce buzzard populations in western France where reduction of \u201chedgerows, woodlots and grasslands areas\" caused a decline of buzzards and in Hampshire, England where more extensive grazing by free-range cattle and horses led to declines of buzzards, probably largely due to the seeming reduction of small mammal populations there. On the contrary, buzzards in central Poland adapted to removal of pine trees and reduction of rodent prey by changing nest sites and prey for a time with no strong change in their local numbers. Extensive urbanization seems to negatively affect buzzards, this species being generally less adaptable to urban areas than their New World counterparts, the red-tailed hawk. Although peri-urban areas can actually increase potential prey populations in a location at times, individual buzzard mortality, nest disturbances and nest site habitat degradation rises significantly in such areas."}
