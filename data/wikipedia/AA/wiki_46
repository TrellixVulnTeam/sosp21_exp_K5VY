{"id": "5668", "revid": "3878156", "url": "https://en.wikipedia.org/wiki?curid=5668", "title": "Calcium", "text": "Calcium is a chemical element with the symbol Ca and atomic number 20. As an alkaline earth metal, calcium is a reactive metal that forms a dark oxide-nitride layer when exposed to air. Its physical and chemical properties are most similar to its heavier homologues strontium and barium. It is the fifth most abundant element in Earth's crust, and the third most abundant metal, after iron and aluminium. The most common calcium compound on Earth is calcium carbonate, found in limestone and the fossilised remnants of early sea life; gypsum, anhydrite, fluorite, and apatite are also sources of calcium. The name derives from Latin \"calx\" \"lime\", which was obtained from heating limestone.\nSome calcium compounds were known to the ancients, though their chemistry was unknown until the seventeenth century. Pure calcium was isolated in 1808 via electrolysis of its oxide by Humphry Davy, who named the element. Calcium compounds are widely used in many industries: in foods and pharmaceuticals for calcium supplementation, in the paper industry as bleaches, as components in cement and electrical insulators, and in the manufacture of soaps. On the other hand, the metal in pure form has few applications due to its high reactivity; still, in small quantities it is often used as an alloying component in steelmaking, and sometimes, as a calcium\u2013lead alloy, in making automotive batteries.\nCalcium is the most abundant metal and the fifth-most abundant element in the human body. As electrolytes, calcium ions play a vital role in the physiological and biochemical processes of organisms and cells: in signal transduction pathways where they act as a second messenger; in neurotransmitter release from neurons; in contraction of all muscle cell types; as cofactors in many enzymes; and in fertilization. Calcium ions outside cells are important for maintaining the potential difference across excitable cell membranes, protein synthesis, and bone formation.\nCharacteristics.\nClassification.\nCalcium is a very ductile silvery metal (sometimes described as pale yellow) whose properties are very similar to the heavier elements in its group, strontium, barium, and radium. A calcium atom has twenty electrons, arranged in the electron configuration [Ar]4s2. Like the other elements placed in group 2 of the periodic table, calcium has two valence electrons in the outermost s-orbital, which are very easily lost in chemical reactions to form a dipositive ion with the stable electron configuration of a noble gas, in this case argon. Hence, calcium is almost always divalent in its compounds, which are usually ionic. Hypothetical univalent salts of calcium would be stable with respect to their elements, but not to disproportionation to the divalent salts and calcium metal, because the enthalpy of formation of MX2 is much higher than those of the hypothetical MX. This occurs because of the much greater lattice energy afforded by the more highly charged Ca2+ cation compared to the hypothetical Ca+ cation.\nCalcium, strontium, barium, and radium are always considered to be alkaline earth metals; the lighter beryllium and magnesium, also in group 2 of the periodic table, are often included as well. Nevertheless, beryllium and magnesium differ significantly from the other members of the group in their physical and chemical behaviour: they behave more like aluminium and zinc respectively and have some of the weaker metallic character of the post-transition metals, which is why the traditional definition of the term \"alkaline earth metal\" excludes them. This classification is mostly obsolete in English-language sources, but is still used in other countries such as Japan. As a result, comparisons with strontium and barium are more germane to calcium chemistry than comparisons with magnesium.\nPhysical.\nCalcium metal melts at 842\u00a0\u00b0C and boils at 1494\u00a0\u00b0C; these values are higher than those for magnesium and strontium, the neighbouring group 2 metals. It crystallises in the face-centered cubic arrangement like strontium; above 450\u00a0\u00b0C, it changes to an anisotropic hexagonal close-packed arrangement like magnesium. Its density of 1.55\u00a0g/cm3 is the lowest in its group. Calcium is harder than lead but can be cut with a knife with effort. While calcium is a poorer conductor of electricity than copper or aluminium by volume, it is a better conductor by mass than both due to its very low density. While calcium is infeasible as a conductor for most terrestrial applications as it reacts quickly with atmospheric oxygen, its use as such in space has been considered.\nChemical.\nThe chemistry of calcium is that of a typical heavy alkaline earth metal. For example, calcium spontaneously reacts with water more quickly than magnesium and less quickly than strontium to produce calcium hydroxide and hydrogen gas. It also reacts with the oxygen and nitrogen in the air to form a mixture of calcium oxide and calcium nitride. When finely divided, it spontaneously burns in air to produce the nitride. In bulk, calcium is less reactive: it quickly forms a hydration coating in moist air, but below 30% relative humidity it may be stored indefinitely at room temperature.\nBesides the simple oxide CaO, the peroxide CaO2 can be made by direct oxidation of calcium metal under a high pressure of oxygen, and there is some evidence for a yellow superoxide Ca(O2)2. Calcium hydroxide, Ca(OH)2, is a strong base, though it is not as strong as the hydroxides of strontium, barium or the alkali metals. All four dihalides of calcium are known. Calcium carbonate (CaCO3) and calcium sulfate (CaSO4) are particularly abundant minerals. Like strontium and barium, as well as the alkali metals and the divalent lanthanides europium and ytterbium, calcium metal dissolves directly in liquid ammonia to give a dark blue solution.\nDue to the large size of the Ca2+ ion, high coordination numbers are common, up to 24 in some intermetallic compounds such as CaZn13. Calcium is readily complexed by oxygen chelates such as EDTA and polyphosphates, which are useful in analytic chemistry and removing calcium ions from hard water. In the absence of steric hindrance, smaller group 2 cations tend to form stronger complexes, but when large polydentate macrocycles are involved the trend is reversed.\nAlthough calcium is in the same group as magnesium and organomagnesium compounds are very commonly used throughout chemistry, organocalcium compounds are not similarly widespread because they are more difficult to make and more reactive, although they have recently been investigated as possible catalysts. Organocalcium compounds tend to be more similar to organoytterbium compounds due to the similar ionic radii of Yb2+ (102\u00a0pm) and Ca2+ (100\u00a0pm). Most of these compounds can only be prepared at low temperatures; bulky ligands tend to favor stability. For example, calcium dicyclopentadienyl, Ca(C5H5)2, must be made by directly reacting calcium metal with mercurocene or cyclopentadiene itself; replacing the C5H5 ligand with the bulkier C5(CH3)5 ligand on the other hand increases the compound's solubility, volatility, and kinetic stability.\nIsotopes.\nNatural calcium is a mixture of five stable isotopes (40Ca, 42Ca, 43Ca, 44Ca, and 46Ca) and one isotope with a half-life so long that it can be considered stable for all practical purposes (48Ca, with a half-life of about 4.3\u00a0\u00d7\u00a01019\u00a0years). Calcium is the first (lightest) element to have six naturally occurring isotopes.\nBy far the most common isotope of calcium in nature is 40Ca, which makes up 96.941% of all natural calcium. It is produced in the silicon-burning process from fusion of alpha particles and is the heaviest stable nuclide with equal proton and neutron numbers; its occurrence is also supplemented slowly by the decay of primordial 40K. Adding another alpha particle leads to unstable 44Ti, which quickly decays via two successive electron captures to stable 44Ca; this makes up 2.806% of all natural calcium and is the second-most common isotope. The other four natural isotopes, 42Ca, 43Ca, 46Ca, and 48Ca, are significantly rarer, each comprising less than 1% of all natural calcium. The four lighter isotopes are mainly products of the oxygen-burning and silicon-burning processes, leaving the two heavier ones to be produced via neutron capture processes. 46Ca is mostly produced in a \"hot\" s-process, as its formation requires a rather high neutron flux to allow short-lived 45Ca to capture a neutron. 48Ca is produced by electron capture in the r-process in type Ia supernovae, where high neutron excess and low enough entropy ensures its survival.\n46Ca and 48Ca are the first \"classically stable\" nuclides with a six-neutron or eight-neutron excess respectively. Although extremely neutron-rich for such a light element, 48Ca is very stable because it is a doubly magic nucleus, having 20 protons and 28 neutrons arranged in closed shells. Its beta decay to 48Sc is very hindered because of the gross mismatch of nuclear spin: 48Ca has zero nuclear spin, being even\u2013even, while 48Sc has spin 6+, so the decay is forbidden by the conservation of angular momentum. While two excited states of 48Sc are available for decay as well, they are also forbidden due to their high spins. As a result, when 48Ca does decay, it does so by double beta decay to 48Ti instead, being the lightest nuclide known to undergo double beta decay. The heavy isotope 46Ca can also theoretically undergo double beta decay to 46Ti as well, but this has never been observed; the lightest and most common isotope 40Ca is also doubly magic and could undergo double electron capture to 40Ar, but this has likewise never been observed. Calcium is the only element to have two primordial doubly magic isotopes. The experimental lower limits for the half-lives of 40Ca and 46Ca are 5.9\u00a0\u00d7\u00a01021\u00a0years and 2.8\u00a0\u00d7\u00a01015\u00a0years respectively.\nApart from the practically stable 48Ca, the longest lived radioisotope of calcium is 41Ca. It decays by electron capture to stable 41K with a half-life of about a hundred thousand years. Its existence in the early Solar System as an extinct radionuclide has been inferred from excesses of 41K: traces of 41Ca also still exist today, as it is a cosmogenic nuclide, continuously reformed through neutron activation of natural 40Ca. Many other calcium radioisotopes are known, ranging from 35Ca to 60Ca. They are all much shorter-lived than 41Ca, the most stable among them being 45Ca (half-life 163\u00a0days) and 47Ca (half-life 4.54\u00a0days). The isotopes lighter than 42Ca usually undergo beta plus decay to isotopes of potassium, and those heavier than 44Ca usually undergo beta minus decay to isotopes of scandium, although near the nuclear drip lines, proton emission and neutron emission begin to be significant decay modes as well.\nLike other elements, a variety of processes alter the relative abundance of calcium isotopes. The best studied of these processes is the mass-dependent fractionation of calcium isotopes that accompanies the precipitation of calcium minerals such as calcite, aragonite and apatite from solution. Lighter isotopes are preferentially incorporated into these minerals, leaving the surrounding solution enriched in heavier isotopes at a magnitude of roughly 0.025% per atomic mass unit (amu) at room temperature. Mass-dependent differences in calcium isotope composition are conventionally expressed by the ratio of two isotopes (usually 44Ca/40Ca) in a sample compared to the same ratio in a standard reference material. 44Ca/40Ca varies by about 1% among common earth materials.\nHistory.\nCalcium compounds were known for millennia, although their chemical makeup was not understood until the 17th century. Lime as a building material and as plaster for statues was used as far back as around 7000\u00a0BC. The first dated lime kiln dates back to 2500\u00a0BC and was found in Khafajah, Mesopotamia. At about the same time, dehydrated gypsum (CaSO4\u00b72H2O) was being used in the Great Pyramid of Giza; this material would later be used for the plaster in the tomb of Tutankhamun. The ancient Romans instead used lime mortars made by heating limestone (CaCO3); the name \"calcium\" itself derives from the Latin word \"calx\" \"lime\". Vitruvius noted that the lime that resulted was lighter than the original limestone, attributing this to the boiling of the water; in 1755, Joseph Black proved that this was due to the loss of carbon dioxide, which as a gas had not been recognised by the ancient Romans.\nIn 1787, Antoine Lavoisier suspected that lime might be an oxide of a fundamental chemical element. In his table of the elements, Lavoisier listed five \"salifiable earths\" (i.e., ores that could be made to react with acids to produce salts (\"salis\" = salt, in Latin): \"chaux\" (calcium oxide), \"magn\u00e9sie\" (magnesia, magnesium oxide), \"baryte\" (barium sulfate), \"alumine\" (alumina, aluminium oxide), and \"silice\" (silica, silicon dioxide)). About these \"elements\", Lavoisier speculated: \nCalcium, along with its congeners magnesium, strontium, and barium, was first isolated by Humphry Davy in 1808. Following the work of J\u00f6ns Jakob Berzelius and Magnus Martin af Pontin on electrolysis, Davy isolated calcium and magnesium by putting a mixture of the respective metal oxides with mercury(II) oxide on a platinum plate which was used as the anode, the cathode being a platinum wire partially submerged into mercury. Electrolysis then gave calcium\u2013mercury and magnesium\u2013mercury amalgams, and distilling off the mercury gave the metal. However, pure calcium cannot be prepared in bulk by this method and a workable commercial process for its production was not found until over a century later.\nOccurrence and production.\nAt 3%, calcium is the fifth most abundant element in the Earth's crust, and the third most abundant metal behind aluminium and iron. It is also the fourth most abundant element in the lunar highlands. Sedimentary calcium carbonate deposits pervade the Earth's surface as fossilized remains of past marine life; they occur in two forms, the rhombohedral calcite (more common) and the orthorhombic aragonite (forming in more temperate seas). Minerals of the first type include limestone, dolomite, marble, chalk, and iceland spar; aragonite beds make up the Bahamas, the Florida Keys, and the Red Sea basins. Corals, sea shells, and pearls are mostly made up of calcium carbonate. Among the other important minerals of calcium are gypsum (CaSO4\u00b72H2O), anhydrite (CaSO4), fluorite (CaF2), and apatite ([Ca5(PO4)3F]).\nThe major producers of calcium are China (about 10000 to 12000 tonnes per year), Russia (about 6000 to 8000 tonnes per year), and the United States (about 2000 to 4000 tonnes per year). Canada and France are also among the minor producers. In 2005, about 24000 tonnes of calcium were produced; about half of the world's extracted calcium is used by the United States, with about 80% of the output used each year. In Russia and China, Davy's method of electrolysis is still used, but is instead applied to molten calcium chloride. Since calcium is less reactive than strontium or barium, the oxide\u2013nitride coating that results in air is stable and lathe machining and other standard metallurgical techniques are suitable for calcium. In the United States and Canada, calcium is instead produced by reducing lime with aluminium at high temperatures.\nGeochemical cycling.\nCalcium cycling provides a link between tectonics, climate, and the carbon cycle. In the simplest terms, uplift of mountains exposes calcium-bearing rocks to chemical weathering and releases Ca2+ into surface water. These ions are transported to the ocean where they react with dissolved CO2 to form limestone (), which in turn settles to the sea floor where it is incorporated into new rocks. Dissolved CO2, along with carbonate and bicarbonate ions, are termed \"dissolved inorganic carbon\" (DIC).\nThe actual reaction is more complicated and involves the bicarbonate ion (HCO) that forms when CO2 reacts with water at seawater pH:\nAt seawater pH, most of the CO2 is immediately converted back into . The reaction results in a net transport of one molecule of CO2 from the ocean/atmosphere into the lithosphere. The result is that each Ca2+ ion released by chemical weathering ultimately removes one CO2 molecule from the surficial system (atmosphere, ocean, soils and living organisms), storing it in carbonate rocks where it is likely to stay for hundreds of millions of years. The weathering of calcium from rocks thus scrubs CO2 from the ocean and atmosphere, exerting a strong long-term effect on climate.\nUses.\nThe largest use of metallic calcium is in steelmaking, due to its strong chemical affinity for oxygen and sulfur. Its oxides and sulfides, once formed, give liquid lime aluminate and sulfide inclusions in steel which float out; on treatment, these inclusions disperse throughout the steel and became small and spherical, improving castability, cleanliness and general mechanical properties. Calcium is also used in maintenance-free automotive batteries, in which the use of 0.1% calcium\u2013lead alloys instead of the usual antimony\u2013lead alloys leads to lower water loss and lower self-discharging. Due to the risk of expansion and cracking, aluminium is sometimes also incorporated into these alloys. These lead\u2013calcium alloys are also used in casting, replacing lead\u2013antimony alloys. Calcium is also used to strengthen aluminium alloys used for bearings, for the control of graphitic carbon in cast iron, and to remove bismuth impurities from lead. Calcium metal is found in some drain cleaners, where it functions to generate heat and calcium hydroxide that saponifies the fats and liquefies the proteins (for example, those in hair) that block drains. Besides metallurgy, the reactivity of calcium is exploited to remove nitrogen from high-purity argon gas and as a getter for oxygen and nitrogen. It is also used as a reducing agent in the production of chromium, zirconium, thorium, and uranium. It can also be used to store hydrogen gas, as it reacts with hydrogen to form solid calcium hydride, from which the hydrogen can easily be re-extracted.\nCalcium isotope fractionation during mineral formation has led to several applications of calcium isotopes. In particular, the 1997 observation by Skulan and DePaolo that calcium minerals are isotopically lighter than the solutions from which the minerals precipitate is the basis of analogous applications in medicine and in paleooceanography. In animals with skeletons mineralized with calcium, the calcium isotopic composition of soft tissues reflects the relative rate of formation and dissolution of skeletal mineral. In humans, changes in the calcium isotopic composition of urine have been shown to be related to changes in bone mineral balance. When the rate of bone formation exceeds the rate of bone resorption, the 44Ca/40Ca ratio in soft tissue rises and vice versa. Because of this relationship, calcium isotopic measurements of urine or blood may be useful in the early detection of metabolic bone diseases like osteoporosis. A similar system exists in seawater, where 44Ca/40Ca tends to rise when the rate of removal of Ca2+ by mineral precipitation exceeds the input of new calcium into the ocean. In 1997, Skulan and DePaolo presented the first evidence of change in seawater 44Ca/40Ca over geologic time, along with a theoretical explanation of these changes. More recent papers have confirmed this observation, demonstrating that seawater Ca2+ concentration is not constant, and that the ocean is never in a \"steady state\" with respect to calcium input and output. This has important climatological implications, as the marine calcium cycle is closely tied to the carbon cycle.\nMany calcium compounds are used in food, as pharmaceuticals, and in medicine, among others. For example, calcium and phosphorus are supplemented in foods through the addition of calcium lactate, calcium diphosphate, and tricalcium phosphate. The last is also used as a polishing agent in toothpaste and in antacids. Calcium lactobionate is a white powder that is used as a suspending agent for pharmaceuticals. In baking, calcium monophosphate is used as a leavening agent. Calcium sulfite is used as a bleach in papermaking and as a disinfectant, calcium silicate is used as a reinforcing agent in rubber, and calcium acetate is a component of liming rosin and is used to make metallic soaps and synthetic resins.\nCalcium is on the World Health Organization's List of Essential Medicines.\nFood sources.\nFoods rich in calcium include dairy products, such as yogurt and cheese, sardines, salmon, soy products, kale, and fortified breakfast cereals.\nBecause of concerns for long-term adverse side effects, including calcification of arteries and kidney stones, both the U.S. Institute of Medicine (IOM) and the European Food Safety Authority (EFSA) set Tolerable Upper Intake Levels (ULs) for combined dietary and supplemental calcium. From the IOM, people of ages 9\u201318 years are not to exceed 3\u00a0g/day combined intake; for ages 19\u201350, not to exceed 2.5\u00a0g/day; for ages 51 and older, not to exceed 2\u00a0g/day. EFSA set the UL for all adults at 2.5\u00a0g/day, but decided the information for children and adolescents was not sufficient to determine ULs.\nBiological and pathological role.\nFunction.\nCalcium is an essential element needed in large quantities. The Ca2+ ion acts as an electrolyte and is vital to the health of the muscular, circulatory, and digestive systems; is indispensable to the building of bone; and supports synthesis and function of blood cells. For example, it regulates the contraction of muscles, nerve conduction, and the clotting of blood. As a result, intra- and extracellular calcium levels are tightly regulated by the body. Calcium can play this role because the Ca2+ ion forms stable coordination complexes with many organic compounds, especially proteins; it also forms compounds with a wide range of solubilities, enabling the formation of the skeleton.\nBinding.\nCalcium ions may be complexed by proteins through binding the carboxyl groups of glutamic acid or aspartic acid residues; through interacting with phosphorylated serine, tyrosine, or threonine residues; or by being chelated by \u03b3-carboxylated amino acid residues. Trypsin, a digestive enzyme, uses the first method; osteocalcin, a bone matrix protein, uses the third. Some other bone matrix proteins such as osteopontin and bone sialoprotein use both the first and the second. Direct activation of enzymes by binding calcium is common; some other enzymes are activated by noncovalent association with direct calcium-binding enzymes. Calcium also binds to the phospholipid layer of the cell membrane, anchoring proteins associated with the cell surface.\nSolubility.\nAs an example of the wide range of solubility of calcium compounds, monocalcium phosphate is very soluble in water, 85% of extracellular calcium is as dicalcium phosphate with a solubility of 2.0\u00a0mM and the hydroxyapatite of bones in an organic matrix is tricalcium phosphate at 100\u00a0\u03bcM.\nNutrition.\nCalcium is a common constituent of multivitamin dietary supplements, but the composition of calcium complexes in supplements may affect its bioavailability which varies by solubility of the salt involved: calcium citrate, malate, and lactate are highly bioavailable, while the oxalate is less. Other calcium preparations include calcium carbonate, calcium citrate malate, and calcium gluconate. The intestine absorbs about one-third of calcium eaten as the free ion, and plasma calcium level is then regulated by the kidneys.\nHormonal regulation of bone formation and serum levels.\nParathyroid hormone and vitamin D promote the formation of bone by allowing and enhancing the deposition of calcium ions there, allowing rapid bone turnover without affecting bone mass or mineral content. When plasma calcium levels fall, cell surface receptors are activated and the secretion of parathyroid hormone occurs; it then proceeds to stimulate the entry of calcium into the plasma pool by taking it from targeted kidney, gut, and bone cells, with the bone-forming action of parathyroid hormone being antagonised by calcitonin, whose secretion increases with increasing plasma calcium levels.\nAbnormal serum levels.\nExcess intake of calcium may cause hypercalcemia. However, because calcium is absorbed rather inefficiently by the intestines, high serum calcium is more likely caused by excessive secretion of parathyroid hormone (PTH) or possibly by excessive intake of vitamin D, both of which facilitate calcium absorption. All these conditions result in excess calcium salts being deposited in the heart, blood vessels, or kidneys. Symptoms include anorexia, nausea, vomiting, memory loss, confusion, muscle weakness, increased urination, dehydration, and metabolic bone disease. Chronic hypercalcaemia typically leads to calcification of soft tissue and its serious consequences: for example, calcification can cause loss of elasticity of vascular walls and disruption of laminar blood flow\u2014and thence to plaque rupture and thrombosis. Conversely, inadequate calcium or vitamin D intakes may result in hypocalcemia, often caused also by inadequate secretion of parathyroid hormone or defective PTH receptors in cells. Symptoms include neuromuscular excitability, which potentially causes tetany and disruption of conductivity in cardiac tissue.\nBone disease.\nAs calcium is required for bone development, many bone diseases can be traced to the organic matrix or the hydroxyapatite in molecular structure or organization of bone. Osteoporosis is a reduction in mineral content of bone per unit volume, and can be treated by supplementation of calcium, vitamin D, and bisphosphonates. Inadequate amounts of calcium, vitamin D, or phosphates can lead to softening of bones, called osteomalacia.\nSafety.\nMetallic calcium.\nBecause calcium reacts exothermically with water and acids, calcium metal coming into contact with bodily moisture results in severe corrosive irritation. When swallowed, calcium metal has the same effect on the mouth, oesophagus, and stomach, and can be fatal. However, long-term exposure is not known to have distinct adverse effects."}
{"id": "5669", "revid": "38455", "url": "https://en.wikipedia.org/wiki?curid=5669", "title": "Chromium", "text": "Chromium is a chemical element with the symbol Cr and atomic number 24. It is the first element in group 6. It is a steely-grey, lustrous, hard, and brittle transition metal. Chromium is the main additive in stainless steel, to which it adds anti-corrosive properties. Chromium is also highly valued as a metal that is able to be highly polished while resisting tarnishing. Polished chromium reflects almost 70% of the visible spectrum, with almost 90% of infrared light being reflected. The name of the element is derived from the Greek word \u03c7\u03c1\u1ff6\u03bc\u03b1, \"chr\u014dma\", meaning color, because many chromium compounds are intensely colored.\nFerrochromium alloy is commercially produced from chromite by silicothermic or aluminothermic reactions and chromium metal by roasting and leaching processes followed by reduction with carbon and then aluminium. Chromium metal is of high value for its high corrosion resistance and hardness. A major development in steel production was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. Stainless steel and chrome plating (electroplating with chromium) together comprise 85% of the commercial use.\nIn the United States, trivalent chromium (Cr(III)) ion is considered an essential nutrient in humans for insulin, sugar, and lipid metabolism. However, in 2014, the European Food Safety Authority, acting for the European Union, concluded that there was insufficient evidence for chromium to be recognized as essential.\nWhile chromium metal and Cr(III) ions are considered non-toxic, hexavalent chromium, Cr(VI), is toxic and carcinogenic. Abandoned chromium production sites often require environmental cleanup.\nPhysical properties.\nAtomic.\nChromium is the fourth transition metal found on the periodic table, and has an electron configuration of [Ar] 3d5 4s1. It is also the first element in the periodic table whose ground-state electron configuration violates the Aufbau principle. This occurs again later in the periodic table with other elements and their electron configurations, such as copper, niobium, and molybdenum. This occurs because electrons in the same orbital repel each other due to their like charges. In the previous elements, the energetic cost of promoting an electron to the next higher energy level is too great to compensate for that released by lessening inter-electronic repulsion. However, in the 3d transition metals, the energy gap between the 3d and the next-higher 4s subshell is very small, and because the 3d subshell is more compact than the 4s subshell, inter-electron repulsion is smaller between 4s electrons than between 3d electrons. This lowers the energetic cost of promotion and increases the energy released by it, so that the promotion becomes energetically feasible and one or even two electrons are always promoted to the 4s subshell. (Similar promotions happen for every transition metal atom but one, palladium.)\nChromium is the first element in the 3d series where the 3d electrons start to sink into the nucleus; they thus contribute less to metallic bonding, and hence the melting and boiling points and the enthalpy of atomisation of chromium are lower than those of the preceding element vanadium. Chromium(VI) is a strong oxidising agent in contrast to the molybdenum(VI) and tungsten(VI) oxides.\nBulk.\nChromium is extremely hard, and is the third hardest element behind carbon (diamond) and boron. Its Mohs hardness is 8.5, which means that it can scratch samples of quartz and topaz, but can be scratched by corundum. Chromium is highly resistant to tarnishing, which makes it useful as a metal that preserves its outermost layer from corroding, unlike other metals such as copper, magnesium, and aluminium.\nChromium has a melting point of 1907\u00a0\u00b0C (3465\u00a0\u00b0F), which is relatively low compared to the majority of transition metals. However, it still has the second highest melting point out of all the Period 4 elements, being topped by vanadium by 3\u00a0\u00b0C (5\u00a0\u00b0F) at 1910\u00a0\u00b0C (3470\u00a0\u00b0F). The boiling point of 2671\u00a0\u00b0C (4840\u00a0\u00b0F), however, is comparatively lower, having the third lowest boiling point out of the Period 4 transition metals alone behind manganese and zinc. The electrical resistivity of chromium at 20\u00a0\u00b0C is 125 nanoohm-meters.\nChromium has a high specular reflection in comparison to other transition metals. In infrared, at 425 \u03bcm, chromium has a maximum reflectance of about 72%, reducing to a minimum of 62% at 750 \u03bcm before rising again to 90% at 4000 \u03bcm. When chromium is used in stainless steel alloys and polished, the specular reflection decreases with the inclusion of additional metals, yet is still high in comparison with other alloys. Between 40% and 60% of the visible spectrum is reflected from polished stainless steel. The explanation on why chromium displays such a high turnout of reflected photon waves in general, especially the 90% in infrared, can be attributed to chromium's magnetic properties. Chromium has unique magnetic properties - chromium is the only elemental solid that shows antiferromagnetic ordering at room temperature and below. Above 38\u00a0\u00b0C, its magnetic ordering becomes paramagnetic. The antiferromagnetic properties, which cause the chromium atoms to temporarily ionize and bond with themselves, are present because the body-centric cubic's magnetic properties are disproportionate to the lattice periodicity. This is due to the magnetic moments at the cube's corners and the unequal, but antiparallel, cube centers. From here, the frequency-dependent relative permittivity of chromium, deriving from Maxwell's equations and chromium's antiferromagnetivity, leaves chromium with a high infrared and visible light reflectance.\nPassivation.\nChromium metal left standing in air is passivated - it forms a thin, protective, surface layer of oxide. This layer has a spinel structure a few atomic layers thick; it is very dense and inhibits the diffusion of oxygen into the underlying metal. In contrast, iron forms a more porous oxide through which oxygen can migrate, causing continued rusting. Passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. Passivation can be removed with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.\nChromium, unlike iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.\nIsotopes.\nNaturally occurring chromium is composed of three stable isotopes; 52Cr, 53Cr and 54Cr, with 52Cr being the most abundant (83.789% natural abundance). 19 radioisotopes have been characterized, with the most stable being 50Cr with a half-life of (more than) 1.8 years, and 51Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority less than 1 minute. Chromium also has two metastable nuclear isomers.\n53Cr is the radiogenic decay product of 53Mn (half-life = 3.74 million years). Chromium isotopes are typically collocated (and compounded) with manganese isotopes. This circumstance is useful in isotope geology. Manganese-chromium isotope ratios reinforce the evidence from 26Al and 107Pd concerning the early history of the Solar System. Variations in 53Cr/52Cr and Mn/Cr ratios from several meteorites indicate an initial 53Mn/55Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of 53Mn in differentiated planetary bodies. Hence 53Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the Solar System.\nThe isotopes of chromium range in atomic mass from 43\u00a0u (43Cr) to 67\u00a0u (67Cr). The primary decay mode before the most abundant stable isotope, 52Cr, is electron capture and the primary mode after is beta decay. 53Cr has been posited as a proxy for atmospheric oxygen concentration.\nChemistry and compounds.\nChromium is a member of group 6, of the transition metals. The +3 and +6 states occur most commonly within chromium compounds, followed by +2; charges of +1, +4 and +5 for chromium are rare, but do nevertheless occasionally exist.\nCommon oxidation states.\nChromium(II).\nMany chromium(II) compounds are known, such as the water-stable chromium(II) chloride that can be made by reducing chromium(III) chloride with zinc. The resulting bright blue solution created from dissolving chromium(II) chloride is only stable at neutral pH. Some other notable chromium(II) compounds include chromium(II) oxide , and chromium(II) sulfate . Many chromous carboxylates are known as well, the most famous of these being the red chromium(II) acetate (Cr2(O2CCH3)4) that features a quadruple bond.\nChromium(III).\nA large number of chromium(III) compounds are known, such as chromium(III) nitrate, chromium(III) acetate, and chromium(III) oxide. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid, but it can also be formed through the reduction of chromium(VI) by cytochrome c7. The ion has a similar radius (63\u00a0pm) to (radius 50\u00a0pm), and they can replace each other in some compounds, such as in chrome alum and alum.\nChromium(III) tends to form octahedral complexes. Commercially available chromium(III) chloride hydrate is the dark green complex [CrCl2(H2O)4]Cl. Closely related compounds are the pale green [CrCl(H2O)5]Cl2 and violet [Cr(H2O)6]Cl3. If anhydrous violet chromium(III) chloride is dissolved in water, the violet solution turns green after some time as the chloride in the inner coordination sphere is replaced by water. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts. A tetrahedral coordination of Chromium(III) has been reported for the Cr-centered Keggin anion [\u03b1-CrW12O40]5\u2013.\nChromium(III) hydroxide (Cr(OH)3) is amphoteric, dissolving in acidic solutions to form [Cr(H2O)6]3+, and in basic solutions to form . It is dehydrated by heating to form the green chromium(III) oxide (Cr2O3), a stable oxide with a crystal structure identical to that of corundum.\nChromium(VI).\nChromium(VI) compounds are oxidants at low or neutral pH. Chromate anions () and dichromate (Cr2O72\u2212) anions are the principal ions at this oxidation state. They exist at an equilibrium, determined by pH:\nChromium(VI) oxyhalides are known also and include chromyl fluoride (CrO2F2) and chromyl chloride (). However, despite several erroneous claims, chromium hexafluoride (as well as all higher hexahalides) remains unknown, as of 2020.\nSodium chromate is produced industrially by the oxidative roasting of chromite ore with sodium carbonate. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.\nBoth the chromate and dichromate anions are strong oxidizing reagents at low pH:\nThey are, however, only moderately oxidizing at high pH:\nChromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO5) is formed, which can be stabilized as an ether adduct .\nChromic acid has the hypothetical formula . It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide , the acid anhydride of chromic acid, is sold industrially as \"chromic acid\". It can be produced by mixing sulfuric acid with dichromate and is a strong oxidizing agent.\nOther oxidation states.\nCompounds of chromium(V) are rather rare; the oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF5). This red solid has a melting point of 30\u00a0\u00b0C and a boiling point of 117\u00a0\u00b0C. It can be prepared by treating chromium metal with fluorine at 400\u00a0\u00b0C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K3[Cr(O2)4]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150\u2013170\u00a0\u00b0C.\nCompounds of chromium(IV) are slightly more common than those of chromium(V). The tetrahalides, CrF4, CrCl4, and CrBr4, can be produced by treating the trihalides () with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water.\nMost chromium(I) compounds are obtained solely by oxidation of electron-rich, octahedral chromium(0) complexes. Other chromium(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4) \u00a0pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.\nMost chromium(0) compounds are derivatives of the compounds chromium hexacarbonyl or bis(benzene)chromium.\nOccurrence.\nChromium is the 21st most abundant element in Earth's crust with an average concentration of 100\u00a0ppm. Chromium compounds are found in the environment from the erosion of chromium-containing rocks, and can be redistributed by volcanic eruptions. Typical background concentrations of chromium in environmental media are: atmosphere &lt;10\u00a0ng/m3; soil &lt;500\u00a0mg/kg; vegetation &lt;0.5\u00a0mg/kg; freshwater &lt;10\u00a0\u03bcg/L; seawater &lt;1\u00a0\u03bcg/L; sediment &lt;80\u00a0mg/kg. Chromium is mined as chromite (FeCr2O4) ore.\nAbout two-fifths of the chromite ores and concentrates in the world are produced in South Africa, about a third in Kazakhstan, while India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa. Although rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamonds.\nThe relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location. In most cases, Cr(III) is the dominating species, but in some areas, the ground water can contain up to 39\u00a0\u00b5g/L of total chromium, of which 30\u00a0\u00b5g/L is Cr(VI).\nHistory.\nEarly applications.\nChromium minerals as pigments came to the attention of the west in the eighteenth century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named \"Siberian red lead\". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite with a formula of PbCrO4. In 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that was discovered to possess useful properties as a pigment in paints. After Pallas, the use of Siberian red lead as a paint pigment began to develop rapidly throughout the region. Crocoite would be the principal source of chromium in pigments until the discovery of chromite many years later.\nIn 1794, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO3) by mixing crocoite with hydrochloric acid. In 1797, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, for which he is credited as the one who truly discovered the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby and emerald.\nDuring the nineteenth century, chromium was primarily used not only as a component of paints, but in tanning salts as well. For quite some time, the crocoite found in Russia was the main source for such tanning materials. In 1827, a larger chromite deposit was discovered near Baltimore, United States, which quickly met the demand for tanning salts much more adequately than the crocoite that had been used previously. This made the United States the largest producer of chromium products until the year 1848, when larger deposits of chromite were uncovered near the city of Bursa, Turkey. With the development of metallurgy and chemical industries in the Western world, the need for chromium increased.\nChromium is also famous for its reflective, metallic luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.\nProduction.\nApproximately 28.8 million metric tons (Mt) of marketable chromite ore was produced in 2013, and converted into 7.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, \"Ferrochromium is the leading end use of chromite ore, [and] stainless steel is the leading end use of ferrochromium.\"\nThe largest producers of chromium ore in 2013 have been South Africa (48%), Kazakhstan (13%), Turkey (11%), and India (10%), with several other countries producing the rest of about 18% of the world production.\nThe two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCr2O4) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.\nFor the production of pure chromium, the iron must be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable Fe2O3. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.\nThe dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.\nApplications.\nThe creation of metal alloys account for 85% of the available chromium's usage. The remainder of chromium is used in the chemical, refractory, and foundry industries.\nMetallurgy.\nThe strengthening effect of forming stable metal carbides at grain boundaries, and the strong increase in corrosion resistance made chromium an important alloying material for steel. High-speed tool steels contain between 3 and 5% chromium. Stainless steel, the primary corrosion-resistant metal alloy, is formed when chromium is introduced to iron in concentrations above 11%. For stainless steel's formation, ferrochromium is added to the molten iron. Also, nickel-based alloys have increased strength due to the formation of discrete, stable, metal, carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials.\nThe high hardness and corrosion resistance of unalloyed chromium makes it a reliable metal for surface coating; it is still the most popular metal for sheet coating, with its above-average durability, compared to other coating metals. A layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: thin, and thick. Thin deposition involves a layer of chromium below 1\u00a0\u00b5m thickness deposited by chrome plating, and is used for decorative surfaces. Thicker chromium layers are deposited if wear-resistant surfaces are needed. Both methods use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development; for most applications of chromium, the previously established process is used.\nIn the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc, and cadmium. This passivation and the self-healing properties of the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating methods are under development.\nChromic acid anodizing (or Type I anodizing) of aluminium is another electrochemical process that does not lead to the deposition of chromium, but uses chromic acid as an electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.\nThe high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium, or at least a change to less toxic chromium(III) compounds.\nPigment.\nThe mineral crocoite (which is also lead chromate PbCrO4) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the United States and for the Postal Service (for example, the Deutsche Post) in Europe. The use of chrome yellow has since declined due to environmental and safety concerns and was replaced by organic pigments or other alternatives that are free from lead and chromium. Other pigments that are based around chromium are, for example, the deep shade of red pigment chrome red, which is simply lead chromate with lead(II) hydroxide (PbCrO4\u00b7Pb(OH)2). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pre-treating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10\u201315\u00a0\u00b5m was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.\nChromium oxides are also used as a green pigment in the field of glassmaking and also as a glaze for ceramics. Green chromium oxide is extremely lightfast and as such is used in cladding coatings. It is also the main ingredient in infrared reflecting paints, used by the armed forces to paint vehicles and to give them the same infrared reflectance as green leaves.\nOther uses.\nChromium(III) ions present in corundum crystals (aluminium oxide) cause them to be colored red; when corundum appears as such, it is known as a ruby. If the corundum is lacking in chromium(III) ions, it is known as a sapphire. A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal. A ruby laser is lasing at 694.3 nanometers, in a deep red color.\nBecause of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood-attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO3 between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution were used in 1996.\nChromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry. Recovery and reuse, direct/indirect recycling, and \"chrome-less\" or \"chrome-free\" tanning are practiced to better manage chromium usage.\nThe high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). \nSeveral chromium compounds are used as catalysts for processing hydrocarbons. For example, the Phillips catalyst, prepared from chromium oxides, is used for the production of about half the world's polyethylene. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.\nBiological role.\nThe biologically beneficial effects of Chromium(III) continue to be debated. Chromium is accepted by the U.S. National Institutes of Health as a trace element for its roles in the action of insulin, a hormone critical to the metabolism and storage of carbohydrate, fat, and protein. The precise mechanism of its actions in the body, however, have not been fully defined, leaving in question whether chromium is essential for healthy people.\nIn contrast, hexavalent chromium (Cr(VI) or Cr6+) is highly toxic and mutagenic when inhaled. Ingestion of chromium(VI) in water has been linked to stomach tumors, and it may also cause allergic contact dermatitis (ACD).\nChromium deficiency, involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor, is controversial. Some studies suggest that the biologically active form of chromium (III) is transported in the body via an oligopeptide called low-molecular-weight chromium-binding substance (LMWCr), which might play a role in the insulin signaling pathway.\nThe chromium content of common foods is generally low (1-13 micrograms per serving). The chromium content of food varies widely, due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. Chromium (and nickel) leach into food cooked in stainless steel, with the effect being largest when the cookware is new. Acidic foods that are cooked for many hours also exacerbate this effect.\nDietary recommendations.\nThere is disagreement on chromium's status as an essential nutrient. Governmental departments from Australia, New Zealand, India, Japan, and the United States consider chromium essential while the European Food Safety Authority (EFSA), representing the European Union, does not.\nThe National Academy of Medicine (NAM) updated the Estimated Average Requirements (EARs) and the Recommended Dietary Allowances (RDAs) for chromium in 2001. For chromium, there was insufficient information to set EARs and RDAs, so its needs are described as estimates for Adequate Intakes (AIs). The current AIs of chromium for women ages 14 through 50 is 25 \u03bcg/day, and the AIs for women ages 50 and above is 20 \u03bcg/day. The AIs for women who are pregnant are 30 \u03bcg/day, and for women who are lactating, the set AIs are 45 \u03bcg/day. The AIs for men ages 14 through 50 are 35 \u03bcg/day, and the AIs for men ages 50 and above are 30 \u03bcg/day. For children ages 1 through 13, the AIs increase with age from 0.2 \u03bcg/day up to 25 \u03bcg/day. As for safety, the NAM sets Tolerable Upper Intake Levels (ULs) for vitamins and minerals when the evidence is sufficient. In the case of chromium, there is not yet enough information, hence no UL has been established. Collectively, the EARs, RDAs, AIs, and ULs are the parameters for the nutrition recommendation system known as Dietary Reference Intake (DRI). Australia and New Zealand consider chromium to be an essential nutrient, with an AI of 35 \u03bcg/day for men, 25 \u03bcg/day for women, 30 \u03bcg/day for women who are pregnant, and 45 \u03bcg/day for women who are lactating. A UL has not been set due to the lack of sufficient data. India considers chromium to be an essential nutrient, with an adult recommended intake of 33 \u03bcg/day. Japan also considers chromium to be an essential nutrient, with an AI of 10 \u03bcg/day for adults, including women who are pregnant or lactating. A UL has not been set. The EFSA of the European Union however, does not consider chromium to be an essential nutrient; chromium is the only mineral for which the United States and the European Union disagree.\nFor U.S. food and dietary supplement labeling purposes, the amount of the substance in a serving is expressed as a percent of the Daily Value (%DV). For chromium labeling purposes, 100% of the Daily Value was 120 \u03bcg. As of May 27, 2016, the percentage of daily value was revised to 35 \u03bcg to bring the chromium intake into a consensus with the official Recommended Dietary Allowance. Compliance with the updated labeling regulations was required by 01 January 2020 for manufacturers with US$10\u00a0million or more in annual food sales, and by 01 January 2021 for manufacturers with lower volume food sales. A table of the old and new adult daily values is provided at Reference Daily Intake.\nFood sources.\nFood composition databases such as those maintained by the U.S. Department of Agriculture do not contain information on the chromium content of foods. A wide variety of animal and vegetable foods contain chromium. Content per serving is influenced by the chromium content of the soil in which the plants are grown, by foodstuffs fed to animals, and by processing methods, as chromium is leached into foods if processed or cooked in stainless steel equipment. One diet analysis study conducted in Mexico reported an average daily chromium intake of 30 micrograms. An estimated 31% of adults in the United States consume multi-vitamin/mineral dietary supplements, which often contain 25 to 60 micrograms of chromium.\nSupplementation.\nChromium is an ingredient in total parenteral nutrition (TPN), because deficiency can occur after months of intravenous feeding with chromium-free TPN. It is also added to nutritional products for preterm infants. Although the mechanism of action in biological roles for chromium is unclear, in the United States chromium-containing products are sold as non-prescription dietary supplements in amounts ranging from 50 to 1,000 \u03bcg. Lower amounts of chromium are also often incorporated into multi-vitamin/mineral supplements consumed by an estimated 31% of adults in the United States. Chemical compounds used in dietary supplements include chromium chloride, chromium citrate, chromium(III) picolinate, chromium(III) polynicotinate, and other chemical compositions. The benefit of supplements has not been proven.\nApproved and disapproved health claims.\nIn 2005, the U.S. Food and Drug Administration had approved a Qualified Health Claim for chromium picolinate with a requirement for very specific label wording: \"One small study suggests that chromium picolinate may reduce the risk of insulin resistance, and therefore possibly may reduce the risk of type 2 diabetes. FDA concludes, however, that the existence of such a relationship between chromium picolinate and either insulin resistance or type 2 diabetes is highly uncertain.\" At the same time, in answer to other parts of the petition, the FDA rejected claims for chromium picolinate and cardiovascular disease, retinopathy or kidney disease caused by abnormally high blood sugar levels. In 2010, chromium(III) picolinate was approved by Health Canada to be used in dietary supplements. Approved labeling statements include: a factor in the maintenance of good health, provides support for healthy glucose metabolism, helps the body to metabolize carbohydrates and helps the body to metabolize fats. The European Food Safety Authority (EFSA) approved claims in 2010 that chromium contributed to normal macronutrient metabolism and maintenance of normal blood glucose concentration, but rejected claims for maintenance or achievement of a normal body weight, or reduction of tiredness or fatigue.\nGiven the evidence for chromium deficiency causing problems with glucose management in the context of intravenous nutrition products formulated without chromium, research interest turned to whether chromium supplementation would benefit people who have type 2 diabetes but are not chromium deficient. Looking at the results from four meta-analyses, one reported a statistically significant decrease in fasting plasma glucose levels (FPG) and a non-significant trend in lower hemoglobin A1C. A second reported the same, a third reported significant decreases for both measures, while a fourth reported no benefit for either. A review published in 2016 listed 53 randomized clinical trials that were included in one or more of six meta-analyses. It concluded that whereas there may be modest decreases in FPG and/or HbA1C that achieve statistical significance in some of these meta-analyses, few of the trials achieved decreases large enough to be expected to be relevant to clinical outcome.\nTwo systematic reviews looked at chromium supplements as a mean of managing body weight in overweight and obese people. One, limited to chromium picolinate, a popular supplement ingredient, reported a statistically significant \u22121.1\u00a0kg (2.4\u00a0lb) weight loss in trials longer than 12 weeks. The other included all chromium compounds and reported a statistically significant \u22120.50\u00a0kg (1.1\u00a0lb) weight change. Change in percent body fat did not reach statistical significance. Authors of both reviews considered the clinical relevance of this modest weight loss as uncertain/unreliable. The European Food Safety Authority reviewed the literature and concluded that there was insufficient evidence to support a claim.\nChromium is promoted as a sports performance dietary supplement, based on the theory that it potentiates insulin activity, with anticipated results of increased muscle mass, and faster recovery of glycogen storage during post-exercise recovery. A review of clinical trials reported that chromium supplementation did not improve exercise performance or increase muscle strength. The International Olympic Committee reviewed dietary supplements for high-performance athletes in 2018 and concluded there was no need to increase chromium intake for athletes, nor support for claims of losing body fat.\nFresh-water fish.\nChromium is naturally present in the environment in trace amounts, but industrial use in rubber and stainless steel manufacturing, chrome plating, dyes for textiles, tanneries and other uses contaminates aquatic systems. In Bangladesh, rivers in or downstream from industrialized areas exhibit heavy metal contamination. Irrigation water standards for chromium are 0.1\u00a0mg/L, but some rivers are more than five times that amount. The standard for fish for human consumption is less than 1\u00a0mg/kg, but many tested samples were more than five times that amount. Chromium, especially hexavalent chromium, is highly toxic to fish because it is easily absorbed across the gills, readily enters blood circulation, crosses cell membranes and bioconcentrates up the food chain. In contrast, the toxicity of trivalent chromium is very low, attributed to poor membrane permeability and little biomagnification.\nAcute and chronic exposure to chromium(VI) affects fish behavior, physiology, reproduction and survival. Hyperactivity and erratic swimming have been reported in contaminated environments. Egg hatching and fingerling survival are affected. In adult fish there are reports of histopathological damage to liver, kidney, muscle, intestines, and gills. Mechanisms include mutagenic gene damage and disruptions of enzyme functions.\nThere is evidence that fish may not require chromium, but benefit from a measured amount in diet. In one study, juvenile fish gained weight on a zero chromium diet, but the addition of 500 \u03bcg of chromium in the form of chromium chloride or other supplement types, per kilogram of food (dry weight), increased weight gain. At 2,000 \u03bcg/kg the weight gain was no better than with the zero chromium diet, and there were increased DNA strand breaks.\nPrecautions.\nWater-insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Acute oral toxicity ranges between 50 and 150\u00a0mg/kg. A 2008 review suggested that moderate uptake of chromium(III) through dietary supplements poses no genetic-toxic risk. In the US, the Occupational Safety and Health Administration (OSHA) has designated an air permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 1\u00a0mg/m3. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5\u00a0mg/m3, time-weighted average. The IDLH (immediately dangerous to life and health) value is 250\u00a0mg/m3.\nChromium(VI) toxicity.\nThe acute oral toxicity for chromium(VI) ranges between 1.5 and 3.3\u00a0mg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidant properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal, and liver failure result. Aggressive dialysis can be therapeutic.\nThe carcinogenity of chromate dust has been known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.\nChromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as \"chrome ulcers\". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.\nEnvironmental issues.\nBecause chromium compounds were used in dyes, paints, and leather tanning compounds, these compounds are often found in soil and groundwater at active and abandoned industrial sites, needing environmental cleanup and remediation. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.\nIn 2010, the Environmental Working Group studied the drinking water in 35 American cities in the first nationwide study. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit.\nThe more toxic hexavalent chromium form can be reduced to the less soluble trivalent oxidation state in soils by organic matter, ferrous iron, sulfides, and other reducing agents, with the rates of such reduction being faster under more acidic conditions than under more alkaline ones. In contrast, trivalent chromium can be oxidized to hexavalent chromium in soils by manganese oxides, such as Mn(III) and Mn(IV) compounds. Since the solubility and toxicity of chromium (VI) are greater that those of chromium (III), the oxidation-reduction conversions between the two oxidation states have implications for movement and bioavailability of chromium in soils, groundwater, and plants."}
{"id": "5671", "revid": "39138398", "url": "https://en.wikipedia.org/wiki?curid=5671", "title": "Cymbal", "text": "A cymbal is a common percussion instrument. Often used in pairs, cymbals consist of thin, normally round plates of various alloys. The majority of cymbals are of indefinite pitch, although small disc-shaped cymbals based on ancient designs sound a definite note (such as crotales). Cymbals are used in many ensembles ranging from the orchestra, percussion ensembles, jazz bands, heavy metal bands, and marching groups. Drum kits usually incorporate at least a crash, ride, or crash/ride, and a pair of hi-hat cymbals. A player of cymbals is known as a cymbalist.\nEtymology and names.\nThe word cymbal is derived from the Latin \"cymbalum\", which is the latinisation of the Greek word \"kymbalon\", \"cymbal\", which in turn derives from \"kymb\u0113\", \"cup, bowl\".\nIn orchestral scores, cymbals may be indicated by the French \"cymbales\"; German \"Becken\", \"Schellbecken\", \"Teller\", or \"Tschinellen\"; Italian \"piatti\" or \"cinelli\"; and Spanish \"platillos\". Many of these derive from the word for plates.\nHistory.\nCymbals have existed since ancient times. Representations of cymbals may be found in reliefs and paintings from Armenian Highlands (7th century BC), Larsa, Babylon, Assyria, ancient Egypt, ancient Greece, and ancient Rome. References to cymbals also appear throughout the Bible, through many Psalms and songs of praise to God. Cymbals may have been introduced to China from Central Asia in the 3rd or 4th century AD.\nIn India, Cymbals have been in use since very ancient times and are still used across almost all major temples and Buddhist sites. Gigantic Aartis along the Ganges which are revered by Hindus all over the world, are incomplete without large cymbals.\nCymbals were employed by Turkish janissaries in the 14th century or earlier. By the 17th century, such cymbals were used in European music, and more commonly played in military bands and orchestras by the mid 18th century. Since the 19th century, some composers have called for larger roles for cymbals in musical works, and a variety of cymbal shapes, techniques, and hardware have been developed in response.\nAnatomy.\nThe anatomy of the cymbal plays a large part in the sound it creates. A hole is drilled in the center of the cymbal, which is used to either mount the cymbal on a stand or for tying straps through (for hand playing). The bell, dome, or cup is the raised section immediately surrounding the hole. The bell produces a higher \"pinging\" pitch than the rest of the cymbal. The bow is the rest of the surface surrounding the bell. The bow is sometimes described in two areas: the ride and crash area. The ride area is the thicker section closer to the bell while the crash area is the thinner tapering section near the edge. The edge or rim is the immediate circumference of the cymbal.\nCymbals are measured by their diameter either in inches or centimeters. The size of the cymbal affects its sound, larger cymbals usually being louder and having longer sustain. The weight describes how thick the cymbal is. Cymbal weights are important to the sound they produce and how they play. Heavier cymbals have a louder volume, more cut, and better stick articulation (when using drum sticks). Thin cymbals have a fuller sound, lower pitch, and faster response.\nThe profile of the cymbal is the vertical distance of the bow from the bottom of the bell to the cymbal edge (higher profile cymbals are more bowl-shaped). The profile affects the pitch of the cymbal: higher profile cymbals have higher pitch.\nTypes.\nOrchestral cymbals.\nCymbals offer a composer nearly endless amounts of color and effect. Their unique timbre allows them to project even against a full orchestra and through the heaviest of orchestrations and enhance articulation and nearly any dynamic. Cymbals have been utilized historically to suggest frenzy, fury or bacchanalian revels, as seen in the Venus music in Wagner's \"Tannh\u00e4user\", Grieg's \"Peer Gynt suite\", and Osmin's aria \"O wie will ich triumphieren\" from Mozart's \"Die Entf\u00fchrung aus dem Serail\".\nClash cymbals.\nOrchestral clash cymbals are traditionally used in pairs, each one having a strap set in the bell of the cymbal by which they are held. Such a pair is known as clash cymbals, crash cymbals, hand cymbals, or plates. Certain sounds can be obtained by rubbing their edges together in a sliding movement for a \"sizzle\", striking them against each other in what is called a \"crash\", tapping the edge of one against the body of the other in what is called a \"tap-crash\", scraping the edge of one from the inside of the bell to the edge for a \"scrape\" or \"zischen\", or shutting the cymbals together and choking the sound in what is called a \"hi-hat\" or \"crush\". A skilled percussionist can obtain an enormous dynamic range from such cymbals. For example, in Beethoven's Symphony No. 9, the percussionist is employed to first play cymbals pianissimo, adding a touch of colour rather than loud crash.\nCrash cymbals are usually damped by pressing them against the percussionist's body. A composer may write \"laissez vibrer\", or, \"let vibrate\" (usually abbreviated l.v.), \"secco\" (dry), or equivalent indications on the score; more usually, the percussionist must judge when to damp based on the written duration of a crash and the context in which it occurs. Crash cymbals have traditionally been accompanied by the bass drum playing an identical part. This combination, played loudly, is an effective way to accentuate a note since it contributes to both very low and very high-frequency ranges and provides a satisfying \"crash-bang-wallop\". In older music the composer sometimes provided one part for this pair of instruments, writing \"senza piatti\" or \"piatti soli\" () if only one is needed. This came from the common practice of having one percussionist play using one cymbal mounted to the shell of the bass drum. The percussionist would crash the cymbals with the left hand and use a mallet to strike the bass drum with the right. This method is nowadays often employed in pit orchestras and called for specifically by composers who desire a certain effect. Stravinsky calls for this in his ballet Petrushka, and Mahler calls for this in his Titan Symphony. The modern convention is for the instruments to have independent parts. However, in kit drumming, a cymbal crash is still most often accompanied by a simultaneous kick to the bass drum, which provides a musical effect and support to the crash.\nHi hats.\nCrash cymbals evolved into the low-sock and from this to the modern hi-hat. Even in a modern drum kit, they remain paired with the bass drum as the two instruments which are played with the player's feet. However, hi-hat cymbals tend to be heavy with little taper, more similar to a ride cymbal than to a clash cymbal as found in a drum kit, and perform a ride rather than a crash function.\nSuspended cymbal.\nAnother use of cymbals is the suspended cymbal. This instrument takes its name from the traditional method of suspending the cymbal by means of a leather strap or rope, thus allowing the cymbal to vibrate as freely as possible for maximum musical effect. Early jazz drumming pioneers borrowed this style of cymbal mounting during the early 1900s and later drummers further developed this instrument into the mounted horizontal or nearly horizontally mounted \"crash\" cymbals of a modern drum kit, However, most modern drum kits do not employ a leather strap suspension system. Many modern drum kits use a mount with felt or otherwise dampening fabric to act as a barrier to hold the cymbals between metal clamps: thus forming the modern-day ride cymbal. Suspended cymbals can be played with yarn-, sponge-, or cord wrapped mallets. The first known instance of using a sponge-headed mallet on a cymbal is the final chord of Hector Berlioz' Symphonie Fantastique. Composers sometimes specifically request other types of mallets like felt mallets or timpani mallets for different attack and sustain qualities. Suspended cymbals can produce bright and slicing tones when forcefully struck, and give an eerie transparent \"windy\" sound when played quietly. A tremolo, or roll (played with two mallets alternately striking on opposing sides of the cymbal) can build in volume from almost inaudible to an overwhelming climax in a satisfyingly smooth manner (as in Humperdinck's Mother Goose Suite). The edge of a suspended cymbal may be hit with the shoulder of a drum stick to obtain a sound somewhat akin to that of clash cymbals. Other methods of playing include scraping a coin or triangle beater rapidly across the ridges on the top of the cymbal, giving a \"zing\" sound (as some percussionists do in the fourth movement of Dvo\u0159\u00e1k's Symphony No. 9). Other effects that can be used include drawing a bass bow across the edge of the cymbal for a sound like squealing car brakes.\nAncient cymbals.\nAncient, antique or tuned cymbals are much more rarely called for. Their timbre is entirely different, more like that of small hand-bells or of the notes of the keyed harmonica. They are not struck full against each other, but by one of their edges, and the note given in by them is higher in proportion as they are thicker and smaller. Berlioz's \"Romeo and Juliet\" calls for two pairs of cymbals, modeled on some old Pompeian instruments no larger than the hand (some are no larger than a crown piece), and tuned to F and B flat. The modern instruments descended from this line are the crotales.\nList of cymbal types.\nCymbal types include:"}
{"id": "5672", "revid": "41369514", "url": "https://en.wikipedia.org/wiki?curid=5672", "title": "Cadmium", "text": "Cadmium is a chemical element with the symbol Cd and atomic number 48. This soft, silvery-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it demonstrates oxidation state +2 in most of its compounds, and like mercury, it has a lower melting point than the transition metals in groups 3 through 11. Cadmium and its congeners in group 12 are often not considered transition metals, in that they do not have partly filled \"d\" or \"f\" electron shells in the elemental or common oxidation states. The average concentration of cadmium in Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.\nCadmium occurs as a minor component in most zinc ores and is a byproduct of zinc production. Cadmium was used for a long time as a corrosion-resistant plating on steel, and cadmium compounds are used as red, orange and yellow pigments, to color glass, and to stabilize plastic. Cadmium use is generally decreasing because it is toxic (it is specifically listed in the European Restriction of Hazardous Substances Directive) and nickel-cadmium batteries have been replaced with nickel-metal hydride and lithium-ion batteries. One of its few new uses is in cadmium telluride solar panels.\nAlthough cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.\nCharacteristics.\nPhysical properties.\nCadmium is a soft, malleable, ductile, silvery-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike most other metals, cadmium is resistant to corrosion and is used as a protective plate on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.\nChemical properties.\nAlthough cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid, and nitric acid dissolve cadmium by forming cadmium chloride (CdCl2), cadmium sulfate (CdSO4), or cadmium nitrate (Cd(NO3)2). The oxidation state +1 can be produced by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd22+ cation, which is similar to the Hg22+ cation in mercury(I) chloride.\nThe structures of many cadmium complexes with nucleobases, amino acids, and vitamins have been determined.\nIsotopes.\nNaturally occurring cadmium is composed of 8 isotopes. Two of them are radioactive, and three are expected to decay but have not done so under laboratory conditions. The two natural radioactive isotopes are 113Cd (beta decay, half-life is ) and 116Cd (two-neutrino double beta decay, half-life is ). The other three are 106Cd, 108Cd (both double electron capture), and 114Cd (double beta decay); only lower limits on these half-lives have been determined. At least three isotopes\u00a0\u2013 110Cd, 111Cd, and 112Cd\u00a0\u2013 are stable. Among the isotopes that do not occur naturally, the most long-lived are 109Cd with a half-life of 462.6\u00a0days, and 115Cd with a half-life of 53.46\u00a0hours. All of the remaining radioactive isotopes have half-lives of less than 2.5\u00a0hours, and the majority have half-lives of less than 5\u00a0minutes. Cadmium has 8 known meta states, with the most stable being 113mCd (\"t\"1\u20442\u00a0= 14.1\u00a0years), 115mCd (\"t\"1\u20442\u00a0= 44.6\u00a0days), and 117mCd (\"t\"1\u20442\u00a0= 3.36\u00a0hours).\nThe known isotopes of cadmium range in atomic mass from 94.950\u00a0u (95Cd) to 131.946\u00a0u (132Cd). For isotopes lighter than 112\u00a0u, the primary decay mode is electron capture and the dominant decay product is element\u00a047 (silver). Heavier isotopes decay mostly through beta emission producing element\u00a049 (indium).\nOne isotope of cadmium, 113Cd, absorbs neutrons with high selectivity: With very high probability, neutrons with energy below the \"cadmium cut-off\" will be absorbed; those higher than the \"cut-off will be transmitted\". The cadmium cut-off is about 0.5\u00a0eV, and neutrons below that level are deemed slow neutrons, distinct from intermediate and fast neutrons.\nCadmium is created via the s-process in low- to medium-mass stars with masses of 0.6\u00a0to 10\u00a0solar masses, over thousands of years. In that process, a silver atom captures a neutron and then undergoes beta decay.\nHistory.\nCadmium (Latin \"cadmia\", Greek \"\u03ba\u03b1\u03b4\u03bc\u03b5\u03af\u03b1\" meaning \"calamine\", a cadmium-bearing mixture of minerals that was named after the Greek mythological character \u039a\u03ac\u03b4\u03bc\u03bf\u03c2, Cadmus, the founder of Thebes) was discovered in contaminated zinc compounds sold in pharmacies in Germany. In 1817 by Friedrich Stromeyer and Karl Samuel Leberecht Hermann simultaneously investigated the discoloration in zinc oxide and found an impurity, first suspected to be arsenic, because of the yellow precipitate with hydrogensulfide. Additionally Stromeyer discovered that one supplier sold zinc carbonate instead of zinc oxide. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, because it was found in this zinc ore. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reducing the sulfide. The potential for cadmium yellow as pigment was recognized in the 1840s, but the lack of cadmium limited this application.\nEven though cadmium and its compounds are toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat \"enlarged joints, scrofulous glands, and chilblains\".\nIn 1907, the International Astronomical Union defined the international \u00e5ngstr\u00f6m in terms of a red cadmium spectral line (1 wavelength = 6438.46963\u00a0\u00c5). This was adopted by the 7th General Conference on Weights and Measures in 1927. In 1960, the definitions of both the metre and \u00e5ngstr\u00f6m were changed to use krypton.\nAfter the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was used for plating. In 1956, 24% of the cadmium in the United States was used for a second application in red, orange and yellow pigments from sulfides and selenides of cadmium.\nThe stabilizing effect of cadmium chemicals like the carboxylates cadmium laurate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The demand for cadmium in pigments, coatings, stabilizers, and alloys declined as a result of environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of to total cadmium consumption was used for plating, and only 10% was used for pigments.\nAt the same time, these decreases in consumption were compensated by a growing demand for cadmium for nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.\nOccurrence.\nCadmium makes up about 0.1\u00a0ppm of Earth's crust. It is much rarer than zinc, which makes up about 65\u00a0ppm. No significant deposits of cadmium-containing ores are known. The only cadmium mineral of importance, greenockite (CdS), is nearly always associated with sphalerite (ZnS). This association is caused by geochemical similarity between zinc and cadmium, with no geological process likely to separate them. Thus, cadmium is produced mainly as a byproduct of mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but wide use began after World War I.\nMetallic cadmium can be found in the Vilyuy River basin in Siberia.\nRocks mined for phosphate fertilizers contain varying amounts of cadmium, resulting in a cadmium concentration of as much as 300\u00a0mg/kg in the fertilizers and a high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in flue dust. Cadmium in soil can be absorbed by crops such as rice. Chinese ministry of agriculture measured in 2002 that 28% of rice it sampled had excess lead and 10% had excess cadmium above limits defined by law. Some plants such as willow trees and poplars have been found to clean both lead and cadmium from soil.\nTypical background concentrations of cadmium do not exceed 5\u00a0ng/m3 in the atmosphere; 2\u00a0mg/kg in soil; 1\u00a0\u03bcg/L in freshwater and 50\u00a0ng/L in seawater. Concentrations of cadmium above 10 \u03bcg/l may be stable in water having low total solute concentrations and \"p\" H and can be difficult to remove by conventional water treatment processes.\nProduction.\nThe British Geological Survey reports that in 2001, China was the top producer of cadmium with almost one-sixth of the world's production, closely followed by South Korea and Japan.\nCadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from sulfidic zinc ores contain up to 1.4% of cadmium. In the 1970s, the output of cadmium was 6.5 pounds per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated from the electrolysis solution.\nApplications.\nCadmium is a common component of electric batteries, pigments, coatings, and electroplating.\nBatteries.\nIn 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2\u00a0V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union put a limit on cadmium in electronics in 2004 of 0.01%, with some exceptions, and in 2006 reduced the limit on cadmium content to 0.002%. Another type of battery based on cadmium is the silver-cadmium battery.\nElectroplating.\nCadmium electroplating, consuming 6% of the global production, is used in the aircraft industry to reduce corrosion of steel components. This coating is passivated by chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels from the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition).\nTitanium embrittlement from cadmium-plated tool residues resulted in banishment of those tools (and the implementation of routine tool testing to detect cadmium contamination) in the A-12/SR-71, U-2, and subsequent aircraft programs that use titanium.\nNuclear fission.\nCadmium is used in the control rods of nuclear reactors, acting as a very effective neutron poison to control neutron flux in nuclear fission. When cadmium rods are inserted in the core of a nuclear reactor, cadmium absorbs neutrons, preventing them from creating additional fission events, thus controlling the amount of reactivity. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.\nTelevisions.\nQLED TVs have been starting to include cadmium in construction. Some companies have been looking to reduce the environmental impact of human exposure and pollution of the material in televisions during production.\nAnticancer drugs.\nComplexes based on heavy metals have great potential for the treatment of a wide variety of cancers but their use is often limited due to toxic side effects. However, scientists are advancing in the field and new promising cadmium complex compounds with reduced toxicity have been discovered.\nCompounds.\nCadmium oxide was used in black and white television phosphors and in the blue and green phosphors of color television cathode ray tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.\nVarious cadmium salts are used in paint pigments, with CdS as a yellow pigment being the most common. Cadmium selenide is a red pigment, commonly called \"cadmium red\". To painters who work with the pigment, cadmium provides the most brilliant and durable yellows, oranges, and reds\u00a0\u2013 so much so that during production, these colors are significantly toned down before they are ground with oils and binders or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Because these pigments are potentially toxic, users should use a barrier cream on the hands to prevent absorption through the skin even though the amount of cadmium absorbed into the body through the skin is reported to be less than 1%.\nIn PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, because it has a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.\nLaboratory uses.\nHelium\u2013cadmium lasers are a common source of blue-ultraviolet laser light. They operate at either 325 or 422\u00a0nm in fluorescence microscopes and various laboratory experiments. Cadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.\nCadmium is a component of some compound semiconductors, such as cadmium sulfide, cadmium selenide, and cadmium telluride, used for light detection and solar cells. HgCdTe is sensitive to infrared light and can be used as an infrared detector, motion detector, or switch in remote control devices.\nIn molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1\u03b1.\nCadmium-selective sensors.\nCadmium-selective sensors based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells. One of the most popular ways to monitor cadmium in aqueous environments is the use of electrochemistry, one example is by attaching a self-assembled monolayer that can help obtain a cadmium selective electrode with a ppt-level sensitivity.\nBiological role and research.\nCadmium has no known function in higher organisms, but a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. Cadmium is considered an environmental pollutant that causes health hazard to living organisms. Administration of cadmium to cells causes oxidative stress and increases the levels of antioxidants produced by cells to protect against macro molecular damage. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. This was discovered with X-ray absorption near edge structure (XANES) spectroscopy.\nThe highest concentration of cadmium is absorbed in the kidneys of humans, and up to about 30\u00a0mg of cadmium is commonly inhaled throughout human childhood and adolescence. Cadmium is under preliminary research for its toxicity in humans, potentially affecting mechanisms and risks of cancer, cardiovascular disease, and osteoporosis.\nEnvironment.\nThe biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment.\nSafety.\nIndividuals and organizations have been reviewing cadmium's bioinorganic aspects for its toxicity. The most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium fumes can result initially in metal fume fever, but may progress to chemical pneumonitis, pulmonary edema, and death.\nCadmium is also an environmental hazard. Human exposure is primarily from fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Bread, root crops, and vegetables also contribute to the cadmium in modern populations.\nThere have been a few instances of general population poisoning as the result of long-term exposure to cadmium in contaminated food and water. Research into an estrogen mimicry that may induce breast cancer is ongoing. In the decades leading up to World War II, mining operations contaminated the Jinz\u016b River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops along the riverbanks downstream of the mines. Some members of the local agricultural communities consumed the contaminated rice and developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria. The victims of this poisoning were almost exclusively post-menopausal women with low iron and low body stores of other minerals. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, although cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors.\nCadmium is one of six substances banned by the European Union's Restriction of Hazardous Substances (RoHS) directive, which regulates hazardous substances in electrical and electronic equipment, but allows for certain exemptions and exclusions from the scope of the law.\nThe International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still uncertainty about the carcinogenicity of cadmium in low environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet is associated with a higher risk of endometrial, breast, and prostate cancer as well as with osteoporosis in humans. A recent study has demonstrated that endometrial tissue is characterized by higher levels of cadmium in current and former smoking females.\nCadmium exposure is associated with a large number of illnesses including kidney disease, early atherosclerosis, hypertension, and cardiovascular diseases. Although studies show a significant correlation between cadmium exposure and occurrence of disease in human populations, a molecular mechanism has not yet been identified. One hypothesis holds that cadmium is an endocrine disruptor and some experimental studies have shown that it can interact with different hormonal signaling pathways. For example, cadmium can bind to the estrogen receptor alpha, and affect signal transduction along the estrogen and MAPK signaling pathways at low doses.\nThe tobacco plant absorbs and accumulates heavy metals such as cadmium from the surrounding soil into its leaves. Following tobacco smoke inhalation, these are readily absorbed into the body of users. Tobacco smoking is the most important single source of cadmium exposure in the general population. An estimated 10% of the cadmium content of a cigarette is inhaled through smoking. Absorption of cadmium through the lungs is more effective than through the gut. As much as 50% of the cadmium inhaled in cigarette smoke may be absorbed.\nOn average, cadmium concentrations in the blood of smokers is 4\u00a0to 5 times greater than non-smokers and in the kidney, 2\u20133 times greater than in non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking.\nIn a non-smoking population, food is the greatest source of exposure. High quantities of cadmium can be found in crustaceans, mollusks, offal, frog legs, cocoa solids, bitter and semi-bitter chocolate, seaweed, fungi and algae products. However, grains, vegetables, and starchy roots and tubers are consumed in much greater quantity in the U.S., and are the source of the greatest dietary exposure there. Most plants bio-accumulate metal toxins such as Cd and when composted to form organic fertilizers, yield a product that often can contain high amounts (e.g., over 0.5\u00a0mg) of metal toxins for every kilo of fertilizer. Fertilizers made from animal dung (e.g., cow dung) or urban waste can contain similar amounts of Cd. The Cd added to the soil from fertilizers (rock phosphates or organic fertilizers) become bio-available and toxic only if the soil pH is low (i.e., acidic soils).\nZn, Cu, Ca, and Fe ions, and selenium with vitamin C are used to treat Cd intoxication, though it is not easily reversed.\nRegulations.\nBecause of the adverse effects of cadmium on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.\nThe EFSA Panel on Contaminants in the Food Chain specifies that 2.5 \u03bcg/kg body weight is a tolerable weekly intake for humans. The Joint FAO/WHO Expert Committee on Food Additives has declared 7 \u03bcg/kg bw to be the provisional tolerable weekly intake level. The state of California requires a food label to carry a warning about potential exposure to cadmium on products such as cocoa powder.\nThe U.S. Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit (PEL) for cadmium at a time-weighted average (TWA) of 0.005 ppm. The National Institute for Occupational Safety and Health (NIOSH) has not set a recommended exposure limit (REL) and has designated cadmium as a known human carcinogen. The IDLH (immediately dangerous to life and health) level for cadmium is 9\u00a0mg/m3.\nProduct recalls.\nIn May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled when the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content in jewelry sold by Claire's and Wal-Mart stores.\nIn June 2010, McDonald's voluntarily recalled more than 12\u00a0million promotional \"Shrek Forever After 3D\" Collectible Drinking Glasses because of the cadmium levels in paint pigments on the glassware. The glasses were manufactured by Arc International, of Millville, NJ, USA."}
{"id": "5675", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=5675", "title": "Curium", "text": "Curium is a transuranic radioactive chemical element with the symbol Cm and atomic number 96. This element of the actinide series was named after Marie and Pierre Curie, both known for their research on radioactivity. Curium was first intentionally produced and identified in July 1944 by the group of Glenn T. Seaborg at the University of California, Berkeley. The discovery was kept secret and only released to the public in November 1947. Most curium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors \u2013 one tonne of spent nuclear fuel contains about 20\u00a0grams of curium.\nCurium is a hard, dense, silvery metal with a relatively high melting point and boiling point for an actinide. Whereas it is paramagnetic at ambient conditions, it becomes antiferromagnetic upon cooling, and other magnetic transitions are also observed for many curium compounds. In compounds, curium usually exhibits valence +3 and sometimes +4, and the +3 valence is predominant in solutions. Curium readily oxidizes, and its oxides are a dominant form of this element. It forms strongly fluorescent complexes with various organic compounds, but there is no evidence of its incorporation into bacteria and archaea. When introduced into the human body, curium accumulates in the bones, lungs and liver, where it promotes cancer.\nAll known isotopes of curium are radioactive and have a small critical mass for a sustained nuclear chain reaction. They predominantly emit \u03b1-particles, and the heat released in this process can serve as a heat source in radioisotope thermoelectric generators, but this application is hindered by the scarcity and high cost of curium isotopes. Curium is used in production of heavier actinides and of the 238Pu radionuclide for power sources in artificial pacemakers and RTGs for spacecraft. It served as the \u03b1-source in the alpha particle X-ray spectrometers installed on several space probes, including the Sojourner, Spirit, Opportunity and Curiosity Mars rovers and the Philae lander on comet 67P/Churyumov\u2013Gerasimenko, to analyze the composition and structure of the surface.\nHistory.\nAlthough curium had likely been produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso. In their experiments, they used a cyclotron.\nCurium was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) at the University of Chicago. It was the third transuranium element to be discovered even though it is the fourth in the series \u2013 the lighter element americium was unknown at the time.\nThe sample was prepared as follows: first plutonium nitrate solution was coated on a platinum foil of about 0.5\u00a0cm2 area, the solution was evaporated and the residue was converted into plutonium(IV) oxide (PuO2) by annealing. Following cyclotron irradiation of the oxide, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid, and further separation was carried out by ion exchange to yield a certain isotope of curium. The separation of curium and americium was so painstaking that the Berkeley group initially called those elements \"pandemonium\" (from Greek for \"all demons\" or \"hell\") and \"delirium\" (from Latin for \"madness\").\nThe curium-242 isotope was produced in July\u2013August 1944 by bombarding 239Pu with \u03b1-particles to produce curium with the release of a neutron:\nCurium-242 was unambiguously identified by the characteristic energy of the \u03b1-particles emitted during the decay:\nThe half-life of this alpha decay was first measured as 150 days and then corrected to 162.8 days.\nAnother isotope 240Cm was produced in a similar reaction in March 1945:\nThe half-life of the 240Cm \u03b1-decay was correctly determined as 26.7 days.\nThe discovery of curium, as well as americium, in 1944 was closely related to the Manhattan Project, so the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children, the Quiz Kids, five days before the official presentation at an American Chemical Society meeting on November 11, 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. The discovery of curium (242Cm and 240Cm), its production, and its compounds was later patented listing only Seaborg as the inventor.\nThe new element was named after Marie Sk\u0142odowska-Curie and her husband Pierre Curie who are noted for discovering radium and for their work in radioactivity. It followed the example of gadolinium, a lanthanide element above curium in the periodic table, which was named after the explorer of the rare earth elements Johan Gadolin:\nThe first curium samples were barely visible, and were identified by their radioactivity. Louis Werner and Isadore Perlman created the first substantial sample of 30\u00a0\u00b5g curium-242 hydroxide at the University of California, Berkeley in 1947 by bombarding americium-241 with neutrons. Macroscopic amounts of curium(III) fluoride were obtained in 1950 by W. W. T. Crane, J. C. Wallmann and B. B. Cunningham. Its magnetic susceptibility was very close to that of GdF3 providing the first experimental evidence for the +3 valence of curium in its compounds. Curium metal was produced only in 1951 by reduction of CmF3 with barium.\nCharacteristics.\nPhysical.\nA synthetic, radioactive element, curium is a hard, dense metal with a silvery-white appearance and physical and chemical properties resembling those of gadolinium. Its melting point of 1344\u00a0\u00b0C is significantly higher than that of the previous transuranic elements neptunium (637\u00a0\u00b0C), plutonium (639\u00a0\u00b0C) and americium (1173\u00a0\u00b0C). In comparison, gadolinium melts at 1312\u00a0\u00b0C. The boiling point of curium is 3556\u00a0\u00b0C. With a density of 13.52\u00a0g/cm3, curium is significantly lighter than neptunium (20.45\u00a0g/cm3) and plutonium (19.8\u00a0g/cm3), but is heavier than most other metals. Between two crystalline forms of curium, the \u03b1-Cm is more stable at ambient conditions. It has a hexagonal symmetry, space group P63/mmc, lattice parameters \"a\" = 365 pm and \"c\" = 1182 pm, and four formula units per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with \u03b1-lanthanum. At pressures above 23 GPa, at room temperature, \u03b1-Cm transforms into \u03b2-Cm, which has a face-centered cubic symmetry, space group Fmm and the lattice constant \"a\" = 493 pm. Upon further compression to 43 GPa, curium transforms to an orthorhombic \u03b3-Cm structure similar to that of \u03b1-uranium, with no further transitions observed up to 52 GPa. These three curium phases are also referred to as Cm I, II and III.\nCurium has peculiar magnetic properties. Whereas its neighbor element americium shows no deviation from Curie-Weiss paramagnetism in the entire temperature range, \u03b1-Cm transforms to an antiferromagnetic state upon cooling to 65\u201352 K, and \u03b2-Cm exhibits a ferrimagnetic transition at about 205 K. Meanwhile, curium pnictides show ferromagnetic transitions upon cooling: 244CmN and 244CmAs at 109 K, 248CmP at 73 K and 248CmSb at 162 K. The lanthanide analogue of curium, gadolinium, as well as its pnictides, also show magnetic transitions upon cooling, but the transition character is somewhat different: Gd and GdN become ferromagnetic, and GdP, GdAs and GdSb show antiferromagnetic ordering.\nIn accordance with magnetic data, electrical resistivity of curium increases with temperature \u2013 about twice between 4 and 60 K \u2013 and then remains nearly constant up to room temperature. There is a significant increase in resistivity over time (about 10 \u00b5\u03a9\u00b7cm/h) due to self-damage of the crystal lattice by alpha radiation. This makes uncertain the absolute resistivity value for curium (about 125 \u00b5\u03a9\u00b7cm). The resistivity of curium is similar to that of gadolinium and of the actinides plutonium and neptunium, but is significantly higher than that of americium, uranium, polonium and thorium.\nUnder ultraviolet illumination, curium(III) ions exhibit strong and stable yellow-orange fluorescence with a maximum in the range about 590\u2013640\u00a0nm depending on their environment. The fluorescence originates from the transitions from the first excited state 6D7/2 and the ground state 8S7/2. Analysis of this fluorescence allows monitoring interactions between Cm(III) ions in organic and inorganic complexes.\nChemical.\nCurium ions in solution almost exclusively assume the oxidation state of +3, which is the most stable oxidation state for curium. The +4 oxidation state is observed mainly in a few solid phases, such as CmO2 and CmF4. Aqueous curium(IV) is only known in the presence of strong oxidizers such as potassium persulfate, and is easily reduced to curium(III) by radiolysis and even by water itself. The chemical behavior of curium is different from the actinides thorium and uranium, and is similar to that of americium and many lanthanides. In aqueous solution, the Cm3+ ion is colorless to pale green, and Cm4+ ion is pale yellow. The optical absorption of Cm3+ ions contains three sharp peaks at 375.4, 381.2 and 396.5 nanometers and their strength can be directly converted into the concentration of the ions. The +6 oxidation state has only been reported once in solution in 1978, as the curyl ion (): this was prepared from the beta decay of americium-242 in the americium(V) ion . Failure to obtain Cm(VI) from oxidation of Cm(III) and Cm(IV) may be due to the high Cm4+/Cm3+ ionization potential and the instability of Cm(V).\nCurium ions are hard Lewis acids and thus form most stable complexes with hard bases. The bonding is mostly ionic, with a small covalent component. Curium in its complexes commonly exhibits a 9-fold coordination environment, within a tricapped trigonal prismatic geometry.\nIsotopes.\nAbout 19 radioisotopes and 7 nuclear isomers between 233Cm and 251Cm are known for curium, none of which are stable. The longest half-lives have been reported for 247Cm (15.6 million years) and 248Cm (348,000 years). Other long-lived isotopes are 245Cm (half-life 8500 years), 250Cm (8,300 years) and 246Cm (4,760 years). Curium-250 is unusual in that it predominantly (about 86%) decays via spontaneous fission. The most commonly used curium isotopes are 242Cm and 244Cm with the half-lives of 162.8 days and 18.1 years, respectively.\nAll isotopes between 242Cm and 248Cm, as well as 250Cm, undergo a self-sustaining nuclear chain reaction and thus in principle can act as a nuclear fuel in a reactor. As in most transuranic elements, the nuclear fission cross section is especially high for the odd-mass curium isotopes 243Cm, 245Cm and 247Cm. These can be used in thermal-neutron reactors, whereas a mixture of curium isotopes is only suitable for fast breeder reactors since the even-mass isotopes are not fissile in a thermal reactor and accumulate as burn-up increases. The mixed-oxide (MOX) fuel, which is to be used in power reactors, should contain little or no curium because the neutron activation of 248Cm will create californium. Californium is a strong neutron emitter, and would pollute the back end of the fuel cycle and increase the dose to reactor personnel. Hence, if the minor actinides are to be used as fuel in a thermal neutron reactor, the curium should be excluded from the fuel or placed in special fuel rods where it is the only actinide present.\nThe adjacent table lists the critical masses for curium isotopes for a sphere, without a moderator and reflector. With a metal reflector (30\u00a0cm of steel), the critical masses of the odd isotopes are about 3\u20134\u00a0kg. When using water (thickness ~20\u201330\u00a0cm) as the reflector, the critical mass can be as small as 59\u00a0gram for 245Cm, 155\u00a0gram for 243Cm and 1550\u00a0gram for 247Cm. There is a significant uncertainty in these critical mass values. Whereas it is usually on the order of 20%, the values for 242Cm and 246Cm were listed as large as 371\u00a0kg and 70.1\u00a0kg, respectively, by some research groups.\nCurium is not currently used as a nuclear fuel due to its low availability and high price. 245Cm and 247Cm have very small critical masses and therefore could be used in tactical nuclear weapons, but none are known to have been produced. Curium-243 is not suitable for this purpose because of its short half-life and strong \u03b1 emission, which would result in excessive heat. Curium-247 would be highly suitable due to its long half-life, which is 647 times longer than plutonium-239 (used in many existing nuclear weapons).\nOccurrence.\nThe longest-lived isotope of curium, 247Cm, has a half-life of 15.6 million years. Therefore, any primordial curium, that is curium present on the Earth during its formation, should have decayed by now, although some of it would be detectable as an extinct radionuclide as an excess of its primordial, long-lived daughter 235U. Trace amounts of curium possibly occur naturally in uranium minerals as a result of neutron capture and beta decay sequences, though this has not been confirmed.\nCurium is produced artificially in small quantities for research purposes. Furthermore, it occurs in spent nuclear fuel. Curium is present in nature in certain areas used for nuclear weapons testing. Analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), besides einsteinium, fermium, plutonium and americium also revealed isotopes of berkelium, californium and curium, in particular 245Cm, 246Cm and smaller quantities of 247Cm, 248Cm and 249Cm.\nAtmospheric curium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 4,000 times higher concentration of curium at the sandy soil particles than in water present in the soil pores. An even higher ratio of about 18,000 was measured in loam soils.\nThe transuranic elements from americium to fermium, including curium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\nSynthesis.\nIsotope preparation.\nCurium is produced in small quantities in nuclear reactors, and by now only kilograms of it have been accumulated for the 242Cm and 244Cm and grams or even milligrams for heavier isotopes. This explains the high price of curium, which has been quoted at 160\u2013185 USD per milligram, with a more recent estimate at US$2,000/g for 242Cm and US$170/g for 244Cm. In nuclear reactors, curium is formed from 238U in a series of nuclear reactions. In the first chain, 238U captures a neutron and converts into 239U, which via \u03b2\u2212 decay transforms into 239Np and 239Pu.\nFurther neutron capture followed by \u03b2\u2212-decay produces the 241Am isotope of americium which further converts into 242Cm:\nFor research purposes, curium is obtained by irradiating not uranium but plutonium, which is available in large amounts from spent nuclear fuel. A much higher neutron flux is used for the irradiation that results in a different reaction chain and formation of 244Cm:\nCurium-244 decays into 240Pu by emission of alpha particle, but it also absorbs neutrons resulting in a small amount of heavier curium isotopes. Among those, 247Cm and 248Cm are popular in scientific research because of their long half-lives. However, the production rate of 247Cm in thermal neutron reactors is relatively low because it is prone to undergo fission induced by thermal neutrons. Synthesis of 250Cm via neutron absorption is also rather unlikely because of the short half-life of the intermediate product 249Cm (64 min), which converts by \u03b2\u2212 decay to the berkelium isotope 249Bk.\nThe above cascade of (n,\u03b3) reactions produces a mixture of different curium isotopes. Their post-synthesis separation is cumbersome, and therefore a selective synthesis is desired. Curium-248 is favored for research purposes because of its long half-life. The most efficient preparation method of this isotope is via \u03b1-decay of the californium isotope 252Cf, which is available in relatively large quantities due to its long half-life (2.65 years). About 35\u201350\u00a0mg of 248Cm is being produced by this method every year. The associated reaction produces 248Cm with isotopic purity of 97%.\nAnother interesting for research isotope 245Cm can be obtained from the \u03b1-decay of 249Cf, and the latter isotope is produced in minute quantities from the \u03b2\u2212-decay of the berkelium isotope 249Bk.\nMetal preparation.\nMost synthesis routines yield a mixture of different actinide isotopes as oxides, from which a certain isotope of curium needs to be separated. An example procedure could be to dissolve spent reactor fuel (e.g. MOX fuel) in nitric acid, and remove the bulk of the uranium and plutonium using a PUREX (Plutonium \u2013 URanium EXtraction) type extraction with tributyl phosphate in a hydrocarbon. The lanthanides and the remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction to give, after stripping, a mixture of trivalent actinides and lanthanides. A curium compound is then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. \"Bis\"-triazinyl bipyridine complex has been recently proposed as such reagent which is highly selective to curium. Separation of curium from a very similar americium can also be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone at elevated temperature. Both americium and curium are present in solutions mostly in the +3 valence state; whereas americium oxidizes to soluble Am(IV) complexes, curium remains unchanged and can thus be isolated by repeated centrifugation.\nMetallic curium is obtained by reduction of its compounds. Initially, curium(III) fluoride was used for this purpose. The reaction was conducted in the environment free from water and oxygen, in the apparatus made of tantalum and tungsten, using elemental barium or lithium as reducing agents.\nAnother possibility is the reduction of curium(IV) oxide using a magnesium-zinc alloy in a melt of magnesium chloride and magnesium fluoride.\nCompounds and reactions.\nOxides.\nCurium readily reacts with oxygen forming mostly Cm2O3 and CmO2 oxides, but the divalent oxide CmO is also known. Black CmO2 can be obtained by burning curium oxalate (), nitrate (), or hydroxide in pure oxygen. Upon heating to 600\u2013650\u00a0\u00b0C in vacuum (about 0.01 Pa), it transforms into the whitish Cm2O3:\nAlternatively, Cm2O3 can be obtained by reducing CmO2 with molecular hydrogen:\nFurthermore, a number of ternary oxides of the type M(II)CmO3 are known, where M stands for a divalent metal, such as barium.\nThermal oxidation of trace quantities of curium hydride (CmH2\u20133) has been reported to produce a volatile form of CmO2 and the volatile trioxide CmO3, one of the two known examples of the very rare +6 state for curium. Another observed species was reported to behave similarly to a supposed plutonium tetroxide and was tentatively characterized as CmO4, with curium in the extremely rare +8 state; however, new experiments seem to indicate that CmO4 does not exist, and have cast doubt on the existence of PuO4 as well.\nHalides.\nThe colorless curium(III) fluoride (CmF3) can be produced by introducing fluoride ions into curium(III)-containing solutions. The brown tetravalent curium(IV) fluoride (CmF4) on the other hand is only obtained by reacting curium(III) fluoride with molecular fluorine:\nA series of ternary fluorides are known of the form A7Cm6F31, where A stands for alkali metal.\nThe colorless curium(III) chloride (CmCl3) is produced in the reaction of curium(III) hydroxide (Cm(OH)3) with anhydrous hydrogen chloride gas. It can further be converted into other halides, such as curium(III) bromide (colorless to light green) and curium(III) iodide (colorless), by reacting it with the ammonia salt of the corresponding halide at elevated temperature of about 400\u2013450\u00a0\u00b0C:\nAn alternative procedure is heating curium oxide to about 600\u00a0\u00b0C with the corresponding acid (such as hydrobromic for curium bromide). Vapor phase hydrolysis of curium(III) chloride results in curium oxychloride:\nChalcogenides and pnictides.\nSulfides, selenides and tellurides of curium have been obtained by treating curium with gaseous sulfur, selenium or tellurium in vacuum at elevated temperature. The pnictides of curium of the type CmX are known for the elements nitrogen, phosphorus, arsenic and antimony. They can be prepared by reacting either curium(III) hydride (CmH3) or metallic curium with these elements at elevated temperatures.\nOrganocurium compounds and biological aspects.\nOrganometallic complexes analogous to uranocene are known also for other actinides, such as thorium, protactinium, neptunium, plutonium and americium. Molecular orbital theory predicts a stable \"curocene\" complex (\u03b78-C8H8)2Cm, but it has not been reported experimentally yet.\nFormation of the complexes of the type , where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-C3H7-BTP and Cm3+ ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with curium and therefore are useful in its selective separation from lanthanides and another actinides. Dissolved Cm3+ ions bind with many organic compounds, such as hydroxamic acid, urea, fluorescein and adenosine triphosphate. Many of these compounds are related to biological activity of various microorganisms. The resulting complexes exhibit strong yellow-orange emission under UV light excitation, which is convenient not only for their detection, but also for studying the interactions between the Cm3+ ion and the ligands via changes in the half-life (of the order ~0.1 ms) and spectrum of the fluorescence.\nCurium has no biological significance. There are a few reports on biosorption of Cm3+ by bacteria and archaea, however no evidence for incorporation of curium into them.\nApplications.\nRadionuclides.\nCurium is one of the most radioactive isolable elements. Its two most common isotopes 242Cm and 244Cm are strong alpha emitters (energy 6\u00a0MeV); they have relatively short half-lives of 162.8 days and 18.1 years, and produce as much as 120 W/g and 3 W/g of thermal energy, respectively. Therefore, curium can be used in its common oxide form in radioisotope thermoelectric generators like those in spacecraft. This application has been studied for the 244Cm isotope, while 242Cm was abandoned due to its prohibitive price of around 2000 USD/g. 243Cm with a ~30 year half-life and good energy yield of ~1.6 W/g could make a suitable fuel, but it produces significant amounts of harmful gamma and beta radiation from radioactive decay products. Though as an \u03b1-emitter, 244Cm requires a much thinner radiation protection shielding, it has a high spontaneous fission rate, and thus the neutron and gamma radiation rate are relatively strong. As compared to a competing thermoelectric generator isotope such as 238Pu, 244Cm emits a 500-fold greater fluence of neutrons, and its higher gamma emission requires a shield that is 20 times thicker\u2014about 2\u00a0inches of lead for a 1\u00a0kW source, as compared to 0.1 in for 238Pu. Therefore, this application of curium is currently considered impractical.\nA more promising application of 242Cm is to produce 238Pu, a more suitable radioisotope for thermoelectric generators such as in cardiac pacemakers. The alternative routes to 238Pu use the (n,\u03b3) reaction of 237Np, or the deuteron bombardment of uranium, which both always produce 236Pu as an undesired by-product\u2014since the latter decays to 232U with strong gamma emission. Curium is also a common starting material for the production of higher transuranic elements and transactinides. Thus, bombardment of 248Cm with neon (22Ne), magnesium (26Mg), or calcium (48Ca) yielded certain isotopes of seaborgium (265Sg), hassium (269Hs and 270Hs), and livermorium (292Lv, 293Lv, and possibly 294Lv). Californium was discovered when a microgram-sized target of curium-242 was irradiated with 35\u00a0MeV alpha particles using the cyclotron at Berkeley:\nOnly about 5,000 atoms of californium were produced in this experiment.\nX-ray spectrometer.\nThe most practical application of 244Cm\u2014though rather limited in total volume\u2014is as \u03b1-particle source in the alpha particle X-ray spectrometers (APXS). These instruments were installed on the Sojourner, Mars, Mars 96, Mars Exploration Rovers and Philae comet lander, as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of planet Mars. APXS was also used in the Surveyor 5\u20137 moon probes but with a 242Cm source.\nAn elaborated APXS setup is equipped with a sensor head containing six curium sources having the total radioactive decay rate of several tens of millicuries (roughly a gigabecquerel). The sources are collimated on the sample, and the energy spectra of the alpha particles and protons scattered from the sample are analyzed (the proton analysis is implemented only in some spectrometers). These spectra contain quantitative information on all major elements in the samples except for hydrogen, helium and lithium.\nSafety.\nOwing to its high radioactivity, curium and its compounds must be handled in appropriate laboratories under special arrangements. Whereas curium itself mostly emits \u03b1-particles which are absorbed by thin layers of common materials, some of its decay products emit significant fractions of beta and gamma radiation, which require a more elaborate protection. If consumed, curium is excreted within a few days and only 0.05% is absorbed in the blood. From there, about 45% goes to the liver, 45% to the bones, and the remaining 10% is excreted. In the bone, curium accumulates on the inside of the interfaces to the bone marrow and does not significantly redistribute with time; its radiation destroys bone marrow and thus stops red blood cell creation. The biological half-life of curium is about 20 years in the liver and 50 years in the bones. Curium is absorbed in the body much more strongly via inhalation, and the allowed total dose of 244Cm in soluble form is 0.3 \u03bcC. Intravenous injection of 242Cm and 244Cm containing solutions to rats increased the incidence of bone tumor, and inhalation promoted pulmonary and liver cancer.\nCurium isotopes are inevitably present in spent nuclear fuel with a concentration of about 20 g/tonne. Among them, the 245Cm\u2013248Cm isotopes have decay times of thousands of years and need to be removed to neutralize the fuel for disposal. The associated procedure involves several steps, where curium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure, nuclear transmutation, while well documented for other elements, is still being developed for curium."}
{"id": "5676", "revid": "6277327", "url": "https://en.wikipedia.org/wiki?curid=5676", "title": "Californium", "text": "Californium is a radioactive chemical element with the symbol Cf and atomic number 98. The element was first synthesized in 1950 at the Lawrence Berkeley National Laboratory (then the University of California Radiation Laboratory), by bombarding curium with alpha particles (helium-4 ions). It is an actinide element, the sixth transuranium element to be synthesized, and has the second-highest atomic mass of all the elements that have been produced in amounts large enough to see with the unaided eye (after einsteinium). The element was named after the university and the U.S. state of California.\nTwo crystalline forms exist for californium under normal pressure: one above and one below . A third form exists at high pressure. Californium slowly tarnishes in air at room temperature. Compounds of californium are dominated by the +3 oxidation state. The most stable of californium's twenty known isotopes is californium-251, which has a half-life of 898 years. This short half-life means the element is not found in significant quantities in the Earth's crust. Californium-252, with a half-life of about 2.645 years, is the most common isotope used and is produced at the Oak Ridge National Laboratory in the United States and the Research Institute of Atomic Reactors in Russia.\nCalifornium is one of the few transuranium elements that have practical applications. Most of these applications exploit the property of certain isotopes of californium to emit neutrons. For example, californium can be used to help start up nuclear reactors, and it is employed as a source of neutrons when studying materials using neutron diffraction and neutron spectroscopy. Californium can also be used in nuclear synthesis of higher mass elements; oganesson (element\u00a0118) was synthesized by bombarding californium-249 atoms with calcium-48 ions. Users of californium must take into account radiological concerns and the element's ability to disrupt the formation of red blood cells by bioaccumulating in skeletal tissue.\nCharacteristics.\nPhysical properties.\nCalifornium is a silvery-white actinide metal with a melting point of and an estimated boiling point of . The pure metal is malleable and is easily cut with a razor blade. Californium metal starts to vaporize above when exposed to a vacuum. Below californium metal is either ferromagnetic or ferrimagnetic (it acts like a magnet), between 48 and 66\u00a0K it is antiferromagnetic (an intermediate state), and above it is paramagnetic (external magnetic fields can make it magnetic). It forms alloys with lanthanide metals but little is known about the resulting materials.\nThe element has two crystalline forms at standard atmospheric pressure: a double-hexagonal close-packed form dubbed alpha (\u03b1) and a face-centered cubic form designated beta (\u03b2). The \u03b1 form exists below 600\u2013800\u00a0\u00b0C with a density of 15.10\u00a0g/cm3 and the \u03b2 form exists above 600\u2013800\u00a0\u00b0C with a density of 8.74\u00a0g/cm3. At 48\u00a0GPa of pressure the \u03b2 form changes into an orthorhombic crystal system due to delocalization of the atom's 5f electrons, which frees them to bond.\nThe bulk modulus of a material is a measure of its resistance to uniform pressure. Californium's bulk modulus is , which is similar to trivalent lanthanide metals but smaller than more familiar metals, such as aluminium (70\u00a0GPa).\nChemical properties and compounds.\nCalifornium exhibits oxidation states of 4, 3, or 2. It typically forms eight or nine bonds to surrounding atoms or ions. Its chemical properties are predicted to be similar to other primarily 3+ valence actinide elements and the element dysprosium, which is the lanthanide above californium in the periodic table. Compounds in the +4 oxidation state are strong oxidizing agents and those in the +2 state are strong reducing agents.\nThe element slowly tarnishes in air at room temperature, with the rate increasing when moisture is added. Californium reacts when heated with hydrogen, nitrogen, or a chalcogen (oxygen family element); reactions with dry hydrogen and aqueous mineral acids are rapid. \nCalifornium is only water-soluble as the californium(III) cation. Attempts to reduce or oxidize the +3 ion in solution have failed. The element forms a water-soluble chloride, nitrate, perchlorate, and sulfate and is precipitated as a fluoride, oxalate, or hydroxide. Californium is the heaviest actinide to exhibit covalent properties, as is observed in the californium borate.\nIsotopes.\nTwenty radioisotopes of californium have been characterized, the most stable being californium-251 with a half-life of 898 years, californium-249 with a half-life of 351 years, californium-250 with a half-life of 13.08 years, and californium-252 with a half-life of 2.645 years. All the remaining isotopes have half-lives shorter than a year, and the majority of these have half-lives shorter than 20 minutes. The isotopes of californium range in mass number from 237 to 256.\nCalifornium-249 is formed from the beta decay of berkelium-249, and most other californium isotopes are made by subjecting berkelium to intense neutron radiation in a nuclear reactor. Although californium-251 has the longest half-life, its production yield is only 10% due to its tendency to collect neutrons (high neutron capture) and its tendency to interact with other particles (high neutron cross-section).\nCalifornium-252 is a very strong neutron emitter, which makes it extremely radioactive and harmful. Californium-252 undergoes alpha decay 96.9% of the time to form curium-248 while the remaining 3.1% of decays are spontaneous fission. One microgram (\u03bcg) of californium-252 emits 2.3\u00a0million neutrons per second, an average of 3.7 neutrons per spontaneous fission. Most of the other isotopes of californium decay to isotopes of curium (atomic number 96) via alpha decay.\nHistory.\nCalifornium was first synthesized at the University of California Radiation Laboratory in Berkeley, by the physics researchers Stanley G. Thompson, Kenneth Street, Jr., Albert Ghiorso, and Glenn T. Seaborg on or about February 9, 1950. It was the sixth transuranium element to be discovered; the team announced its discovery on March 17, 1950.\nTo produce californium, a microgram-sized target of curium-242 () was bombarded with 35\u00a0MeV-alpha particles () in the cyclotron at Berkeley, which produced californium-245 () plus one free neutron ().\nTo identify and separate out the element, ion exchange and adsorsion methods were undertaken. Only about 5,000 atoms of californium were produced in this experiment, and these atoms had a half-life of 44 minutes.\nThe discoverers named the new element after the university and the state. This was a break from the convention used for elements 95 to 97, which drew inspiration from how the elements directly above them in the periodic table were named. However, the element directly above element\u00a098 in the periodic table, dysprosium, has a name that simply means \"hard to get at\" so the researchers decided to set aside the informal naming convention. They added that \"the best we can do is to point out [that] ... searchers a century ago found it difficult to get to California.\"\nWeighable quantities of californium were first produced by the irradiation of plutonium targets at the Materials Testing Reactor at the National Reactor Testing Station in eastern Idaho; and these findings were reported in 1954. The high spontaneous fission rate of californium-252 was observed in these samples. The first experiment with californium in concentrated form occurred in 1958. The isotopes californium-249 to californium-252 were isolated that same year from a sample of plutonium-239 that had been irradiated with neutrons in a nuclear reactor for five years. Two years later, in 1960, Burris Cunningham and James Wallman of the Lawrence Radiation Laboratory of the University of California created the first californium compounds\u2014californium trichloride, californium oxychloride, and californium oxide\u2014by treating californium with steam and hydrochloric acid.\nThe High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, started producing small batches of californium in the 1960s. By 1995, the HFIR nominally produced of californium annually. Plutonium supplied by the United Kingdom to the United States under the 1958 US-UK Mutual Defence Agreement was used for californium production.\nThe Atomic Energy Commission sold californium-252 to industrial and academic customers in the early 1970s for $10 per microgram and an average of of californium-252 were shipped each year from 1970 to 1990. Californium metal was first prepared in 1974 by Haire and Baybarz who reduced californium(III) oxide with lanthanum metal to obtain microgram amounts of sub-micrometer thick films.\nOccurrence.\nTraces of californium can be found near facilities that use the element in mineral prospecting and in medical treatments. The element is fairly insoluble in water, but it adheres well to ordinary soil; and concentrations of it in the soil can be 500 times higher than in the water surrounding the soil particles.\nFallout from atmospheric nuclear testing prior to 1980 contributed a small amount of californium to the environment. Californium isotopes with mass numbers 249, 252, 253, and 254 have been observed in the radioactive dust collected from the air after a nuclear explosion. Californium is not a major radionuclide at United States Department of Energy legacy sites since it was not produced in large quantities.\nCalifornium was once believed to be produced in supernovas, as their decay matches the 60-day half-life of 254Cf. However, subsequent studies failed to demonstrate any californium spectra, and supernova light curves are now thought to follow the decay of nickel-56.\nThe transuranic elements from americium to fermium, including californium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\nProduction.\nCalifornium is produced in nuclear reactors and particle accelerators. Californium-250 is made by bombarding berkelium-249 () with neutrons, forming berkelium-250 () via neutron capture (n,\u03b3) which, in turn, quickly beta decays (\u03b2\u2212) to californium-250 () in the following reaction:\nBombardment of californium-250 with neutrons produces californium-251 and californium-252.\nProlonged irradiation of americium, curium, and plutonium with neutrons produces milligram amounts of californium-252 and microgram amounts of californium-249. As of 2006, curium isotopes 244 to 248 are irradiated by neutrons in special reactors to produce primarily californium-252 with lesser amounts of isotopes 249 to 255.\nMicrogram quantities of californium-252 are available for commercial use through the U.S. Nuclear Regulatory Commission. Only two sites produce californium-252: the Oak Ridge National Laboratory in the United States, and the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. As of 2003, the two sites produce 0.25 grams and 0.025 grams of californium-252 per year, respectively.\nThree californium isotopes with significant half-lives are produced, requiring a total of 15 neutron captures by uranium-238 without nuclear fission or alpha decay occurring during the process. Californium-253 is at the end of a production chain that starts with uranium-238, includes several isotopes of plutonium, americium, curium, berkelium, and the californium isotopes 249 to 253 (see diagram).\nApplications.\nCalifornium-252 has a number of specialized applications as a strong neutron emitter, and each microgram of fresh californium produces 139\u00a0million neutrons per minute. This property makes californium useful as a neutron startup source for some nuclear reactors and as a portable (non-reactor based) neutron source for neutron activation analysis to detect trace amounts of elements in samples. Neutrons from californium are employed as a treatment of certain cervical and brain cancers where other radiation therapy is ineffective. It has been used in educational applications since 1969 when the Georgia Institute of Technology received a loan of 119\u00a0\u03bcg of californium-252 from the Savannah River Plant. It is also used with online elemental coal analyzers and bulk material analyzers in the coal and cement industries.\nNeutron penetration into materials makes californium useful in detection instruments such as fuel rod scanners; neutron radiography of aircraft and weapons components to detect corrosion, bad welds, cracks and trapped moisture; and in portable metal detectors. Neutron moisture gauges use californium-252 to find water and petroleum layers in oil wells, as a portable neutron source for gold and silver prospecting for on-the-spot analysis, and to detect ground water movement. The major uses of californium-252 in 1982 were, in order of use, reactor start-up (48.3%), fuel rod scanning (25.3%), and activation analysis (19.4%). By 1994, most californium-252 was used in neutron radiography (77.4%), with fuel rod scanning (12.1%) and reactor start-up (6.9%) as important but distant secondary uses.\nCalifornium-251 has a very small calculated critical mass of about , high lethality, and a relatively short period of toxic environmental irradiation. The low critical mass of californium led to some exaggerated claims about possible uses for the element.\nIn October 2006, researchers announced that three atoms of oganesson (element 118) had been identified at the Joint Institute for Nuclear Research in Dubna, Russia, as the product of bombardment of californium-249 with calcium-48, making it the heaviest element ever synthesized. The target for this experiment contained about 10\u00a0mg of californium-249 deposited on a titanium foil of 32\u00a0cm2 area. Californium has also been used to produce other transuranium elements; for example, element 103 (later named lawrencium) was first synthesized in 1961 by bombarding californium with boron nuclei.\nPrecautions.\nCalifornium that bioaccumulates in skeletal tissue releases radiation that disrupts the body's ability to form red blood cells. The element plays no natural biological role in any organism due to its intense radioactivity and low concentration in the environment.\nCalifornium can enter the body from ingesting contaminated food or drinks or by breathing air with suspended particles of the element. Once in the body, only 0.05% of the californium will reach the bloodstream. About 65% of that californium will be deposited in the skeleton, 25% in the liver, and the rest in other organs, or excreted, mainly in urine. Half of the californium deposited in the skeleton and liver are gone in 50 and 20 years, respectively. Californium in the skeleton adheres to bone surfaces before slowly migrating throughout the bone.\nThe element is most dangerous if taken into the body. In addition, californium-249 and californium-251 can cause tissue damage externally, through gamma ray emission. Ionizing radiation emitted by californium on bone and in the liver can cause cancer."}
{"id": "5677", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=5677", "title": "Cerebral Spinal Fluid", "text": ""}
{"id": "5679", "revid": "5515781", "url": "https://en.wikipedia.org/wiki?curid=5679", "title": "Christian Social Union in Bavaria", "text": "The Christian Social Union in Bavaria (, CSU) is a Christian-democratic and conservative political party in Germany. Having a regionalist identity, the CSU operates only in Bavaria while its larger counterpart, the Christian Democratic Union (CDU), operates in the other fifteen states of Germany. It differs from the CDU by being somewhat more conservative in social matters, following the Catholic social teaching. The CSU is considered the \"de facto\" successor of the Weimar-era Catholic Bavarian People's Party.\nAt the federal level, the CSU forms a common faction in the Bundestag with the CDU which is frequently referred to as the Union Faction (\"die Unionsfraktion\") or simply CDU/CSU. The CSU has had 46 seats in the Bundestag since the 2017 federal election, making it currently the smallest of the seven parties represented. The CSU is a member of the European People's Party and the International Democrat Union.\nThe CSU currently has three ministers in the cabinet of Germany of the federal government in Berlin, including former party leader Horst Seehofer who is Federal Minister of the Interior while party leader Markus S\u00f6der serves as Minister-President of Bavaria, a position that CSU representatives have held from 1946 to 1954 and again since 1957.\nHistory.\nFranz Josef Strau\u00df (1915\u20131988) had left behind the strongest legacy as a leader of the party, having led the party from 1961 until his death in 1988. His political career in the federal cabinet was unique in that he had served four ministerial posts in the years between 1953 and 1969. From 1978 until his death in 1988, Strau\u00df served as the Minister-President of Bavaria. Strau\u00df was the first leader of the CSU to be a candidate for the German chancellery in 1980. In the 1980 federal election, Strau\u00df ran against the incumbent Helmut Schmidt of the Social Democratic Party of Germany (SPD), but lost thereafter as the SPD and the Free Democratic Party (FDP) managed to secure an absolute majority together, forming a social-liberal coalition.\nThe CSU has led the Bavarian state government since it came into existence in 1946, save from 1954 to 1957 when the SPD formed a state government in coalition with the Bavaria Party and the state branches of the GB/BHE and FDP.\nInitially, the separatist Bavaria Party (BP) successfully competed for the same electorate as the CSU, as both parties saw and presented themselves as successors to the BVP. The CSU was ultimately able to win this power struggle for itself. Among other things, the BP was involved in the \"casino affair\" under dubious circumstances by the CSU at the end of the 1950s and lost considerable prestige and votes. In the 1966 state election, the BP finally left the state parliament.\nBefore the 2008 elections in Bavaria, the CSU perennially achieved absolute majorities at the state level by itself. This level of dominance is unique among Germany's 16 states. Edmund Stoiber took over the CSU leadership in 1999. He ran for Chancellor of Germany in 2002, but his preferred CDU/CSU\u2013FDP coalition lost against the SPD candidate Gerhard Schr\u00f6der's SPD\u2013Green alliance.\nIn the 2003 Bavarian state election, the CSU won 60.7% of the vote and 124 of 180 seats in the state parliament. This was the first time any party had won a two-thirds majority in a German state parliament. \"The Economist\" later suggested that this exceptional result was due to a backlash against Schr\u00f6der's government in Berlin. The CSU's popularity declined in subsequent years. Stoiber stepped down from the posts of Minister-President and CSU chairman in September 2007. A year later, the CSU lost its majority in the 2008 Bavarian state election, with its vote share dropping from 60.7% to 43.4%. The CSU remained in power by forming a coalition with the FDP. In the 2009 general election, the CSU received only 42.5% of the vote in Bavaria in the 2009 election, which constitutes its weakest showing in the party's history.\nThe CSU made gains in the 2013 Bavarian state election and the 2013 federal election, which were held a week apart in September 2013. The CSU regained their majority in the Bavarian Landtag and remained in government in Berlin. They have three ministers in Angela Merkel's current cabinet, namely Horst Seehofer (Minister of the Interior, Building and Community), Andreas Scheuer (Minister of Transport and Digital Infrastructure) and Gerd M\u00fcller (Minister for Economic Cooperation and Development).\nAfter the 2018 Bavarian state election on 14 October 2018, the CSU formed a new government with partner Free Voters of Bavaria.\nRelationship with the CDU.\nThe CSU is the sister party of the Christian Democratic Union (CDU). Together, they are called The Union. The CSU operates only within Bavaria and the CDU operates in all other states, but not Bavaria. While virtually independent, at the federal level the parties form a common CDU/CSU faction. No Chancellor has ever come from the CSU, although Strau\u00df and Edmund Stoiber were CDU/CSU candidates for Chancellor in the 1980 federal election and the 2002 federal election, respectively, which were both won by the Social Democratic Party of Germany (SPD). Below the federal level, the parties are entirely independent.\nSince its formation, the CSU has been more conservative than the CDU. The CSU and the state of Bavaria decided not to sign the \"Grundgesetz\" of the Federal Republic of Germany as they could not agree with the division of Germany into two states after World War II. Although Bavaria like all German states has a separate police and justice system (distinctive and non-federal), the CSU has actively participated in all political affairs of the German Parliament, the German government, the German Bundesrat, the parliamentary elections of the German President, the European Parliament and meetings with Mikhail Gorbachev in Russia.\nLike the CDU, the CSU is pro-European, although some Eurosceptic tendencies were shown in the past.\nLeaders.\nMinisters-President.\nThe CSU has contributed eleven of the twelve Ministers-President of Bavaria since 1945, with only Wilhelm Hoegner (1945\u20131946, 1954\u20131957) of the SPD also holding the office."}
{"id": "5680", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5680", "title": "CEO", "text": ""}
{"id": "5681", "revid": "12120664", "url": "https://en.wikipedia.org/wiki?curid=5681", "title": "Corporate title", "text": "Corporate titles or business titles are given to company and organization officials to show what duties and responsibilities they have in the organization. Such titles are used by publicly and privately held for-profit corporations. In addition, many non-profit organizations, educational institutions, partnerships, and sole proprietorships also confer corporate titles.\nVariations.\nThere are considerable variations in the composition and responsibilities of corporate titles.\nWithin the corporate office or corporate center of a company, some companies have a chairman and chief executive officer (CEO) as the top-ranking executive, while the number two is the president and chief operating officer (COO); other companies have a president and CEO but no official deputy. Typically, senior managers are \"higher\" than vice presidents, although many times a senior officer may also hold a vice president title, such as executive vice president and chief financial officer (CFO). The board of directors is technically not part of management itself, although its chairman may be considered part of the corporate office if he or she is an executive chairman.\nA corporation often consists of different businesses, whose senior executives report directly to the CEO or COO, but depends on the form of the business. If organized as a division then the top manager is often known as an executive vice president (EVP). If that business is a subsidiary which has considerably more independence, then the title might be chairman and CEO.\nIn many countries, particularly in Europe and Asia, there is a separate executive board for day-to-day business and supervisory board (elected by shareholders) for control purposes. In these countries, the CEO presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board. This seemingly allows for clear lines of authority. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service.\nIn the United States and other countries that follow a single-board corporate structure, the board of directors (elected by the shareholders) is often equivalent to the European or Asian supervisory board, while the functions of the executive board may be vested either in the board of directors or in a separate committee, which may be called an operating committee (J.P. Morgan Chase), management committee (Goldman Sachs), executive committee (Lehman Brothers), or executive council (Hewlett-Packard), or executive board (HeiG) composed of the division/subsidiary heads and senior officers that report directly to the CEO.\nUnited States.\nState laws in the United States traditionally required certain positions to be created within every corporation, such as president, secretary and treasurer. Today, the approach under the \"Model Business Corporation Act\", which is employed in many states, is to grant companies discretion in determining which titles to have, with the only mandated organ being the board of directors.\nSome states that do not employ the MBCA continue to require that certain offices be established. Under the law of Delaware, where most large US corporations are established, stock certificates must be signed by two officers with titles specified by law (e.g. a president and secretary or a president and treasurer). Every corporation incorporated in California must have a chairman of the board or a president (or both), as well as a secretary and a chief financial officer.\nLimited liability company (LLC)-structured companies are generally run directly by their members, but the members can agree to appoint officers such as a CEO or to appoint \"managers\" to operate the company.\nAmerican companies are generally led by a CEO. In some companies, the CEO also has the title of \"president\". In other companies, a president is a different person, and the primary duties of the two positions are defined in the company's bylaws (or the laws of the governing legal jurisdiction). Many companies also have a CFO, a chief operating officer (COO) and other senior positions such as chief information officer (CIO), chief business officer (CBO), chief marketing officer (CMO), etc. that report to the president and CEO as \"senior vice presidents\" of the company. The next level, which are not executive positions, is middle management and may be called \"vice presidents\", \"directors\" or \"managers\", depending on the size and required managerial depth of the company.\nUnited Kingdom.\nIn British English, the title of managing director is generally synonymous with that of chief executive officer. Managing directors do not have any particular authority under the \"Companies Act\" in the UK, but do have implied authority based on the general understanding of what their position entails, as well as any authority expressly delegated by the board of directors.\nJapan and South Korea.\nIn Japan, corporate titles are roughly standardized across companies and organizations; although there is variation from company to company, corporate titles within a company are always consistent, and the large companies in Japan generally follow the same outline. These titles are the formal titles that are used on business cards. Korean corporate titles are similar to those of Japan.\nLegally, Japanese and Korean companies are only required to have a board of directors with at least one representative director. In Japanese, a company director is called a \"torishimariyaku\" (\u53d6\u7de0\u5f79) and a representative director is called a \"daihy\u014d torishimariyaku\" (\u4ee3\u8868\u53d6\u7de0\u5f79). The equivalent Korean titles are \"isa\" (\uc774\uc0ac, \u7406\u4e8b) and \"daepyo-isa\" (\ub300\ud45c\uc774\uc0ac, \u4ee3\u8868\u7406\u4e8b). These titles are often combined with lower titles, e.g. \"senmu torishimariyaku\" or \"j\u014dmu torishimariyaku\" for Japanese executives who are also board members. Most Japanese companies also have statutory auditors, who operate alongside the board of directors in supervisory roles.\nThe typical structure of executive titles in large companies includes the following:\nThe top management group, comprising \"jomu\"/\"sangmu\" and above, is often referred to collectively as \"senior management\" (\u5e79\u90e8 or \u91cd\u5f79; \"kambu\" or \"juyaku\" in Japanese; \"ganbu\" or \"jungy\u014fk\" in Korean).\nSome Japanese and Korean companies have also adopted American-style titles, but these are not yet widespread and their usage varies. For example, although there is a Korean translation for chief operating officer (\"\ucd5c\uace0\uc6b4\uc601\ucc45\uc784\uc790, choego uny\u014fng chaegimja\"), not many companies have yet adopted it with an exception of a few multi-national companies such as Samsung and CJ (a spin-off from Samsung), while the CFO title is often used alongside other titles such as \"bu-sajang\" (SEVP) or \"J\u014fnmu\" (EVP).\nSince the late 1990s, many Japanese companies have introduced the title of \"shikk\u014d yakuin\" (\u57f7\u884c\u5f79\u54e1) or \"officer\", seeking to emulate the separation of directors and officers found in American companies. In 2002, the statutory title of \"shikk\u014d yaku\" (\u57f7\u884c\u5f79) was introduced for use in companies that introduced a three-committee structure in their board of directors. The titles are frequently given to \"buch\u014d\" and higher-level personnel. Although the two titles are very similar in intent and usage, there are several legal distinctions: \"shikk\u014d yaku\" make their own decisions in the course of performing work delegated to them by the board of directors, and are considered managers of the company rather than employees, with a legal status similar to that of directors. \"Shikk\u014d yakuin\" are considered employees of the company that follow the decisions of the board of directors, although in some cases directors may have the \"shikk\u014d yakuin\" title as well.\nSenior management.\nThe highest-level executives in senior management usually have titles beginning with \"chief\" and ending with \"officer\", forming what is often called the \"C-suite\" or \"CxO\", where \"x\" is a variable that could be any functional area; not to be confused with CXO. The traditional three such officers are CEO, COO, and CFO. Depending on the management structure, titles may exist instead of, or be blended/overlapped with, other traditional executive titles, such as \"president\", various designations of \"vice presidents\" (e.g. VP of marketing), and \"general managers\" or \"directors\" of various divisions (such as director of marketing); the latter may or may not imply membership of the \"board of directors\".\nCertain other prominent positions have emerged, some of which are sector-specific. For example, chief audit executive (CAE), chief procurement officer (CPO) and chief risk officer (CRO) positions are often found in many types of financial services companies. Technology companies of all sorts now tend to have a chief technology officer (CTO) to manage technology development. A CIO oversees information technology (IT) matters, either in companies that specialize in IT or in any kind of company that relies on it for supporting infrastructure.\nMany companies now also have a chief marketing officer (CMO), particularly mature companies in competitive sectors, where brand management is a high priority. A chief value officer (CVO) is introduced in companies where business processes and organizational entities are focused on the creation and maximization of value. A chief administrative officer may be found in many large complex organizations that have various departments or divisions. Additionally, many companies now call their top diversity leadership position the chief diversity officer (CDO). However, this and many other nontraditional and lower-ranking titles are not universally recognized as corporate officers, and they tend to be specific to particular organizational cultures or the preferences of employees.\nSpecific corporate officer positions.\nChairman of the board \u2013 presiding officer of the corporate board of directors. The chairman influences the board of directors, which in turn elects and removes the officers of a corporation and oversees the human, financial, environmental and technical operations of a corporation."}
{"id": "5683", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=5683", "title": "Computer expo", "text": ""}
{"id": "5685", "revid": "38612296", "url": "https://en.wikipedia.org/wiki?curid=5685", "title": "Cambridge, Massachusetts", "text": "Cambridge ( ) is a city in Middlesex County, Massachusetts, and part of the Boston metropolitan area as a major suburb of Boston. , it was the fifth most populous city in the state, behind Boston, Worcester, Springfield, and Lowell. According to the 2010 Census, the city's population was 105,162. It is one of two de jure county seats of Middlesex County, although the county's government was abolished in 1997. Situated directly north of Boston, across the Charles River, it was named in honor of the University of Cambridge in England, once also an important center of the Puritan theology embraced by the town's founders.\nHarvard University, the Massachusetts Institute of Technology (MIT), Lesley University, and Hult International Business School are in Cambridge, as was Radcliffe College before it merged with Harvard. Kendall Square in Cambridge has been called \"the most innovative square mile on the planet\" owing to the high concentration of successful startups that have emerged in the vicinity of the square since 2010.\nHistory.\nIn December 1630, the site of what would become Cambridge was chosen because it was safely upriver from Boston Harbor, making it easily defensible from attacks by enemy ships. Thomas Dudley, his daughter Anne Bradstreet, and her husband Simon Willard were the town's founders. The first houses were built in the spring of 1631. The settlement was initially referred to as \"the newe towne\". Official Massachusetts records show the name rendered as Newe Towne by 1632, and as Newtowne by 1638.\nLocated at the first convenient Charles River crossing west of Boston, Newtowne was one of several towns (including Boston, Dorchester, Watertown, and Weymouth) founded by the 700 original Puritan colonists of the Massachusetts Bay Colony under Governor John Winthrop. Its first preacher was Thomas Hooker, who led many of its original inhabitants west in 1636 to found Hartford and the Connecticut Colony; before leaving, they sold their plots to more recent immigrants from England. The original village site is now within Harvard Square. The marketplace where farmers sold crops from surrounding towns at the edge of a salt marsh (since filled) remains within a small park at the corner of John F. Kennedy and Winthrop Streets.\nIn 1636, the Newe College (later renamed Harvard College after benefactor John Harvard) was founded by the Massachusetts Bay Colony to train ministers. According to Cotton Mather, Newtowne was chosen for the site of the college by the Great and General Court (the Massachusetts legislature) primarily for its proximity to the popular and highly respected Puritan preacher Thomas Shepard. In May 1638, the settlement's name was changed to Cambridge in honor of the university in Cambridge, England.\nThe town comprised a much larger area than the present city, with various outlying parts becoming independent towns over the years: Cambridge Village (later Newtown and now Newton) in 1688, Cambridge Farms (now Lexington) in 1712 or 1713, and Little or South Cambridge (now Brighton) and Menotomy or West Cambridge (now Arlington) in 1807. In the late 19th century, various schemes for annexing Cambridge to Boston were pursued and rejected.\nNewtowne's ministers, Hooker and Shepard, the college's first president, the college's major benefactor, and the first schoolmaster Nathaniel Eaton were all Cambridge alumni, as was the colony's governor John Winthrop. In 1629, Winthrop had led the signing of the founding document of the city of Boston, which was known as the Cambridge Agreement, after the university. In 1650, Governor Thomas Dudley signed the charter creating the corporation that still governs Harvard College.\nCambridge grew slowly as an agricultural village by road from Boston, the colony's capital. By the American Revolution, most residents lived near the Common and Harvard College, with most of the town comprising farms and estates. Most inhabitants were descendants of the original Puritan colonists, but there was also a small elite of Anglican \"worthies\" who were not involved in village life, made their livings from estates, investments, and trade, and lived in mansions along \"the Road to Watertown\" (today's Brattle Street, still known as Tory Row).\nComing north from Virginia, George Washington took command of the volunteer American soldiers camped on Cambridge Common on July 3, 1775, now reckoned the birthplace of the U.S. Army. Most of the Tory estates were confiscated after the Revolution. On January 24, 1776, Henry Knox arrived with artillery captured from Fort Ticonderoga, which enabled Washington to drive the British army out of Boston.\nBetween 1790 and 1840, Cambridge grew rapidly, with the construction of the West Boston Bridge in 1792 connecting Cambridge directly to Boston, so that it was no longer necessary to travel through the Boston Neck, Roxbury, and Brookline to cross the Charles River. A second bridge, the Canal Bridge, opened in 1809 alongside the new Middlesex Canal. The new bridges and roads made what were formerly estates and marshland into prime industrial and residential districts.\nIn the mid-19th century, Cambridge was the center of a literary revolution. It was home to some of the famous Fireside Poets\u2014so called because their poems would often be read aloud by families in front of their evening fires. The Fireside Poets\u2014Henry Wadsworth Longfellow, James Russell Lowell, and Oliver Wendell Holmes\u2014were highly popular and influential in their day.\nSoon after, turnpikes were built: the Cambridge and Concord Turnpike (today's Broadway and Concord Ave.), the Middlesex Turnpike (Hampshire St. and Massachusetts Ave. northwest of Porter Square), and what are today's Cambridge, Main, and Harvard Streets connected various areas of Cambridge to the bridges. In addition, the town was connected to the Boston &amp; Maine Railroad, leading to the development of Porter Square as well as the creation of neighboring Somerville from the formerly rural parts of Charlestown.\nCambridge was incorporated as a city in 1846 despite persistent tensions between East Cambridge, Cambridgeport, and Old Cambridge stemming from differences in culture, sources of income, and the national origins of the residents. The city's commercial center began to shift from Harvard Square to Central Square, which became the city's downtown around that time.\nBetween 1850 and 1900, Cambridge took on much of its present character\u2014streetcar suburban development along the turnpikes, with working-class and industrial neighborhoods focused on East Cambridge, comfortable middle-class housing on the old Cambridgeport and Mid-Cambridge estates, and upper-class enclaves near Harvard University and on the minor hills. The coming of the railroad to North Cambridge and Northwest Cambridge led to three major changes: the development of massive brickyards and brickworks between Massachusetts Ave., Concord Ave. and Alewife Brook; the ice-cutting industry launched by Frederic Tudor on Fresh Pond; and the carving up of the last estates into residential subdivisions to house the thousands of immigrants who arrived to work in the new industries.\nFor many decades, the city's largest employer was the New England Glass Company, founded in 1818. By the middle of the 19th century, it was the world's largest and most modern glassworks. In 1888, Edward Drummond Libbey moved all production to Toledo, Ohio, where it continues today under the name Owens-Illinois. The company's flint glassware with heavy lead content is prized by antique glass collectors. There is none on public display in Cambridge, but the Toledo Museum of Art has a large collection. The Museum of Fine Arts, Boston, and the Sandwich Glass Museum on Cape Cod also have a few pieces.\nIn 1895, Edwin Ginn, founder of Ginn and Company built the Athenaeum Press Building for his publishing textbook empire.\nBy 1920, Cambridge was one of New England's main industrial cities, with nearly 120,000 residents. Among the largest businesses in Cambridge during the period of industrialization was Carter's Ink Company, whose neon sign long adorned the Charles River and which was for many years the world's largest ink manufacturer. Next door was the Athenaeum Press. Confectionery and snack manufacturers in the Cambridgeport-Area 4-Kendall corridor included the Kennedy Biscuit Factory (later part of Nabisco and originator of the Fig Newton), Necco, Squirrel Brands, George Close Company (1861\u20131930s), Page &amp; Shaw, Daggett Chocolate (1892\u20131960s, recipes bought by Necco), Fox Cross Company (1920\u20131980, originator of the Charleston Chew, and now part of Tootsie Roll Industries), Kendall Confectionery Company, and James O. Welch (1927\u20131963, originator of Junior Mints, Sugar Daddies, Sugar Mamas, and Sugar Babies, now part of Tootsie Roll Industries).\nOnly the Cambridge Brands subsidiary of Tootsie Roll Industries remains in town, still manufacturing Junior Mints in the old Welch factory on Main Street. The Blake and Knowles Steam Pump Company (1886), the Kendall Boiler and Tank Company (1880, now in Chelmsford, Massachusetts), and the New England Glass Company (1818\u20131878) were among the industrial manufacturers in what are now Kendall Square and East Cambridge.\nIn 1935, the Cambridge Housing Authority and the Public Works Administration demolished an integrated low-income tenement neighborhood with African Americans and European immigrants, built in its place the whites-only \"Newtowne Court\" public housing development and the adjoining segregated \"Washington Elms\" project for Black people in 1940, and the city required segregation in its other public housing projects as well.\nAs industry in New England began to decline during the Great Depression and after World War II, Cambridge lost much of its industrial base. It also began to become an intellectual, rather than an industrial, center. Harvard University had always been important as both a landowner and an institution, but it began to play a more dominant role in the city's life and culture. When Radcliffe College was established in 1879 the town became a mecca for some of the nation's most academically talented female students. The Massachusetts Institute of Technology's move from Boston in 1916 reinforced Cambridge's status as an intellectual center of the United States.\nAfter the 1950s, the city's population began to decline slowly as families tended to be replaced by single people and young couples. In Cambridge Highlands, the technology company Bolt, Beranek, &amp; Newman produced the first network router in 1969 and hosted the invention of computer-to-computer email in 1971. The 1980s brought a wave of high-technology startups. Those selling advanced minicomputers were overtaken by the microcomputer. Cambridge-based VisiCorp made the first spreadsheet software for personal computers, Visicalc, and helped propel the Apple II to major consumer success. It was overtaken and purchased by Cambridge-based Lotus Development, maker of Lotus 1-2-3. (This was in turn replaced in the market by Microsoft Excel).\nThe city continues to be home to many startups. Kendall Square was a major software hub through the dot-com boom and today hosts offices of such technology companies as Google, Microsoft, and Amazon. The Square also now houses the headquarters of Akamai.\nIn 1976, Harvard's plans to start experiments with recombinant DNA led to a three-month moratorium and a citizen review panel. In the end, Cambridge decided to allow such experiments but passed safety regulations in 1977. This led to regulatory certainty and acceptance when Biogen opened a lab in 1982, in contrast to the hostility that caused the Genetic Institute (a Harvard spinoff) to abandon Somerville and Boston for Cambridge. The biotech and pharmaceutical industries have since thrived in Cambridge, which now includes headquarters for Biogen and Genzyme; laboratories for Novartis, Teva, Takeda, Alnylam, Ironwood, Catabasis, Moderna Therapeutics, Editas Medicine; support companies such as Cytel; and many smaller companies.\nBy the end of the 20th century, Cambridge had one of the most costly housing markets in the Northeastern United States. While considerable class, race, and age diversity persisted, it became harder for those who grew up in the city to afford to stay. The end of rent control in 1994 prompted many Cambridge renters to move to more affordable housing in Somerville and other cities or towns.\nUntil recently, Cambridge's mix of amenities and proximity to Boston kept housing prices relatively stable despite the bursting of the United States housing bubble. Cambridge has been a sanctuary city since 1985 and reaffirmed its status as such in 2006.\nGeography.\nAccording to the United States Census Bureau, Cambridge has a total area of , of which is land and (9.82%) is water.\nAdjacent municipalities.\nCambridge is located in eastern Massachusetts, bordered by:\nThe border between Cambridge and the neighboring city of Somerville passes through densely populated neighborhoods which are connected by the MBTA Red Line. Some of the main squares, Inman, Porter, and to a lesser extent, Harvard and Lechmere, are very close to the city line, as are Somerville's Union and Davis Squares.\nThrough the City of Cambridge's exclusive municipal water system, the city further controls two exclave areas, one being Payson Park Reservoir and Gatehouse, a 2009 listed American Water Landmark located roughly one mile west of Fresh Pond and surrounded by the town of Belmont. The second area is the larger Hobbs Brook and Stony Brook watersheds, which share borders with neighboring towns and cities including Lexington, Lincoln, Waltham and Weston.\nNeighborhoods.\nSquares.\nCambridge has been called the \"City of Squares\", as most of its commercial districts are major street intersections known as squares. Each square acts as a neighborhood center. These include:\nOther neighborhoods.\nCambridge's residential neighborhoods border but are not defined by the squares.\nClimate.\nIn the Koppen-Geiger classification Cambridge has a warm continental summer climate (Dfa) that can appear in the southern end of New England's interior. Abundant rain falls on the city; it has no dry season. The average January temperature is 26.6\u00a0\u00b0F (- 3\u00a0\u00b0C), making Cambridge part of Group D, independent of the isotherm. There are four well-defined seasons.\nDemographics.\nAs of the census of 2010, there were 105,162 people, 44,032 households, and 17,420 families residing in the city. The population density was 16,354.9 people per square mile (6,314.6/km2). There were 47,291 housing units at an average density of 7,354.7 per square mile (2,840.3/km2). The racial makeup of the city was 66.60% White, 11.70% Black or African American, 0.20% Native American, 15.10% Asian (3.7% Chinese, 1.4% Asian Indian, 1.2% Korean, 1.0% Japanese), 0.01% Pacific Islander, 2.10% from other races, and 4.30% from two or more races. 7.60% of the population were Hispanic or Latino of any race (1.6% Puerto Rican, 1.4% Mexican, 0.6% Dominican, 0.5% Colombian &amp; Salvadoran, 0.4% Spaniard). Non-Hispanic Whites were 62.1% of the population in 2010, down from 89.7% in 1970. An individual resident of Cambridge is known as a Cantabrigian.\nIn 2010, there were 44,032 households, out of which 16.9% had children under the age of 18 living with them, 28.9% were married couples living together, 8.4% had a female householder with no husband present, and 60.4% were non-families. 40.7% of all households were made up of individuals, and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.00 and the average family size was 2.76.\nIn the city, the population was spread out, with 13.3% of the population under the age of 18, 21.2% from 18 to 24, 38.6% from 25 to 44, 17.8% from 45 to 64, and 9.2% who were 65 years of age or older. The median age was 30.5 years. For every 100 females, there were 96.1 males. For every 100 females age 18 and over, there were 94.7 males.\nThe median income for a household in the city was $47,979, and the median income for a family was $59,423 (these figures had risen to $58,457 and $79,533 respectively ). Males had a median income of $43,825 versus $38,489 for females. The per capita income for the city was $31,156. About 8.7% of families and 12.9% of the population were below the poverty line, including 15.1% of those under age 18 and 12.9% of those age 65 or over.\nCambridge has been ranked as one of the most liberal cities in America. Locals living in and near the city jokingly refer to it as \"The People's Republic of Cambridge.\" For 2016, the residential property tax rate in Cambridge was $6.99 per $1,000. Cambridge enjoys the highest possible bond credit rating, AAA, with all three Wall Street rating agencies.\nIn 2000, 11.0% of city residents were of Irish ancestry; 7.2% were of English, 6.9% Italian, 5.5% West Indian and 5.3% German ancestry. 69.4% spoke only English at home, while 6.9% spoke Spanish, 3.2% Chinese or Mandarin, 3.0% Portuguese, 2.9% French Creole, 2.3% French, 1.5% Korean, and 1.0% Italian.\nIncome.\nData is from the 2009\u20132013 American Community Survey 5-Year Estimates.\nEconomy.\nManufacturing was an important part of Cambridge's economy in the late 19th and early 20th century, but educational institutions are its biggest employers today. Harvard and MIT together employ about 20,000. As a cradle of technological innovation, Cambridge was home to technology firms Analog Devices, Akamai, Bolt, Beranek, and Newman (BBN Technologies) (now part of Raytheon), General Radio (later GenRad), Lotus Development Corporation (now part of IBM), Polaroid, Symbolics, and Thinking Machines.\nIn 1996, Polaroid, Arthur D. Little, and Lotus were Cambridge's top employers, with over 1,000 employees, but they faded out a few years later. Health care and biotechnology firms such as Genzyme, Biogen Idec, bluebird bio, Millennium Pharmaceuticals, Sanofi, Pfizer and Novartis have significant presences in the city. Though headquartered in Switzerland, Novartis continues to expand its operations in Cambridge.\nOther major biotech and pharmaceutical firms expanding their presence in Cambridge include GlaxoSmithKline, AstraZeneca, Shire, and Pfizer. Most of Cambridge's biotech firms are in Kendall Square and East Cambridge, which decades ago were the city's center of manufacturing. Some others are in University Park at MIT, a new development in another former manufacturing area.\nNone of the high-technology firms that once dominated the economy was among the 25 largest employers in 2005, but by 2008 Akamai and ITA Software were. Google, IBM Research, Microsoft Research, and Philips Research maintain offices in Cambridge. In late January 2012\u2014less than a year after acquiring Billerica-based analytic database management company, Vertica\u2014Hewlett-Packard announced it would also be opening its first offices in Cambridge. Also around that time, e-commerce giants Staples and Amazon.com said they would be opening research and innovation centers in Kendall Square. And LabCentral provides a shared laboratory facility for approximately 25 emerging biotech companies.\nThe proximity of Cambridge's universities has also made the city a center for nonprofit groups and think tanks, including the National Bureau of Economic Research, the Smithsonian Astrophysical Observatory, the Lincoln Institute of Land Policy, Cultural Survival, and One Laptop per Child.\nIn September 2011, the City of Cambridge launched the \"Entrepreneur Walk of Fame\" initiative. The Walk recognizes people who have made contributions to innovation in global business.\nTop employers.\n, the city's ten largest employers are:\nArts and culture.\nPublic art.\nCambridge has a large and varied collection of permanent public art, on both city property (managed by the Cambridge Arts Council) and the Harvard and MIT campuses. Temporary public artworks are displayed as part of the annual Cambridge River Festival on the banks of the Charles River, during winter celebrations in Harvard and Central Squares, and at university campus sites. Experimental forms of public artistic and cultural expression include the Central Square World's Fair, the annual Somerville-based Honk! Festival, and If This House Could Talk, a neighborhood art and history event.\nStreet musicians and other performers entertain tourists and locals in Harvard Square during the warmer months. The performances are coordinated through a public process that has been developed collaboratively by the performers, city administrators, private organizations and business groups. The Cambridge public library contains four Works Progress Administration murals completed in 1935 by Elizabeth Tracy Montminy: \"Religion\", \"Fine Arts\", \"History of Books and Paper\", and \"The Development of the Printing Press\".\nArchitecture.\nDespite intensive urbanization during the late 19th century and the 20th century, Cambridge has several historic buildings, including some from the 17th century. The city also has abundant contemporary architecture, largely built by Harvard and MIT.\nNotable historic buildings in the city include:\nContemporary architecture:\nMusic.\nThe city has an active music scene, from classical performances to the latest popular bands. Beyond its colleges and universities, Cambridge has many music venues, including The Middle East, Club Passim, The Plough and Stars, and the Nameless Coffeehouse.\nParks and recreation.\nConsisting largely of densely built residential space, Cambridge lacks significant tracts of public parkland. Easily accessible open space on the university campuses, including Harvard Yard, the Radcliffe Yard, and MIT's Great Lawn, as well as the considerable open space of Mount Auburn Cemetery and Fresh Pond Reservation, partly compensates for this. At Cambridge's western edge, the cemetery is known as a garden cemetery because of its landscaping (the oldest planned landscape in the country) and arboretum. Although known as a Cambridge landmark, much of the cemetery lies within Watertown. It is also an Important Bird Area (IBA) in the Greater Boston area. Fresh Pond Reservation is the largest open green space in Cambridge with 162 acres (656,000 m\u00b2) of land around a 155-acre (627,000 m\u00b2) kettle hole lake. This land includes a 2.25-mile walking trail around the reservoir and a public 9-hole golf course.\nPublic parkland includes the esplanade along the Charles River, which mirrors its Boston counterpart; Cambridge Common, a busy and historic public park adjacent to Harvard's campus; Danehy Park, formerly a landfill; and the Alewife Brook Reservation.\nGovernment.\nFederal and state representation.\nCambridge is split between Massachusetts's 5th and 7th U.S. congressional districts. The 5th district seat is held by Democrat Katherine Clark, who replaced now-Senator Ed Markey in a 2013 special election; the 7th is represented by Democrat Ayanna Pressley, elected in 2018. The state's senior United States Senator is Democrat Elizabeth Warren, elected in 2012, who lives in Cambridge. The governor of Massachusetts is Republican Charlie Baker, elected in 2014.\nCambridge is represented in six districts in the Massachusetts House of Representatives: the 24th Middlesex (which includes parts of Belmont and Arlington), the 25th and 26th Middlesex (the latter of which includes a portion of Somerville), the 29th Middlesex (which includes a small part of Watertown), and the Eighth and Ninth Suffolk (both including parts of the City of Boston). The city is represented in the Massachusetts Senate as a part of the 2nd Middlesex, Middlesex and Suffolk, and 1st Suffolk and Middlesex districts.\nPolitics.\nFrom 1868\u20131880, Republicans Ulysses S. Grant, Rutherford B. Hayes, and James Garfield each won Cambridge, Grant doing so by margins of over 20 points in both of his campaigns. Following that, from 1884\u20131892, Grover Cleveland won Cambridge in all three of his presidential campaigns, by less than ten points each time.\nThen from 1896\u20131924, Cambridge became something of a \"swing\" city with a slight Republican lean. GOP nominees carried the city in five of the eight presidential elections during that time frame, with five of the elections resulting in either a plurality or a margin of victory of fewer than ten points.\nThe city of Cambridge is extremely Democratic in modern times, however. In the last 23 presidential elections dating back to the nomination of Al Smith in 1928, the Democratic nominee has carried Cambridge in every election. Every Democratic nominee since Massachusetts native John F. Kennedy in 1960 has received at least 70% of the vote, except for Jimmy Carter in 1976 and 1980. Since 1928, the only Republican nominee to come within ten points of carrying Cambridge is Dwight Eisenhower in his 1956 re-election bid.\nCity government.\nCambridge has a city government led by a mayor and a nine-member city council. There is also a six-member school committee that functions alongside the superintendent of public schools. The councilors and school committee members are elected every two years using the single transferable vote (STV) system. Cambridge is one of a small number of local governments in the U.S. to use ranked-choice voting; by 2019, the State of Maine and about 18 local governments have adopted similar systems.\nThe mayor is elected by the city councilors from among themselves and serves as the chair of city council meetings. The mayor also sits on the school committee. The mayor is not the city's chief executive. Rather, the city manager, who is appointed by the city council, serves in that capacity.\nUnder the city's Plan E form of government, the city council does not have the power to appoint or remove city officials who are under the direction of the city manager. The city council and its members are also forbidden from giving orders to any subordinate of the city manager.\nLouis DePasquale is the City Manager, having succeeded Lisa C. Peterson, the Acting City Manager and Cambridge's first woman City Manager, on November 14, 2016. Peterson became Acting City Manager on September 30, 2016, after Richard C. Rossi announced that he would opt out of his contract renewal. Rossi succeeded Robert W. Healy, who retired in June 2013 after 32 years in the position. In recent history, the media has highlighted the salary of the city manager as one of the highest for a Massachusetts civic employee.\n\"* = current mayor\"\n\"** = former mayor\"\nCounty government.\nCambridge was a county seat of Middlesex County, along with Lowell, until the abolition of county government. Though the county government was abolished in 1997, the county still exists as a geographical and political region. The employees of Middlesex County courts, jails, registries, and other county agencies now work directly for the state. The county's registrars of Deeds and Probate remain in Cambridge, but the Superior Court and District Attorney have had their operations transferred to Woburn. Third District Court has shifted operations to Medford, and the county Sheriff's office awaits near-term relocation.\nEducation.\nHigher education.\nCambridge is perhaps best known as an academic and intellectual center. Its colleges and universities include:\nAt least 129 of the world's total 780 Nobel Prize winners have at some point in their careers been affiliated with universities in Cambridge.\nThe American Academy of Arts and Sciences is also based in Cambridge.\nPrimary and secondary public education.\nFive upper schools offer grades 6\u20138 in some of the same buildings as the elementary schools:\nCambridge has three district public high school programs, the principal one being Cambridge Rindge and Latin School (CRLS).\nOther public charter schools include Benjamin Banneker Charter School, which serves grades K\u20136; Community Charter School of Cambridge in Kendall Square, which serves grades 7\u201312; and Prospect Hill Academy, a charter school whose upper school is in Central Square though it is not a part of the Cambridge Public School District.\nPrimary and secondary private education.\nCambridge also has several private schools, including:\nMedia.\nNewspapers.\nCambridge is served by the \"Cambridge Chronicle\", the oldest surviving weekly paper in the United States. Another popular online newspaper is Cambridge Day.\nRadio.\nCambridge is home to the following commercially licensed and student-run radio stations:\nTelevision and broadband.\nCambridge Community Television (CCTV) has served the city since its inception in 1988. CCTV operates Cambridge's public access television facility and three television channels, 8, 9, and 96, on the Cambridge cable system (Comcast). The city has invited tenders from other cable providers, but Comcast remains its only fixed television and broadband utility, though services from American satellite TV providers are available. In October 2014, Cambridge City Manager Richard Rossi appointed a citizen Broadband Task Force to \"examine options to increase competition, reduce pricing, and improve speed, reliability and customer service for both residents and businesses.\"\nInfrastructure.\nWater department.\nCambridge obtains water from Hobbs Brook (in Lincoln and Waltham) and Stony Brook (Waltham and Weston), as well as an emergency connection to the Massachusetts Water Resources Authority. The city owns over of land in other towns that includes these reservoirs and portions of their watershed. Water from these reservoirs flows by gravity through an aqueduct to Fresh Pond in Cambridge. It is then treated in an adjacent plant and pumped uphill to an elevation of above sea level at the Payson Park Reservoir (Belmont). The water is then redistributed downhill via gravity to individual users in the city. A new water treatment plant opened in 2001.\nIn October 2016, the City of Cambridge announced that, owing to drought conditions, they would begin buying water from the MWRA. On January 3, 2017, Cambridge announced that \"As a result of continued rainfall each month since October 2016, we have been able to significantly reduce the need to use MWRA water. We have not purchased any MWRA water since December 12, 2016 and if 'average' rainfall continues this could continue for several months.\"\nTransportation.\nRoad.\nSeveral major roads lead to Cambridge, including Route 2, Route 16, and the McGrath Highway (Route 28). The Massachusetts Turnpike does not pass through Cambridge but provides access by an exit in nearby Allston. Both U.S. Route 1 and Interstate 93 also provide additional access on the eastern end of Cambridge at Leverett Circle in Boston. Route 2A runs the length of the city, chiefly along Massachusetts Avenue. The Charles River forms the southern border of Cambridge and is crossed by 11 bridges connecting Cambridge to Boston, including the Longfellow Bridge and the Harvard Bridge, eight of which are open to motorized road traffic.\nCambridge has an irregular street network because many of the roads date from the colonial era. Contrary to popular belief, the road system did not evolve from longstanding cow-paths. Roads connected various village settlements with each other and nearby towns and were shaped by geographic features, most notably streams, hills, and swampy areas. Today, the major \"squares\" are typically connected by long, mostly straight roads, such as Massachusetts Avenue between Harvard Square and Central Square, or Hampshire Street between Kendall Square and Inman Square.\nMass transit.\nCambridge is served by the MBTA, including the Porter Square Station on the regional Commuter Rail; the Lechmere Station on the Green Line; and the Red Line at Alewife, Porter Square, Harvard Square, Central Square, and Kendall Square/MIT Stations. Alewife Station, the terminus of the Red Line, has a large multi-story parking garage (at a rate of $7 per day ).\nThe Harvard bus tunnel, under Harvard Square, connects to the Red Line underground. This tunnel was originally opened for streetcars in 1912 and served trackless trolleys (trolleybuses) and buses as the routes were converted; four lines of the MBTA trolleybus system continue to use it. The tunnel was partially reconfigured when the Red Line was extended to Alewife in the early 1980s.\nBesides the state-owned transit agency, the city is also served by the Charles River Transportation Management Agency (CRTMA) shuttles which are supported by some of the largest companies operating in the city, in addition to the municipal government itself.\nCycling.\nCambridge has several bike paths, including one along the Charles River, and the Linear Park connecting the Minuteman Bikeway at Alewife with the Somerville Community Path. A connection to Watertown is under construction. Bike parking is common and there are bike lanes on many streets, although concerns have been expressed regarding the suitability of many of the lanes. On several central MIT streets, bike lanes transfer onto the sidewalk. Cambridge bans cycling on certain sections of sidewalk where pedestrian traffic is heavy.\nWhile \"Bicycling Magazine\" in 2006 rated Boston as one of the worst cities in the nation for bicycling, it has given Cambridge honorable mention as one of the best and was called by the magazine \"Boston's Great Hope\". Boston has since then followed the example of Cambridge and made considerable efforts to improve bicycling safety and convenience.\nCambridge has an official bicycle committee. The LivableStreets Alliance, headquartered in Cambridge, is an advocacy group for bicyclists, pedestrians, and walkable neighborhoods.\nWalking.\nWalking is a popular activity in Cambridge. In 2000, among US cities with more than 100,000 residents, Cambridge had the highest percentage of commuters who walked to work. Cambridge's major historic squares have changed into modern walking neighborhoods, including traffic calming features based on the needs of pedestrians rather than of motorists.\nIntercity.\nThe Boston intercity bus and train stations at South Station, Boston, and Logan International Airport in East Boston, are accessible by subway. The Fitchburg Line rail service from Porter Square connects to some western suburbs. Since October 2010, there has also been intercity bus service between Alewife Station (Cambridge) and New York City.\nPolice department.\nIn addition to the Cambridge Police Department, the city is patrolled by the Fifth (Brighton) Barracks of Troop H of the Massachusetts State Police. Owing, however, to proximity, the city also practices functional cooperation with the Fourth (Boston) Barracks of Troop H, as well. The campuses of Harvard and MIT are patrolled by the Harvard University Police Department and MIT Police Department, respectively.\nFire department.\nThe city of Cambridge is protected by the Cambridge Fire Department. Established in 1832, the CFD operates eight engine companies, four ladder companies, one rescue company, and two paramedic squad companies from eight fire stations located throughout the city. The Acting Chief is Gerard Mahoney.\nEmergency medical services (EMS).\nThe city of Cambridge receives emergency medical services from PRO EMS, a privately contracted ambulance service. \nPublic library services.\nFurther educational services are provided at the Cambridge Public Library. The large modern main building was built in 2009, and connects to the restored 1888 Richardson Romanesque building. It was founded as the private Cambridge Athenaeum in 1849 and was acquired by the city in 1858, and became the Dana Library. The 1888 building was a donation of Frederick H. Rindge.\nSister Cities \u2013 Twin Towns.\nCambridge has seven official sister cities with active relationships:\nCambridge has ten additional official sister city relationships that are not active:"}
{"id": "5686", "revid": "1276024", "url": "https://en.wikipedia.org/wiki?curid=5686", "title": "Cambridge (disambiguation)", "text": "Cambridge is a city and the county town of Cambridgeshire, United Kingdom, famous for being the location of the University of Cambridge.\nCambridge may also refer to:"}
{"id": "5687", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5687", "title": "Cambridge University", "text": ""}
{"id": "5688", "revid": "892079", "url": "https://en.wikipedia.org/wiki?curid=5688", "title": "Colin Dexter", "text": "Norman Colin Dexter (29 September 1930 \u2013 21 March 2017) was an English crime writer known for his \"Inspector Morse\" series of novels, which were written between 1975 and 1999 and adapted as an ITV television series, \"Inspector Morse\", from 1987 to 2000. His characters have spawned a sequel series, \"Lewis\", and a prequel series, \"Endeavour\".\nEarly life and career.\nDexter was born in Stamford, Lincolnshire, to Alfred and Dorothy Dexter. He had an elder brother, John, a fellow classicist, who taught Classics at The King's School, Peterborough, and a sister, Avril. Alfred ran a small garage and taxi company from premises in Scotgate, Stamford. Dexter was educated at St. John's Infants School, Bluecoat Junior School, from which he gained a scholarship to Stamford School, a boys' public school, where one of his contemporaries was the England international cricket captain and England international rugby player M. J. K. Smith.\nAfter leaving school, Dexter completed his national service with the Royal Corps of Signals and then read Classics at Christ's College, Cambridge, graduating in 1953 and receiving a master's degree in 1958.\nIn 1954, Dexter began his teaching career in the East Midlands, becoming assistant Classics master at Wyggeston School, Leicester. There he helped the Christian Union school society. However, in 2000 he stated that he shared the same views on politics and religion as Inspector Morse, who was portrayed in the final Morse novel, \"The Remorseful Day\", as an atheist.\nA post at Loughborough Grammar School followed in 1957 before he took up the position of senior Classics teacher at Corby Grammar School, Northamptonshire, in 1959. \nIn 1966, he was forced by the onset of deafness to retire from teaching and took up the post of senior assistant secretary at the University of Oxford Delegacy of Local Examinations (UODLE) in Oxford, a job he held until his retirement in 1988.\nIn November 2008, Dexter featured prominently in the BBC programme \"How to Solve a Cryptic Crossword\" as part of the \"Time Shift\" series, in which he recounted some of the crossword clues solved by Morse.\nWriting career.\nThe initial books written by Dexter were general studies text books. He began writing mysteries in 1972 during a family holiday. \"Last Bus to Woodstock\" was published in 1975 and introduced the character of Inspector Morse, the irascible detective whose penchants for cryptic crosswords, English literature, cask ale, and music by composer Wagner reflect Dexter's own enthusiasms. Dexter's plots used false leads and other red herrings.\nThe success of the 33 two-hour episodes of the ITV television series \"Inspector Morse\", produced between 1987 and 2000, brought further attention to Dexter's writings, featuring both Morse and his assistant Sergeant Robert Lewis. In the manner of Alfred Hitchcock, Dexter made a cameo appearance in almost all episodes. From 2006 to 2016, Morse's assistant featured in a 33-episode ITV series titled \"Lewis\" (\"Inspector Lewis\" in the United States). \nA prequel series, \"Endeavour\", featuring a young Morse and starring Shaun Evans and Roger Allam, began airing on the ITV network in 2012. Dexter was a consultant in the first few years of the programme. As with \"Morse\", Dexter occasionally made cameo appearances in \"Lewis\" and \"Endeavour\". \"Endeavour\" has aired seven series, with the eighth series expected to air in early 2021, taking young Morse's career into 1971.\nPart of the audio in the television episodes includes playing the Morse code for Morse's name. It is part of Dexter's life that his military service was as a Morse code operator in the Royal Corps of Signals. These are false clues for why Dexter named his character; he named his character for his friend Sir Jeremy Morse, a crossword devotee like Dexter.\nAwards and honours.\nDexter received several Crime Writers' Association awards: two Silver Daggers for \"Service of All the Dead\" in 1979 and \"The Dead of Jericho\" in 1981; two Gold Daggers for \"The Wench is Dead\" in 1989 and \"The Way Through the Woods\" in 1992; and a Cartier Diamond Dagger for lifetime achievement in 1997. In 1996, Dexter received a Macavity Award for his short story \"Evans Tries an O-Level\". In 1980, he was elected a member of the by-invitation-only Detection Club. In 2005 Dexter became a Fellow by Special Election of St Cross College, Oxford.\nIn 2000 Dexter was appointed an Officer of the Order of the British Empire for services to literature. In 2001 he was awarded the Freedom of the City of Oxford. In September 2011, the University of Lincoln awarded Dexter an honorary Doctor of Letters degree.\nPersonal life.\nIn 1956 he married Dorothy Cooper They had a daughter, Sally, and a son, Jeremy.\nDeath.\nOn 21 March 2017 Dexter's publisher, Macmillan, said in a statement \"With immense sadness, Macmillan announces the death of Colin Dexter who died peacefully at his home in Oxford this morning.\""}
{"id": "5689", "revid": "541218", "url": "https://en.wikipedia.org/wiki?curid=5689", "title": "College", "text": "A college (Latin: \"collegium\") is an educational institution or a constituent part of one. A college may be a degree-awarding tertiary educational institution, a part of a collegiate or federal university, an institution offering vocational education, or a secondary school.\nIn most of the world, a college may be a high school or secondary school, a college of further education, a training institution that awards trade qualifications, a higher-education provider that does not have university status (often without its own degree-awarding powers), or a constituent part of a university. In the United States, a college offers undergraduate programs; it may be independent or the undergraduate program of a university, it is generally also used as a synonym for a university while in some instances a college may also be a residential college. A \u2014France (see secondary education in France), Belgium, and Switzerland\u2014provides secondary education. However, the Coll\u00e8ge de France is a prestigious advanced research institute in Paris.\nEtymology.\nThe word \"college\" is from the Latin verb \"lego, legere, legi, lectum\", \"to collect, gather together, pick\", plus the preposition \"cum\", \"with\", thus meaning \"selected together\". Thus \"colleagues\" are literally \"persons who have been selected to work together\". In ancient Rome a \"collegium\" was a\n\"body, guild, corporation united in colleagueship; of magistrates, praetors, tribunes, priests, augurs; a political club or trade guild\". Thus a college was a form of corporation or corporate body, an artificial legal person (body/corpus) with its own legal personality, with the capacity to enter into legal contracts, to sue and be sued. In mediaeval England there were colleges of priests, for example in chantry chapels; modern survivals include the Royal College of Surgeons in England (originally the Guild of Surgeons Within the City of London), the College of Arms in London (a body of heralds enforcing heraldic law), an electoral college (to elect representatives), etc., all groups of persons \"selected in common\" to perform a specified function and appointed by a monarch, founder or other person in authority. As for the modern \"college of education\", it was a body created for that purpose, for example Eton College was founded in 1440 by letters patent of King Henry VI for the constitution of a college \"of Fellows, priests, clerks, choristers, poor scholars, and old poor men, with one master or governor\", whose duty it shall be to instruct these scholars and any others who may resort thither from any part of England in the knowledge of letters, and especially of grammar, without payment\".\nOverview.\nHigher education.\nWithin higher education, the term can be used to refer to:\nFurther education.\nA sixth form college or college of further education is an educational institution in England, Wales, Northern Ireland, Belize, the Caribbean, Malta, Norway, Brunei, or Southern Africa, among others, where students aged 16 to 19 typically study for advanced school-level qualifications, such as A-levels, BTEC, HND or its equivalent and the International Baccalaureate Diploma, or school-level qualifications such as GCSEs. In Singapore and India, this is known as a junior college. The municipal government of the city of Paris uses the phrase \"sixth form college\" as the English name for a lyc\u00e9e.\nSecondary education.\nIn some national education systems, secondary schools may be called \"colleges\" or have \"college\" as part of their title.\nIn Australia the term \"college\" is applied to any private or independent (non-government) primary and, especially, secondary school as distinct from a state school. Melbourne Grammar School, Cranbrook School, Sydney and The King's School, Parramatta are considered colleges.\nThere has also been a recent trend to rename or create government secondary schools as \"colleges\". In the state of Victoria, some state high schools are referred to as \"secondary colleges\", although the pre-eminent government secondary school for boys in Melbourne is still named Melbourne High School. In Western Australia, South Australia and the Northern Territory, \"college\" is used in the name of all state high schools built since the late 1990s, and also some older ones. In New South Wales, some high schools, especially multi-campus schools resulting from mergers, are known as \"secondary colleges\". In Queensland some newer schools which accept primary and high school students are styled \"state college\", but state schools offering only secondary education are called \"State High School\". In Tasmania and the Australian Capital Territory, \"college\" refers to the final two years of high school (years 11 and 12), and the institutions which provide this. In this context, \"college\" is a system independent of the other years of high school. Here, the expression is a shorter version of \"matriculation college\".\nIn a number of Canadian cities, many government-run secondary schools are called \"collegiates\" or \"collegiate institutes\" (C.I.), a complicated form of the word \"college\" which avoids the usual \"post-secondary\" connotation. This is because these secondary schools have traditionally focused on academic, rather than vocational, subjects and ability levels (for example, collegiates offered Latin while vocational schools offered technical courses). Some private secondary schools (such as Upper Canada College, Vancouver College) choose to use the word \"college\" in their names nevertheless. Some secondary schools elsewhere in the country, particularly ones within the separate school system, may also use the word \"college\" or \"collegiate\" in their names.\nIn New Zealand the word \"college\" normally refers to a secondary school for ages 13 to 17 and \"college\" appears as part of the name especially of private or integrated schools. \"Colleges\" most frequently appear in the North Island, whereas \"high schools\" are more common in the South Island.\nIn the Netherlands, \"college\" is equivalent to HBO (Higher professional education). It is oriented towards professional training with clear occupational outlook, unlike universities which are scientifically oriented.\nIn South Africa, some secondary schools, especially private schools on the English public school model, have \"college\" in their title. Thus no less than six of South Africa's Elite Seven high schools call themselves \"college\" and fit this description. A typical example of this category would be St John's College.\nPrivate schools that specialize in improving children's marks through intensive focus on examination needs are informally called \"cram-colleges\".\nIn Sri Lanka the word \"college\" (known as \"Vidyalaya\" in \"Sinhala\") normally refers to a secondary school, which usually signifies above the 5th standard. During the British colonial period a limited number of exclusive secondary schools were established based on English public school model (Royal College Colombo, S. Thomas' College, Mount Lavinia, Trinity College, Kandy) these along with several Catholic schools (St. Joseph's College, Colombo, St Anthony's College) traditionally carry their name as colleges. Following the start of free education in 1931 large group of central colleges were established to educate the rural masses. Since Sri Lanka gained Independence in 1948, many schools that have been established have been named as \"college\".\nOther.\nAs well as an educational institution, the term can also refer, following its etymology, to any formal group of colleagues set up under statute or regulation; often under a Royal Charter. Examples are an electoral college, the College of Arms, a college of canons, and the College of Cardinals. Other collegiate bodies include professional associations, particularly in medicine and allied professions. In the UK these include the Royal College of Nursing and the Royal College of Physicians. Examples in the United States include the American College of Physicians, the American College of Surgeons, and the American College of Dentists. An example in Australia is the Royal Australian College of General Practitioners.\nCollege by country.\nAustralia.\nIn Australia a college may be an institution of tertiary education that is smaller than a university, run independently or as part of a university. Following a reform in the 1980s many of the formerly independent colleges now belong to a larger universities.\nReferring to parts of a university, there are \"residential colleges\" which provide residence for students, both undergraduate and postgraduate, called university colleges. These colleges often provide additional tutorial assistance, and some host theological study. Many colleges have strong traditions and rituals, so are a combination of dormitory style accommodation and fraternity or sorority culture.\nMost technical and further education institutions (TAFEs), which offer certificate and diploma vocational courses, are styled \"TAFE colleges\" or \"Colleges of TAFE\".\nSome senior high schools are also referred to as colleges.\nCanada.\nIn Canadian English, the term \"college\" usually refers to a trades school, applied arts/science/technology/business/health school or community college. These are post-secondary institutions granting certificates, diplomas, associate degree, and in some cases bachelor's degrees. In Quebec, the term is seldom used; with the French acronym for public colleges, CEGEP (\"Coll\u00e8ge d'enseignement g\u00e9n\u00e9ral et professionnel\", \"college of general and professional education\") is more commonly used. CEGEP is a collegiate level institutions in Quebec, that a student typically enrols in if they wish to continue onto university in the Quebec education system., or to learn a trade. In Ontario and Alberta, there are also institutions which are designated university colleges, as they only grant undergraduate degrees. This is to differentiate between universities, which have both undergraduate and graduate programs and those that do not.\nIn Canada, there is a strong distinction between \"college\" and \"university\". In conversation, one specifically would say either \"they are going to university\" (i.e., studying for a three- or four-year degree at a university) or \"they are going to college\" (i.e., studying at a technical/career training).\nUsage in a university setting.\nThe term \"college\" also applies to distinct entities that formally act as an affiliated institution of the university, formally referred to as federated college, or affiliated colleges. A university may also formally include several constituent colleges, forming a collegiate university. Examples of collegiate universities in Canada include Trent University, and the University of Toronto. These types of institutions act independently, maintaining their own endowments, and properties. However, they remain either affiliated, or federated with the overarching university, with the overarching university being the institution that formally grants the degrees. For example, Trinity College was once an independent institution, but later became federated with the University of Toronto. Several centralized universities in Canada have mimicked the collegiate university model; although constituent colleges in a centralized university remains under the authority of the central administration. Centralized universities that have adopted the collegiate model to a degree includes the University of British Columbia, with Green College and St. John's College; and the Memorial University of Newfoundland, with Sir Wilfred Grenfell College.\nOccasionally, \"college\" refers to a subject specific faculty within a university that, while distinct, are neither \"federated\" nor \"affiliated\"\u2014College of Education, College of Medicine, College of Dentistry, College of Biological Science among others.\nThe Royal Military College of Canada is a military college which trains officers for the Canadian Armed Forces. The institution is a full-fledged university, with the authority to issue graduate degrees, although it continues to word the term \"college\" in its name. The institution's sister schools, Royal Military College Saint-Jean also uses the term college in its name, although it academic offering is akin to a CEGEP institution in Quebec. A number of post-secondary art schools in Canada formerly used the word \"college\" in their names, despite formally being universities. However, most of these institutions were renamed, or re-branded in the early 21st century, omitting the word \"college\" from its name.\nUsage in secondary education.\nThe word \"college\" continues to be used in the names public separate secondary schools in Ontario. A number of independent schools across Canada also use the word \"college\" in its name.\nPublic secular school boards in Ontario also refer to their secondary schools as \"collegiate institutes\". However, usage of the word \"collegiate institute\" varies between school boards. \"Collegiate institute\" is the predominant name for secondary schools in Lakehead District School Board, and Toronto District School Board, although most school boards in Ontario use \"collegiate institute\" alongside \"high school\", and \"secondary school\" in the names of their institutions. Similarly, secondary schools in Regina, and Saskatoon are referred to as \"Collegiate\".\nChile.\nIn Chile, the term \"college\" is usually used in the name of some bilingual schools, like Santiago College, Saint George's College etc.\nSince 2009 the Pontifical Catholic University of Chile incorporated college as a bachelor's degree, it has a Bachelor of Natural Sciences and Mathematics, a Bachelor of Social Science and a Bachelor of Arts and Humanities. It has the same system as the American universities, it combines majors and minors. And it let the students continue a higher degree in the same university once finished.\nGreece.\nKollegio (in Greek \u039a\u03bf\u03bb\u03bb\u03ad\u03b3\u03b9\u03bf) refers to the Centers of Post-Lyceum Education (in Greek \u039a\u03ad\u03bd\u03c4\u03c1\u03bf \u039c\u03b5\u03c4\u03b1\u03bb\u03c5\u03ba\u03b5\u03b9\u03b1\u03ba\u03ae\u03c2 \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2, abbreviated as KEME), which are principally private and belong to the Greek post-secondary education system. Some of them have links to EU or US higher education institutions or accreditation organizations, such as the NEASC. \"Kollegio\" (or \"Kollegia\" in plural) may also refer to private non-tertiary schools, such as the Athens College.\nHong Kong.\nIn Hong Kong, the term 'college' is used by tertiary institutions as either part of their names or to refer to a constituent part of the university, such as the colleges in the collegiate The Chinese University of Hong Kong; or to a residence hall of a university, such as St. John's College, University of Hong Kong. Many older secondary schools have the term 'college' as part of their names.\nIndia.\nThe modern system of education was heavily influenced by the British starting in 1835.\nIn India, the term \"college\" is commonly reserved for institutions that offer high school diplomas at year 12 (\"Junior College\", similar to American \"high schools\"), and those that offer the bachelor's degree; some colleges, however, offer programmes up to PhD level. Generally, colleges are located in different parts of a state and all of them are affiliated to a regional university. The colleges offer programmes leading to degrees of that university. Colleges may be either Autonomous or non-autonomous. Autonomous Colleges are empowered to establish their own syllabus, and conduct and assess their own examinations; in non-autonomous colleges, examinations are conducted by the university, at the same time for all colleges under its affiliation. There are several hundred universities and each university has affiliated colleges, often a large number.\nThe first liberal arts and sciences college in India was \"Cottayam College\" or the \"Syrian College\",Kerala,in 1815. The First inter linguistic residential education institution in Asia was started at this college. At present it is a Theological seminary which is popularly known as Orthodox Theological Seminary or Old Seminary. After that, CMS College, Kottayam,established in 1817, and the Presidency College, Kolkata, also 1817, initially known as Hindu College. The first college for the study of Christian theology and ecumenical enquiry was Serampore College (1818). The first Missionary institution to impart Western style education in India was the Scottish Church College, Calcutta (1830). The first commerce and economics college in India was Sydenham College, Mumbai (1913).\nIn India a new term has been introduced that is Autonomous Institutes &amp; Colleges. An autonomous Colleges are colleges which\u00a0 need to be affiliated to a certain university. These colleges can conduct their own admission procedure, examination syllabus, fees structure etc. However, at the end of course completion, they cannot issue their own degree or diploma. The final degree or diploma is issued by the affiliated university. You can have a look at other colleges in India. \nAlso, some significant changes can pave way under the NEP (New Education Policy 2020) which may affect the present guidelines for universities and colleges.\nIreland.\nIn Ireland the term \"college\" is normally used to describe an institution of tertiary education. University students often say they attend \"college\" rather than \"university\". Until 1989, no university provided teaching or research directly; they were formally offered by a constituent college of the university.\nThere are number of secondary education institutions that traditionally used the word \"college\" in their names: these are either older, private schools (such as Belvedere College, Gonzaga College, Castleknock College, and St. Michael's College) or what were formerly a particular kind of secondary school. These secondary schools, formerly known as \"technical colleges,\" were renamed \"community colleges,\" but remain secondary schools.\nThe country's only ancient university is the University of Dublin. Created during the reign of Elizabeth I, it is modelled on the collegiate universities of Cambridge and Oxford. However, only one constituent college was ever founded, hence the curious position of Trinity College Dublin today; although both are usually considered one and the same, the university and college are completely distinct corporate entities with separate and parallel governing structures.\nAmong more modern foundations, the National University of Ireland, founded in 1908, consisted of constituent colleges and recognised colleges until 1997. The former are now referred to as constituent universities \u2013 institutions that are essentially universities in their own right. The National University can trace its existence back to 1850 and the creation of the Queen's University of Ireland and the creation of the Catholic University of Ireland in 1854. From 1880, the degree awarding roles of these two universities was taken over by the Royal University of Ireland, which remained until the creation of the National University in 1908 and Queen's University Belfast.\nThe state's two new universities, Dublin City University and University of Limerick, were initially National Institute for Higher Education institutions. These institutions offered university level academic degrees and research from the start of their existence and were awarded university status in 1989 in recognition of this.\nThird level technical education in the state has been carried out in the Institutes of Technology, which were established from the 1970s as Regional Technical Colleges. These institutions have \"delegated authority\" which entitles them to give degrees and diplomas from Quality and Qualifications Ireland (QQI) in their own names.\nA number of private colleges exist such as Dublin Business School, providing undergraduate and postgraduate courses validated by QQI and in some cases by other universities.\nOther types of college include colleges of education, such as the Church of Ireland College of Education. These are specialist institutions, often linked to a university, which provide both undergraduate and postgraduate academic degrees for people who want to train as teachers.\nA number of state-funded further education colleges exist \u2013 which offer vocational education and training in a range of areas from business studies and information and communications technology to sports injury therapy. These courses are usually one, two or less often three years in duration and are validated by QQI at Levels 5 or 6, or for the BTEC Higher National Diploma award, which is a Level 6/7 qualification, validated by Edexcel. There are numerous private colleges (particularly in Dublin and Limerick) which offer both further and higher education qualifications. These degrees and diplomas are often certified by foreign universities/international awarding bodies and are aligned to the National Framework of Qualifications at Levels 6, 7 and 8.\nIsrael.\nIn Israel, any non-university higher-learning facility is called a college. Institutions accredited by the Council for Higher Education in Israel (CHE) to confer a bachelor's degree are called \"Academic Colleges\". These colleges (at least 4 for 2012) may also offer master's degrees and act as Research facilities. There are also over twenty teacher training colleges or seminaries, most of which may award only a Bachelor of Education (BEd) degree.\nMacau.\nFollowing the Portuguese usage, the term \"college\" (\"col\u00e9gio\") in Macau has traditionally been used in the names for private (and non-governmental) pre-university educational institutions, which correspond to form one to form six level tiers. Such schools are usually run by the Roman Catholic church or missionaries in Macau. Examples include Chan Sui Ki Perpetual Help College, Yuet Wah College, and Sacred Heart Canossian College.\nNetherlands.\nIn the Netherlands there are 3 main educational routes after high school.\nHBO graduates can be awarded two titles, which are Baccalaureus (bc.) and Ingenieur (ing.). At a WO institution, many more bachelor's and master's titles can be awarded. Bachelor's degrees: Bachelor of Arts (BA), Bachelor of Science (BSc) and Bachelor of Laws (LLB). Master's degrees: Master of Arts (MA), Master of Laws (LLM) and Master of Science (MSc). The PhD title is a research degree awarded upon completion and defense of a doctoral thesis.\nNew Zealand.\nThe constituent colleges of the former University of New Zealand (such as Canterbury University College) have become independent universities. Some halls of residence associated with New Zealand universities retain the name of \"college\", particularly at the University of Otago (which although brought under the umbrella of the University of New Zealand, already possessed university status and degree awarding powers). The institutions formerly known as \"Teacher-training colleges\" now style themselves \"College of education\".\nSome universities, such as the University of Canterbury, have divided their university into constituent administrative \"Colleges\" \u2013 the College of Arts containing departments that teach Arts, Humanities and Social Sciences, College of Science containing Science departments, and so on. This is largely modelled on the Cambridge model, discussed above.\nLike the United Kingdom some professional bodies in New Zealand style themselves as \"colleges\", for example, the Royal Australasian College of Surgeons, the Royal Australasian College of Physicians.\nIn some parts of the country, secondary school is often referred to as college and the term is used interchangeably with high school. This sometimes confuses people from other parts of New Zealand. But in all parts of the country many secondary schools have \"College\" in their name, such as Rangitoto College, New Zealand's largest secondary.\nPhilippines.\nIn the Philippines, colleges usually refer to institutions of learning that grant degrees but whose scholastic fields are not as diverse as that of a university (University of Santo Tomas, University of the Philippines, Ateneo de Manila University, De La Salle University, Far Eastern University, and AMA University), such as the San Beda College which specializes in law, AMA Computer College whose campuses are spread all over the Philippines which specializes in information and computing technologies, and the Map\u00faa Institute of Technology which specializes in engineering, or to component units within universities that do not grant degrees but rather facilitate the instruction of a particular field, such as a College of Science and College of Engineering, among many other colleges of the University of the Philippines.\nA state college may not have the word \"college\" on its name, but may have several component colleges, or departments. Thus, the Eulogio Amang Rodriguez Institute of Science and Technology is a state college by classification.\nUsually, the term \"college\" is also thought of as a hierarchical demarcation between the term \"university\", and quite a number of colleges seek to be recognized as universities as a sign of improvement in academic standards (Colegio de San Juan de Letran, San Beda College), and increase in the diversity of the offered degree programs (called \"courses\"). For private colleges, this may be done through a survey and evaluation by the Commission on Higher Education and accrediting organizations, as was the case of Urios College which is now the Fr. Saturnino Urios University. For state colleges, it is usually done by a legislation by the Congress or Senate. In common usage, \"going to college\" simply means attending school for an undergraduate degree, whether it's from an institution recognized as a college or a university.\nWhen it comes to referring to the level of education, \"college\" is the term more used to be synonymous to tertiary or higher education. A student who is or has studied his/her undergraduate degree at either an institution with \"college\" or \"university\" in its name is considered to be going to or have gone to \"college\".\nPortugal.\nPresently in Portugal, the term \"col\u00e9gio\" (college) is normally used as a generic reference to a private (non-government) school that provides from basic to secondary education. Many of the private schools include the term \"col\u00e9gio\" in their name. Some special public schools \u2013 usually of the boarding school type \u2013 also include the term in their name, with a notable example being the \"Col\u00e9gio Militar\" (Military College). The term \"col\u00e9gio interno\" (literally \"internal college\") is used specifically as a generic reference to a boarding school.\nUntil the 19th century, a \"col\u00e9gio\" was usually a secondary or pre-university school, of public or religious nature, where the students usually lived together. A model for these colleges was the Royal College of Arts and Humanities, founded in Coimbra by King John III of Portugal in 1542.\nSingapore.\nThe term \"college\" in Singapore is generally only used for pre-university educational institutions called \"Junior Colleges\", which provide the final two years of secondary education (equivalent to sixth form in British terms or grades 11\u201312 in the American system). Since 1 January 2005, the term also refers to the three campuses of the Institute of Technical Education with the introduction of the \"collegiate system\", in which the three institutions are called ITE College East, ITE College Central, and ITE College West respectively.\nThe term \"university\" is used to describe higher-education institutions offering locally conferred degrees. Institutions offering diplomas are called \"polytechnics\", while other institutions are often referred to as \"institutes\" and so forth.\nSouth Africa.\nAlthough the term \"college\" is hardly used in any context at any university in South Africa, some non-university tertiary institutions call themselves colleges. These include teacher training colleges, business colleges and wildlife management colleges. See: List of universities in South Africa#Private colleges and universities; List of post secondary institutions in South Africa.\nSri Lanka.\nThere are several professional and vocational institutions that offer post-secondary education without granting degrees that are referred to as \"colleges\". This includes the Sri Lanka Law College, the many Technical Colleges and Teaching Colleges.\nUnited Kingdom.\nSecondary education and further education.\nFurther education (FE) colleges and sixth form colleges are institutions providing further education to students over 16. Some of these also provide higher education courses (see below). In the context of secondary education, 'college' is used in the names of some private schools, e.g. Eton College and Winchester College.\nHigher education.\nIn higher education, a college is normally a provider that does not hold university status, although it can also refer to a constituent part of a collegiate or federal university or a grouping of academic faculties or departments within a university. Traditionally the distinction between colleges and universities was that colleges did not award degrees while universities did, but this is no longer the case with NCG having gained taught degree awarding powers (the same as some universities) on behalf of its colleges, and many of the colleges of the University of London holding full degree awarding powers and being effectively universities. Most colleges, however, do not hold their own degree awarding powers and continue to offer higher education courses that are validated by universities or other institutions that can award degrees.\nIn England, , over 60% of the higher education providers directly funded by HEFCE (208/340) are sixth-form or further education colleges, often termed colleges of further and higher education, along with 17 colleges of the University of London, one university college, 100 universities, and 14 other providers (six of which use 'college' in their name). Overall, this means over two-thirds of state-supported higher education providers in England are colleges of one form or another. Many private providers are also called colleges, e.g. the New College of the Humanities and St Patrick's College, London.\nColleges within universities vary immensely in their responsibilities. The large constituent colleges of the University of London are effectively universities in their own right; colleges in some universities, including those of the University of the Arts London and smaller colleges of the University of London, run their own degree courses but do not award degrees; those at the University of Roehampton provide accommodation and pastoral care as well as delivering the teaching on university courses; those at Oxford and Cambridge deliver some teaching on university courses as well as providing accommodation and pastoral care; and those in Durham, Kent, Lancaster and York provide accommodation and pastoral care but do not normally participate in formal teaching. The legal status of these colleges also varies widely, with University of London colleges being independent corporations and recognised bodies, Oxbridge colleges, colleges of the University of the Highlands and Islands (UHI) and some Durham colleges being independent corporations and listed bodies, most Durham colleges being owned by the university but still listed bodies, and those of other collegiate universities not having formal recognition. When applying for undergraduate courses through UCAS, University of London colleges are treated as independent providers, colleges of Oxford, Cambridge, Durham and UHI are treated as locations within the universities that can be selected by specifying a 'campus code' in addition to selecting the university, and colleges of other universities are not recognised.\nThe UHI and the University of Wales Trinity Saint David (UWTSD) both include further education colleges. However, while the UHI colleges integrate FE and HE provision, UWTSD maintains a separation between the university campuses (Lampeter, Carmarthen and Swansea) and the two colleges (\"Coleg Sir G\u00e2r\" and \"Coleg Ceredigion\"; n.b. \"coleg\" is Welsh for college), which although part of the same group are treated as separate institutions rather than colleges within the university.\nA university college is an independent institution with the power to award taught degrees, but which has not been granted university status. University College is a protected title that can only be used with permission, although note that University College London, University College, Oxford and University College, Durham are colleges within their respective universities and not university colleges (in the case of UCL holding full degree awarding powers that set it above a university college), while University College Birmingham is a university in its own right and also not a university college.\nUnited States.\nIn the United States, there are over 7021 colleges and universities. A \"college\" in the US formally denotes a constituent part of a university, but in popular usage, the word \"college\" is the generic term for any post-secondary undergraduate education. Americans \"go to college\" after high school, regardless of whether the specific institution is formally a college or a university. Some students choose to dual-enroll, by taking college classes while still in high school. The word and its derivatives are the standard terms used to describe the institutions and experiences associated with American post-secondary undergraduate education.\nStudents must pay for college before taking classes. Some borrow the money via loans, and some students fund their educations with cash, scholarships, grants, or some combination of these payment methods. In 2011, the state or federal government subsidized $8,000 to $100,000 for each undergraduate degree. For state-owned schools (called \"public\" universities), the subsidy was given to the college, with the student benefiting from lower tuition. The state subsidized on average 50% of public university tuition.\nColleges vary in terms of size, degree, and length of stay. Two-year colleges, also known as junior or community colleges, usually offer an associate degree, and four-year colleges usually offer a bachelor's degree. Often, these are entirely undergraduate institutions, although some have graduate school programs.\nFour-year institutions in the U.S. that emphasize a liberal arts curriculum are known as liberal arts colleges. Until the 20th century, liberal arts, law, medicine, theology, and divinity were about the only form of higher education available in the United States. These schools have traditionally emphasized instruction at the undergraduate level, although advanced research may still occur at these institutions.\nWhile there is no national standard in the United States, the term \"university\" primarily designates institutions that provide undergraduate and graduate education. A university typically has as its core and its largest internal division an undergraduate college teaching a liberal arts curriculum, also culminating in a bachelor's degree. What often distinguishes a university is having, in addition, one or more graduate schools engaged in both teaching graduate classes and in research. Often these would be called a School of Law or School of Medicine, (but may also be called a college of law, or a faculty of law). An exception is Vincennes University, Indiana, which is styled and chartered as a \"university\" even though almost all of its academic programs lead only to two-year associate degrees. Some institutions, such as Dartmouth College and The College of William &amp; Mary, have retained the term \"college\" in their names for historical reasons. In one unique case, Boston College and Boston University, the former located in Chestnut Hill, Massachusetts and the latter located in Boston, Massachusetts, are completely separate institutions.\nUsage of the terms varies among the states. In 1996, for example, Georgia changed all of its four-year institutions previously designated as colleges to universities, and all of its vocational technology schools to technical colleges.\nThe terms \"university\" and \"college\" do not exhaust all possible titles for an American institution of higher education. Other options include \"institute\" (Worcester Polytechnic Institute) and (Massachusetts Institute of Technology), \"academy\" (United States Military Academy), \"union\" (Cooper Union), \"conservatory\" (New England Conservatory), and \"school\" (Juilliard School). In colloquial use, they are still referred to as \"college\" when referring to their undergraduate studies.\nThe term \"college\" is also, as in the United Kingdom, used for a constituent semi-autonomous part of a larger university but generally organized on academic rather than residential lines. For example, at many institutions, the undergraduate portion of the university can be briefly referred to as the college (such as The College of the University of Chicago, Harvard College at Harvard, or Columbia College at Columbia) while at others, such as the University of California, Berkeley, each of the faculties may be called a \"college\" (the \"college of engineering\", the \"college of nursing\", and so forth). There exist other variants for historical reasons; for example, Duke University, which was called Trinity College until the 1920s, still calls its main undergraduate subdivision Trinity College of Arts and Sciences.\nResidential colleges.\nSome American universities, such as Princeton, Rice, and Yale have established residential colleges (sometimes, as at Harvard, the first to establish such a system in the 1930s, known as houses) along the lines of Oxford or Cambridge. Unlike the Oxbridge colleges, but similarly to Durham, these residential colleges are not autonomous legal entities nor are they typically much involved in education itself, being primarily concerned with room, board, and social life. At the University of Michigan, University of California, San Diego and the University of California, Santa Cruz, however, each of the residential colleges does teach its own core writing courses and has its own distinctive set of graduation requirements.\nMany U.S. universities have placed increased emphasis on their residential colleges in recent years. This is exemplified by the creation of new colleges at Ivy League schools such as Yale University and Princeton University, and efforts to strengthen the contribution of the residential colleges to student education, including through a 2016 taskforce at Princeton on residential colleges.\nOrigin of the U.S. usage.\nThe founders of the first institutions of higher education in the United States were graduates of the University of Oxford and the University of Cambridge. The small institutions they founded would not have seemed to them like universities \u2013 they were tiny and did not offer the higher degrees in medicine and theology. Furthermore, they were not composed of several small colleges. Instead, the new institutions felt like the Oxford and Cambridge colleges they were used to \u2013 small communities, housing and feeding their students, with instruction from residential tutors (as in the United Kingdom, described above). When the first students graduated, these \"colleges\" assumed the right to confer degrees upon them, usually with authority\u2014for example, The College of William &amp; Mary has a Royal Charter from the British monarchy allowing it to confer degrees while Dartmouth College has a charter permitting it to award degrees \"as are usually granted in either of the universities, or any other college in our realm of Great Britain.\"\nThe leaders of Harvard College (which granted America's first degrees in 1642) might have thought of their college as the first of many residential colleges that would grow up into a New Cambridge university. However, over time, few new colleges were founded there, and Harvard grew and added higher faculties. Eventually, it changed its title to university, but the term \"college\" had stuck and \"colleges\" have arisen across the United States.\nIn U.S. usage, the word \"college\" embodies not only a particular type of school, but has historically been used to refer to the general concept of higher education when it is not necessary to specify a school, as in \"going to college\" or \"college savings accounts\" offered by banks.\nIn a survey of more than 2,000 college students in 33 states and 156 different campuses, the U.S. Public Interest Research Group found the average student spends as much as $1,200 each year on textbooks and supplies alone. By comparison, the group says that's the equivalent of 39 percent of tuition and fees at a community college, and 14 percent of tuition and fees at a four-year public university.\nMorrill Land-Grant Act.\nIn addition to private colleges and universities, the U.S. also has a system of government funded, public universities. Many were founded under the Morrill Land-Grant Colleges Act of 1862. A movement had arisen to bring a form of more practical higher education to the masses, as \"...many politicians and educators wanted to make it possible for all young Americans to receive some sort of advanced education.\" The Morrill Act \"...made it possible for the new western states to establish colleges for the citizens.\" Its goal was to make higher education more easily accessible to the citizenry of the country, specifically to improve agricultural systems by providing training and scholarship in the production and sales of agricultural products, and to provide formal education in \"...agriculture, home economics, mechanical arts, and other professions that seemed practical at the time.\"\nThe act was eventually extended to allow all states that had remained with the Union during the American Civil War, and eventually all states, to establish such institutions. Most of the colleges established under the Morrill Act have since become full universities, and some are among the elite of the world.\nBenefits of college.\nSelection of a four-year college as compared to a two-year junior college, even by marginal students such as those with a C+ grade average in high school and SAT scores in the mid 800s, increases the probability of graduation and confers substantial economic and social benefits.\nZimbabwe.\nThe term college is mainly used by private or independent secondary schools with Advanced Level (Upper 6th formers) and also Polytechnic Colleges which confer diplomas only. A student can complete secondary education (International General Certificate of Secondary Education, IGCSE) at 16 years and proceed straight to a poly-technical college or they can proceed to Advanced level (16 to 19 years) and obtain a General Certificate of Education (GCE) certificate which enables them to enrol at a university, provided they have good grades. Alternatively, with lower grades the GCE certificate holders will have an added advantage over their GCSE counterparts if they choose to enrol at a poly-technical college. Some schools in Zimbabwe choose to offer the International Baccalaureate studies as an alternative to the IGCSE and GCE."}
{"id": "5690", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=5690", "title": "Chalmers University of Technology", "text": "Chalmers University of Technology (, often shortened to Chalmers) is a Swedish university located in Gothenburg that focuses on research and education in technology, natural sciences, architecture, mathematics, maritime and other management areas.\nHistory.\nThe university was founded in 1829 following a donation by William Chalmers, a director of the Swedish East India Company. He donated part of his fortune for the establishment of an \"industrial school\". Chalmers was run as a private institution until 1937, when the institute became a state-owned university. In 1994, the school was incorporated as an\naktiebolag under the control of the Swedish Government, the faculty and the Student Union. Chalmers is one of only three universities in Sweden which are named after a person, the other two being Karolinska Institutet and Linnaeus University.\nDepartments.\nBeginning 1 May 2017, Chalmers has 13 departments.\nFurthermore, Chalmers is home to eight Areas of Advance and six national competence centers in key fields such as materials, mathematical modelling, environmental science, and vehicle safety.\nResearch infrastructure.\nChalmers University of Technology's research infrastructure includes everything from advanced real or virtual labs to large databases, computer capacity for large-scale calculations and research facilities.\nRankings and reputation.\nSince 2012, Chalmers has been achieved the highest reputation for Swedish Universities by the Kantar Sifo's Reputation Index. According to the survey, Chalmers is the most well-known university in Sweden regarded as a successful and competitive high-class institution with large contribution in society and credibility in media.\nIn 2018, a benchmarking report from MIT ranked Chalmers top 10 in the world of engineering education while in 2019, the European Commission recognized Chalmers as one of Europe's top universities, based on the U-Multirank rankings. \nFurthermore, in 2020, the World University Research Rankings placed Chalmers 12th in the world based on the evaluation of three key research aspects, namely research multi-disciplinarity, research impact, and research cooperativeness, while the QS World University Rankings, placed Chalmers 81st in the world in graduate employability and the Academic Ranking of World Universities, placed Chalmers 51\u201375 in the world in the field of electrical &amp; electronic engineering and 47th in the world for telecommunication engineering.\nAdditionally, in 2021, the QS World University Rankings, placed Chalmers 79th in the world in the field of engineering &amp; technology, the Times Higher Education World University Rankings, ranked Chalmers 68th in the world for engineering &amp; technology and the U.S. News &amp; World Report Best Global University Ranking placed Chalmers 84th in the world for engineering.\nIn the 2011 International Professional Ranking of Higher Education Institutions, which is established on the basis of the number of alumni holding a post of Chief Executive Officer (CEO) or equivalent in one of the Fortune Global 500 companies, Chalmers ranked 38th in the world, ranking 1st in Sweden and 15th in Europe.\nTies and partnerships.\nChalmers has partnerships with major industries mostly in the Gothenburg region such as Ericsson, Volvo, and SKF.\nThe University has general exchange agreements with many European and U.S. universities and maintains a special exchange program agreement with National Chiao Tung University (NCTU) in Taiwan where the exchange students from the two universities maintains offices for, among other things, helping local students with applying and preparing for an exchange year as well as acting as representatives. It contributes also to the Top Industrial Managers for Europe (TIME) network.\nA close collaboration between the Department of Computer Science and Engineering at Chalmers and ICVR at ETH Zurich is being established. As of 2014, Chalmers University of Technology is a member of the IDEA League network.\nStudents.\nApproximately 40% of Sweden's graduate engineers and architects are educated at Chalmers. Each year, around 250 post graduate degrees are awarded as well as 850 graduate degrees. About 1,000 post-graduate students attend programmes at the university, and many students are taking Master of Science engineering programmes and the Master of Architecture programme. Since 2007, all master's programmes are taught in English for both national and international students. This was a result of the adaptation to the Bologna process that started in 2004 at Chalmers (as the first technical university in Sweden).\nCurrently, about 10% of all students at Chalmers come from countries outside Sweden to enroll in a master's or PhD program.\nAround 2,700 students also attend Bachelor of Science engineering programmes, merchant marine and other undergraduate courses at Campus Lindholmen. Chalmers also shares some students with Gothenburg University in the joint IT University project. The IT University focuses exclusively on information technology and offers bachelor's and master's programmes with degrees issued from either Chalmers or Gothenburg University, depending on the programme.\nChalmers confers honorary doctoral degrees to people outside the university who have shown great merit in their research or in society.\nOrganization.\nChalmers is an aktiebolag with 100 shares \u00e0 1,000 SEK, all of which are owned by the Chalmers University of Technology Foundation, a private foundation, which appoints the university board and the president. The foundation has its members appointed by the Swedish government (4 to 8 seats), the departments appoints one member, the student union appoints one member and the president automatically gains one chair. Each department is led by a department head, usually a member of the faculty of that department. The faculty senate represents members of the faculty when decisions are taken.\nCampuses.\nIn 1937, the school moved from the city center to the new Gibraltar Campus, named after the mansion which owned the grounds, where it is now located. The Lindholmen College Campus was created in the early 1990s and is located on the island Hisingen. Campus Johanneberg and Campus Lindholmen, as they are now called, are connected by bus lines.\nStudent societies and traditions.\nTraditions include the graduation ceremony and the Cort\u00e8ge procession, an annual public event.\nPresidents.\nAlthough the official Swedish title for the head is \"rektor\", the university now uses \"President\" as the English translation."}
{"id": "5691", "revid": "9755426", "url": "https://en.wikipedia.org/wiki?curid=5691", "title": "Codex", "text": "The codex (plural codices () was the historical ancestor of the modern book. Instead of being composed of sheets of paper, it used sheets of vellum, papyrus, or other materials. The term \"codex\" is often used for ancient manuscript books, with handwritten contents. A codex much like the modern book is bound by stacking the pages and securing one set of edges in a form analogous to modern bookbinding by a variety of methods over the centuries. Modern books are divided into paperback or softback and those bound with stiff boards, called hardbacks. Elaborate historical bindings are called treasure bindings.\nAt least in the Western world, the main alternative to the paged codex format for a long document was the continuous scroll, which was the dominant form of document in the ancient world. Some codices are continuously folded like a concertina, in particular the Maya codices and Aztec codices, which are actually long sheets of paper or animal skin folded into pages.\nThe Ancient Romans developed the form from wax tablets. The gradual replacement of the scroll by the codex has been called the most important advance in book making before the invention of the printing press. The codex transformed the shape of the book itself, and offered a form that has lasted ever since. The spread of the codex is often associated with the rise of Christianity, which early on adopted the format for the Bible. First described by the 1st century AD Roman poet Martial, who praised its convenient use, the codex achieved numerical parity with the scroll around 300 AD, and had completely replaced it throughout what was by then a Christianized Greco-Roman world by the 6th century.\nEtymology and origins.\nThe word codex comes from the Latin word \"caudex\", meaning \"trunk of a tree\", \u201cblock of wood\u201d or \u201cbook\u201d). The codex began to replace the scroll almost as soon as it was invented. In Egypt, by the fifth century, the codex outnumbered the scroll by ten to one based on surviving examples. By the sixth century, the scroll had almost vanished as a medium for literature. The change from rolls to codices roughly coincides with the transition from papyrus to parchment as the preferred writing material, but the two developments are unconnected. In fact, any combination of codices and scrolls with papyrus and parchment is technically feasible and common in the historical record.\nTechnically, even modern paperbacks are codices, but publishers and scholars reserve the term for manuscript (hand-written) books produced from Late antiquity until the Middle Ages. The scholarly study of these manuscripts from the point of view of the bookbinding craft is called codicology. The study of ancient documents in general is called paleography.\nThe codex provided considerable advantages over other book formats, primarily its compactness, sturdiness, economic use of materials by using both sides (recto and verso), and ease of reference (a codex accommodates random access, as opposed to a scroll, which uses sequential access.)\nHistory.\nThe Romans used precursors made of reusable wax-covered tablets of wood for taking notes and other informal writings. Two ancient polyptychs, a \"pentaptych\" and \"octoptych\" excavated at Herculaneum, used a unique connecting system that presages later sewing on of thongs or cords. Julius Caesar may have been the first Roman to reduce scrolls to bound pages in the form of a note-book, possibly even as a papyrus codex. At the turn of the 1st century AD, a kind of folded parchment notebook called \"pugillares membranei\" in Latin became commonly used for writing in the Roman Empire. Theodore Cressy Skeat theorized that this form of notebook was invented in Rome and then spread rapidly to the Near East.\nCodices are described in certain works by the Classical Latin poet, Martial. He wrote a series of five couplets meant to accompany gifts of literature that Romans exchanged during the festival of Saturnalia. Three of these books are specifically described by Martial as being in the form of a codex; the poet praises the compendiousness of the form (as opposed to the scroll), as well as the convenience with which such a book can be read on a journey. In another poem by Martial, the poet advertises a new edition of his works, specifically noting that it is produced as a codex, taking less space than a scroll and being more comfortable to hold in one hand. According to Theodore Cressy Skeat, this might be the first recorded known case of an entire edition of a literary work (not just a single copy) being published in codex form, though it was likely an isolated case and was not a common practice until a much later time.\nIn his discussion of one of the earliest parchment codices to survive from Oxyrhynchus in Egypt, Eric Turner seems to challenge Skeat's notion when stating, \"its mere existence is evidence that this book form had a prehistory\", and that \"early experiments with this book form may well have taken place outside of Egypt.\" Early codices of parchment or papyrus appear to have been widely used as personal notebooks, for instance in recording copies of letters sent (Cicero \"Fam.\" 9.26.1). The parchment notebook pages were \"more durable, and could withstand being folded and stitched to other sheets\". Parchments whose writing was no longer needed were commonly washed or scraped for re-use, creating a palimpsest; the erased text, which can often be recovered, is older and usually more interesting than the newer text which replaced it. Consequently, writings in a codex were often considered informal and impermanent. Parchment (animal skin) was expensive, and therefore it was used primarily by the wealthy and powerful, who were also able to pay for textual design and color. \"Official documents and deluxe manuscripts [in the late Middle Ages] were written in gold and silver ink on parchment...dyed or painted with costly purple pigments as an expression of imperial power and wealth.\"\nAs early as the early 2nd century, there is evidence that a codex\u2014usually of papyrus\u2014was the preferred format among Christians. In the library of the Villa of the Papyri, Herculaneum (buried in AD 79), all the texts (of Greek literature) are scrolls (see Herculaneum papyri). However, in the Nag Hammadi library, hidden about AD 390, all texts (Gnostic) are codices. Despite this comparison, a fragment of a non-Christian parchment codex of Demosthenes' \"De Falsa Legatione\" from Oxyrhynchus in Egypt demonstrates that the surviving evidence is insufficient to conclude whether Christians played a major or central role in the development of early codices\u2014or if they simply adopted the format to distinguish themselves from Jews.\nThe earliest surviving fragments from codices come from Egypt, and are variously dated (always tentatively) towards the end of the 1st century or in the first half of the 2nd. This group includes the Rylands Library Papyrus P52, containing part of St John's Gospel, and perhaps dating from between 125 and 160.\nIn Western culture, the codex gradually replaced the scroll. Between the 4th century, when the codex gained wide acceptance, and the Carolingian Renaissance in the 8th century, many works that were not converted from scroll to codex were lost. The codex improved on the scroll in several ways. It could be opened flat at any page for easier reading, pages could be written on both front and back (recto and verso), and the protection of durable covers made it more compact and easier to transport.\nThe ancients stored codices with spines facing inward, and not always vertically. The spine could be used for the incipit, before the concept of a proper title developed in medieval times. Though most early codices were made of papyrus, papyrus was fragile and supplied from Egypt, the only place where papyrus grew and was made into paper, became scanty. The more durable parchment and vellum gained favor, despite the cost.\nThe codices of pre-Columbian Mesoamerica (Mexico and Central America) had a similar appearance when closed to the European codex, but were instead made with long folded strips of either fig bark (amatl) or plant fibers, often with a layer of whitewash applied before writing. New World codices were written as late as the 16th century (see Maya codices and Aztec codices). Those written before the Spanish conquests seem all to have been single long sheets folded concertina-style, sometimes written on both sides of the local \"amatl\" paper. There are significant codices produced in the colonial era, with pictorial and alphabetic texts in Spanish or an indigenous language such as Nahuatl.\nIn East Asia, the scroll remained standard for far longer than in the Mediterranean world. There were intermediate stages, such as scrolls folded concertina-style and pasted together at the back and books that were printed only on one side of the paper. This replaced traditional Chinese writing mediums such as bamboo and wooden slips, as well as silk and paper scrolls. The evolution of the codex in China began with folded-leaf pamphlets in the 9th century, during the late Tang Dynasty (618-907), improved by the 'butterfly' bindings of the Song dynasty (960-1279), the wrapped back binding of the Yuan dynasty (1271-1368), the stitched binding of the Ming (1368-1644) and Qing dynasties (1644-1912), and finally the adoption of Western-style bookbinding in the 20th century. The initial phase of this evolution, the accordion-folded palm-leaf-style book, most likely came from India and was introduced to China via Buddhist missionaries and scriptures.\nJudaism still retains the Torah scroll, at least for ceremonial use.\nFrom scrolls to codex.\nAmong the experiments of earlier centuries, scrolls were sometimes unrolled horizontally, as a succession of columns. (The Dead Sea Scrolls are a famous example of this format.) This made it possible to fold the scroll as an accordion. The next evolutionary step was to cut the folios and sew and glue them at their centers, making it easier to use the papyrus or vellum recto-verso as with a modern book.\nTraditional bookbinders would call one of these assembled, trimmed and bound folios (that is, the \"pages\" of the book as a whole, comprising the front matter and contents) a \"codex\" in contradistinction to the cover or \"case,\" producing the format of book now colloquially known as a \"hardcover\". In the hardcover bookbinding process, the procedure of binding the codex is very different to that of producing and attaching the case.\nPreparation.\nThe first stage in creating a codex is to prepare the animal skin. The skin is washed with water and lime but not together. The skin is soaked in the lime for a couple of days. The hair is removed, and the skin is dried by attaching it to a frame, called a herse. The parchment maker attaches the skin at points around the circumference. The skin attaches to the herse by cords. To prevent it from being torn, the maker wraps the area of the skin attached to the cord around a pebble called a pippin. After completing that, the maker uses a crescent shaped knife called a \"lunarium\" or \"lunellum\" to remove any remaining hairs. Once the skin completely dries, the maker gives it a deep clean and processes it into sheets. The number of sheets from a piece of skin depends on the size of the skin and the final product dimensions. For example, the average calfskin can provide three-and-a-half medium sheets of writing material, which can be doubled when they are folded into two conjoint leaves, also known as a \"bifolium\". Historians have found evidence of manuscripts in which the scribe wrote down the medieval instructions now followed by modern membrane makers. Defects can often be found in the membrane, whether they are from the original animal, human error during the preparation period, or from when the animal was killed. Defects can also appear during the writing process. Unless the manuscript is kept in perfect condition, defects can also appear later in its life.\nPreparation of pages for writing.\nFirstly, the membrane must be prepared. The first step is to set up the quires. The quire is a group of several sheets put together. Raymond Clemens and Timothy Graham point out, in \"Introduction to Manuscript Studies\", that \"the quire was the scribe's basic writing unit throughout the Middle Ages\": \"Pricking is the process of making holes in a sheet of parchment (or membrane) in preparation of it ruling. The lines were then made by ruling between the prick marks... The process of entering ruled lines on the page to serve as a guide for entering text. Most manuscripts were ruled with horizontal lines that served as the baselines on which the text was entered and with vertical bounding lines that marked the boundaries of the columns.\"\nForming quire.\nFrom the Carolingian period to the end of the Middle Ages, different styles of folding the quire came about. For example, in Mainland Europe throughout the Middle Ages, the quire was put into a system in which each side folded on to the same style. The hair side met the hair side and the flesh side to the flesh side. This was not the same style used in the British Isles, where the membrane was folded so that it turned out an eight-leaf quire, with single leaves in the third and sixth positions. The next stage was tacking the quire. Tacking is when the scribe would hold together the leaves in quire with thread. Once threaded together, the scribe would then sew a line of parchment up the \"spine\" of the manuscript to protect the tacking."}
{"id": "5692", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=5692", "title": "Calf", "text": "A calf (plural calves) is a young domestic cow or bull. Calves are reared to become adult cattle or are slaughtered for their meat, called veal, and hide.\nThe term \"calf\" is also used for some other species. See \"Other animals\" below.\nTerminology.\n\"Calf\" is the term used from birth to weaning, when it becomes known as a \"weaner\" or \"weaner calf\", though in some areas the term \"calf\" may be used until the animal is a yearling. The birth of a calf is known as \"calving\". A calf that has lost its mother is an orphan calf, also known as a \"poddy\" or \"poddy-calf\" in British English. \"Bobby calves\" are young calves which are to be slaughtered for human consumption. A \"vealer\" is a fat calf weighing less than about which is at about eight to nine months of age. A young female calf from birth until she has had a calf of her own is called a \"heifer\"\n(). In the American Old West, a motherless or small, runty calf was sometimes referred to as a \"dogie,\" (pronounced with a long \"o\").\nThe term \"calf\" is also used for some other species. See \"Other animals\" below.\nEarly development.\nCalves may be produced by natural means, or by artificial breeding using artificial insemination or embryo transfer.\nCalves are born after nine months. They usually stand within a few minutes of calving, and suckle within an hour. However, for the first few days they are not easily able to keep up with the rest of the herd, so young calves are often left hidden by their mothers, who visit them several times a day to suckle them. By a week old the calf is able to follow the mother all the time.\nSome calves are ear tagged soon after birth, especially those that are stud cattle in order to correctly identify their dams (mothers), or in areas (such as the EU) where tagging is a legal requirement for cattle. Typically when the calves are about two months old they are branded, ear marked, castrated and vaccinated.\nCalf rearing systems.\nThe \"single suckler\" system of rearing calves is similar to that occurring naturally in wild cattle, where each calf is suckled by its own mother until it is weaned at about nine months old. This system is commonly used for rearing beef cattle throughout the world.\nCows kept on poor forage (as is typical in subsistence farming) produce a limited amount of milk. A calf left with such a mother all the time can easily drink all the milk, leaving none for human consumption. For dairy production under such circumstances, the calf's access to the cow must be limited, for example by penning the calf and bringing the mother to it once a day after partly milking her. The small amount of milk available for the calf under such systems may mean that it takes a longer time to rear, and in subsistence farming it is therefore common for cows to calve only in alternate years.\nIn more intensive dairy farming, cows can easily be bred and fed to produce far more milk than one calf can drink. In the \"multi-suckler\" system, several calves are fostered onto one cow in addition to her own, and these calves' mothers can then be used wholly for milk production. More commonly, calves of dairy cows are fed formula milk from soon after birth, usually from a bottle or bucket.\nPurebred female calves of dairy cows are reared as replacement dairy cows. Most purebred dairy calves are produced by artificial insemination (AI). By this method each bull can serve many cows, so only a very few of the purebred dairy male calves are needed to provide bulls for breeding. The remainder of the male calves may be reared for beef or veal; however, some extreme dairy breeds carry so little muscle that rearing the purebred male calves may be uneconomic, and in this case they are often killed soon after birth and disposed of. Only a proportion of purebred heifers are needed to provide replacement cows, so often some of the cows in dairy herds are put to a beef bull to produce crossbred calves suitable for rearing as beef.\nVeal calves may be reared entirely on milk formula and killed at about 18 or 20 weeks as \"white\" veal, or fed on grain and hay and killed at 22 to 35 weeks to produce red or pink veal.\nGrowth.\nA commercial steer or bull calf is expected to put on about per month. A nine-month-old steer or bull is therefore expected to weigh about . Heifers will weigh at least at eight months of age.\nCalves are usually weaned at about eight to nine months of age, but depending on the season and condition of the dam, they might be weaned earlier. They may be paddock weaned, often next to their mothers, or weaned in stockyards. The latter system is preferred by some as it accustoms the weaners to the presence of people and they are trained to take feed other than grass. Small numbers may also be weaned with their dams with the use of weaning nose rings or nosebands which results in the mothers rejecting the calves' attempts to suckle. Many calves are also weaned when they are taken to the large weaner auction sales that are conducted in the south eastern states of Australia. Victoria and New South Wales have yardings of up to 8,000 weaners (calves) for auction sale in one day. The best of these weaners may go to the butchers. Others will be purchased by re-stockers to grow out and fatten on grass or as potential breeders. In the United States these weaners may be known as \"feeders\" and would be placed directly into feedlots.\nAt about 12 months old a beef heifer reaches puberty if she is well grown.\nDiseases.\nCalves suffer from few congenital abnormalities but the Akabane virus is widely distributed in temperate to tropical regions of the world. The virus is a teratogenic pathogen which causes abortions, stillbirths, premature births and congenital abnormalities, but occurs only during some years.\nUses.\nCalf meat for human consumption is called veal, and is usually produced from the male calves of Dairy cattle. Also eaten are calf's brains and calf liver. The hide is used to make calfskin, or tanned into leather and called calf leather, or sometimes in the US \"novillo\", the Spanish term. The fourth compartment of the stomach of slaughtered milk-fed calves is the source of rennet. The intestine is used to make Goldbeater's skin, and is the source of Calf Intestinal Alkaline Phosphatase (CIP).\nDairy cows can only produce milk after having calved, and dairy cows need to produce one calf each year in order to remain in production. Female calves will become a replacement dairy cow. Male dairy calves are generally reared for beef or veal; relatively few are kept for breeding purposes.\nOther animals.\nIn English the term \"calf\" is used by extension for the young of various other large species of mammal. In addition to other bovid species (such as bison, yak and water buffalo), these include the young of camels, dolphins, elephants, giraffes, hippopotamuses, deer (such as moose, elk (wapiti) and red deer), rhinoceroses, porpoises, whales, walruses and larger seals. However, common domestic species tend to have their own specific names, such as lamb, or foal used for all \"Equidae\"."}
{"id": "5693", "revid": "82818", "url": "https://en.wikipedia.org/wiki?curid=5693", "title": "Claude Shannon", "text": "Claude Elwood Shannon (April 30, 1916 \u2013 February 24, 2001) was an American mathematician, electrical engineer, and cryptographer known as \"the father of information theory\". Shannon is noted for having founded information theory with a landmark paper, \"A Mathematical Theory of Communication\", which he published in 1948.\nHe is also well known for founding digital circuit design theory in 1937, when\u2014as a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT)\u2014he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical numerical relationship. Shannon contributed to the field of cryptanalysis for national defense during World War II, including his fundamental work on codebreaking and secure telecommunications.\nBiography.\nChildhood.\nThe Shannon family lived in Gaylord, Michigan, and Claude was born in a hospital in nearby Petoskey. His father, Claude Sr. (1862\u20131934) was a businessman and for a while, a judge of probate in Gaylord. His mother, Mabel Wolf Shannon (1890\u20131945), was a language teacher, who also served as the principal of Gaylord High School. Claude Sr. was a descendant of New Jersey settlers, while Mabel was a child of German immigrants.\nMost of the first 16 years of Shannon's life were spent in Gaylord, where he attended public school, graduating from Gaylord High School in 1932. Shannon showed an inclination towards mechanical and electrical things. His best subjects were science and mathematics. At home he constructed such devices as models of planes, a radio-controlled model boat and a barbed-wire telegraph system to a friend's house a half-mile away. While growing up, he also worked as a messenger for the Western Union company.\nShannon's childhood hero was Thomas Edison, who he later learned was a distant cousin. Both Shannon and Edison were descendants of John Ogden (1609\u20131682), a colonial leader and an ancestor of many distinguished people.\nLogic circuits.\nIn 1932, Shannon entered the University of Michigan, where he was introduced to the work of George Boole. He graduated in 1936 with two bachelor's degrees: one in electrical engineering and the other in mathematics.\nIn 1936, Shannon began his graduate studies in electrical engineering at MIT, where he worked on Vannevar Bush's differential analyzer, an early analog computer. While studying the complicated \"ad hoc\" circuits of this analyzer, Shannon designed switching circuits based on Boole's concepts. In 1937, he wrote his master's degree thesis, \"A Symbolic Analysis of Relay and Switching Circuits\". A paper from this thesis was published in 1938. In this work, Shannon proved that his switching circuits could be used to simplify the arrangement of the electromechanical relays that were used then in telephone call routing switches. Next, he expanded this concept, proving that these circuits could solve all problems that Boolean algebra could solve. In the last chapter, he presented diagrams of several circuits, including a 4-bit full adder.\nUsing this property of electrical switches to implement logic is the fundamental concept that underlies all electronic digital computers. Shannon's work became the foundation of digital circuit design, as it became widely known in the electrical engineering community during and after World War II. The theoretical rigor of Shannon's work superseded the \"ad hoc\" methods that had prevailed previously. Howard Gardner called Shannon's thesis \"possibly the most important, and also the most noted, master's thesis of the century.\"\nShannon received his PhD from MIT in 1940. Vannevar Bush had suggested that Shannon should work on his dissertation at the Cold Spring Harbor Laboratory, in order to develop a mathematical formulation for Mendelian genetics. This research resulted in Shannon's PhD thesis, called \"An Algebra for Theoretical Genetics\".\nIn 1940, Shannon became a National Research Fellow at the Institute for Advanced Study in Princeton, New Jersey. In Princeton, Shannon had the opportunity to discuss his ideas with influential scientists and mathematicians such as Hermann Weyl and John von Neumann, and he also had occasional encounters with Albert Einstein and Kurt G\u00f6del. Shannon worked freely across disciplines, and this ability may have contributed to his later development of mathematical information theory.\nWartime research.\nShannon then joined Bell Labs to work on fire-control systems and cryptography during World War II, under a contract with section D-2 (Control Systems section) of the National Defense Research Committee (NDRC).\nShannon is credited with the invention of signal-flow graphs, in 1942. He discovered the topological gain formula while investigating the functional operation of an analog computer.\nFor two months early in 1943, Shannon came into contact with the leading British mathematician Alan Turing. Turing had been posted to Washington to share with the U.S. Navy's cryptanalytic service the methods used by the British Government Code and Cypher School at Bletchley Park to break the ciphers used by the Kriegsmarine U-boats in the north Atlantic Ocean. He was also interested in the encipherment of speech and to this end spent time at Bell Labs. Shannon and Turing met at teatime in the cafeteria. Turing showed Shannon his 1936 paper that defined what is now known as the \"Universal Turing machine\". This impressed Shannon, as many of its ideas complemented his own.\nIn 1945, as the war was coming to an end, the NDRC was issuing a summary of technical reports as a last step prior to its eventual closing down. Inside the volume on fire control, a special essay titled \"Data Smoothing and Prediction in Fire-Control Systems\", coauthored by Shannon, Ralph Beebe Blackman, and Hendrik Wade Bode, formally treated the problem of smoothing the data in fire-control by analogy with \"the problem of separating a signal from interfering noise in communications systems.\" In other words, it modeled the problem in terms of data and signal processing and thus heralded the coming of the Information Age.\nShannon's work on cryptography was even more closely related to his later publications on communication theory. At the close of the war, he prepared a classified memorandum for Bell Telephone Labs entitled \"A Mathematical Theory of Cryptography\", dated September 1945. A declassified version of this paper was published in 1949 as \"Communication Theory of Secrecy Systems\" in the \"Bell System Technical Journal\". This paper incorporated many of the concepts and mathematical formulations that also appeared in his \"A Mathematical Theory of Communication\". Shannon said that his wartime insights into communication theory and cryptography developed simultaneously and that \"they were so close together you couldn\u2019t separate them\". In a footnote near the beginning of the classified report, Shannon announced his intention to \"develop these results \u2026 in a forthcoming memorandum on the transmission of information.\"\nWhile he was at Bell Labs, Shannon proved that the cryptographic one-time pad is unbreakable in his classified research that was later published in October 1949. He also proved that any unbreakable system must have essentially the same characteristics as the one-time pad: the key must be truly random, as large as the plaintext, never reused in whole or part, and be kept secret.\nInformation theory.\nIn 1948, the promised memorandum appeared as \"A Mathematical Theory of Communication\", an article in two parts in the July and October issues of the \"Bell System Technical Journal\". This work focuses on the problem of how best to encode the information a sender wants to transmit. In this fundamental work, he used tools in probability theory, developed by Norbert Wiener, which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure of the information content in a message, which is a measure of uncertainty reduced by the message, while essentially inventing the field of information theory.\nThe book \"The Mathematical Theory of Communication\" reprints Shannon's 1948 article and Warren Weaver's popularization of it, which is accessible to the non-specialist. Weaver pointed out that the word \"information\" in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's \"Symbols, Signals, and Noise\".\nInformation theory's fundamental contribution to natural language processing and computational linguistics was further established in 1951, in his article \"Prediction and Entropy of Printed English\", showing upper and lower bounds of entropy on the statistics of English \u2013 giving a statistical foundation to language analysis. In addition, he proved that treating whitespace as the 27th letter of the alphabet actually lowers uncertainty in written language, providing a clear quantifiable link between cultural practice and probabilistic cognition.\nAnother notable paper published in 1949 is \"Communication Theory of Secrecy Systems\", a declassified version of his wartime work on the mathematical theory of cryptography, in which he proved that all theoretically unbreakable ciphers must have the same requirements as the one-time pad. He is also credited with the introduction of sampling theory, which is concerned with representing a continuous-time signal from a (uniform) discrete set of samples. This theory was essential in enabling telecommunications to move from analog to digital transmissions systems in the 1960s and later.\nHe returned to MIT to hold an endowed chair in 1956.\nTeaching at MIT.\nIn 1956 Shannon joined the MIT faculty to work in the Research Laboratory of Electronics (RLE). He continued to serve on the MIT faculty until 1978.\nLater life.\nShannon developed Alzheimer's disease and spent the last few years of his life in a nursing home; he died in 2001, survived by his wife, a son and daughter, and two granddaughters.\nHobbies and inventions.\nOutside of Shannon's academic pursuits, he was interested in juggling, unicycling, and chess. He also invented many devices, including a Roman numeral computer called THROBAC, juggling machines, and a flame-throwing trumpet. He built a device that could solve the Rubik's Cube puzzle.\nShannon designed the Minivac 601, a digital computer trainer to teach business people about how computers functioned. It was sold by the Scientific Development Corp starting in 1961.\nHe is also considered the co-inventor of the first wearable computer along with Edward O. Thorp. The device was used to improve the odds when playing roulette.\nPersonal life.\nShannon married Norma Levor, a wealthy, Jewish, left-wing intellectual in January 1940. The marriage ended in divorce after about a year. Levor later married Ben Barzman.\nShannon met his second wife Betty Shannon (n\u00e9e Mary Elizabeth Moore) when she was a numerical analyst at Bell Labs. They were married in 1949. Betty assisted Claude in building some of his most famous inventions. They had three children.\nShannon was apolitical and an atheist.\nTributes.\nThere are six statues of Shannon sculpted by Eugene Daub: one at the University of Michigan; one at MIT in the Laboratory for Information and Decision Systems; one in Gaylord, Michigan; one at the University of California, San Diego; one at Bell Labs; and another at AT&amp;T Shannon Labs. After the breakup of the Bell System, the part of Bell Labs that remained with AT&amp;T Corporation was named Shannon Labs in his honor.\nAccording to Neil Sloane, an AT&amp;T Fellow who co-edited Shannon's large collection of papers in 1993, the perspective introduced by Shannon's communication theory (now called information theory) is the foundation of the digital revolution, and every device containing a microprocessor or microcontroller is a conceptual descendant of Shannon's publication in 1948: \"He's one of the great men of the century. Without him, none of the things we know today would exist. The whole digital revolution started with him.\" The unit shannon is named after Claude Shannon.\n\"A Mind at Play\", a biography of Shannon written by Jimmy Soni and Rob Goodman, was published in 2017.\nOn April 30, 2016 Shannon was honored with a Google Doodle to celebrate his life on what would have been his 100th birthday.\n\"The Bit Player\", a feature film about Shannon directed by Mark Levinson premiered at the World Science Festival in 2019. Drawn from interviews conducted with Shannon in his house in the 1980s, the film was released on Amazon Prime in August 2020.\nOther work.\nShannon's mouse.\n\"Theseus\", created in 1950, was a mechanical mouse controlled by an electromechanical relay circuit that enabled it to move around a labyrinth of 25 squares. The maze configuration was flexible and it could be modified arbitrarily by rearranging movable partitions. The mouse was designed to search through the corridors until it found the target. Having travelled through the maze, the mouse could then be placed anywhere it had been before, and because of its prior experience it could go directly to the target. If placed in unfamiliar territory, it was programmed to search until it reached a known location and then it would proceed to the target, adding the new knowledge to its memory and learning new behavior. Shannon's mouse appears to have been the first artificial learning device of its kind.\nShannon's estimate for the complexity of chess.\nIn 1949 Shannon completed a paper (published in March 1950) which estimates the game-tree complexity of chess, which is approximately 10120. This number is now often referred to as the \"Shannon number\", and is still regarded today as an accurate estimate of the game's complexity. The number is often cited as one of the barriers to solving the game of chess using an exhaustive analysis (i.e. brute force analysis).\nShannon's computer chess program.\nOn March 9, 1949, Shannon presented a paper called \"Programming a Computer for playing Chess\". The paper was presented at the National Institute for Radio Engineers Convention in New York. He described how to program a computer to play chess based on position scoring and move selection. He proposed basic strategies for restricting the number of possibilities to be considered in a game of chess. In March 1950 it was published in \"Philosophical Magazine\", and is considered one of the first articles published on the topic of programming a computer for playing chess, and using a computer to solve the game.\nHis process for having the computer decide on which move to make was a minimax procedure, based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. \"Material\" was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting \u00bd point for each doubled pawn, backward pawn, and isolated pawn; \"mobility\" was incorporated by adding 0.1 point for each legal move available.\nShannon's maxim.\nShannon formulated a version of Kerckhoffs' principle as \"The enemy knows the system\". In this form it is known as \"Shannon's maxim\".\nCommemorations.\nShannon centenary.\nThe Shannon centenary, 2016, marked the life and influence of Claude Elwood Shannon on the hundredth anniversary of his birth on April 30, 1916. It was inspired in part by the Alan Turing Year. An ad hoc committee of the IEEE Information Theory Society including Christina Fragouli, R\u00fcdiger Urbanke, Michelle Effros, Lav Varshney and Sergio Verd\u00fa, coordinated worldwide events. The initiative was announced in the History Panel at the 2015 IEEE Information Theory Workshop Jerusalem and the IEEE Information Theory Society Newsletter.\nA detailed listing of confirmed events was available on the website of the IEEE Information Theory Society.\nSome of the planned activities included:\nAwards and honors list.\nThe Claude E. Shannon Award was established in his honor; he was also its first recipient, in 1972."}
{"id": "5694", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=5694", "title": "Cracking", "text": "Cracking may refer to:\nIn computing:"}
{"id": "5695", "revid": "21155", "url": "https://en.wikipedia.org/wiki?curid=5695", "title": "Community", "text": "A community is a social unit (a group of living things) with commonality such as norms, religion, values, customs, or identity. Communities may share a sense of place situated in a given geographical area (e.g. a country, village, town, or neighbourhood) or in virtual space through communication platforms. Durable relations that extend beyond immediate genealogical ties also define a sense of community, important to their identity, practice, and roles in social institutions such as family, home, work, government, society, or humanity at large. Although communities are usually small relative to personal social ties, \"community\" may also refer to large group affiliations such as national communities, international communities, and virtual communities.\nThe English-language word \"community\" derives from the Old French \"comunet\u00e9\" (currently \"Communaut\u00e9\"), which comes from the Latin \"communitas\" \"community\", \"public spirit\" (from Latin \"communis\", \"common\").\nHuman communities may have intent, belief, resources, preferences, needs, and risks in common, affecting the identity of the participants and their degree of cohesiveness.\nPerspectives of various disciplines.\nArchaeology.\nArchaeological studies of social communities use the term \"community\" in two ways, paralleling usage in other areas. The first is an informal definition of community as a place where people used to live. In this sense it is synonymous with the concept of an ancient settlement - whether a hamlet, village, town, or city. The second meaning resembles the usage of the term in other social sciences: a community is a group of people living near one another who interact socially. Social interaction on a small scale can be difficult to identify with archaeological data. Most reconstructions of social communities by archaeologists rely on the principle that social interaction in the past was conditioned by physical distance. Therefore, a small village settlement likely constituted a social community and spatial subdivisions of cities and other large settlements may have formed communities. Archaeologists typically use similarities in material culture\u2014from house types to styles of pottery\u2014to reconstruct communities in the past. This classification method relies on the assumption that people or households will share more similarities in the types and styles of their material goods with other members of a social community than they will with outsiders.\nEcology.\nIn ecology, a community is an assemblage of populations of different species, interacting with one another. Community ecology is the branch of ecology that studies interactions between and among species. It considers how such interactions, along with interactions between species and the abiotic environment, affect community structure and species richness, diversity and patterns of abundance. Species interact in three ways: competition, predation and mutualism. Competition typically results in a double negative\u2014that is both species lose in the interaction. Predation is a win/lose situation with one species winning. Mutualism, on the other hand, involves both species cooperating in some way, with both winning. The two main types of communities are major which are self-sustaining and self-regulating (such as a forest or a lake) and minor communities which rely on other communities (like fungi decomposing a log) and are the building blocks of major communities.\nKey concepts.\n\"Gemeinschaft and Gesellschaft\".\nIn \"Gemeinschaft und Gesellschaft\" (1887), German sociologist Ferdinand T\u00f6nnies described two types of human association: \"Gemeinschaft\" (usually translated as \"community\") and \"Gesellschaft\" (\"society\" or \"association\"). T\u00f6nnies proposed the \"Gemeinschaft\u2013Gesellschaft\" dichotomy as a way to think about social ties. No group is exclusively one or the other. \"Gemeinschaft\" stress personal social interactions, and the roles, values, and beliefs based on such interactions. \"Gesellschaft\" stress indirect interactions, impersonal roles, formal values, and beliefs based on such interactions.\nSense of community.\nIn a seminal 1986 study, McMillan and Chavis identify four elements of \"sense of community\":\nA \"sense of community index\" (SCI) was developed by Chavis and colleagues, and revised and adapted by others. Although originally designed to assess sense of community in neighborhoods, the index has been adapted for use in schools, the workplace, and a variety of types of communities.\nStudies conducted by the APPA indicate that young adults who feel a sense of belonging in a community, particularly small communities, develop fewer psychiatric and depressive disorders than those who do not have the feeling of love and belonging.\nSocialization.\nThe process of learning to adopt the behavior patterns of the community is called socialization. The most fertile time of socialization is usually the early stages of life, during which individuals develop the skills and knowledge and learn the roles necessary to function within their culture and social environment. For some psychologists, especially those in the psychodynamic tradition, the most important period of socialization is between the ages of one and ten. But socialization also includes adults moving into a significantly different environment where they must learn a new set of behaviors.\nSocialization is influenced primarily by the family, through which children first learn community norms. Other important influences include schools, peer groups, people, mass media, the workplace, and government. The degree to which the norms of a particular society or community are adopted determines one's willingness to engage with others. The norms of tolerance, reciprocity, and trust are important \"habits of the heart,\" as de Tocqueville put it, in an individual's involvement in community.\nCommunity development.\nCommunity development is often linked with community work or community planning, and may involve stakeholders, foundations, governments, or contracted entities including non-government organisations (NGOs), universities or government agencies to progress the social well-being of local, regional and, sometimes, national communities. More grassroots efforts, called community building or community organizing, seek to empower individuals and groups of people by providing them with the skills they need to effect change in their own communities. These skills often assist in building political power through the formation of large social groups working for a common agenda. Community development practitioners must understand both how to work with individuals and how to affect communities' positions within the context of larger social institutions. Public administrators, in contrast, need to understand community development in the context of rural and urban development, housing and economic development, and community, organizational and business development.\nFormal accredited programs conducted by universities, as part of degree granting institutions, are often used to build a knowledge base to drive curricula in public administration, sociology and community studies. The General Social Survey from the National Opinion Research Center at the University of Chicago and the Saguaro Seminar at the John F. Kennedy School of Government at Harvard University are examples of national community development in the United States. The Maxwell School of Citizenship and Public Affairs at Syracuse University in New York State offers core courses in community and economic development, and in areas ranging from non-profit development to US budgeting (federal to local, community funds). In the United Kingdom, the University of Oxford has led in providing extensive research in the field through its \" Community Development Journal,\" used worldwide by sociologists and community development practitioners.\nAt the intersection between community \"development\" and community \"building\" are a number of programs and organizations with community development tools. One example of this is the program of the Asset Based Community Development Institute of Northwestern University. The institute makes available downloadable tools to assess community assets and make connections between non-profit groups and other organizations that can help in community building. The Institute focuses on helping communities develop by \"mobilizing neighborhood assets\"\u00a0\u2013 building from the inside out rather than the outside in. In the disability field, community building was prevalent in the 1980s and 1990s with roots in John McKnight's approaches.\nCommunity building and organizing.\nIn \"The Different Drum: Community-Making and Peace\" (1987) Scott Peck argues that the almost accidental sense of community that exists at times of crisis can be consciously built. Peck believes that conscious community building is a process of deliberate design based on the knowledge and application of certain rules. He states that this process goes through four stages:\nIn 1991, Peck remarked that building a sense of community is easy but maintaining this sense of community is difficult in the modern world.\nThe three basic types of community organizing are grassroots organizing, coalition building, and \"institution-based community organizing,\" (also called \"broad-based community organizing,\" an example of which is faith-based community organizing, or Congregation-based Community Organizing).\nCommunity building can use a wide variety of practices, ranging from simple events (e.g., potlucks, small book clubs) to larger-scale efforts (e.g., mass festivals, construction projects that involve local participants rather than outside contractors).\nCommunity building that is geared toward citizen action is usually termed \"community organizing.\" In these cases, organized community groups seek accountability from elected officials and increased direct representation within decision-making bodies. Where good-faith negotiations fail, these constituency-led organizations seek to pressure the decision-makers through a variety of means, including picketing, boycotting, sit-ins, petitioning, and electoral politics. \nCommunity organizing can focus on more than just resolving specific issues. Organizing often means building a widely accessible power structure, often with the end goal of distributing power equally throughout the community. Community organizers generally seek to build groups that are open and democratic in governance. Such groups facilitate and encourage consensus decision-making with a focus on the general health of the community rather than a specific interest group.\nIf communities are developed based on something they share in common, whether location or values, then one challenge for developing communities is how to incorporate individuality and differences. Rebekah Nathan suggests in her book, \"My Freshman Year\", we are drawn to developing communities totally based on sameness, despite stated commitments to diversity, such as those found on university websites.\nTypes of community.\nA number of ways to categorize types of community have been proposed. One such breakdown is as follows:\nThe usual categorizations of community relations have a number of problems: (1) they tend to give the impression that a particular community can be defined as just this kind or another; (2) they tend to conflate modern and customary community relations; (3) they tend to take sociological categories such as ethnicity or race as given, forgetting that different ethnically defined persons live in different kinds of communities \u2014grounded, interest-based, diasporic, etc.\nIn response to these problems, Paul James and his colleagues have developed a taxonomy that maps community relations, and recognizes that actual communities can be characterized by different kinds of relations at the same time:\nIn these terms, communities can be nested and/or intersecting; one community can contain another\u2014for example a location-based community may contain a number of ethnic communities. Both lists above can used in a cross-cutting matrix in relation to each other.\nInternet communities.\nIn general, virtual communities value knowledge and information as currency or social resource. What differentiates virtual communities from their physical counterparts is the extent and impact of \"weak ties\", which are the relationships acquaintances or strangers form to acquire information through online networks. Relationships among members in a virtual community tend to focus on information exchange about specific topics. A survey conducted by Pew Internet and The American Life Project in 2001 found those involved in entertainment, professional, and sports virtual-groups focused their activities on obtaining information.\nAn epidemic of bullying and harassment has arisen from the exchange of information between strangers, especially among teenagers, in virtual communities. Despite attempts to implement anti-bullying policies, Sheri Bauman, professor of counselling at the University of Arizona, claims the \"most effective strategies to prevent bullying\" may cost companies revenue.\nVirtual Internet-mediated communities can interact with offline real-life activity, potentially forming strong and tight-knit groups such as QAnon."}
{"id": "5696", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5696", "title": "Community college", "text": "A community college is a type of educational institution. The term can have different meanings in different countries: many community colleges have an \"open enrollment\" for students who have graduated from high school (also known as senior secondary school). The term usually refers to a higher educational institution that provides workforce education and college transfer academic programs. Some institutions maintain athletic teams and dormitories similar to their university counterparts.\nAustralia.\nIn Australia, the term \"community college\" refers to small private businesses running short (e.g. 6 weeks) courses generally of a self-improvement or hobbyist nature. Equivalent to the American notion of community colleges are Tertiary and Further Education colleges or TAFEs; these are institutions regulated mostly at state and territory level. There are also an increasing number of private providers, which are colloquially called \"colleges\".\nTAFEs and other providers carry on the tradition of adult education, which was established in Australia around the mid-19th century, when evening classes were held to help adults enhance their numeracy and literacy skills. Most Australian universities can also be traced back to such forerunners, although obtaining a university charter has always changed their nature. In TAFEs and colleges today, courses are designed for personal development of an individual or for employment outcomes. Educational programs cover a variety of topics such as arts, languages, business and lifestyle. They usually are scheduled to run two, three or four days of the week, depending on the level of the course undertaken. A Certificate I may only run for 4 hours twice a week for a term of 9 weeks. A full-time Diploma course might have classes 4 days per week for a year (36 weeks). Some courses may be offered in the evenings or weekends to accommodate people working full-time. Funding for colleges may come from government grants and course fees. Many are not-for-profit organisations. Such TAFES are located in metropolitan, regional and rural locations of Australia.\nEducation offered by TAFEs and colleges has changed over the years. By the 1980s, many colleges had recognised a community need for computer training. Since then thousands of people have increased skills through IT courses. The majority of colleges by the late 20th century had also become Registered Training Organisations. They offer individuals a nurturing, non-traditional education venue to gain skills that better prepare them for the workplace and potential job openings. TAFEs and colleges have not traditionally offered bachelor's degrees, instead providing pathway arrangements with universities to continue towards degrees. The American innovation of the associate degree is being developed at some institutions. Certificate courses I to IV, diplomas and advanced diplomas are typically offered, the latter deemed equivalent to an undergraduate qualification, albeit typically in more vocational areas. Recently, some TAFE institutes (and private providers) have also become higher education providers in their own right and are now starting to offer bachelor's degree programs.\nCanada.\nIn Canada, colleges are adult educational institutions that provide higher education and tertiary education, and grant certificates and diplomas. Alternatively, Canadian colleges are often called \u201cinstitutes\u201d or \u201cpolytechnic institutes\u201d. As well, in Ontario, the 24 colleges of applied arts and technology have been mandated to offer their own stand-alone degrees as well as to offer joint degrees with universities through \"articulation agreements\" that often result in students emerging with both a diploma and a degree. Thus, for example, the University of Guelph \"twins\" with Humber College and York University does the same with Seneca College. More recently, however, colleges have been offering a variety of their own degrees, often in business, technology, science, and other technical fields. Each province has its own educational system, as prescribed by the Canadian federalism model of governance. In the mid-1960s and early 1970s, most Canadian colleges began to provide practical education and training for the emerging and booming generation, and for immigrants from around the world who were entering Canada in increasing numbers at that time. A formative trend was the merging of the then separate vocational training and adult education (night school) institutions.\nCanadian colleges are either publicly funded or private post-secondary institutions (run for profit).\nIn terms of academic pathways, Canadian colleges and universities collaborate with each other with the purpose of providing college students the opportunity to academically upgrade their education. Students can transfer their diplomas and earn transfer credits through their completed college credits towards undergraduate university degrees.\nThe term associate degree is used in western Canada to refer to a two-year college arts or science degree, similar to how the term is used in the United States. In other parts of Canada, the term advanced degree is used to indicate a three- or four-year college program.\nIn the province of Quebec, three years is the norm for a university degree because a year of credit is earned in the CEGEP (college) system. Even when speaking in English, people often refer to all colleges as C\u00e9geps; however, the term is an acronym more correctly applied specifically to the French-language public system: Coll\u00e8ge d'enseignement g\u00e9n\u00e9ral et professionnel (CEGEP); in English: College of General and Vocational Education. The word \"college\" can also refer to a private high school in Quebec.\nIndia.\nIn India, 98 community colleges are recognized by the University Grants Commission. The courses offered by these colleges are diplomas, advance diplomas and certificate courses. The duration of these courses usually ranges from six months to two years.\nMalaysia.\nCommunity colleges in Malaysia are a network of educational institutions whereby vocational and technical skills training could be provided at all levels for school leavers before they entered the workforce. The community colleges also provide an infrastructure for rural communities to gain skills training through short courses as well as providing access to a post-secondary education.\nAt the moment, most community colleges award qualifications up to Level 3 in the Malaysian Qualifications Framework (Certificate 3) in both the Skills sector (Sijil Kemahiran Malaysia or the Malaysian Skills Certificate) as well as the Vocational and Training sector but the number of community colleges that are starting to award Level 4 qualifications (Diploma) are increasing. This is two levels below a bachelor's degree (Level 6 in the MQF) and students within the system who intend to further their studies to that level will usually seek entry into Advanced Diploma programs in public universities, polytechnics or accredited private providers.\nPhilippines.\nIn the Philippines, a community school functions as elementary or secondary school at daytime and towards the end of the day convert into a community college. This type of institution offers night classes under the supervision of the same principal, and the same faculty members who are given part-time college teaching load.\nThe concept of community college dates back to the time of the former Minister of Education, Culture and Sports (MECS) that had under its wings the Bureaus of Elementary Education, Secondary Education, Higher Education and Vocational-Technical Education. MECS Secretary, Dr. Cecilio Putong, who in 1971 wrote that a community school is a school established in the community, by the community, and for the community itself. Dr. Pedro T. Orata of Pangasinan shared the same idea, hence the establishment of a community college, now called the City College of Urdaneta.\nA community college like the one in Abuyog, Leyte can operate with only a PHP 124,000 annual budget in a two-storey structure housing more than 700 students.\nUnited Kingdom.\nIn the United Kingdom, except for Scotland, this term is not commonly used. When it is, a community college is a school which not only provides education for the school-age population (11\u201318) of the locality, but also additional services and education to adults and other members of the community. This education includes but is not limited to sports, adult literacy and lifestyle education. Usually when students finish their secondary school studies at age 16, they move on to a sixth form college where they study for their A-levels (although some secondary schools have integrated sixth forms). After the two-year A-level period, they may proceed to a college of further education or a university. The former is also known as a technical college.\nUnited States.\nIn the United States, community colleges, sometimes called junior colleges, technical colleges, two-year colleges, or city colleges, are primarily two-year public institutions providing lower-level tertiary education, also known as continuing education. They grant certificates, diplomas, and associate degrees. After graduating from a community college, most students transfer to a four-year liberal arts college or university for two to three years to complete a bachelor's degree.\nBefore the 1970s, community colleges in the United States were more commonly referred to as junior colleges. That term is still used at some institutions. However, the term \"junior college\" is generally applied to private two-year institutions, whereas the term \"community college\" is used to describe publicly funded two-year institutions. Community colleges primarily attract and accept students from the local community, and are often supported by local tax revenue. They may also work with local businesses to ensure students are being prepared for the local workforce.\nResearch.\nSome research organizations and publications focus upon the activities of community college, junior college, and technical college institutions. Many of these institutions and organizations present the most current research and practical outcomes at annual community college conferences.\nSeveral peer-reviewed journals extensively publish research on community colleges:"}
{"id": "5697", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5697", "title": "Civil Rights Memorial", "text": "The Civil Rights Memorial is an American memorial in Montgomery, Alabama created by Maya Lin. The names of 41 people are inscribed on the granite fountain as martyrs who were killed in the civil rights movement. The memorial is sponsored by the Southern Poverty Law Center.\nDesign.\nThe names included in the memorial belong to those who were killed between 1954 and 1968. Those dates were chosen because in 1954 the U.S. Supreme Court ruled that racial segregation in schools was unlawful and 1968 is the year of the assassination of Martin Luther King Jr. The monument was created by Maya Lin, who is best known for creating the Vietnam Veterans Memorial in Washington, D.C. The Civil Rights Memorial was dedicated in 1989.\nThe concept of Lin's design is based on the soothing and healing effect of water. It was inspired by a passage from King's \"I Have a Dream\" speech \"...we will not be satisfied \"until justice rolls down like waters and righteousness like a mighty stream...\" The quotation in the passage, which is inscribed on the memorial, is a direct paraphrase of , as translated in the American Standard Version of the Bible. The memorial is a fountain in the form of an asymmetric inverted stone cone. A film of water flows over the base of the cone, which contains the 41 names included. It is possible to touch the smooth film of water and to alter it temporarily, which quickly returns to smoothness. As such, the memorial represents the aspirations of the civil rights movement to end legal racial segregation.\nTours and location.\nThe memorial is in downtown Montgomery, at 400 Washington Avenue, in an open plaza in front of the Civil Rights Memorial Center, which was the offices of the Southern Poverty Law Center until it moved across the street into a new building in 2001. The memorial may be visited freely 24 hours a day, 7 days a week.\nThe Civil Rights Memorial Center offers guided group tours, lasting approximately one hour. Tours are available by appointment, Monday to Saturday.\nThe memorial is only a few blocks from other historic sites, including the Dexter Avenue King Memorial Baptist Church, the Alabama State Capitol, the Alabama Department of Archives and History, the corners where Claudette Colvin and Rosa Parks boarded buses in 1955 on which they would later refuse to give up their seats, and the Rosa Parks Library and Museum.\nNames included.\n\"Civil Rights Martyrs\".\nThe 41 names included in the Civil Rights Memorial are those of:\n\"The Forgotten\".\n\"The Forgotten\" are 74 people who are identified in a display at the Civil Rights Memorial Center. These names were not inscribed on the Memorial because there was insufficient information about their deaths at the time the Memorial was created. However, it is thought that these people were killed as a result of racially motivated violence between 1952 and 1968."}
{"id": "5698", "revid": "36932876", "url": "https://en.wikipedia.org/wiki?curid=5698", "title": "Charles Babbage", "text": "Charles Babbage (; 26 December 1791\u00a0\u2013 18 October 1871) was an English polymath. A mathematician, philosopher, inventor and mechanical engineer, Babbage originated the concept of a digital programmable computer.\nBabbage is considered by some to be \"father of the computer\". Babbage is credited with inventing the first mechanical computer, the Difference Engine, that eventually led to more complex electronic designs, though all the essential ideas of modern computers are to be found in Babbage's Analytical Engine. His varied work in other fields has led him to be described as \"pre-eminent\" among the many polymaths of his century.\nBabbage, who died before the complete successful engineering of many of his designs, including his Difference Engine and Analytical Engine, remained a prominent figure in the ideating of computing. Parts of Babbage's incomplete mechanisms are on display in the Science Museum in London. In 1991, a functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.\nEarly life.\nBabbage's birthplace is disputed, but according to the \"Oxford Dictionary of National Biography\" he was most likely born at 44 Crosby Row, Walworth Road, London, England. A blue plaque on the junction of Larcom Street and Walworth Road commemorates the event.\nHis date of birth was given in his obituary in \"The Times\" as 26 December 1792; but then a nephew wrote to say that Babbage was born one year earlier, in 1791. The parish register of St. Mary's, Newington, London, shows that Babbage was baptised on 6 January 1792, supporting a birth year of 1791.\nBabbage was one of four children of Benjamin Babbage and Betsy Plumleigh Teape. His father was a banking partner of William Praed in founding Praed's &amp; Co. of Fleet Street, London, in 1801. In 1808, the Babbage family moved into the old Rowdens house in East Teignmouth. Around the age of eight, Babbage was sent to a country school in Alphington near Exeter to recover from a life-threatening fever. For a short time he attended King Edward VI Grammar School in Totnes, South Devon, but his health forced him back to private tutors for a time.\nBabbage then joined the 30-student Holmwood Academy, in Baker Street, Enfield, Middlesex, under the Reverend Stephen Freeman. The academy had a library that prompted Babbage's love of mathematics. He studied with two more private tutors after leaving the academy. The first was a clergyman near Cambridge; through him Babbage encountered Charles Simeon and his evangelical followers, but the tuition was not what he needed. He was brought home, to study at the Totnes school: this was at age 16 or 17. The second was an Oxford tutor, under whom Babbage reached a level in Classics sufficient to be accepted by Cambridge.\nAt the University of Cambridge.\nBabbage arrived at Trinity College, Cambridge, in October 1810. He was already self-taught in some parts of contemporary mathematics; he had read in Robert Woodhouse, Joseph Louis Lagrange, and Marie Agnesi. As a result, he was disappointed in the standard mathematical instruction available at the university.\nBabbage, John Herschel, George Peacock, and several other friends formed the Analytical Society in 1812; they were also close to Edward Ryan. As a student, Babbage was also a member of other societies such as The Ghost Club, concerned with investigating supernatural phenomena, and the Extractors Club, dedicated to liberating its members from the madhouse, should any be committed to one.\nIn 1812, Babbage transferred to Peterhouse, Cambridge. He was the top mathematician there, but did not graduate with honours. He instead received a degree without examination in 1814. He had defended a thesis that was considered blasphemous in the preliminary public disputation; but it is not known whether this fact is related to his not sitting the examination.\nAfter Cambridge.\nConsidering his reputation, Babbage quickly made progress. He lectured to the Royal Institution on astronomy in 1815, and was elected a Fellow of the Royal Society in 1816. After graduation, on the other hand, he applied for positions unsuccessfully, and had little in the way of career. In 1816 he was a candidate for a teaching job at Haileybury College; he had recommendations from James Ivory and John Playfair, but lost out to Henry Walter. In 1819, Babbage and Herschel visited Paris and the Society of Arcueil, meeting leading French mathematicians and physicists. That year Babbage applied to be professor at the University of Edinburgh, with the recommendation of Pierre Simon Laplace; the post went to William Wallace.\nWith Herschel, Babbage worked on the electrodynamics of Arago's rotations, publishing in 1825. Their explanations were only transitional, being picked up and broadened by Michael Faraday. The phenomena are now part of the theory of eddy currents, and Babbage and Herschel missed some of the clues to unification of electromagnetic theory, staying close to Amp\u00e8re's force law.\nBabbage purchased the actuarial tables of George Barrett, who died in 1821 leaving unpublished work, and surveyed the field in 1826 in \"Comparative View of the Various Institutions for the Assurance of Lives\". This interest followed a project to set up an insurance company, prompted by Francis Baily and mooted in 1824, but not carried out. Babbage did calculate actuarial tables for that scheme, using Equitable Society mortality data from 1762 onwards.\nDuring this whole period Babbage depended awkwardly on his father's support, given his father's attitude to his early marriage, of 1814: he and Edward Ryan wedded the Whitmore sisters. He made a home in Marylebone in London, and founded a large family. On his father's death in 1827, Babbage inherited a large estate (value around \u00a3100,000, equivalent to \u00a3 or $ today), making him independently wealthy. After his wife's death in the same year he spent time travelling. In Italy he met Leopold II, Grand Duke of Tuscany, foreshadowing a later visit to Piedmont. In April 1828 he was in Rome, and relying on Herschel to manage the difference engine project, when he heard that he had become professor at Cambridge, a position he had three times failed to obtain (in 1820, 1823 and 1826).\nRoyal Astronomical Society.\nBabbage was instrumental in founding the Royal Astronomical Society in 1820, initially known as the Astronomical Society of London. Its original aims were to reduce astronomical calculations to a more standard form, and to circulate data. These directions were closely connected with Babbage's ideas on computation, and in 1824 he won its Gold Medal, cited \"for his invention of an engine for calculating mathematical and astronomical tables\".\nBabbage's motivation to overcome errors in tables by mechanisation had been a commonplace since Dionysius Lardner wrote about it in 1834 in the \"Edinburgh Review\" (under Babbage's guidance). The context of these developments is still debated. Babbage's own account of the origin of the difference engine begins with the Astronomical Society's wish to improve \"The Nautical Almanac\". Babbage and Herschel were asked to oversee a trial project, to recalculate some part of those tables. With the results to hand, discrepancies were found. This was in 1821 or 1822, and was the occasion on which Babbage formulated his idea for mechanical computation. The issue of the \"Nautical Almanac\" is now described as a legacy of a polarisation in British science caused by attitudes to Sir Joseph Banks, who had died in 1820.\nBabbage studied the requirements to establish a modern postal system, with his friend Thomas Frederick Colby, concluding there should be a uniform rate that was put into effect with the introduction of the Uniform Fourpenny Post supplanted by the Uniform Penny Post in 1839 and 1840. Colby was another of the founding group of the Society. He was also in charge of the Survey of Ireland. Herschel and Babbage were present at a celebrated operation of that survey, the remeasuring of the Lough Foyle baseline.\nBritish Lagrangian School.\nThe Analytical Society had initially been no more than an undergraduate provocation. During this period it had some more substantial achievements. In 1816 Babbage, Herschel and Peacock published a translation from French of the lectures of Sylvestre Lacroix, which was then the state-of-the-art calculus textbook.\nReference to Lagrange in calculus terms marks out the application of what are now called formal power series. British mathematicians had used them from about 1730 to 1760. As re-introduced, they were not simply applied as notations in differential calculus. They opened up the fields of functional equations (including the difference equations fundamental to the difference engine) and operator (D-module) methods for differential equations. The analogy of difference and differential equations was notationally changing \u0394 to D, as a \"finite\" difference becomes \"infinitesimal\". These symbolic directions became popular, as operational calculus, and pushed to the point of diminishing returns. The Cauchy concept of limit was kept at bay. Woodhouse had already founded this second \"British Lagrangian School\" with its treatment of Taylor series as formal.\nIn this context function composition is complicated to express, because the chain rule is not simply applied to second and higher derivatives. This matter was known to Woodhouse by 1803, who took from Louis Fran\u00e7ois Antoine Arbogast what is now called Fa\u00e0 di Bruno's formula. In essence it was known to Abraham De Moivre (1697). Herschel found the method impressive, Babbage knew of it, and it was later noted by Ada Lovelace as compatible with the analytical engine. In the period to 1820 Babbage worked intensively on functional equations in general, and resisted both conventional finite differences and Arbogast's approach (in which \u0394 and D were related by the simple additive case of the exponential map). But via Herschel he was influenced by Arbogast's ideas in the matter of iteration, i.e. composing a function with itself, possibly many times. Writing in a major paper on functional equations in the \"Philosophical Transactions\" (1815/6), Babbage said his starting point was work of Gaspard Monge.\nAcademic.\nFrom 1828 to 1839, Babbage was Lucasian Professor of Mathematics at Cambridge. Not a conventional resident don, and inattentive to his teaching responsibilities, he wrote three topical books during this period of his life. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1832. Babbage was out of sympathy with colleagues: George Biddell Airy, his predecessor as Lucasian Professor of Mathematics at Trinity College, Cambridge, thought an issue should be made of his lack of interest in lecturing. Babbage planned to lecture in 1831 on political economy. Babbage's reforming direction looked to see university education more inclusive, universities doing more for research, a broader syllabus and more interest in applications; but William Whewell found the programme unacceptable. A controversy Babbage had with Richard Jones lasted for six years. He never did give a lecture.\nIt was during this period that Babbage tried to enter politics. Simon Schaffer writes that his views of the 1830s included disestablishment of the Church of England, a broader political franchise, and inclusion of manufacturers as stakeholders. He twice stood for Parliament as a candidate for the borough of Finsbury. In 1832 he came in third among five candidates, missing out by some 500 votes in the two-member constituency when two other reformist candidates, Thomas Wakley and Christopher Temple, split the vote. In his memoirs Babbage related how this election brought him the friendship of Samuel Rogers: his brother Henry Rogers wished to support Babbage again, but died within days. In 1834 Babbage finished last among four. In 1832, Babbage, Herschel and Ivory were appointed Knights of the Royal Guelphic Order, however they were not subsequently made knights bachelor to entitle them to the prefix \"Sir\", which often came with appointments to that foreign order (though Herschel was later created a baronet).\n\"Declinarians\", learned societies and the BAAS.\nBabbage now emerged as a polemicist. One of his biographers notes that all his books contain a \"campaigning element\". His \"Reflections on the Decline of Science and some of its Causes\" (1830) stands out, however, for its sharp attacks. It aimed to improve British science, and more particularly to oust Davies Gilbert as President of the Royal Society, which Babbage wished to reform. It was written out of pique, when Babbage hoped to become the junior secretary of the Royal Society, as Herschel was the senior, but failed because of his antagonism to Humphry Davy. Michael Faraday had a reply written, by Gerrit Moll, as \"On the Alleged Decline of Science in England\" (1831). On the front of the Royal Society Babbage had no impact, with the bland election of the Duke of Sussex to succeed Gilbert the same year. As a broad manifesto, on the other hand, his \"Decline\" led promptly to the formation in 1831 of the British Association for the Advancement of Science (BAAS).\nThe \"Mechanics' Magazine\" in 1831 identified as Declinarians the followers of Babbage. In an unsympathetic tone it pointed out David Brewster writing in the \"Quarterly Review\" as another leader; with the barb that both Babbage and Brewster had received public money.\nIn the debate of the period on statistics (\"qua\" data collection) and what is now statistical inference, the BAAS in its Statistical Section (which owed something also to Whewell) opted for data collection. This Section was the sixth, established in 1833 with Babbage as chairman and John Elliot Drinkwater as secretary. The foundation of the Statistical Society followed. Babbage was its public face, backed by Richard Jones and Robert Malthus.\n\"On the Economy of Machinery and Manufactures\".\nBabbage published \"On the Economy of Machinery and Manufactures\" (1832), on the organisation of industrial production. It was an influential early work of operational research. John Rennie the Younger in addressing the Institution of Civil Engineers on manufacturing in 1846 mentioned mostly surveys in encyclopaedias, and Babbage's book was first an article in the \"Encyclop\u00e6dia Metropolitana\", the form in which Rennie noted it, in the company of related works by John Farey, Jr., Peter Barlow and Andrew Ure. From \"An essay on the general principles which regulate the application of machinery to manufactures and the mechanical arts\" (1827), which became the \"Encyclop\u00e6dia Metropolitana\" article of 1829, Babbage developed the schematic classification of machines that, combined with discussion of factories, made up the first part of the book. The second part considered the \"domestic and political economy\" of manufactures.\nThe book sold well, and quickly went to a fourth edition (1836). Babbage represented his work as largely a result of actual observations in factories, British and abroad. It was not, in its first edition, intended to address deeper questions of political economy; the second (late 1832) did, with three further chapters including one on piece rate. The book also contained ideas on rational design in factories, and profit sharing.\n\"Babbage principle\".\nIn \"Economy of Machinery\" was described what is now called the \"Babbage principle\". It pointed out commercial advantages available with more careful division of labour. As Babbage himself noted, it had already appeared in the work of Melchiorre Gioia in 1815. The term was introduced in 1974 by Harry Braverman. Related formulations are the \"principle of multiples\" of Philip Sargant Florence, and the \"balance of processes\".\nWhat Babbage remarked is that skilled workers typically spend parts of their time performing tasks that are below their skill level. If the labour process can be divided among several workers, labour costs may be cut by assigning only high-skill tasks to high-cost workers, restricting other tasks to lower-paid workers. He also pointed out that training or apprenticeship can be taken as fixed costs; but that returns to scale are available by his approach of standardisation of tasks, therefore again favouring the factory system. His view of human capital was restricted to minimising the time period for recovery of training costs.\nPublishing.\nAnother aspect of the work was its detailed breakdown of the cost structure of book publishing. Babbage took the unpopular line, from the publishers' perspective, of exposing the trade's profitability. He went as far as to name the organisers of the trade's restrictive practices. Twenty years later he attended a meeting hosted by John Chapman to campaign against the Booksellers Association, still a cartel.\nInfluence.\nIt has been written that \"what Arthur Young was to agriculture, Charles Babbage was to the factory visit and machinery\". Babbage's theories are said to have influenced the layout of the 1851 Great Exhibition, and his views had a strong effect on his contemporary George Julius Poulett Scrope. Karl Marx argued that the source of the productivity of the factory system was exactly the combination of the division of labour with machinery, building on Adam Smith, Babbage and Ure. Where Marx picked up on Babbage and disagreed with Smith was on the motivation for division of labour by the manufacturer: as Babbage did, he wrote that it was for the sake of profitability, rather than productivity, and identified an impact on the concept of a trade.\nJohn Ruskin went further, to oppose completely what manufacturing in Babbage's sense stood for. Babbage also affected the economic thinking of John Stuart Mill. George Holyoake saw Babbage's detailed discussion of profit sharing as substantive, in the tradition of Robert Owen and Charles Fourier, if requiring the attentions of a benevolent captain of industry, and ignored at the time.\nWorks by Babbage and Ure were published in French translation in 1830; \"On the Economy of Machinery\" was translated in 1833 into French by \u00c9douard Biot, and into German the same year by Gottfried Friedenberg. The French engineer and writer on industrial organisation L\u00e9on Lalanne was influenced by Babbage, but also by the economist Claude Lucien Bergery, in reducing the issues to \"technology\". William Jevons connected Babbage's \"economy of labour\" with his own labour experiments of 1870. The Babbage principle is an inherent assumption in Frederick Winslow Taylor's scientific management.\nMary Everest Boole claimed that there was profound influence\u2014via her uncle George Everest\u2014of Indian thought in general and Indian logic, in particular, on Babbage and on her husband George Boole, as well as on Augustus De Morgan:\nThink what must have been the effect of the intense Hinduizing of three such men as Babbage, De Morgan, and George Boole on the mathematical atmosphere of 1830\u201365. What share had it in generating the Vector Analysis and the mathematics by which investigations in physical science are now conducted? \nNatural theology.\nIn 1837, responding to the series of eight \"Bridgewater Treatises\", Babbage published his \"Ninth Bridgewater Treatise\", under the title \"On the Power, Wisdom and Goodness of God, as manifested in the Creation\". In this work Babbage weighed in on the side of uniformitarianism in a current debate. He preferred the conception of creation in which a God-given natural law dominated, removing the need for continuous \"contrivance\".\nThe book is a work of natural theology, and incorporates extracts from related correspondence of Herschel with Charles Lyell. Babbage put forward the thesis that God had the omnipotence and foresight to create as a divine legislator. In this book, Babbage dealt with relating interpretations between science and religion; on the one hand, he insisted that \"there exists no fatal collision between the words of Scripture and the facts of nature;\" on the one hand, he wrote the Book of Genesis was not meant to be read literally in relation to scientific terms. Against those who said these were in conflict, he wrote \"that the contradiction they have imagined can have no real existence, and that whilst the testimony of Moses remains unimpeached, we may also be permitted to confide in the testimony of our senses.\"\nThe Ninth Bridgewater Treatise was quoted extensively in \"Vestiges of the Natural History of Creation\". The parallel with Babbage's computing machines is made explicit, as allowing plausibility to the theory that transmutation of species could be pre-programmed.\nJonar Ganeri, author of \"Indian Logic\", believes Babbage may have been influenced by Indian thought; one possible route would be through Henry Thomas Colebrooke. Mary Everest Boole argues that Babbage was introduced to Indian thought in the 1820s by her uncle George Everest:\nSome time about 1825, [Everest] came to England for two or three years, and made a fast and lifelong friendship with Herschel and with Babbage, who was then quite young. I would ask any fair-minded mathematician to read Babbage's Ninth Bridgewater Treatise and compare it with the works of his contemporaries in England; and then ask himself whence came the peculiar conception of the nature of miracle which underlies Babbage's ideas of Singular Points on Curves (Chap, viii) \u2013 from European Theology or Hindu Metaphysic? Oh! how the English clergy of that day hated Babbage's book!\nReligious views.\nBabbage was raised in the Protestant form of the Christian faith, his family having inculcated in him an orthodox form of worship. He explained:\nRejecting the Athanasian Creed as a \"direct contradiction in terms\", in his youth he looked to Samuel Clarke's works on religion, of which \"Being and Attributes of God\" (1704) exerted a particularly strong influence on him. Later in life, Babbage concluded that \"the true value of the Christian religion rested, not on speculative [theology] \u2026 but \u2026 upon those doctrines of kindness and benevolence which that religion claims and enforces, not merely in favour of man himself but of every creature susceptible of pain or of happiness.\"\nIn his autobiography (1864), Babbage wrote a whole chapter on the topic of religion, where he identified three sources of divine knowledge:\nHe stated, on the basis of the design argument, that studying the works of nature had been the more appealing evidence, and the one which led him to actively profess the existence of God. Advocating for natural theology, he wrote:\nLike Samuel Vince, Babbage also wrote a defence of the belief in divine miracles. Against objections previously posed by David Hume, Babbage advocated for the belief of divine agency, stating \"we must not measure the credibility or incredibility of an event by the narrow sphere of our own experience, nor forget that there is a Divine energy which overrides what we familiarly call the laws of nature.\" He alluded to the limits of human experience, expressing: \"all that we see in a miracle is an effect which is new to our observation, and whose cause is concealed. The cause may be beyond the sphere of our observation, and would be thus beyond the familiar sphere of nature; but this does not make the event a violation of any law of nature. The limits of man's observation lie within very narrow boundaries, and it would be arrogance to suppose that the reach of man's power is to form the limits of the natural world.\"\nLater life.\nThe British Association was consciously modelled on the Deutsche Naturforscher-Versammlung, founded in 1822. It rejected romantic science as well as metaphysics, and started to entrench the divisions of science from literature, and professionals from amateurs. Belonging as he did to the \"Wattite\" faction in the BAAS, represented in particular by James Watt the younger, Babbage identified closely with industrialists. He wanted to go faster in the same directions, and had little time for the more gentlemanly component of its membership. Indeed, he subscribed to a version of conjectural history that placed industrial society as the culmination of human development (and shared this view with Herschel). A clash with Roderick Murchison led in 1838 to his withdrawal from further involvement. At the end of the same year he sent in his resignation as Lucasian professor, walking away also from the Cambridge struggle with Whewell. His interests became more focussed, on computation and metrology, and on international contacts.\nMetrology programme.\nA project announced by Babbage was to tabulate all physical constants (referred to as \"constants of nature\", a phrase in itself a neologism), and then to compile an encyclopaedic work of numerical information. He was a pioneer in the field of \"absolute measurement\". His ideas followed on from those of Johann Christian Poggendorff, and were mentioned to Brewster in 1832. There were to be 19 categories of constants, and Ian Hacking sees these as reflecting in part Babbage's \"eccentric enthusiasms\". Babbage's paper \"On Tables of the Constants of Nature and Art\" was reprinted by the Smithsonian Institution in 1856, with an added note that the physical tables of Arnold Henry Guyot \"will form a part of the important work proposed in this article\".\nExact measurement was also key to the development of machine tools. Here again Babbage is considered a pioneer, with Henry Maudslay, William Sellers, and Joseph Whitworth.\nEngineer and inventor.\nThrough the Royal Society Babbage acquired the friendship of the engineer Marc Brunel. It was through Brunel that Babbage knew of Joseph Clement, and so came to encounter the artisans whom he observed in his work on manufactures. Babbage provided an introduction for Isambard Kingdom Brunel in 1830, for a contact with the proposed Bristol &amp; Birmingham Railway. He carried out studies, around 1838, to show the superiority of the broad gauge for railways, used by Brunel's Great Western Railway.\nIn 1838, Babbage invented the pilot (also called a cow-catcher), the metal frame attached to the front of locomotives that clears the tracks of obstacles; he also constructed a dynamometer car. His eldest son, Benjamin Herschel Babbage, worked as an engineer for Brunel on the railways before emigrating to Australia in the 1850s.\nBabbage also invented an ophthalmoscope, which he gave to Thomas Wharton Jones for testing. Jones, however, ignored it. The device only came into use after being independently invented by Hermann von Helmholtz.\nCryptography.\nBabbage achieved notable results in cryptography, though this was still not known a century after his death. Letter frequency was category 18 of Babbage's tabulation project. Joseph Henry later defended interest in it, in the absence of the facts, as relevant to the management of movable type.\nAs early as 1845, Babbage had solved a cipher that had been posed as a challenge by his nephew Henry Hollier, and in the process, he made a discovery about ciphers that were based on Vigen\u00e8re tables. Specifically, he realised that enciphering plain text with a keyword rendered the cipher text subject to modular arithmetic. During the Crimean War of the 1850s, Babbage broke Vigen\u00e8re's autokey cipher as well as the much weaker cipher that is called Vigen\u00e8re cipher today. His discovery was kept a military secret, and was not published. Credit for the result was instead given to Friedrich Kasiski, a Prussian infantry officer, who made the same discovery some years later. However, in 1854, Babbage published the solution of a Vigen\u00e8re cipher, which had been published previously in the \"Journal of the Society of Arts\". In 1855, Babbage also published a short letter, \"Cypher Writing\", in the same journal. Nevertheless, his priority was not established until 1985.\nPublic nuisances.\nBabbage involved himself in well-publicised but unpopular campaigns against public nuisances. He once counted all the broken panes of glass of a factory, publishing in 1857 a \"Table of the Relative Frequency of the Causes of Breakage of Plate Glass Windows\": Of 464 broken panes, 14 were caused by \"drunken men, women or boys\".\nBabbage's distaste for commoners (the Mob) included writing \"Observations of Street Nuisances\" in 1864, as well as tallying up 165 \"nuisances\" over a period of 80 days. He especially hated street music, and in particular the music of organ grinders, against whom he railed in various venues. The following quotation is typical:\nBabbage was not alone in his campaign. A convert to the cause was the MP Michael Thomas Bass.\nIn the 1860s, Babbage also took up the anti-hoop-rolling campaign. He blamed hoop-rolling boys for driving their iron hoops under horses' legs, with the result that the rider is thrown and very often the horse breaks a leg. Babbage achieved a certain notoriety in this matter, being denounced in debate in Commons in 1864 for \"commencing a crusade against the popular game of tip-cat and the trundling of hoops.\"\nComputing pioneer.\nBabbage's machines were among the first mechanical computers. That they were not actually completed was largely because of funding problems and clashes of personality, most notably with George Biddell Airy, the Astronomer Royal.\nBabbage directed the building of some steam-powered machines that achieved some modest success, suggesting that calculations could be mechanised. For more than ten years he received government funding for his project, which amounted to \u00a317,000, but eventually the Treasury lost confidence in him.\nWhile Babbage's machines were mechanical and unwieldy, their basic architecture was similar to a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make conditional jumps, and the machine had a separate I/O unit.\nBackground on mathematical tables.\nIn Babbage's time, printed mathematical tables were calculated by human computers; in other words, by hand. They were central to navigation, science and engineering, as well as mathematics. Mistakes were known to occur in transcription as well as calculation.\nAt Cambridge, Babbage saw the fallibility of this process, and the opportunity of adding mechanisation into its management. His own account of his path towards mechanical computation references a particular occasion: \nThere was another period, seven years later, when his interest was aroused by the issues around computation of mathematical tables. The French official initiative by Gaspard de Prony, and its problems of implementation, were familiar to him. After the Napoleonic Wars came to a close, scientific contacts were renewed on the level of personal contact: in 1819 Charles Blagden was in Paris looking into the printing of the stalled de Prony project, and lobbying for the support of the Royal Society. In works of the 1820s and 1830s, Babbage referred in detail to de Prony's project.\nDifference engine.\nBabbage began in 1822 with what he called the difference engine, made to compute values of polynomial functions. It was created to calculate a series of values automatically. By using the method of finite differences, it was possible to avoid the need for multiplication and division.\nFor a prototype difference engine, Babbage brought in Joseph Clement to implement the design, in 1823. Clement worked to high standards, but his machine tools were particularly elaborate. Under the standard terms of business of the time, he could charge for their construction, and would also own them. He and Babbage fell out over costs around 1831.\nSome parts of the prototype survive in the Museum of the History of Science, Oxford. This prototype evolved into the \"first difference engine.\" It remained unfinished and the finished portion is located at the Science Museum in London. This first difference engine would have been composed of around 25,000 parts, weighed fifteen tons (13,600\u00a0kg), and would have been tall. Although Babbage received ample funding for the project, it was never completed. He later (1847\u20131849) produced detailed drawings for an improved version,\"Difference Engine No. 2\", but did not receive funding from the British government. His design was finally constructed in 1989\u20131991, using his plans and 19th-century manufacturing tolerances. It performed its first calculation at the Science Museum, London, returning results to 31 digits.\nNine years later, in 2000, the Science Museum completed the printer Babbage had designed for the difference engine.\nCompleted models.\nThe Science Museum has constructed two Difference Engines according to Babbage's plans for the Difference Engine No 2. One is owned by the museum. The other, owned by the technology multimillionaire Nathan Myhrvold, went on exhibition at the Computer History Museum in Mountain View, California on 10 May 2008. The two models that have been constructed are not replicas; Myhrvold's engine is the first design by Babbage, and the Science Museum's is a later model.\nAnalytical Engine.\nAfter the attempt at making the first difference engine fell through, Babbage worked to design a more complex machine called the Analytical Engine. He hired C. G. Jarvis, who had previously worked for Clement as a draughtsman. The Analytical Engine marks the transition from mechanised arithmetic to fully-fledged general purpose computation. It is largely on it that Babbage's standing as computer pioneer rests.\nThe major innovation was that the Analytical Engine was to be programmed using punched cards: the Engine was intended to use loops of Jacquard's punched cards to control a mechanical calculator, which could use as input the results of preceding computations. The machine was also intended to employ several features subsequently used in modern computers, including sequential control, branching and looping. It would have been the first mechanical device to be, in principle, Turing-complete. The Engine was not a single physical machine, but rather a succession of designs that Babbage tinkered with until his death in 1871.\nAda Lovelace and Italian followers.\nAda Lovelace, who corresponded with Babbage during his development of the Analytical Engine, is credited with developing an algorithm that would enable the Engine to calculate a sequence of Bernoulli numbers. Despite documentary evidence in Lovelace's own handwriting, some scholars dispute to what extent the ideas were Lovelace's own. For this achievement, she is often described as the first computer programmer; though no programming language had yet been invented.\nLovelace also translated and wrote literature supporting the project. Describing the engine's programming by punch cards, she wrote: \"We may say most aptly that the Analytical Engine weaves algebraical patterns just as the Jacquard loom weaves flowers and leaves.\"\nBabbage visited Turin in 1840 at the invitation of Giovanni Plana. In 1842 Charles Wheatstone approached Lovelace to translate a paper of Luigi Menabrea, who had taken notes of Babbage's Turin talks; and Babbage asked her to add something of her own. Fortunato Prandi who acted as interpreter in Turin was an Italian exile and follower of Giuseppe Mazzini.\nSwedish followers.\nPer Georg Scheutz wrote about the difference engine in 1830, and experimented in automated computation. After 1834 and Lardner's \"Edinburgh Review\" article he set up a project of his own, doubting whether Babbage's initial plan could be carried out. This he pushed through with his son, Edvard Scheutz. Another Swedish engine was that of Martin Wiberg (1860).\nLegacy.\nIn 2011, researchers in Britain proposed a multimillion-pound project, \"Plan 28\", to construct Babbage's Analytical Engine. Since Babbage's plans were continually being refined and were never completed, they intended to engage the public in the project and crowd-source the analysis of what should be built. It would have the equivalent of 675 bytes of memory, and run at a clock speed of about 7\u00a0Hz. They hope to complete it by the 150th anniversary of Babbage's death, in 2021.\nAdvances in MEMS and nanotechnology have led to recent high-tech experiments in mechanical computation. The benefits suggested include operation in high radiation or high temperature environments. These modern versions of mechanical computation were highlighted in \"The Economist\" in its special \"end of the millennium\" black cover issue in an article entitled \"Babbage's Last Laugh\".\nDue to his association with the town Babbage was chosen in 2007 to appear on the 5 Totnes pound note. An image of Babbage features in the British cultural icons section of the newly designed British passport in 2015.\nFamily.\nOn 25 July 1814, Babbage married Georgiana Whitmore at St. Michael's Church in Teignmouth, Devon; her sister Louisa married Sir Edward Ryan five months later. The couple lived at Dudmaston Hall, Shropshire (where Babbage engineered the central heating system), before moving to 5 Devonshire Street, London in 1815.\nCharles and Georgiana had eight children, but only four \u2013 Benjamin Herschel, Georgiana Whitmore, Dugald Bromhead and Henry Prevost \u2013 survived childhood. Charles' wife Georgiana died in Worcester on 1 September 1827, the same year as his father, their second son (also named Charles) and their newborn son Alexander.\nHis youngest surviving son, Henry Prevost Babbage (1824\u20131918), went on to create six small demonstration pieces for Difference Engine No. 1 based on his father's designs, one of which was sent to Harvard University where it was later discovered by Howard H. Aiken, pioneer of the Harvard Mark I. Henry Prevost's 1910 Analytical Engine Mill, previously on display at Dudmaston Hall, is now on display at the Science Museum.\nDeath.\nBabbage lived and worked for over 40 years at 1 Dorset Street, Marylebone, where he died, at the age of 79, on 18 October 1871; he was buried in London's Kensal Green Cemetery. According to Horsley, Babbage died \"of renal inadequacy, secondary to cystitis.\" He had declined both a knighthood and baronetcy. He also argued against hereditary peerages, favouring life peerages instead.\nAutopsy report.\nIn 1983, the autopsy report for Charles Babbage was discovered and later published by his great-great-grandson. A copy of the original is also available. Half of Babbage's brain is preserved at the Hunterian Museum in the Royal College of Surgeons in London. The other half of Babbage's brain is on display in the Science Museum, London.\nMemorials.\nThere is a black plaque commemorating the 40 years Babbage spent at 1 Dorset Street, London. Locations, institutions and other things named after Babbage include:\nIn fiction and film.\nBabbage frequently appears in steampunk works; he has been called an iconic figure of the genre. Other works in which Babbage appears include:"}
{"id": "5700", "revid": "81778", "url": "https://en.wikipedia.org/wiki?curid=5700", "title": "Cross-dressing", "text": "Cross-dressing is the act of wearing items of clothing not commonly associated with one's sex. Cross-dressing has been used for purposes of disguise, comfort, and self-expression in modern times and throughout history.\nAlmost every human society throughout history has had expected norms for each gender relating to style, color, or type of clothing they are expected to wear, and likewise most societies have had a set of guidelines, views or even laws defining what type of clothing is appropriate for each gender.\nThe term \"cross-dressing\" refers to an action or a behavior, without attributing or implying any specific causes or motives for that behavior. Cross-dressing is not synonymous with being transgender.\nTerminology.\nThe phenomenon of cross-dressing is an old recorded practice, being referred to as far back as the Hebrew Bible. However, the terms to describe it change. The Anglo-Saxon \"cross-dresser\" has largely superseded the Latinate \"transvestite\", which has come to be seen as outdated and derogatory. This is because the latter was historically used to diagnose psychiatric disorders (e.g. transvestic fetishism), but the former was coined by the transgender community. The Oxford English Dictionary gives 1911 as the earliest citation, by Edward Carpenter: \"Cross-dressing must be taken as a general indication of, and a cognate phenomenon to, homosexuality\". In 1928 Havelock Ellis used the two terms, \"cross-dressing\" and \"transvestism\", interchangeably. The earliest citations for \"cross-dress\" and \"cross-dresser\" are 1966 and 1976 respectively.\nHistory.\nCross-dressing has been practiced throughout much of recorded history, in many societies, and for many reasons. Examples exist in Greek, Norse, and Hindu mythology. Cross-dressing can be found in folklore, literature, theater, and music, such as Kabuki and Korean shamanism. In the British and European context, theatrical troupes (\"playing companies\") were all-male, with the female parts undertaken by boy players.\nA variety of historical figures are known to have cross-dressed to varying degrees. Many women found they had to disguise themselves as men in order to participate in the wider world. For example, Margaret King cross-dressed in the early nineteenth century to attend medical school, as none would accept female students. A century later, Vita Sackville-West dressed as a young soldier in order to \"walk out\" with her girlfriend Violet Keppel, to avoid the street harassment that two women would have faced. The prohibition on women wearing male garb, once strictly applied, still has echoes today in some Western societies which require girls and women to wear skirts, for example as part of school uniform or office dress codes. In some countries, even in casual settings, women are still prohibited from wearing traditionally male clothing. Sometimes all trousers, no matter how loose and long, are automatically considered \"indecent\", which may render their wearer subject to severe punishment, as in the case of Lubna al-Hussein in Sudan in 2009.\nVarieties.\nThere are many different kinds of cross-dressing and many different reasons why an individual might engage in cross-dressing behavior. Some people cross-dress as a matter of comfort or style, a personal preference for clothing associated with the opposite sex. Some people cross-dress to shock others or challenge social norms; others will limit their cross-dressing to underwear, so that it is not apparent. Some people attempt to pass as a member of the opposite sex in order to gain access to places or resources they would not otherwise be able to reach.\nGender disguise.\nGender disguise has been used by women and girls to pass as male, and by men and boys to pass as female. Gender disguise has also been used as a plot device in storytelling, particularly in narrative ballads, and is a recurring motif in literature, theater, and film. Historically, some women have cross-dressed to take up male-dominated or male-exclusive professions, such as military service. Conversely, some men have cross-dressed to escape from mandatory military service or as a disguise to assist in political or social protest, as men in Wales did in the Rebecca Riots and when conducting Ceffyl Pren as a form of mob justice.\nUndercover journalism may require cross-dressing, as with Norah Vincent's project \"Self-Made Man\".\nSome girls in Afghanistan, long after the fall of the Taliban, are still disguised by their families as boys. This is known as \"bacha posh\".\nTheater and performance.\nSingle-sex theatrical troupes often have some performers who cross-dress to play roles written for members of the opposite sex (travesti and trouser roles). Cross-dressing, particularly the depiction of males wearing dresses, is often used for comic effect onstage and on-screen.\nDrag is a special form of performance art based on the act of cross-dressing. A drag queen is usually a male-assigned person who performs as an exaggeratedly feminine character, in heightened costuming sometimes consisting of a showy dress, high-heeled shoes, obvious make-up, and wig. A drag queen may imitate famous female film or pop-music stars. A faux queen is a female-assigned person employing the same techniques. A drag king is a counterpart of the drag queen - a female-assigned person who adopts a masculine persona in performance or imitates a male film or pop-music star. Some female-assigned people undergoing gender reassignment therapy also self-identify as \"drag kings\" although this use of \"drag king\" would generally be considered inaccurate.\nThe modern activity of battle reenactments has raised the question of women passing as male soldiers. In 1989, Lauren Burgess dressed as a male soldier in a U.S. National Park Service reenactment of the Battle of Antietam, and was ejected after she was discovered to be a woman. Burgess sued the Park Service for sexual discrimination. The case spurred spirited debate among Civil War buffs. In 1993, a federal judge ruled in Burgess's favor.\n\"Wigging\" refers to the practice of male stunt doubles taking the place of an actress, parallel to \"paint downs\", where white stunt doubles are made up to resemble black actors. Female stunt doubles have begun to protest this norm of \"historical sexism\", saying that it restricts their already limited job possibilities.\nSexual fetishes.\nA transvestic fetishist is a person who cross-dresses as part of a sexual fetish. According to the fourth edition of \"Diagnostic and Statistical Manual of Mental Disorders\", this fetishism was limited to heterosexual men; however, DSM-5 does not have this restriction, and opens it to women and men, regardless of their sexual orientation.\nSometimes either member of a heterosexual couple will cross-dress in order to arouse the other. For example, the male might wear skirts or lingerie and/or the female will wear boxers or other male clothing. (See also forced feminization)\nPassing or not.\nSome people who cross-dress may endeavor to project a complete impression of belonging to another gender, including mannerisms, speech patterns, and emulation of sexual characteristics. This is referred to as passing or \"trying to pass,\" depending how successful the person is. An observer who sees through the cross-dresser's attempt to pass is said to have \"read\" or \"clocked\" them. There are videos, books, and magazines on how a man may look more like a woman.\nOthers may choose to take a mixed approach, adopting some feminine traits and some masculine traits in their appearance. For instance, a man might wear both a dress and a beard. This is sometimes known as \"genderfuck\". In a broader context, cross-dressing may also refer to other actions undertaken to pass as a particular sex, such as packing (accentuating the male crotch bulge) or, the opposite, tucking (concealing the male crotch bulge).\nClothes.\nThe actual determination of cross-dressing is largely socially constructed. For example, in Western society, trousers have long been adopted for usage by women, and it is no longer regarded as cross-dressing. In cultures where men have traditionally worn skirt-like garments such as the kilt or sarong, these are not seen as women's clothing, and wearing them is not seen as cross-dressing for men. As societies are becoming more global in nature, both men's and women's clothing are adopting styles of dress associated with other cultures.\nCosplaying may also involve cross-dressing, for some females may wish to dress as a male, and vice versa (see Crossplay). Breast binding (for females) is not uncommon and is one of the things likely needed to cosplay a male character.\nIn most parts of the world it remains socially disapproved for men to wear clothes traditionally associated with women. Attempts are occasionally made, e.g. by fashion designers, to promote the acceptance of skirts as everyday wear for men. Cross-dressers have complained that society permits women to wear pants or jeans and other masculine clothing, while condemning any man who wants to wear clothing sold for women.\nWhile creating a more feminine figure, male cross-dressers will often utilize different types and styles of breast forms, which are silicone prostheses traditionally used by women who have undergone mastectomies to recreate the visual appearance of a breast.\nWhile most male cross-dressers utilize clothing associated with modern women, some are involved in subcultures that involve dressing as little girls or in vintage clothing. Some such men have written that they enjoy dressing as femininely as possible, so they wear frilly dresses with lace and ribbons, bridal gowns complete with veils, as well as multiple petticoats, corsets, girdles and/or garter belts with nylon stockings.\nThe term \"underdressing\" is used by male cross-dressers to describe wearing female undergarments such as panties under their male clothes. The famous low-budget film-maker Edward D. Wood, Jr. said he often wore women's underwear under his military uniform as a Marine during World War II. \"Female masking\" is a form of cross-dressing in which men wear masks that present them as female.\nSocial issues.\nCross-dressers may begin wearing clothing associated with the opposite sex in childhood, using the clothes of a sibling, parent, or friend. Some parents have said they allowed their children to cross-dress and, in many cases, the child stopped when they became older. The same pattern often continues into adulthood, where there may be confrontations with a spouse, partner, family member or friend. Married cross-dressers can experience considerable anxiety and guilt if their spouse objects to their behavior.\nSometimes because of guilt or other reasons cross-dressers dispose of all their clothing, a practice called \"purging\", only to start collecting other gender's clothing again.\nFestivals.\nCelebrations of cross-dressing occur in widespread cultures. The Abissa festival in C\u00f4te d'Ivoire, Ofudamaki in Japan, and Kottankulangara Festival in India are all examples of this.\nAnalysis.\nAdvocacy for social change has done much to relax the constrictions of gender roles on men and women, but they are still subject to prejudice from some people. It is noticeable that as 'transgender' is becoming more socially accepted as a normal human condition, the prejudices against cross-dressing are changing quite quickly, just as the similar prejudices against homosexuals have changed rapidly in recent decades.\nThe reason it is so hard to have statistics for female-assigned cross-dressers is that the line where cross-dressing stops and cross-dressing begins has become blurred, whereas the same line for men is as well defined as ever. This is one of the many issues being addressed by third wave feminism as well as the modern-day masculist movement.\nThe general culture has very mixed views about cross-dressing. A woman who wears her husband's shirt to bed is considered attractive while a man who wears his wife's nightgown to bed may be considered transgressive. Marlene Dietrich in a tuxedo was considered very erotic; Jack Lemmon in a dress was considered ridiculous. All this may result from an overall gender role rigidity for males; that is, because of the prevalent gender dynamic throughout the world, men frequently encounter discrimination when deviating from masculine gender norms, particularly violations of heteronormativity. A man's adoption of feminine clothing is often considered a going down in the gendered social order whereas a woman's adoption of what are traditionally men's clothing (at least in the English-speaking world) has less of an impact because women have been traditionally subordinate to men, unable to affect serious change through style of dress. Thus when a male cross-dresser puts on his clothes, he transforms into the quasi-female and thereby becomes an embodiment of the conflicted gender dynamic. Following the work of Butler, gender proceeds along through ritualized performances, but in male cross-dressing it becomes a performative \"breaking\" of the masculine and a \"subversive repetition\" of the feminine.\nPsychoanalysts today do not regard cross-dressing by itself as a psychological problem, unless it interferes with a person's life. \"For instance,\" said Dr. Joseph Merlino, senior editor of \"Freud at 150: 21st Century Essays on a Man of Genius\", \"[suppose that]...I'm a cross-dresser and I don't want to keep it confined to my circle of friends, or my party circle, and I want to take that to my wife and I don't understand why she doesn't accept it, or I take it to my office and I don't understand why they don't accept it, then it's become a problem because it's interfering with my relationships and environment.\"\nBritish pantomime, television and comedy.\nCross-dressing is a traditional popular trope in British comedy. The Pantomime dame in British pantomime dates from the 19th century, which is part of the theatrical tradition of female characters portrayed by male actors in drag. Widow Twankey (Aladdin\u2019s mother) is a popular pantomime dame: in 2004 Ian McKellen played the role.\nThe Monty Python comedy troupe donned frocks and makeup, playing female roles while speaking in falsetto. Character comics such as Benny Hill and Dick Emery drew upon several female identities. In the BBC's long-running sketch show \"The Dick Emery Show\" (broadcast from 1963 to 1981), Emery played Mandy, a busty peroxide blonde whose catchphrase, \"Ooh, you are awful ... but I like you!\", was given in response to a seemingly innocent remark made by her interviewer, but perceived by her as ribald double entendre. The popular tradition of cross dressing in British comedy extended to the 1984 music video for Queen's \"I Want to Break Free\" where the band parody several female characters from the soap opera \"Coronation Street\".\nLiterature.\nWomen dressed as men, and less often men dressed as women, is a common trope in fiction and folklore. For example, in Norse myth, Thor disguised himself as Freya. These disguises were also popular in Gothic fiction, such as in works by Charles Dickens, Alexandre Dumas, p\u00e8re, and Eug\u00e8ne Sue, and in a number of Shakespeare's plays, such as \"Twelfth Night\". In \"The Wind in the Willows\", Toad dresses as a washerwoman, and in \"Lord of the Rings\", \u00c9owyn pretends to be a man.\nIn science fiction, fantasy and women's literature, this literary motif is occasionally taken further, with literal transformation of a character from male to female or vice versa. Virginia Woolf's \"\" focuses on a man who becomes a woman, as does a warrior in Peter S. Beagle's \"The Innkeeper's Song\"; while in Geoff Ryman's \"The Warrior Who Carried Life\", Cara magically transforms herself into a man.\nOther popular examples of gender disguise include \"Madame Doubtfire\" (published as \"Alias Madame Doubtfire\" in the United States) and its movie adaptation \"Mrs. Doubtfire\", featuring a man disguised as a woman. Similarly, the movie \"Tootsie\" features Dustin Hoffman disguised as a woman, while the movie \"The Associate\" features Whoopi Goldberg disguised as a man.\nMedical views.\nThe 10th edition of the International Statistical Classification of Diseases and Related Health Problems lists \"dual-role transvestism\" (non-sexual cross-dressing) and \"fetishistic transvestism\" (cross-dressing for sexual pleasure) as disorders. Both listings were removed for the 11th edition. \nTransvestic fetishism is a paraphilia and a psychiatric diagnosis in the DSM-5 version of the \"Diagnostic and Statistical Manual of Mental Disorders\"."}
{"id": "5702", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=5702", "title": "Channel Tunnel", "text": "The Channel Tunnel (), also referred to as the Eurotunnel or Chunnel, is a railway tunnel that connects Folkestone (Kent, England, UK) with Coquelles (Hauts-de-France, France) beneath the English Channel at the Strait of Dover. It is the only fixed link between the island of Great Britain and the European mainland. At its lowest point, it is deep below the sea bed and below sea level. At , the tunnel has the longest underwater section of any tunnel in the world, and is the third longest railway tunnel in the world. The speed limit for trains through the tunnel is 160\u00a0km/h (100\u00a0mph).\nThe tunnel carries high-speed Eurostar passenger trains, the Eurotunnel Shuttle for road vehicles \u2013 the largest such transport in the world \u2013 and international freight trains. The tunnel connects end-to-end with the high-speed railway lines of the LGV Nord in France and High Speed 1 in England. In 2017, through rail services carried 10.3 million passengers and 1.22 million tonnes of freight, and the Shuttle carried 10.4 million passengers, 2.6 million cars, 51,000 coaches, and 1.6 million lorries (equivalent to 21.3 million tonnes of freight). This compares with 11.7 million passengers, 2.6 million lorries and 2.2 million cars by sea through the Port of Dover.\nPlans to build a cross-Channel fixed link appeared as early as 1802, but British political and media pressure over the compromising of national security had disrupted attempts to build a tunnel. An early unsuccessful attempt at building a tunnel was made in the late 19th century, on the English side, \"in the hope of forcing the hand of the English Government\". The eventual successful project, organised by Eurotunnel, began construction in 1988 and opened in 1994. Valued at \u00a35.5 billion in 1985, it was at the time the most expensive construction project ever proposed. The cost finally amounted to \u00a39 billion (equivalent to \u00a3 billion in ), well over its predicted budget.\nSince its construction, the tunnel has experienced a few mechanical problems. Both fires and cold weather have temporarily disrupted its operation.\nSince at least 1997, people have attempted to use the tunnel to travel illegally to the UK, causing many migrants to head towards Calais and creating ongoing issues of human rights violations, illegal immigration, diplomatic disagreement, and violence.\nOrigins.\nEarlier proposals.\nIn 1802, Albert Mathieu-Favier, a French mining engineer, put forward a proposal to tunnel under the English Channel, with illumination from oil lamps, horse-drawn coaches, and an artificial island positioned mid-Channel for changing horses. Mathieu-Favier's design envisaged a bored two-level tunnel with the top tunnel used for transport and the bottom one for groundwater flows.\nIn 1839, Aim\u00e9 Thom\u00e9 de Gamond, a Frenchman, performed the first geological and hydrographical surveys on the Channel, between Calais and Dover. Thom\u00e9 de Gamond explored several schemes and, in 1856, he presented a proposal to Napoleon III for a mined railway tunnel from Cap Gris-Nez to East Wear Point with a port/airshaft on the Varne sandbank at a cost of 170\u00a0million francs, or less than \u00a37\u00a0million.\nIn 1865, a deputation led by George Ward Hunt proposed the idea of a tunnel to the Chancellor of the Exchequer of the day, William Ewart Gladstone.\nAround 1866, William Low and Sir John Hawkshaw promoted ideas, but apart from preliminary geological studies none were implemented.\nAn official Anglo-French protocol was established in 1876 for a cross-Channel railway tunnel.\nIn 1881, the British railway entrepreneur Sir Edward Watkin and Alexandre Lavalley, a French Suez Canal contractor, were in the Anglo-French Submarine Railway Company that conducted exploratory work on both sides of the Channel. On the English side a diameter Beaumont-English boring machine dug a pilot tunnel from Shakespeare Cliff. On the French side, a similar machine dug from Sangatte. The project was abandoned in May 1882, owing to British political and press campaigns asserting that a tunnel would compromise Britain's national defences. These early works were encountered more than a century later during the TML project.\nA 1907 film, \"Tunnelling the English Channel\" by pioneer filmmaker Georges M\u00e9li\u00e8s, depicts King Edward VII and President Armand Falli\u00e8res dreaming of building a tunnel under the English Channel.\nIn 1919, during the Paris Peace Conference, the British prime minister, David Lloyd George, repeatedly brought up the idea of a Channel tunnel as a way of reassuring France about British willingness to defend against another German attack. The French did not take the idea seriously, and nothing came of Lloyd George's proposal.\nIn the 1920s, Winston Churchill had advocated for the Channel Tunnel, using that exact name in an essay entitled \"Should Strategists Veto The Tunnel?\" The essay was published on 27 July 1924 in the \"Weekly Dispatch\", and argued vehemently against the idea that the tunnel could be used by a Continental enemy in an invasion of Britain. Churchill expressed his enthusiasm for the project again in an article for the \"Daily Mail\" on 12 February 1936, \"Why Not A Channel Tunnel?\"\nThere was another proposal in 1929, but nothing came of this discussion and the idea was shelved. Proponents estimated the construction cost at US$150 million. The engineers had addressed the concerns of both nations' military leaders by designing two sumps\u2014one near the coast of each country\u2014that could be flooded at will to block the tunnel. But this did not appease military leaders, and other concerns about hordes of tourists who would disrupt English life. Military fears continued during the Second World War. After the fall of France, as Britain prepared for an expected German invasion, a Royal Navy officer in the Directorate of Miscellaneous Weapons Development calculated that Hitler could use slave labour to build two Channel tunnels in 18 months. The estimate caused rumours that Germany had already begun digging.\nA British film from Gaumont Studios, \"The Tunnel\" (also called \"TransAtlantic Tunnel\"), was released in 1935 as a futuristic science fiction project concerning the creation of a transatlantic tunnel. It referred briefly to its protagonist, a Mr. McAllan, as having completed a British Channel tunnel successfully in 1940, five years into the future of the film's release.\nBy 1955, defence arguments had become less relevant due to the dominance of air power, and both the British and French governments supported technical and geological surveys. In 1958 the 1881 workings were cleared in preparation for a \u00a3100,000 geological survey by the Channel Tunnel Study Group. 30% of the funding came from the Channel Tunnel Co Ltd, the largest shareholder of which was the British Transport Commission, as successor to the South Eastern Railway. A detailed geological survey was carried out in 1964 and 1965.\nAlthough the two countries agreed to build a tunnel in 1964, the phase 1 initial studies and signing of a second agreement to cover phase 2 took until 1973. Construction work of this government-funded project to create two tunnels designed to accommodate car shuttle wagons on either side of a service tunnel started on both sides of the Channel in 1974.\nOn 20 January 1975, to the dismay of their French partners, the then-governing Labour Party in Britain cancelled the project due to uncertainty about EEC membership, doubling cost estimates and the general economic crisis at the time. By this time the British tunnel boring machine was ready and the Ministry of Transport was able to do a experimental drive. This short tunnel was reused as the starting and access point for tunnelling operations from the British side. The cancellation costs were estimated to be \u00a317 million. On the French side, a tunnel boring machine had been installed underground in a stub tunnel. It lay there for 14 years until 1988, when it was sold, dismantled, refurbished and shipped to Turkey where it was used to drive the Moda tunnel for the Istanbul Sewerage Scheme, designed and supervised by British Civil Engineers Binnie &amp; Partners, and officially opened by Margaret Thatcher in 1989.\nInitiation of project.\nIn 1979, the \"Mouse-hole Project\" was suggested when the Conservatives came to power in Britain. The concept was a single-track rail tunnel with a service tunnel, but without shuttle terminals. The British government took no interest in funding the project, but the British Prime Minister Margaret Thatcher did not object to a privately funded project, although she said she assumed it would be for cars rather than trains. In 1981, Thatcher and the French president Fran\u00e7ois Mitterrand agreed to establish a working group to evaluate a privately funded project. In June 1982 the Franco-British study group favoured a twin tunnel to accommodate conventional trains and a vehicle shuttle service. In April 1985 promoters were invited to submit scheme proposals. Four submissions were shortlisted:\nThe cross-Channel ferry industry protested under the name \"Flexilink\". In 1975 there was no campaign protesting against a fixed link, with one of the largest ferry operators (Sealink) being state-owned. Flexilink continued rousing opposition throughout 1986 and 1987. Public opinion strongly favoured a drive-through tunnel, but concerns about ventilation, accident management and driver mesmerisation led to the only shortlisted rail submission, CTG/F-M, being awarded the project in January 1986. Reasons given for the selection included that it caused least disruption to shipping in the Channel, least environmental disruption, was the best protected against terrorism, and was the most likely to attract sufficient private finance.\nArrangement.\nThe British \"Channel Tunnel Group\" consisted of two banks and five construction companies, while their French counterparts, \"France\u2013Manche\", consisted of three banks and five construction companies. The role of the banks was to advise on financing and secure loan commitments. On 2 July 1985, the groups formed Channel Tunnel Group/France\u2013Manche (CTG/F\u2013M). Their submission to the British and French governments was drawn from the 1975 project, including 11\u00a0volumes and a substantial environmental impact statement.\nAnglo-French Treaty on the Channel Tunnel was signed by both governments in Canterbury Cathedral. The Treaty of Canterbury (1986) prepared the Concession for the construction and operation of the Fixed Link by privately owned companies. It outlines the methods to be used for arbitration in the event of a dispute. It sets up the Intergovernmental Commission (IGC) which is responsible for monitoring all matters associated with the construction and operation of the Tunnel on behalf of the British and French governments, together with a Safety Authority to advise the IGC.\nIt draws a land frontier between the two countries in the middle of the Channel tunnel \u2013 the first of its kind.\nThe design and construction was done by the ten construction companies in the CTG/F-M group. The French terminal and boring from Sangatte was done by the five French construction companies in the joint venture group \"GIE Transmanche Construction\". The English Terminal and boring from Shakespeare Cliff was done by the five British construction companies in the \"Translink Joint Venture\". The two partnerships were linked by a bi-national project organisation called TransManche Link (TML). The \"Ma\u00eetre d'Oeuvre\" was a supervisory engineering body employed by Eurotunnel under the terms of the concession that monitored the project and reported to the governments and banks.\nIn France, with its long tradition of infrastructure investment, the project had widespread approval. The French National Assembly approved it unanimously in April 1987, and after a public inquiry, the Senate approved it unanimously in June. In Britain, select committees examined the proposal, making history by holding hearings away from Westminster, in Kent. In February 1987, the third reading of the Channel Tunnel Bill took place in the House of Commons, and passed by 94\u00a0votes to 22. The Channel Tunnel Act gained Royal assent and passed into law in July. Parliamentary support for the project came partly from provincial members of Parliament on the basis of promises of regional Eurostar through train services that never materialised; the promises were repeated in 1996 when the contract for construction of the Channel Tunnel Rail Link was awarded.\nCost.\nThe tunnel is a build-own-operate-transfer (BOOT) project with a concession.&lt;ref name=\"Flyvbjerg p. 96/97\"&gt;Flyvbjerg et al. pp.\u00a096\u201397&lt;/ref&gt; TML would design and build the tunnel, but financing was through a separate legal entity, Eurotunnel. Eurotunnel absorbed CTG/F-M and signed a construction contract with TML, but the British and French governments controlled final engineering and safety decisions, now in the hands of the Channel Tunnel Safety Authority. The British and French governments gave Eurotunnel a 55-year operating concession (from 1987; extended by 10 years to 65 years in 1993) to repay loans and pay dividends. A Railway Usage Agreement was signed between Eurotunnel, British Rail and SNCF guaranteeing future revenue in exchange for the railways obtaining half of the tunnel's capacity.\nPrivate funding for such a complex infrastructure project was of unprecedented scale. An initial equity of \u00a345\u00a0million was raised by CTG/F-M, increased by \u00a3206\u00a0million private institutional placement, \u00a3770\u00a0million was raised in a public share offer that included press and television advertisements, a syndicated bank loan and letter of credit arranged \u00a35\u00a0billion. Privately financed, the total investment costs at 1985 prices were \u00a32.6\u00a0billion. At the 1994 completion actual costs were, in 1985 prices, \u00a34.65\u00a0billion: an 80% cost overrun. The cost overrun was partly due to enhanced safety, security, and environmental demands. Financing costs were 140% higher than forecast.\nConstruction.\nWorking from both the English side and the French side of the Channel, eleven tunnel boring machines or TBMs cut through chalk marl to construct two rail tunnels and a service tunnel. The vehicle shuttle terminals are at Cheriton (part of Folkestone) and Coquelles, and are connected to the English M20 and French A16 motorways respectively.\nTunnelling commenced in 1988, and the tunnel began operating in 1994. In 1985 prices, the total construction cost was \u00a34.65\u00a0billion (equivalent to \u00a3\u00a0billion in 2015), an 80% cost overrun. At the peak of construction 15,000\u00a0people were employed with daily expenditure over \u00a33\u00a0million. Ten workers, eight of them British, were killed during construction between 1987 and 1993, most in the first few months of boring.\nCompletion.\nA 50\u00a0mm (2\u00a0in) diameter pilot hole allowed the service tunnel to break through without ceremony on 30 October 1990. On 1 December 1990, Englishman Graham Fagg and Frenchman Phillippe Cozette broke through the service tunnel with the media watching. Eurotunnel completed the tunnel on time. (A BBC TV television commentator called Graham Fagg \"the first man to cross the Channel by land for 8000 years\".) The two tunnelling efforts met each other with an offset of only 36.2\u00a0cm.\nThe tunnel was officially opened, one year later than originally planned, by Queen Elizabeth II and the French president, Fran\u00e7ois Mitterrand, in a ceremony held in Calais on 6 May 1994. The Queen travelled through the tunnel to Calais on a Eurostar train, which stopped nose to nose with the train that carried President Mitterrand from Paris. Following the ceremony President Mitterrand and the Queen travelled on Le Shuttle to a similar ceremony in Folkestone. A full public service did not start for several months. The first freight train, however, ran on 1 June 1994 and carried Rover and Mini cars being exported to Italy.\nThe Channel Tunnel Rail Link (CTRL), now called High Speed 1, runs from St Pancras railway station in London to the tunnel portal at Folkestone in Kent. It cost \u00a35.8\u00a0billion. On 16 September 2003 the prime minister, Tony Blair, opened the first section of High Speed 1, from Folkestone to north Kent. On 6 November 2007 the Queen officially opened High Speed 1 and St Pancras International station, replacing the original slower link to Waterloo International railway station. High Speed 1 trains travel at up to , the journey from London to Paris taking 2\u00a0hours 15\u00a0minutes, to Brussels 1\u00a0hour 51\u00a0minutes.\nIn 1994, the American Society of Civil Engineers elected the tunnel as one of the seven modern Wonders of the World. In 1995, the American magazine \"Popular Mechanics\" published the results.\nOpening dates.\nOpening was phased for various services offered as the Channel Tunnel Safety Authority, the IGC, gave permission for various services to begin at several dates over the period 1994/1995 but start up dates were a few days later.\nEngineering.\nSurveying undertaken in the 20 years before construction confirmed earlier speculations that a tunnel could be bored through a chalk marl stratum. The chalk marl is conducive to tunnelling, with impermeability, ease of excavation and strength. The chalk marl runs along the entire length of the English side of the tunnel, but on the French side a length of has variable and difficult geology. The tunnel consists of three bores: two diameter rail tunnels, apart, in length with a diameter service tunnel in between. The three bores are connected by cross-passages and piston relief ducts. The service tunnel was used as a pilot tunnel, boring ahead of the main tunnels to determine the conditions. English access was provided at Shakespeare Cliff, French access from a shaft at Sangatte. The French side used five tunnel boring machines (TBMs), the English side six. The service tunnel uses Service Tunnel Transport System (STTS) and Light Service Tunnel Vehicles (LADOGS). Fire safety was a critical design issue.\nBetween the portals at Beussingue and Castle Hill the tunnel is long, with under land on the French side and on the UK side, and under sea. It is the third-longest rail tunnel in the world, behind the Gotthard Base Tunnel in Switzerland and the Seikan Tunnel in Japan, but with the longest under-sea section. The average depth is below the seabed. On the UK side, of the expected of spoil approximately was used for fill at the terminal site, and the remainder was deposited at Lower Shakespeare Cliff behind a seawall, reclaiming of land. This land was then made into the Samphire Hoe Country Park. Environmental impact assessment did not identify any major risks for the project, and further studies into safety, noise, and air pollution were overall positive. However, environmental objections were raised over a high-speed link to London.\nGeology.\nSuccessful tunnelling required a sound understanding of the topography and geology and the selection of the best rock strata through which to dig. The geology of this site generally consists of northeasterly dipping Cretaceous strata, part of the northern limb of the Wealden-Boulonnais dome. Characteristics include:\nOn the English side, the stratum dip is less than 5\u00b0; on the French side this increases to 20\u00b0. Jointing and faulting are present on both sides. On the English side, only minor faults of displacement less than exist; on the French side, displacements of up to are present owing to the Quenocs anticlinal fold. The faults are of limited width, filled with calcite, pyrite and remoulded clay. The increased dip and faulting restricted the selection of route on the French side. To avoid confusion, microfossil assemblages were used to classify the chalk marl. On the French side, particularly near the coast, the chalk was harder, more brittle and more fractured than on the English side. This led to the adoption of different tunnelling techniques on the two sides.\nThe Quaternary undersea valley Fosse Dangaered, and Castle Hill landslip at the English portal, caused concerns. Identified by the 1964\u201365 geophysical survey, the Fosse Dangaered is an infilled valley system extending below the seabed, south of the tunnel route in mid-channel. A 1986 survey showed that a tributary crossed the path of the tunnel, and so the tunnel route was made as far north and deep as possible. The English terminal had to be located in the Castle Hill landslip, which consists of displaced and tipping blocks of lower chalk, glauconitic marl and gault debris. Thus the area was stabilised by buttressing and inserting drainage adits. The service tunnel acted as a pilot preceding the main ones, so that the geology, areas of crushed rock, and zones of high water inflow could be predicted. Exploratory probing took place in the service tunnel, in the form of extensive forward probing, vertical downward probes and sideways probing.\nSurveying.\nMarine soundings and samplings by Thom\u00e9 de Gamond were carried out during 1833\u201367, establishing the seabed depth at a maximum of and the continuity of geological strata (layers). Surveying continued over many years, with 166\u00a0marine and 70\u00a0land-deep boreholes being drilled and over 4,000-line-kilometres of marine geophysical survey completed. Surveys were undertaken in 1958\u20131959, 1964\u20131965, 1972\u20131974 and 1986\u20131988.\nThe surveying in 1958\u201359 catered for immersed tube and bridge designs as well as a bored tunnel, and thus a wide area was investigated. At this time, marine geophysics surveying for engineering projects was in its infancy, with poor positioning and resolution from seismic profiling. The 1964\u201365 surveys concentrated on a northerly route that left the English coast at Dover harbour; using 70\u00a0boreholes, an area of deeply weathered rock with high permeability was located just south of Dover harbour.\nGiven the previous survey results and access constraints, a more southerly route was investigated in the 1972\u201373 survey, and the route was confirmed to be feasible. Information for the tunnelling project also came from work before the 1975 cancellation. On the French side at Sangatte, a deep shaft with adits was made. On the English side at Shakespeare Cliff, the government allowed of diameter tunnel to be driven. The actual tunnel alignment, method of excavation and support were essentially the same as the 1975 attempt. In the 1986\u201387 survey, previous findings were reinforced, and the characteristics of the gault clay and the tunnelling medium (chalk marl that made up 85% of the route) were investigated. Geophysical techniques from the oil industry were employed.\nTunnelling.\nTunnelling was a major engineering challenge, with the only precedent being the undersea Seikan Tunnel in Japan, which opened in 1988. A serious health and safety risk with building tunnels underwater is major water inflow due to the high hydrostatic pressure from the sea above, under weak ground conditions. The tunnel also had the challenge of time: being privately funded, early financial return was paramount.\nThe objective was to construct two rail tunnels, apart, in length; a service tunnel between the two main ones; pairs of cross-passages linking the rail tunnels to the service one at spacing; piston relief ducts in diameter connecting the rail tunnels apart; two undersea crossover caverns to connect the rail tunnels, with the service tunnel always preceding the main ones by at least to ascertain the ground conditions. There was plenty of experience with excavating through chalk in the mining industry, while the undersea crossover caverns were a complex engineering problem. The French one was based on the Mount Baker Ridge freeway tunnel in Seattle; the UK cavern was dug from the service tunnel ahead of the main ones, to avoid delay.\nPrecast segmental linings in the main TBM drives were used, but two different solutions were used. On the French side, neoprene and grout sealed bolted linings made of cast iron or high-strength reinforced concrete were used; on the English side, the main requirement was for speed so bolting of cast-iron lining segments was only carried out in areas of poor geology. In the UK rail tunnels, eight lining segments plus a key segment were used; in the French side, five segments plus a key. On the French side, a diameter deep grout-curtained shaft at Sangatte was used for access. On the English side, a marshalling area was below the top of Shakespeare Cliff, the New Austrian Tunnelling method (NATM) was first applied in the chalk marl here. On the English side, the land tunnels were driven from Shakespeare Cliff \u2013 same place as the marine tunnels \u2013 not from Folkestone. The platform at the base of the cliff was not large enough for all of the drives and, despite environmental objections, tunnel spoil was placed behind a reinforced concrete seawall, on condition of placing the chalk in an enclosed lagoon, to avoid wide dispersal of chalk fines. Owing to limited space, the precast lining factory was on the Isle of Grain in the Thames estuary, which used Scottish granite aggregate delivered by ship from the Foster Yeoman coastal super quarry at Glensanda in Loch Linnhe on the west coast of Scotland.\nOn the French side, owing to the greater permeability to water, earth pressure balance TBMs with open and closed modes were used. The TBMs were of a closed nature during the initial , but then operated as open, boring through the chalk marl stratum. This minimised the impact to the ground, allowed high water pressures to be withstood and it also alleviated the need to grout ahead of the tunnel. The French effort required five TBMs: two main marine machines, one main land machine (the short land drives of allowed one TBM to complete the first drive then reverse direction and complete the other), and two service tunnel machines. On the English side, the simpler geology allowed faster open-faced TBMs. Six machines were used; all commenced digging from Shakespeare Cliff, three marine-bound and three for the land tunnels. Towards the completion of the undersea drives, the UK TBMs were driven steeply downwards and buried clear of the tunnel. These buried TBMs were then used to provide an electrical earth. The French TBMs then completed the tunnel and were dismantled. A gauge railway was used on the English side during construction.\nIn contrast to the English machines, which were given alphanumeric names, the French tunnelling machines were all named after women: Brigitte, Europa, Catherine, Virginie, Pascaline, S\u00e9verine.\nAt the end of the tunnelling, one machine was on display at the side of the M20 motorway in Folkestone until Eurotunnel sold it on eBay for \u00a339,999 to a scrap metal merchant. Another machine (T4 \"Virginie\") still survives on the French side, adjacent to Junction 41 on the A16, in the middle of the D243E3/D243E4 roundabout. On it are the words \"hommage aux b\u00e2tisseurs du tunnel\", meaning \"tribute to the builders of the tunnel\".\nTunnel boring machines.\nThe eleven tunnel boring machines were designed and manufactured through a joint venture between the Robbins Company of Kent, Washington, United States; Markham &amp; Co. of Chesterfield; and Kawasaki Heavy Industries of Japan.\nRailway design.\nLoading Gauge.\nThe loading gauge height is .\nCommunications.\nThere are three communication systems: concession radio (CR) for mobile vehicles and personnel within Eurotunnel's Concession (terminals, tunnels, coastal shafts); track-to-train radio (TTR) for secure speech and data between trains and the railway control centre; Shuttle internal radio (SIR) for communication between shuttle crew and to passengers over car radios. This service was discontinued within one year of opening because of drivers' difficulty setting their radios to the correct frequency (88.8\u00a0MHz).\nPower supply.\nPower is delivered to the locomotives via an overhead line (catenary) at . with a normal overhead clearance of . All tunnel services run on electricity, shared equally from English and French sources. There are two sub-stations fed at 400 kV at each terminal, but in an emergency the tunnel's lighting (about 20,000 light fittings) and plant can be powered solely from either England or France.\nThe traditional railway south of London uses a 750\u00a0V\u00a0DC third rail to deliver electricity, but since the opening of High Speed 1 there is no longer any need for tunnel trains to use the third rail system. High Speed 1, the tunnel and the LGV Nord all have power provided via overhead catenary at 25\u00a0kV\u00a050\u00a0Hz. The railways on \"classic\" lines in Belgium are also electrified by overhead wires, but at 3000\u00a0V\u00a0DC.\nSignalling.\nA cab signalling system gives information directly to train drivers on a display. There is a train protection system that stops the train if the speed exceeds that indicated on the in-cab display. TVM430, as used on LGV Nord and High Speed 1, is used in the tunnel. The TVM signalling is interconnected with the signalling on the high-speed lines either side, allowing trains to enter and exit the tunnel system without stopping. The maximum speed is .\nSignalling in the tunnel is coordinated from two control centres: The main control centre at the Folkestone terminal, and a backup at the Calais terminal, which is staffed at all times and can take over all operations in the event of a breakdown or emergency.\nTrack system.\nConventional ballasted tunnel-track was ruled out owing to the difficulty of maintenance and lack of stability and precision. The Sonneville International Corporation's track system was chosen based on reliability and cost-effectiveness based on good performance in Swiss tunnels and worldwide. The type of track used is known as Low Vibration Track (LVT). Like ballasted track the LVT is of the free floating type, held in place by gravity and friction. Reinforced concrete blocks of 100\u00a0kg support the rails every 60\u00a0cm and are held by 12\u00a0mm thick closed cell polymer foam pads placed at the bottom of rubber boots. The latter separate the blocks' mass movements from the lean encasement concrete. Ballastless track provides extra overhead clearance necessary for the passage of larger trains. The corrugated rubber walls of the boots add a degree of isolation of horizontal wheel-rail vibrations, and are insulators of the track signal circuit in the humid tunnel environment. UIC60 (60\u00a0kg/m) rails of 900A grade rest on rail pads, which fit the RN/Sonneville bolted dual leaf-springs. The rails, LVT-blocks and their boots with pads were assembled outside the tunnel, in a fully automated process developed by the LVT inventor, Mr. Roger Sonneville. About 334,000 Sonneville blocks were made on the Sangatte site.\nMaintenance activities are less than projected. Initially the rails were ground on a yearly basis or after approximately 100MGT of traffic. Ride quality continues to be noticeably smooth and of low noise. Maintenance is facilitated by the existence of two tunnel junctions or crossover facilities, allowing for two-way operation in each of the six tunnel segments thereby created, and thus providing safe access for maintenance of one isolated tunnel segment at a time. The two crossovers are the largest artificial undersea caverns ever built; 150 m long, 10 m high and 18 m wide. The English crossover is from Shakespeare Cliff, and the French crossover is from Sangatte.\nVentilation, cooling and drainage.\nThe ventilation system maintains the air pressure in the service tunnel higher than in the rail tunnels, so that in the event of a fire, smoke does not enter the service tunnel from the rail tunnels. Two cooling water pipes in each rail tunnel circulate chilled water to remove heat generated by the rail traffic. Pumping stations remove water in the tunnels from rain, seepage, and so on.\nOperators.\nEurotunnel Shuttle.\nInitially 38\u00a0Le Shuttle locomotives were commissioned, with one at each end of a shuttle train. The shuttles have two separate halves: single and double deck. Each half has two loading/unloading wagons and 12 carrier wagons. Eurotunnel's original order was for nine tourist shuttles.\nHeavy goods vehicle (HGV) shuttles also have two halves, with each half containing one loading wagon, one unloading wagon and 14\u00a0carrier wagons. There is a club car behind the leading locomotive, where haulage drivers must stay during the journey. Eurotunnel originally ordered six HGV shuttle rakes.\nFreight locomotives.\nForty-six Class 92 locomotives for hauling freight trains and overnight passenger trains (the Nightstar project, which was abandoned) were commissioned, running on both overhead AC and third-rail DC power. However, RFF does not let these run on French railways, so there are plans to certify Alstom Prima II locomotives for use in the tunnel.\nInternational passenger.\nThirty-one Eurostar trains, based on the French TGV, built to UK loading gauge with many modifications for safety within the tunnel, were commissioned, with ownership split between British Rail, French national railways (SNCF) and Belgian national railways (SNCB). British Rail ordered seven more for services north of London. Around 2010, Eurostar ordered ten trains from Siemens based on its Velaro product. The Class 374 entered service in 2016 and have been operating through the Channel Tunnel ever since alongside the current Class 373.\nGermany (DB) has since around 2005 tried to get permission to run train services to London. At the end of 2009, extensive fire-proofing requirements were dropped and DB received permission to run German Intercity-Express (ICE) test trains through the tunnel. In June 2013 DB was granted access to the tunnel. In June 2014 the plans were shelved, because there are special safety rules that requires custom made trains (DB calls them Class 407).\nService locomotives.\nDiesel locomotives for rescue and shunting work are Eurotunnel Class 0001 and Eurotunnel Class 0031.\nOperation.\nThe following chart presents the estimated number of passengers and tonnes of freight, respectively, annually transported through the Channel Tunnel since 1994, in millions:\nUsage and services.\nTransport services offered by the tunnel are as follows:\nBoth the freight and passenger traffic forecasts that led to the construction of the tunnel were overestimated; in particular, Eurotunnel's commissioned forecasts were over-predictions. Although the captured share of Channel crossings was forecast correctly, high competition (especially from budget airlines which expanded rapidly in the 1990s and 2000s) and reduced tariffs led to low revenue. Overall cross-Channel traffic was overestimated.\nWith the EU's liberalisation of international rail services, the tunnel and High Speed 1 have been open to competition since 2010. There have been a number of operators interested in running trains through the tunnel and along High Speed 1 to London. In June 2013, after several years, DB obtained a licence to operate Frankfurt \u2013 London trains, not expected to run before 2016 because of delivery delays of the custom-made trains.\nPlans for the service to Frankfurt seem to have been shelved in 2018.\nPassenger traffic volumes.\nCross-tunnel passenger traffic volumes peaked at 18.4\u00a0million in 1998, dropped to 14.9\u00a0million in 2003 and has increased substantially since then.\nAt the time of the decision about building the tunnel, 15.9\u00a0million passengers were predicted for Eurostar trains in the opening year. In 1995, the first full year, actual numbers were a little over 2.9\u00a0million, growing to 7.1\u00a0million in 2000, then dropping to 6.3\u00a0million in 2003. Eurostar was initially limited by the lack of a high-speed connection on the British side. After the completion of High Speed 1 in two stages in 2003 and 2007, traffic increased. In 2008, Eurostar carried 9,113,371 passengers, a 10% increase over the previous year, despite traffic limitations due to the 2008 Channel Tunnel fire. Eurostar passenger numbers continued to increase.\nFreight traffic volumes.\nFreight volumes have been erratic, with a major decrease during 1997 due to a closure caused by a fire in a freight shuttle. Freight crossings increased over the period, indicating the substitutability of the tunnel by sea crossings. The tunnel has achieved a market share close to or above Eurotunnel's 1980s predictions but Eurotunnel's 1990 and 1994 predictions were overestimates.\nFor through freight trains, the first year prediction was 7.2\u00a0million gross tonnes; the actual 1995 figure was 1.3M gross tonnes. Through freight volumes peaked in 1998 at 3.1M tonnes. This fell back to 1.21M tonnes in 2007, increasing slightly to 1.24M tonnes in 2008. Together with that carried on freight shuttles, freight growth has occurred since opening, with 6.4M tonnes carried in 1995, 18.4M tonnes recorded in 2003 and 19.6M tonnes in 2007. Numbers fell back in the wake of the 2008 fire.\nEurotunnel's freight subsidiary is Europorte 2. In September 2006 EWS, the UK's largest rail freight operator, announced that owing to cessation of UK-French government subsidies of \u00a352\u00a0million per annum to cover the tunnel \"Minimum User Charge\" (a subsidy of around \u00a313,000 per train, at a traffic level of 4,000\u00a0trains per annum), freight trains would stop running after 30 November.\nEconomic performance.\nShares in Eurotunnel were issued at \u00a33.50 per share on 9 December 1987. By mid-1989 the price had risen to \u00a311.00. Delays and cost overruns led to the price dropping; during demonstration runs in October 1994 it reached an all-time low. Eurotunnel suspended payment on its debt in September 1995 to avoid bankruptcy. In December 1997 the British and French governments extended Eurotunnel's operating concession by 34\u00a0years, to 2086. Financial restructuring of Eurotunnel occurred in mid-1998, reducing debt and financial charges. Despite the restructuring, \"The Economist\" reported in 1998 that to break even Eurotunnel would have to increase fares, traffic and market share for sustainability. A cost benefit analysis of the tunnel indicated that there were few impacts on the wider economy and few developments associated with the project, and that the British economy would have been better off if it had not been constructed.\nUnder the terms of the Concession, Eurotunnel was obliged to investigate a cross-Channel road tunnel. In December 1999 road and rail tunnel proposals were presented to the British and French governments, but it was stressed that there was not enough demand for a second tunnel. A three-way treaty between the United Kingdom, France and Belgium governs border controls, with the establishment of \"control zones\" wherein the officers of the other nation may exercise limited customs and law enforcement powers. For most purposes these are at either end of the tunnel, with the French border controls on the UK side of the tunnel and vice versa. For some city-to-city trains, the train is a control zone. A binational emergency plan coordinates UK and French emergency activities.\nIn 1999 Eurostar posted its first net profit, having made a loss of \u00a3925m in 1995. In 2005 Eurotunnel was described as being in a serious situation. In 2013, operating profits rose 4 percent from 2012, to \u00a354 million.\nSecurity.\nThere is a need for full passport controls, since this is the border between the Schengen Area and the Common Travel Area. There are juxtaposed controls, meaning that passports are checked before boarding first by officials belonging to departing country and then officials of the destination country. These are placed only at the main Eurostar stations: French officials operate at London St Pancras, Ebbsfleet International and Ashford International, while British officials operate at Calais-Fr\u00e9thun, Lille-Europe, Marne-la-Vall\u00e9e\u2013Chessy, Brussels-South and Paris-Gare du Nord. There are security checks before boarding as well. For the shuttle road-vehicle trains, there are juxtaposed passport controls before boarding the trains.\nFor Eurostar trains travelling from places south of Paris, there is no passport and security check before departure, and those trains must stop in Lille at least 30 minutes to allow all passengers to be checked. No checks are done on board. There have been plans for services from Amsterdam, Frankfurt and Cologne to London, but a major reason to cancel them was the need for a stop in Lille. A direct service from London to Amsterdam started on 4 April 2018; following the building of check-in terminals at Amsterdam and Rotterdam and intergovernmental agreement, a direct service from the two Dutch cities to London will start on 30 April 2020.\nThe reason for juxtaposed controls is a wish to prevent illegal immigration before reaching British soil, and because a check of all passengers on a train can take 30 minutes, which creates long queues if done at arrival.\nNo-deal Brexit plan.\nUnder a planned legislation published on 12 February 2019, the UK and France would have had time to renegotiate the terms under which the railway service operates. Trains would have been permitted to use the Channel Tunnel for three months if the UK left the EU without a deal, under a proposed European Commission law.\nThe UK has since left the EU with an agreement.\nTerminals.\nThe terminals' sites are at Cheriton (near Folkestone in the United Kingdom) and Coquelles (near Calais in France). The terminals are designed to transfer vehicles from the motorway onto trains at a rate of 700\u00a0cars and 113\u00a0heavy vehicles per hour. The UK site uses the M20 motorway for access. The terminals are organised with the frontier controls juxtaposed with the entry to the system to allow travellers to go onto the motorway at the destination country immediately after leaving the shuttle.\nTo achieve design output at the French terminal, the shuttles accept cars on double-deck wagons; for flexibility, ramps were placed inside the shuttles to provide access to the top decks. At Folkestone there are of main-line track, 45\u00a0turnouts and eight platforms. At Calais there are of track and 44\u00a0turnouts. At the terminals the shuttle trains traverse a figure eight to reduce uneven wear on the wheels. There is a freight marshalling yard west of Cheriton at Dollands Moor Freight Yard.\nRegional impact.\nA 1996 report from the European Commission predicted that Kent and Nord-Pas de Calais had to face increased traffic volumes due to general growth of cross-Channel traffic and traffic attracted by the tunnel. In Kent, a high-speed rail line to London would transfer traffic from road to rail. Kent's regional development would benefit from the tunnel, but being so close to London restricts the benefits. Gains are in the traditional industries and are largely dependent on the development of Ashford International railway station, without which Kent would be totally dependent on London's expansion. Nord-Pas-de-Calais enjoys a strong internal symbolic effect of the Tunnel which results in significant gains in manufacturing.\nThe removal of a bottleneck by means like the tunnel does not necessarily induce economic gains in all adjacent regions. The image of a region being connected to the European high-speed transport and active political response are more important for regional economic development. Some small-medium enterprises located in the immediate vicinity of the terminal have used the opportunity to re-brand the profile of their business with positive effect, such as \"The New Inn\" at Etchinghill which was able to commercially exploit its unique selling point as being 'the closest pub to the Channel Tunnel'. Tunnel-induced regional development is small compared to general economic growth. The South East of England is likely to benefit developmentally and socially from faster and cheaper transport to continental Europe, but the benefits are unlikely to be equally distributed throughout the region. The overall environmental impact is almost certainly negative.\nSince the opening of the tunnel, small positive impacts on the wider economy have been felt, but it is difficult to identify major economic successes directly attributed to the tunnel.&lt;ref name=\"Flyvbjerg p. 68/69\"&gt;Flyvbjerg et al. p.\u00a068\u201369&lt;/ref&gt; The Eurotunnel does operate profitably, offering an alternative transportation mode unaffected by poor weather. High costs of construction did delay profitability, however, and companies involved in the tunnel's construction and operation early in operation relied on government aid to deal with debts amounted.\nIllegal immigration.\nIllegal immigrants and would-be asylum seekers have used the tunnel to attempt to enter Britain. By 1997, the problem had attracted international press attention, and by 1999, the French Red Cross opened the first migrant centre at Sangatte, using a warehouse once used for tunnel construction; by 2002, it housed up to 1,500\u00a0people at a time, most of them trying to get to the UK. In 2001, most came from Afghanistan, Iraq, and Iran, but African countries were also represented.\nEurotunnel, the company that operates the crossing, said that more than 37,000 migrants were intercepted between January and July 2015. Approximately 3,000 migrants, mainly from Ethiopia, Eritrea, Sudan and Afghanistan, were living in the temporary camps erected in Calais at the time of an official count in July 2015. An estimated 3,000 to 5,000 migrants were waiting in Calais for a chance to get to England.\nBritain and France operate a system of juxtaposed controls on immigration and customs, where investigations happen before travel. France is part of the Schengen Agreement, which has largely abolished border checks between member nations, but the United Kingdom is not.\nMost illegal immigrants and would-be asylum seekers who got into Britain found some way to ride a freight train. Trucks are loaded onto freight trains. In a few instances, groups of migrants were able to stow away in the cargo area of a tanker truck carrying liquid chocolate and managed to survive, though they did not enter the UK in one attempt. Although the facilities were fenced, airtight security was deemed impossible; migrants would even jump from bridges onto moving trains. In several incidents people were injured during the crossing; others tampered with railway equipment, causing delays and requiring repairs. Eurotunnel said it was losing \u00a35m per month because of the problem.\nIn 2001 and 2002, several riots broke out at Sangatte, and groups of migrants (up to 550 in a December 2001 incident) stormed the fences and attempted to enter \"en masse\".\nOther migrants use the Eurostar passenger train. They arrive as legitimate Eurostar passengers, but without proper entry papers.\nDiplomatic efforts.\nLocal authorities in both France and the UK called for the closure of the Sangatte migrant camp, and Eurotunnel twice sought an injunction against the centre. The United Kingdom blamed France for allowing Sangatte to open, and France blamed both the UK for its lax asylum rules, and the EU for not having a uniform immigration policy. The \"cause c\u00e9l\u00e8bre\" nature of the problem even included journalists detained as they followed migrants onto railway property.\nIn 2002, after the European Commission told France that it was in breach of European Union rules on the free transfer of goods because of the delays and closures as a result of its poor security, a double fence was built at a cost of \u00a35\u00a0million, reducing the numbers of migrants detected each week reaching Britain on goods trains from 250 to almost none. Other measures included CCTV cameras and increased police patrols. At the end of 2002, the Sangatte centre was closed after the UK agreed to absorb some migrants.\nOn 23 and 30 June 2015, striking workers associated with MyFerryLink damaged the sections of track by burning car tires, leading to all trains being cancelled and a backlog of vehicles. Hundreds seeking to reach Britain made use of the situation to attempt to stow away inside and underneath transport trucks destined for the United Kingdom. Extra security measures included a \u00a32 million upgrade of detection technology, \u00a31 million extra for dog searches, and \u00a312 million (over three years) towards a joint fund with France for security surrounding the Port of Calais.\nIllegal attempts to cross and deaths.\nMigrants take great risks to evade security precautions. In 2002, a dozen migrants died in crossing attempts. In the two months from June to July 2015, ten migrants died near the French tunnel terminal, during a period when 1,500 attempts to evade security precautions were being made each day.\nOn 6 July 2015, a migrant died while attempting to climb onto a freight train while trying to reach Britain from the French side of the Channel. The previous month an Eritrean man was killed under similar circumstances.\nDuring the night of 28 July 2015, one person, aged 25\u201330, was found dead after a night in which 1,500\u20132,000 migrants had attempted to enter the Eurotunnel terminal.\nOn 4 August 2015, a Sudanese migrant walked nearly the entire length of one of the tunnels. He was arrested close to the British side, after having walked about through the tunnel.\nMechanical incidents.\nFires.\nThere have been three fires in the tunnel, all on the heavy goods vehicle (HGV) shuttles, that were significant enough to close the tunnel, as well as other more minor incidents.\nOn 9 December 1994, during an \"invitation only\" testing phase, a fire broke out in a Ford Escort car whilst its owner was loading it onto the upper deck of a tourist shuttle. The fire started at about 10:00, with the shuttle train stationary in the Folkestone terminal and was put out about 40 minutes later with no passenger injuries.\nOn 18 November 1996, a fire broke out on an HGV shuttle wagon in the tunnel, but nobody was seriously hurt. The exact cause is unknown, although it was neither a Eurotunnel equipment nor rolling stock problem; it may have been due to arson of a heavy goods vehicle. It is estimated that the heart of the fire reached , with the tunnel severely damaged over , with some affected to some extent. Full operation recommenced six months after the fire.\nOn 21 August 2006, the tunnel was closed for several hours when a truck on an HGV shuttle train caught fire.\nOn 11 September 2008, a fire occurred in the Channel Tunnel at 13:57 GMT. The incident started on an HGV shuttle train travelling towards France. The event occurred from the French entrance to the tunnel. No one was killed but several people were taken to hospitals suffering from smoke inhalation, and minor cuts and bruises. The tunnel was closed to all traffic, with the undamaged South Tunnel reopening for limited services two days later. Full service resumed on 9 February 2009 after repairs costing \u20ac60\u00a0million.\nOn 29 November 2012, the tunnel was closed for several hours after a truck on an HGV shuttle caught fire.\nOn 17 January 2015, both tunnels were closed following a lorry fire which filled the midsection of Running Tunnel North with smoke. Eurostar cancelled all services. The shuttle train had been heading from Folkestone to Coquelles and stopped adjacent to cross-passage CP 4418 just before 12:30 UTC. Thirty-eight passengers and four members of Eurotunnel staff were evacuated into the service tunnel, and then transported to France using special STTS road vehicles in the Service Tunnel. The passengers and crew were taken to the Eurotunnel Fire/Emergency Management Centre close to the French portal.\nTrain failures.\nOn the night of 19/20 February 1996, about 1,000 passengers became trapped in the Channel Tunnel when Eurostar trains from London broke down owing to failures of electronic circuits caused by snow and ice being deposited and then melting on the circuit boards.\nOn 3 August 2007, an electrical failure lasting six hours caused passengers to be trapped in the tunnel on a shuttle.\nOn the evening of 18 December 2009, during the December 2009 European snowfall, five London-bound Eurostar trains failed inside the tunnel, trapping 2,000 passengers for approximately 16 hours, during the coldest temperatures in eight years. A Eurotunnel spokesperson explained that snow had evaded the train's winterisation shields, and the transition from cold air outside to the tunnel's warm atmosphere had melted the snow, resulting in electrical failures. One train was turned back before reaching the tunnel; two trains were hauled out of the tunnel by Eurotunnel Class 0001 diesel locomotives. The blocking of the tunnel led to the implementation of Operation Stack, the transformation of the M20 motorway into a linear car park.\nThe occasion was the first time that a Eurostar train was evacuated inside the tunnel; the failing of four at once was described as \"unprecedented\". The Channel Tunnel reopened the following morning. Nirj Deva, Member of the European Parliament for South East England, had called for Eurostar chief executive Richard Brown to resign over the incidents. An independent report by Christopher Garnett (former CEO of Great North Eastern Railway) and Claude Gressier (a French transport expert) on the 18/19 December 2009 incidents was issued in February 2010, making 21 recommendations.\nOn 7 January 2010, a Brussels\u2013London Eurostar broke down in the tunnel. The train had 236 passengers on board and was towed to Ashford; other trains that had not yet reached the tunnel were turned back.\nSafety.\nThe Channel Tunnel Safety Authority is responsible for some aspects of safety regulation in the tunnel; it reports to the Intergovernmental Commission (IGC).\nThe service tunnel is used for access to technical equipment in cross-passages and equipment rooms, to provide fresh-air ventilation and for emergency evacuation. The Service Tunnel Transport System (STTS) allows fast access to all areas of the tunnel. The service vehicles are rubber-tyred with a buried wire guidance system. The 24 STTS vehicles are used mainly for maintenance but also for firefighting and in emergencies. \"Pods\" with different purposes, up to a payload of , are inserted into the side of the vehicles. The vehicles cannot turn around within the tunnel, and are driven from either end. The maximum speed is when the steering is locked. A fleet of 15 Light Service Tunnel Vehicles (LADOGS) was introduced to supplement the STTSs. The LADOGS have a short wheelbase with a turning circle, allowing two-point turns within the service tunnel. Steering cannot be locked like the STTS vehicles, and maximum speed is . Pods up to can be loaded onto the rear of the vehicles. Drivers in the tunnel sit on the right, and the vehicles drive on the left. Owing to the risk of French personnel driving on their native right side of the road, sensors in the vehicles alert the driver if the vehicle strays to the right side.\nThe three tunnels contain of air that needs to be conditioned for comfort and safety. Air is supplied from ventilation buildings at Shakespeare Cliff and Sangatte, with each building capable of providing 100% standby capacity. Supplementary ventilation also exists on either side of the tunnel. In the event of a fire, ventilation is used to keep smoke out of the service tunnel and move smoke in one direction in the main tunnel to give passengers clean air. The tunnel was the first main-line railway tunnel to have special cooling equipment. Heat is generated from traction equipment and drag. The design limit was set at , using a mechanical cooling system with refrigeration plants on both sides that run chilled water circulating in pipes within the tunnel.\nTrains travelling at high speed create piston effect pressure changes that can affect passenger comfort, ventilation systems, tunnel doors, fans and the structure of the trains, and which drag on the trains. Piston relief ducts of diameter were chosen to solve the problem, with 4\u00a0ducts per kilometre to give close to optimum results. Unfortunately this design led to unacceptable lateral forces on the trains so a reduction in train speed was required and restrictors were installed in the ducts.\nThe safety issue of a possible fire on a passenger-vehicle shuttle garnered much attention, with Eurotunnel noting that fire was the risk attracting the most attention in a 1994 safety case for three reasons: the opposition of ferry companies to passengers being allowed to remain with their cars; Home Office statistics indicating that car fires had doubled in ten years; and the long length of the tunnel. Eurotunnel commissioned the UK Fire Research Station \u2013 now part of the Building Research Establishment \u2013 to give reports of vehicle fires, and liaised with Kent Fire Brigade to gather vehicle fire statistics over one year. Fire tests took place at the French Mines Research Establishment with a mock wagon used to investigate how cars burned. The wagon door systems are designed to withstand fire inside the wagon for 30\u00a0minutes, longer than the transit time of 27\u00a0minutes. Wagon air conditioning units help to purge dangerous fumes from inside the wagon before travel. Each wagon has a fire detection and extinguishing system, with sensing of ions or ultraviolet radiation, smoke and gases that can trigger halon gas to quench a fire. Since the HGV wagons are not covered, fire sensors are located on the loading wagon and in the tunnel. A water main in the service tunnel provides water to the main tunnels at intervals. The ventilation system can control smoke movement. Special arrival sidings accept a train that is on fire, as the train is not allowed to stop whilst on fire in the tunnel, unless continuing its journey would lead to a worse outcome. Eurotunnel has banned a wide range of hazardous goods from travelling in the tunnel. Two STTS (Service Tunnel Transportation System) vehicles with firefighting pods are on duty at all times, with a maximum delay of 10\u00a0minutes before they reach a burning train.\nUnusual traffic.\nTrains.\nIn 1999, the \"Kosovo Train for Life\" passed through the tunnel en route to Pristina, in Kosovo.\nOther.\nIn 2009, former F1 racing champion John Surtees drove a Ginetta G50 EV electric sports car prototype from England to France, using the service tunnel, as part of a charity event. He was required to keep to the speed limit. To celebrate the 2014 Tour de France's transfer from its opening stages in Britain to France in July of that year, Chris Froome of Team Sky rode a bicycle through the service tunnel, becoming the first solo rider to do so. The crossing took under an hour, reaching speeds of \u2013faster than most cross-channel ferries.\nMobile network coverage.\nSince 2012, French operators Bouygues Telecom, Orange and SFR have covered Running Tunnel South, the tunnel bore normally used for travel from France to Britain.\nIn January 2014, UK operators EE and Vodafone signed ten-year contracts with Eurotunnel for Running Tunnel North. The agreements will enable both operators' subscribers to use 2G and 3G services. Both EE and Vodafone planned to offer LTE services on the route; EE said it expected to cover the route with LTE connectivity by summer 2014. EE and Vodafone will offer Channel Tunnel network coverage for travellers from the UK to France. Eurotunnel said it also held talks with Three UK but has yet to reach an agreement with the operator.\nIn May 2014, Eurotunnel announced that they had installed equipment from Alcatel-Lucent to cover Running Tunnel North and simultaneously to provide mobile service (GSM 900/1800\u00a0MHz and UMTS 2100\u00a0MHz) by EE, O2 and Vodafone. The service of EE and Vodafone commenced on the same date as the announcement. O2 service was expected to be available soon afterwards.\nIn November 2014, EE announced that it had previously switched on LTE earlier in September 2014. O2 turned on 2G, 3G and 4G services in November 2014, whilst Vodafone's 4G was due to go live later.\nOther (non-transport) services.\nThe tunnel also houses the 1,000\u00a0MW ElecLink interconnector, which is currently being constructed to transfer power between the British and French electricity networks."}
{"id": "5703", "revid": "11462353", "url": "https://en.wikipedia.org/wiki?curid=5703", "title": "Cyberpunk", "text": "Cyberpunk is a subgenre of science fiction in a dystopian futuristic setting that tends to focus on a \"combination of low-life and high tech\" featuring advanced technological and scientific achievements, such as artificial intelligence and cybernetics, juxtaposed with a degree of breakdown or radical change in the social order. Much of cyberpunk is rooted in the New Wave science fiction movement of the 1960s and 1970s, when writers like Philip K. Dick, Roger Zelazny, John Brunner, J.\u00a0G. Ballard, Philip Jos\u00e9 Farmer and Harlan Ellison examined the impact of drug culture, technology, and the sexual revolution while avoiding the utopian tendencies of earlier science fiction.\nComics exploring cyberpunk themes began appearing as early as Judge Dredd, first published in 1977. Released in 1984, William Gibson's influential debut novel \"Neuromancer\" would help solidify cyberpunk as a genre, drawing influence from punk subculture and early hacker culture. Other influential cyberpunk writers included Bruce Sterling and Rudy Rucker. The Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series \"Akira\", with its 1988 anime film adaptation later popularizing the subgenre.\nEarly films in the genre include Ridley Scott's 1982 film \"Blade Runner\", one of several of Philip K. Dick's works that have been adapted into films. The films \"Johnny Mnemonic\" (1995) and \"New Rose Hotel\" (1998), both based upon short stories by William Gibson, flopped commercially and critically. \"The Matrix trilogy\" (1999\u20132003) were some of the most successful cyberpunk films. Newer cyberpunk media includes \"Blade Runner 2049\" (2017), a sequel to the original 1982 film, as well as \"Upgrade\" (2018), \"\" (2019) based on the 1990s Japanese manga \"Battle Angel Alita\", the 2018 Netflix TV series \"Altered Carbon\" based on Richard K. Morgan's 2002 novel of the same name, and the video game \"Cyberpunk 2077\" (2020), based on the table-top role-playing game \"Cyberpunk\".\nBackground.\nLawrence Person has attempted to define the content and ethos of the cyberpunk literary movement stating: \nCyberpunk plots often center on conflict among artificial intelligences, hackers, and megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's \"Foundation\" or Frank Herbert's \"Dune\". The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and the use of technology in ways never anticipated by its original inventors (\"the street finds its own uses for things\"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction. There are sources who view that cyberpunk has shifted from a literary movement to a mode of science fiction due to the limited number of writers and its transition to a more generalized cultural formation.\nHistory and origins.\nThe origins of cyberpunk are rooted in the New Wave science fiction movement of the 1960s and 70s, where \"New Worlds\", under the editorship of Michael Moorcock, began inviting and encouraging stories that examined new writing styles, techniques, and archetypes. Reacting to conventional storytelling, New Wave authors attempted to present a world where society coped with a constant upheaval of new technology and culture, generally with dystopian outcomes. Writers like Roger Zelazny, J.G. Ballard, Philip Jose Farmer, and Harlan Ellison often examined the impact of drug culture, technology, and the sexual revolution with an avant-garde style influenced by the Beat Generation (especially William S. Burroughs' own SF), Dadaism, and their own ideas. Ballard attacked the idea that stories should follow the \"archetypes\" popular since the time of Ancient Greece, and the assumption that these would somehow be the same ones that would call to modern readers, as Joseph Campbell argued in \"The Hero with a Thousand Faces\". Instead, Ballard wanted to write a new myth for the modern reader, a style with \"more psycho-literary ideas, more meta-biological and meta-chemical concepts, private time systems, synthetic psychologies and space-times, more of the sombre half-worlds one glimpses in the paintings of schizophrenics.\"\nThis had a profound influence on a new generation of writers, some of whom would come to call their movement \"Cyberpunk\". One, Bruce Sterling, later said:\nBallard, Zelazny, and the rest of New Wave was seen by the subsequent generation as delivering more \"realism\" to science fiction, and they attempted to build on this.\nSimilarly influential, and generally cited as proto-cyberpunk , is the Philip K. Dick novel \"Do Androids Dream of Electric Sheep\", first published in 1968. Presenting precisely the general feeling of dystopian post-economic-apocalyptic future as Gibson and Sterling later deliver, it examines ethical and moral problems with cybernetic, artificial intelligence in a way more \"realist\" than the Isaac Asimov \"Robot\" series that laid its philosophical foundation. Dick's protege and friend K. W. Jeter wrote a very dark and imaginative novel called \"Dr. Adder\" in 1972 that, Dick lamented, might have been more influential in the field had it been able to find a publisher at that time. It was not published until 1984, after which Jeter made it the first book in a trilogy, followed by \"The Glass Hammer\" (1985) and \"Death Arms\" (1987). Jeter wrote other standalone cyberpunk novels before going on to write three authorized sequels to \"Do Androids Dream of electric sheep\", named \"Blade Runner 2: The Edge of Human\" (1995), \"Blade Runner 3: Replicant Night\" (1996), and \"Blade Runner 4: Eye and Talon\".\n\"Do Androids Dream of Electric Sheep\" was made into the seminal movie \"Blade Runner\", released in 1982. This was one year after William Gibson's story, \"Johnny Mnemonic\" helped move proto-cyberpunk concepts into the mainstream. That story, which also became a film years later in 1995, involves another dystopian future, where human couriers deliver computer data, stored cybernetically in their own minds.\nThe term \"cyberpunk\" first appeared as the title of a short story written by Bruce Bethke, written in 1980 and published in \"Amazing Stories\" in 1983. It was picked up by Gardner Dozois, editor of \"Isaac Asimov's Science Fiction Magazine\" and popularized in his editorials.\nBethke says he made two lists of words, one for technology, one for troublemakers, and experimented with combining them variously into compound words, consciously attempting to coin a term that encompassed both punk attitudes and high technology. He described the idea thus:\nAfterward, Dozois began using this term in his own writing, most notably in a \"Washington Post\" article where he said \"About the closest thing here to a self-willed esthetic 'school' would be the purveyors of bizarre hard-edged, high-tech stuff, who have on occasion been referred to as 'cyberpunks' \u2014 Sterling, Gibson, Shiner, Cadigan, Bear.\"\nAbout that time in 1984, William Gibson's novel \"Neuromancer\" was published, delivering a glimpse of a future encompassed by what became an archetype of cyberpunk \"virtual reality\", with the human mind being fed light-based worldscapes through a computer interface. Some, perhaps ironically including Bethke himself, argued at the time that the writers whose style Gibson's books epitomized should be called \"Neuromantics\", a pun on the name of the novel plus \"New Romantics\", a term used for a New Wave pop music movement that had just occurred in Britain, but this term did not catch on. Bethke later paraphrased Michael Swanwick's argument for the term: \"the movement writers should properly be termed neuromantics, since so much of what they were doing was clearly Imitation \"Neuromancer\".\nSterling was another writer who played a central role, often consciously, in the cyberpunk genre, variously seen as either keeping it on track, or distorting its natural path into a stagnant formula. In 1986 he edited a volume of cyberpunk stories called \", an attempt to establish what cyberpunk was, from Sterling's perspective.\nIn the subsequent decade, the motifs of Gibson's \"Neuromancer\" became formulaic, climaxing in the satirical extremes of Neal Stephenson's \"Snow Crash\" in 1992.\nBookending the Cyberpunk era, Bethke himself published a novel in 1995 called \"Headcrash\", like \"Snow Crash\" a satirical attack on the genre's excesses. Fittingly, it won an honor named after cyberpunk's spiritual founder, the Philip K. Dick Award.\nIt satirized the genre in this way:\nThe impact of cyberpunk, though, has been long-lasting. Elements of both the setting and storytelling have become normal in science fiction in general, and a slew of sub-genres now have -punk tacked onto their names, most obviously Steampunk, but also a host of other Cyberpunk derivatives.\nStyle and ethos.\nPrimary figures in the cyberpunk movement include William Gibson, Neal Stephenson, Bruce Sterling, Bruce Bethke, Pat Cadigan, Rudy Rucker, and John Shirley. Philip K. Dick (author of \"Do Androids Dream of Electric Sheep?\", from which the film \"Blade Runner\" was adapted) is also seen by some as prefiguring the movement.\n\"Blade Runner\" can be seen as a quintessential example of the cyberpunk style and theme. Video games, board games, and tabletop role-playing games, such as \"Cyberpunk 2020\" and \"Shadowrun\", often feature storylines that are heavily influenced by cyberpunk writing and movies. Beginning in the early 1990s, some trends in fashion and music were also labeled as cyberpunk. Cyberpunk is also featured prominently in anime and manga (Japanese cyberpunk), with \"Akira\", \"Ghost in the Shell\" and \"Cowboy Bebop\" being among the most notable.\nSetting.\nCyberpunk writers tend to use elements from crime fiction\u2014particularly hardboiled detective fiction and film noir\u2014and postmodernist prose to describe an often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards utopian SF in his 1981 short story \"The Gernsback Continuum,\" which pokes fun at and, to a certain extent, condemns utopian science fiction.\nIn some cyberpunk writing, much of the action takes place online, in cyberspace, blurring the line between actual and virtual reality. A typical trope in such work is a direct connection between the human brain and computer systems. Cyberpunk settings are dystopias with corruption, computers and internet connectivity. Giant, multinational corporations have for the most part replaced governments as centers of political, economic, and even military power.\nThe economic and technological state of Japan is a regular theme in the Cyberpunk literature of the '80s. Of Japan's influence on the genre, William Gibson said, \"Modern Japan simply was cyberpunk.\" Cyberpunk is often set in urbanized, artificial landscapes, and \"city lights, receding\" was used by Gibson as one of the genre's first metaphors for cyberspace and virtual reality. The cityscapes of Hong Kong has had major influences in the urban backgrounds, ambiance and settings in many cyberpunk works such as \"Blade Runner\" and \"Shadowrun\". Ridley Scott envisioned the landscape of cyberpunk Los Angeles in \"Blade Runner\" to be \"Hong Kong on a very bad day\". The streetscapes of the \"Ghost in the Shell\" film were based on Hong Kong. Its director Mamoru Oshii felt that Hong Kong's strange and chaotic streets where \"old and new exist in confusing relationships\", fit the theme of the film well. Hong Kong's Kowloon Walled City is particularly notable for its disorganized hyper-urbanization and breakdown in traditional urban planning to be an inspiration to cyberpunk landscapes.\nProtagonists.\nOne of the cyberpunk genre's prototype characters is Case, from Gibson's \"Neuromancer\". Case is a \"console cowboy,\" a brilliant hacker who has betrayed his organized criminal partners. Robbed of his talent through a crippling injury inflicted by the vengeful partners, Case unexpectedly receives a once-in-a-lifetime opportunity to be healed by expert medical care but only if he participates in another criminal enterprise with a new crew.\nLike Case, many cyberpunk protagonists are manipulated, placed in situations where they have little or no choice, and although they might see things through, they do not necessarily come out any further ahead than they previously were. These anti-heroes\u2014\"criminals, outcasts, visionaries, dissenters and misfits\"\u2014call to mind the private eye of detective fiction. This emphasis on the misfits and the malcontents is the \"punk\" component of cyberpunk.\nSociety and government.\nCyberpunk can be intended to disquiet readers and call them to action. It often expresses a sense of rebellion, suggesting that one could describe it as a type of cultural revolution in science fiction. In the words of author and critic David Brin:\n...a closer look [at cyberpunk authors] reveals that they nearly always portray future societies in which governments have become wimpy and pathetic ...Popular science fiction tales by Gibson, Williams, Cadigan and others \"do\" depict Orwellian accumulations of power in the next century, but nearly always clutched in the secretive hands of a wealthy or corporate elite.\nCyberpunk stories have also been seen as fictional forecasts of the evolution of the Internet. The earliest descriptions of a global communications network came long before the World Wide Web entered popular awareness, though not before traditional science-fiction writers such as Arthur C. Clarke and some social commentators such as James Burke began predicting that such networks would eventually form.\nSome observers cite that cyberpunk tends to marginalize sectors of society such as women and Africans. For instance, it is claimed that cyberpunk depicts fantasies that ultimately empower masculinity using fragmentary and decentered aesthetic that culminate in a masculine genre populated by male outlaws. Critics also note the absence of any reference to Africa or an African-American character in the quintessential cyberpunk film \"Blade Runner\" while other films reinforce stereotypes.\nMedia.\nLiterature.\nMinnesota writer Bruce Bethke coined the term in 1983 for his short story \"Cyberpunk,\" which was published in an issue of \"Amazing Science Fiction Stories\". The term was quickly appropriated as a label to be applied to the works of William Gibson, Bruce Sterling, Pat Cadigan and others. Of these, Sterling became the movement's chief ideologue, thanks to his fanzine \"Cheap Truth.\" John Shirley wrote articles on Sterling and Rucker's significance. John Brunner's 1975 novel \"The Shockwave Rider\" is considered by many to be the first cyberpunk novel with many of the tropes commonly associated with the genre, some five years before the term was popularized by Dozois.\nWilliam Gibson with his novel \"Neuromancer\" (1984) is arguably the most famous writer connected with the term cyberpunk. He emphasized style, a fascination with surfaces, and atmosphere over traditional science-fiction tropes. Regarded as ground-breaking and sometimes as \"the archetypal cyberpunk work,\" \"Neuromancer\" was awarded the Hugo, Nebula, and Philip K. Dick Awards. \"Count Zero\" (1986) and \"Mona Lisa Overdrive\" (1988) followed after Gibson's popular debut novel. According to the Jargon File, \"Gibson's near-total ignorance of computers and the present-day hacker culture enabled him to speculate about the role of computers and hackers in the future in ways hackers have since found both irritatingly na\u00efve and tremendously stimulating.\"\nEarly on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, however, some critics arose to challenge its status as a revolutionary movement. These critics said that the SF New Wave of the 1960s was much more innovative as far as narrative techniques and styles were concerned. Furthermore, while \"Neuromancer\"'s narrator may have had an unusual \"voice\" for science fiction, much older examples can be found: Gibson's narrative voice, for example, resembles that of an updated Raymond Chandler, as in his novel \"The Big Sleep\" (1939). Others noted that almost all traits claimed to be uniquely cyberpunk could in fact be found in older writers' works\u2014often citing J. G. Ballard, Philip K. Dick, Harlan Ellison, Stanis\u0142aw Lem, Samuel R. Delany, and even William S. Burroughs. For example, Philip K. Dick's works contain recurring themes of social decay, artificial intelligence, paranoia, and blurred lines between objective and subjective realities. The influential cyberpunk movie \"Blade Runner\" (1982) is based on his book, \"Do Androids Dream of Electric Sheep?\". Humans linked to machines are found in Pohl and Kornbluth's \"Wolfbane\" (1959) and Roger Zelazny's \"Creatures of Light and Darkness\" (1968).\nIn 1994, scholar Brian Stonehill suggested that Thomas Pynchon's 1973 novel \"Gravity's Rainbow\" \"not only curses but precurses what we now glibly dub cyberspace.\" Other important predecessors include Alfred Bester's two most celebrated novels, \"The Demolished Man\" and \"The Stars My Destination\", as well as Vernor Vinge's novella \"True Names\".\nReception and impact.\nScience-fiction writer David Brin describes cyberpunk as \"the finest free promotion campaign ever waged on behalf of science fiction.\" It may not have attracted the \"real punks,\" but it did ensnare many new readers, and it provided the sort of movement that postmodern literary critics found alluring. Cyberpunk made science fiction more attractive to academics, argues Brin; in addition, it made science fiction more profitable to Hollywood and to the visual arts generally. Although the \"self-important rhetoric and whines of persecution\" on the part of cyberpunk fans were irritating at worst and humorous at best, Brin declares that the \"rebels did shake things up. We owe them a debt.\"\nFredric Jameson considers cyberpunk the \"supreme literary expression if not of postmodernism, then of late capitalism itself\".\nCyberpunk further inspired many professional writers who were not among the \"original\" cyberpunks to incorporate cyberpunk ideas into their own works, such as George Alec Effinger's \"When Gravity Fails\". \"Wired\" magazine, created by Louis Rossetto and Jane Metcalfe, mixes new technology, art, literature, and current topics in order to interest today's cyberpunk fans, which Paula Yoo claims \"proves that hardcore hackers, multimedia junkies, cyberpunks and cellular freaks are poised to take over the world.\"\nFilm and television.\nThe film \"Blade Runner\" (1982)\u2014adapted from Philip K. Dick's \"Do Androids Dream of Electric Sheep?\"\u2014is set in 2019 in a dystopian future in which manufactured beings called replicants are slaves used on space colonies and are legal prey on Earth to various bounty hunters who \"retire\" (kill) them. Although \"Blade Runner\" was largely unsuccessful in its first theatrical release, it found a viewership in the home video market and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson would later reveal that upon first viewing the film, he was surprised at how the look of this film matched his vision for \"Neuromancer\", a book he was then working on. The film's tone has since been the staple of many cyberpunk movies, such as \"The Matrix trilogy\" (1999-2003), which uses a wide variety of cyberpunk elements.\nThe number of films in the genre or at least using a few genre elements has grown steadily since \"Blade Runner\". Several of Philip K. Dick's works have been adapted to the silver screen. The films \"Johnny Mnemonic\" and \"New Rose Hotel\", both based upon short stories by William Gibson, flopped commercially and critically. These box offices misses significantly slowed the development of cyberpunk as a literary or cultural form although a sequel to the 1982 film \"Blade Runner\" was released in October 2017 with Harrison Ford reprising his role from the original film.\nIn addition, \"tech-noir\" film as a hybrid genre, means a work of combining neo-noir and science fiction or cyberpunk. It includes many cyberpunk films such as \"Blade Runner\", \"Burst City\", \"Robocop\", \"12 Monkeys\", \"The Lawnmower Man\", \"Hackers\", \"Hardware\", and \"Strange Days.\"\nAnime and manga.\nThe Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series \"Akira\", with its 1988 anime film adaptation, which Otomo directed, later popularizing the subgenre. \"Akira\" inspired a wave of Japanese cyberpunk works, including manga and anime series such as \"Ghost in the Shell\", \"Battle Angel Alita\", \"Cowboy Bebop\", and \"Serial Experiments Lain\". Other early Japanese cyberpunk works include the 1982 film \"Burst City\", the 1985 original video animation \"Megazone 23\", and the 1989 film \"\".\nIn contrast to Western cyberpunk which has roots in New Wave science fiction literature, Japanese cyberpunk has roots in underground music culture, specifically the Japanese punk subculture that arose from the Japanese punk music scene in the 1970s. The filmmaker Sogo Ishii introduced this subculture to Japanese cinema with the punk film \"Panic High School\" (1978) and the punk biker film \"Crazy Thunder Road\" (1980), both portraying the rebellion and anarchy associated with punk, and the latter featuring a punk biker gang aesthetic. Ishii's punk films paved the way for Otomo's seminal cyberpunk work \"Akira\".\nCyberpunk themes are widely visible in anime and manga. In Japan, where cosplay is popular and not only teenagers display such fashion styles, cyberpunk has been accepted and its influence is widespread. William Gibson's \"Neuromancer,\" whose influence dominated the early cyberpunk movement, was also set in Chiba, one of Japan's largest industrial areas, although at the time of writing the novel Gibson did not know the location of Chiba and had no idea how perfectly it fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the 1980s has allowed it to seep into the Japanese culture.\nCyberpunk anime and manga draw upon a futuristic vision which has elements in common with Western science fiction and therefore have received wide international acceptance outside Japan. \"The conceptualization involved in cyberpunk is more of forging ahead, looking at the new global culture. It is a culture that does not exist right now, so the Japanese concept of a cyberpunk future, seems just as valid as a Western one, especially as Western cyberpunk often incorporates many Japanese elements.\" William Gibson is now a frequent visitor to Japan, and he came to see that many of his visions of Japan have become a reality:\nModern Japan simply was cyberpunk. The Japanese themselves knew it and delighted in it. I remember my first glimpse of Shibuya, when one of the young Tokyo journalists who had taken me there, his face drenched with the light of a thousand media-suns\u2014all that towering, animated crawl of commercial information\u2014said, \"You see? You see? It is \"Blade Runner\" town.\" And it was. It so evidently was.\nCyberpunk themes have appeared in many anime and manga, including the ground-breaking \"Appleseed\", \"Ghost in the Shell\", \"Ergo Proxy\", \"Megazone 23\", \"Goku Midnight Eye\", \"Cyber City Oedo 808\", \"Bubblegum Crisis\", \"\", \"Angel Cop\", \"Blame!\", \"Armitage III\", \"Texhnolyze\", \"Psycho-Pass\" and \"No Guns Life\".\nInfluence.\n\"Akira\" (1982 manga) and its 1988 anime film adaptation have influenced numerous works in animation, comics, film, music, television and video games. \"Akira\" has been cited as a major influence on Hollywood films such as \"The Matrix\", \"Chronicle\", \"Looper\", \"Midnight Special\", and \"Inception\", as well as cyberpunk-influenced video games such as Hideo Kojima's \"Snatcher\" and \"Metal Gear Solid\", Valve's \"Half-Life\" series and Dontnod Entertainment's \"Remember Me\". \"Akira\" has also influenced the work of musicians such as Kanye West, who paid homage to \"Akira\" in the \"Stronger\" music video, and Lupe Fiasco, whose album \"Tetsuo &amp; Youth\" is named after Tetsuo Shima. The popular bike from the film, Kaneda's Motorbike, appears in \"Steven Spielberg\"'s film \"Ready Player One\", and CD Projekt's video game \"Cyberpunk 2077\".\n\"Ghost in the Shell\" (1995) influenced a number of prominent filmmakers, most notably the Wachowskis in \"The Matrix\" (1999) and its sequels. \"The Matrix\" series took several concepts from the film, including the Matrix digital rain, which was inspired by the opening credits of \"Ghost in the Shell\", and the way characters access the Matrix through holes in the back of their necks. Other parallels have been drawn to James Cameron's \"Avatar\", Steven Spielberg's \"A.I. Artificial Intelligence\", and Jonathan Mostow's \"Surrogates\". James Cameron cited \"Ghost in the Shell\" as a source of inspiration, citing it as an influence on \"Avatar\".\nThe original video animation \"Megazone 23\" (1985) has a number of similarities to \"The Matrix\". \"Battle Angel Alita\" (1990) has had a notable influence on filmmaker James Cameron, who was planning to adapt it into a film since 2000. It was an influence on his TV series \"Dark Angel\", and he is the producer of the 2018 film adaptation \"\".\nGames.\nThere are cyberpunk video games. Popular series include \"Final Fantasy VII\" and its spin-offs and remake, the \"Megami Tensei\" series, Kojima's \"Snatcher\" and \"Metal Gear\" series, \"Deus Ex\" series, \"Syndicate\" series, and \"System Shock\" and its sequel. Other games, like \"Blade Runner\", \"Ghost in the Shell\", and the \"Matrix\" series, are based upon genre movies, or role-playing games (for instance the various \"Shadowrun\" games).\nSeveral RPGs called \"Cyberpunk\" exist: \"Cyberpunk\", \"Cyberpunk 2020\", \"Cyberpunk 2077\", and \"Cyberpunk v3\", by R. Talsorian Games, and \"GURPS Cyberpunk\", published by Steve Jackson Games as a module of the GURPS family of RPGs. \"Cyberpunk 2020\" was designed with the settings of William Gibson's writings in mind, and to some extent with his approval, unlike the approach taken by FASA in producing the transgenre \"Shadowrun\" game. Both are set in the near future, in a world where cybernetics are prominent. In addition, Iron Crown Enterprises released an RPG named \"Cyberspace\", which was out of print for several years until recently being re-released in online PDF form. CD Projekt Red released \"Cyberpunk 2077,\" a cyberpunk first-person open world Role-playing video game (RPG) based on the tabletop RPG \"Cyberpunk 2020\", on December 10, 2020.\nIn 1990, in a convergence of cyberpunk art and reality, the United States Secret Service raided Steve Jackson Games's headquarters and confiscated all their computers. Officials denied that the target had been the \"GURPS Cyberpunk\" sourcebook, but Jackson would later write that he and his colleagues \"were never able to secure the return of the complete manuscript; [...] The Secret Service at first flatly refused to return anything \u2013 then agreed to let us copy files, but when we got to their office, restricted us to one set of out-of-date files \u2013 then agreed to make copies for us, but said \"tomorrow\" every day from March 4 to March 26. On March 26 we received a set of disks which purported to be our files, but the material was late, incomplete and well-nigh useless.\" Steve Jackson Games won a lawsuit against the Secret Service, aided by the new Electronic Frontier Foundation. This event has achieved a sort of notoriety, which has extended to the book itself as well. All published editions of \"GURPS Cyberpunk\" have a tagline on the front cover, which reads \"The book that was seized by the U.S. Secret Service!\" Inside, the book provides a summary of the raid and its aftermath.\nCyberpunk has also inspired several tabletop, miniature and board games such as \"Necromunda\" by Games Workshop. \"Netrunner\" is a collectible card game introduced in 1996, based on the \"Cyberpunk 2020\" role-playing game. \"Tokyo NOVA\", debuting in 1993, is a cyberpunk role-playing game that uses playing cards instead of dice.\n\"Cyberpunk 2077\" set a new record for the largest number of simultaneous players in a single player game, with a record 1,003,262 playing just after the December 10th launch, according to Steam Database. That tops the previous Steam record of 472,962 players set by Fallout 4 back in 2015.\nMusic.\nInvariably the origin of cyberpunk music lies in the synthesizer-heavy scores of cyberpunk films such as \"Escape from New York\" (1981) and \"Blade Runner\" (1982). Some musicians and acts have been classified as cyberpunk due to their aesthetic style and musical content. Often dealing with dystopian visions of the future or biomechanical themes, some fit more squarely in the category than others. Bands whose music has been classified as cyberpunk include Psydoll, Front Line Assembly, Clock DVA, Angelspit and Sigue Sigue Sputnik.\nSome musicians not normally associated with cyberpunk have at times been inspired to create concept albums exploring such themes. Albums such as Gary Numan's \"Replicas\", \"The Pleasure Principle\" and \"Telekon\" were heavily inspired by the works of Philip K. Dick. Kraftwerk's \"The Man-Machine\" and \"Computer World\" albums both explored the theme of humanity becoming dependent on technology. Nine Inch Nails' concept album \"Year Zero\" also fits into this category. Fear Factory concept albums are heavily based upon future dystopia, cybernetics, clash between man and machines, virtual worlds. Billy Idol's \"Cyberpunk\" drew heavily from cyberpunk literature and the cyberdelic counter culture in its creation. \"1. Outside\", a cyberpunk narrative fueled concept album by David Bowie, was warmly met by critics upon its release in 1995. Many musicians have also taken inspiration from specific cyberpunk works or authors, including Sonic Youth, whose albums \"Sister\" and \"Daydream Nation\" take influence from the works of Philip K. Dick and William Gibson respectively. Madonna's 2001 Drowned World Tour opened with a cyberpunk section, where costumes, asethetics and stage props were used to accentuate the dystopian nature of the theatrical concert.\nVaporwave and synthwave are also influenced by cyberpunk. The former has been inspired by one of the messages of cyberpunk and is interpreted as a dystopian critique of capitalism in the vein of cyberpunk and the latter is more surface-level, inspired only by the aesthetic of cyberpunk as a nostalgic retrofuturistic revival of aspects of cyberpunk's origins.\nSocial impact.\nArt and architecture.\nSome Neo-Futurism artworks and cityscapes have been influenced by cyberpunk. Writers David Suzuki and Holly Dressel describe the cafes, brand-name stores and video arcades of the Sony Center in the Potsdamer Platz public square of Berlin, Germany, as \"a vision of a cyberpunk, corporate urban future\".\nSociety and counterculture.\nSeveral subcultures have been inspired by cyberpunk fiction. These include the cyberdelic counter culture of the late 1980s and early 90s. Cyberdelic, whose adherents referred to themselves as \"cyberpunks\", attempted to blend the psychedelic art and drug movement with the technology of cyberculture. Early adherents included Timothy Leary, Mark Frauenfelder and R. U. Sirius. The movement largely faded following the dot-com bubble implosion of 2000.\nCybergoth is a fashion and dance subculture which draws its inspiration from cyberpunk fiction, as well as rave and Gothic subcultures. In addition, a distinct cyberpunk fashion of its own has emerged in recent years which rejects the raver and goth influences of cybergoth, and draws inspiration from urban street fashion, \"post apocalypse\", functional clothing, high tech sports wear, tactical uniform and multifunction. This fashion goes by names like \"tech wear\", \"goth ninja\" or \"tech ninja\".\nThe Kowloon Walled City in Hong Kong (demolished in 1994) is often referenced as the model cyberpunk/dystopian slum as, given its poor living conditions at the time coupled with the city's political, physical, and economic isolation has caused many in academia to be fascinated by the ingenuity of its spawning.\nRelated genres.\nAs a wider variety of writers began to work with cyberpunk concepts, new subgenres of science fiction emerged, some of which could be considered as playing off the cyberpunk label, others which could be considered as legitimate explorations into newer territory. These focused on technology and its social effects in different ways. One prominent subgenre is \"steampunk,\" which is set in an alternate history Victorian era that combines anachronistic technology with cyberpunk's bleak film noir world view. The term was originally coined around 1987 as a joke to describe some of the novels of Tim Powers, James P. Blaylock, and K.W. Jeter, but by the time Gibson and Sterling entered the subgenre with their collaborative novel \"The Difference Engine\" the term was being used earnestly as well.\nAnother subgenre is \"biopunk\" (cyberpunk themes dominated by biotechnology) from the early 1990s, a derivative style building on biotechnology rather than informational technology. In these stories, people are changed in some way not by mechanical means, but by genetic manipulation. Paul Di Filippo is seen as the most prominent biopunk writer, including his half-serious ribofunk. Bruce Sterling's Shaper/Mechanist cycle is also seen as a major influence. In addition, some people consider works such as Neal Stephenson's \"The Diamond Age\" to be postcyberpunk.\nCyberpunk works have been described as well situated within postmodern literature.\nRegistered trademark status.\nIn the United States, the term \"Cyberpunk\" is a registered trademark by R. Talsorian Games Inc. for its tabletop role-playing game.\nWithin the European Union, the \"Cyberpunk\" trademark is owned by two parties: CD Projekt SA for \"games and online gaming services\" (particularly for the video game adaptation of the former) and by Sony Music for use outside games."}
{"id": "5704", "revid": "16495000", "url": "https://en.wikipedia.org/wiki?curid=5704", "title": "Comic strip", "text": "A comic strip is a sequence of drawings, often cartoon, arranged in interrelated panels to display brief humor or form a narrative, often serialized, with text in balloons and captions. Traditionally, throughout the 20th and into the 21st century, these have been published in newspapers and magazines, with daily horizontal strips printed in black-and-white in newspapers, while Sunday papers offered longer sequences in special color comics sections. With the advent of the internet, online comic strips began to appear as webcomics.\nStrips are written and drawn by a comics artist, known as a cartoonist. As the word \"comic\" implies, strips are frequently humorous. Examples of these gag-a-day strips are \"Blondie\", \"Bringing Up Father\", \"Marmaduke\", and \"Pearls Before Swine\". In the late 1920s, comic strips expanded from their mirthful origins to feature adventure stories, as seen in \"Popeye\", \"Captain Easy\", \"Buck Rogers\", \"Tarzan\", and \"Terry and the Pirates\". In the 1940s, soap-opera-continuity strips such as \"Judge Parker\" and \"Mary Worth\" gained popularity. Because \"comic\" strips are not always funny, cartoonist Will Eisner has suggested that sequential art would be a better genre-neutral name.\nEvery day in American newspapers, for most of the 20th century, there were at least 200 different comic strips and cartoon panels, which makes 73,000 per year. Comic strips have appeared inside American magazines such as \"Liberty\" and \"Boys' Life\", but also on the front covers, such as the \"Flossy Frills\" series on \"The American Weekly\" Sunday newspaper supplement. In the UK and the rest of Europe, comic strips are also serialized in \"comic book magazines\", with a strip's story sometimes continuing over three pages or more.\nHistory.\nStorytelling using a sequence of pictures has existed through history. One medieval European example in textile form is the Bayeux Tapestry. Printed examples emerged in 19th-century Germany and in 18th-century England, where some of the first satirical or humorous sequential narrative drawings were produced. William Hogarth's 18th century English cartoons include both narrative sequences, such as \"A Rake's Progress\", and single panels.\nThe \"Biblia pauperum\" (\"Paupers' Bible\"), a tradition of picture Bibles beginning in the later Middle Ages, sometimes depicted Biblical events with words spoken by the figures in the miniatures written on scrolls coming out of their mouths\u2014which makes them to some extent ancestors of the modern cartoon strips.\nIn China, with its traditions of block printing and of the incorporation of text with image, experiments with what became \"lianhuanhua\" date back to 1884.\nNewspapers.\nThe first newspaper comic strips appeared in North America in the late 19th century. \"The Yellow Kid\" is usually credited as one of the first newspaper strips. However, the art form combining words and pictures developed gradually and there are many examples which led up to the comic strip.\nSwiss author and caricature artist Rodolphe T\u00f6pffer (Geneva, 1799\u20131846) is considered the father of the modern comic strips. His illustrated stories such as \"Histoire de M. Vieux Bois\" (1827), first published in the USA in 1842 as \"The Adventures of Obadiah Oldbuck\" or \"Histoire de Monsieur Jabot\" (1831), inspired subsequent generations of German and American comic artists. In 1865, German painter, author, and caricaturist Wilhelm Busch created the strip \"Max and Moritz\", about two trouble-making boys, which had a direct influence on the American comic strip. \"Max and Moritz\" was a series of seven severely moralistic tales in the vein of German children's stories such as \"Struwwelpeter\" (\"Shockheaded Peter\"). In the story's final act, the boys, after perpetrating some mischief, are tossed into a sack of grain, run through a mill, and consumed by a flock of geese (without anybody mourning their demise). \"Max and Moritz\" provided an inspiration for German immigrant Rudolph Dirks, who created the \"Katzenjammer Kids\" in 1897 \u2013 a strip starring two German-American boys visually modelled on \"Max and Moritz\". Familiar comic-strip iconography such as stars for pain, sawing logs for snoring, speech balloons, and thought balloons originated in Dirks' strip.\nHugely popular, \"Katzenjammer Kids\" occasioned one of the first comic-strip copyright ownership suits in the history of the medium. When Dirks left William Randolph Hearst for the promise of a better salary under Joseph Pulitzer, it was an unusual move, since cartoonists regularly deserted Pulitzer for Hearst. In a highly unusual court decision, Hearst retained the rights to the name \"Katzenjammer Kids\", while creator Dirks retained the rights to the characters. Hearst promptly hired Harold Knerr to draw his own version of the strip. Dirks renamed his version \"Hans and Fritz\" (later, \"The Captain and the Kids\"). Thus, two versions distributed by rival syndicates graced the comics pages for decades. Dirks' version, eventually distributed by United Feature Syndicate, ran until 1979.\nIn the United States, the great popularity of comics sprang from the . \"The Little Bears\" (1893\u201396) was the first American comic strip with recurring characters, while the first color comic supplement was published by the \"Chicago Inter-Ocean\" sometime in the latter half of 1892, followed by the \"New York Journal\"s first color Sunday comic pages in 1897. On January 31, 1912, Hearst introduced the nation's first full daily comic page in his \"New York Evening Journal\". The history of this newspaper rivalry and the rapid appearance of comic strips in most major American newspapers is discussed by Ian Gordon. Numerous events in newspaper comic strips have reverberated throughout society at large, though few of these events occurred in recent years, owing mainly to the declining use of continuous storylines on newspaper comic strips, which since the 1970s had been waning as an entertainment form. From 1903 to 1905 Gustave Verbeek, wrote his comic series \"The UpsideDowns of Old Man Muffaroo and Little Lady Lovekins\". These comics were made in such a way that one could read the 6 panel comic, flip the book and keep reading. He made 64 such comics in total.\nThe longest-running American comic strips are:\nMost newspaper comic strips are syndicated; a syndicate hires people to write and draw a strip and then distributes it to many newspapers for a fee. Some newspaper strips begin or remain exclusive to one newspaper. For example, the \"Pogo\" comic strip by Walt Kelly originally appeared only in the \"New York Star\" in 1948 and was not picked up for syndication until the following year.\nNewspaper comic strips come in two different types: daily strips and Sunday strips. In the United States, a daily strip appears in newspapers on weekdays, Monday through Saturday, as contrasted with a Sunday strip, which typically only appears on Sundays. Daily strips usually are printed in black and white, and Sunday strips are usually in color. However, a few newspapers have published daily strips in color, and some newspapers have published Sunday strips in black and white.\nPopularity.\nWhile in the early 20th century comic strips were a frequent target for detractors of \"yellow journalism\", by the 1920s the medium became wildly popular. While radio, and later, television surpassed newspapers as a means of entertainment, most comic strip characters were widely recognizable until the 1980s, and the \"funny pages\" were often arranged in a way they appeared at the front of Sunday editions. In 1931, George Gallup's first poll had the comic section as the most important part of the newspaper, with additional surveys pointing out that the comic strips were the second most popular feature after the picture page. During the 1930s, many comic sections had between 12 and 16 pages, although in some cases, these had up to 24 pages.\nThe popularity and accessibility of strips meant they were often clipped and saved; authors including John Updike and Ray Bradbury have written about their childhood collections of clipped strips. Often posted on bulletin boards, clipped strips had an ancillary form of distribution when they were faxed, photocopied or mailed. \"The Baltimore Sun\"s Linda White recalled, \"I followed the adventures of \"Winnie Winkle\", \"Moon Mullins\" and \"Dondi\", and waited each fall to see how Lucy would manage to trick Charlie Brown into trying to kick that football. (After I left for college, my father would clip out that strip each year and send it to me just to make sure I didn\u2019t miss it.)\"\nProduction and format.\nThe two conventional formats for newspaper comics are strips and single gag panels. The strips are usually displayed horizontally, wider than they are tall. Single panels are square, circular or taller than they are wide. Strips usually, but not always, are broken up into several smaller panels with continuity from panel to panel. A horizontal strip can also be used for a single panel with a single gag, as seen occasionally in Mike Peters' \"Mother Goose and Grimm\".\nEarly daily strips were large, often running the entire width of the newspaper, and were sometimes three or more inches high. Initially, a newspaper page included only a single daily strip, usually either at the top or the bottom of the page. By the 1920s, many newspapers had a comics page on which many strips were collected together. During the 1930s, the original art for a daily strip could be drawn as large as 25\u00a0inches wide by six inches high. Over decades, the size of daily strips became smaller and smaller, until by 2000, four standard daily strips could fit in an area once occupied by a single daily strip. As strips have become smaller, the number of panels have been reduced.\nProof sheets were the means by which syndicates provided newspapers with black-and-white line art for the reproduction of strips (which they arranged to have colored in the case of Sunday strips). Michigan State University Comic Art Collection librarian Randy Scott describes these as \"large sheets of paper on which newspaper comics have traditionally been distributed to subscribing newspapers. Typically each sheet will have either six daily strips of a given title or one Sunday strip. Thus, a week of \"Beetle Bailey\" would arrive at the \"Lansing State Journal\" in two sheets, printed much larger than the final version and ready to be cut apart and fitted into the local comics page.\" Comic strip historian Allan Holtz described how strips were provided as mats (the plastic or cardboard trays in which molten metal is poured to make plates) or even plates ready to be put directly on the printing press. He also notes that with electronic means of distribution becoming more prevalent printed sheets \"are definitely on their way out.\"\nNEA Syndicate experimented briefly with a two-tier daily strip, \"Star Hawks\", but after a few years, \"Star Hawks\" dropped down to a single tier.\nIn Flanders, the two-tier strip is the standard publication style of most daily strips like \"Spike and Suzy\" and \"Nero\". They appear Monday through Saturday; until 2003 there were no Sunday papers in Flanders. In the last decades, they have switched from black and white to color.\nCartoon panels.\nSingle panels usually, but not always, are not broken up and lack continuity. The daily \"Peanuts\" is a strip, and the daily \"Dennis the Menace\" is a single panel. J. R. Williams' long-run \"Out Our Way\" continued as a daily panel even after it expanded into a Sunday strip, \"Out Our Way with the Willets\". Jimmy Hatlo's \"They'll Do It Every Time\" was often displayed in a two-panel format with the first panel showing some deceptive, pretentious, unwitting or scheming human behavior and the second panel revealing the truth of the situation.\nSunday comics.\nSunday newspapers traditionally included a special color section. Early Sunday strips (known colloquially as \"the funny papers\", shortened to \"the funnies\"), such as \"Thimble Theatre\" and \"Little Orphan Annie\", filled an entire newspaper page, a format known to collectors as full page. Sunday pages during the 1930s and into the 1940s often carried a secondary strip by the same artist as the main strip. No matter whether it appeared above or below a main strip, the extra strip was known as the topper, such as \"The Squirrel Cage\" which ran along with \"Room and Board\", both drawn by Gene Ahern.\nDuring the 1930s, the original art for a Sunday strip was usually drawn quite large. For example, in 1930, Russ Westover drew his \"Tillie the Toiler\" Sunday page at a size of 17\" \u00d7 37\". In 1937, the cartoonist Dudley Fisher launched the innovative \"Right Around Home\", drawn as a huge single panel filling an entire Sunday page.\nFull-page strips were eventually replaced by strips half that size. Strips such as \"The Phantom\" and \"Terry and the Pirates\" began appearing in a format of two strips to a page in full-size newspapers, such as the \"New Orleans Times Picayune\", or with one strip on a tabloid page, as in the \"Chicago Sun-Times\". When Sunday strips began to appear in more than one format, it became necessary for the cartoonist to allow for rearranged, cropped or dropped panels. During World War II, because of paper shortages, the size of Sunday strips began to shrink. After the war, strips continued to get smaller and smaller because of increased paper and printing costs. The last full-page comic strip was the \"Prince Valiant\" strip for 11 April 1971.\nComic strips have also been published in Sunday newspaper magazines. Russell Patterson and Carolyn Wells' \"New Adventures of Flossy Frills\" was a continuing strip series seen on Sunday magazine covers. Beginning January 26, 1941, it ran on the front covers of Hearst's \"American Weekly\" newspaper magazine supplement, continuing until March 30 of that year. Between 1939 and 1943, four different stories featuring Flossy appeared on \"American Weekly\" covers.\nSunday comics sections employed offset color printing with multiple print runs imitating a wide range of colors. Printing plates were created with four or more colors\u2014traditionally, the CMYK color model: cyan, magenta, yellow and \"K\" for black. With a screen of tiny dots on each printing plate, the dots allowed an image to be printed in a halftone that appears to the eye in different gradations. The semi-opaque property of ink allows halftone dots of different colors to create an optical effect of full-color imagery.\nUnderground comic strips.\nThe decade of the 1960s saw the rise of underground newspapers, which often carried comic strips, such as \"Fritz the Cat\" and \"The Fabulous Furry Freak Brothers\". \"Zippy the Pinhead\" initially appeared in underground publications in the 1970s before being syndicated. \"Bloom County\" and \"Doonesbury\" began as strips in college newspapers under different titles, and later moved to national syndication. Underground comic strips covered subjects that are usually taboo in newspaper strips, such as sex and drugs. Many underground artists, notably Vaughn Bode, Dan O'Neill, Gilbert Shelton, and Art Spiegelman went on to draw comic strips for magazines such as \"Playboy\", \"National Lampoon\", and Pete Millar's \"CARtoons\". Jay Lynch graduated from undergrounds to alternative weekly newspapers to \"Mad\" and children's books.\nWebcomics.\n\"Webcomics\", also known as \"online comics\" and \"internet comics\", are comics that are available to read on the Internet. Many are exclusively published online, but the majority of traditional newspaper comic strips have some Internet presence. King Features Syndicate and other syndicates often provide archives of recent strips on their websites. Some, such as Scott Adams, creator of \"Dilbert\", include an email address in each strip.\nConventions and genres.\nMost comic strip characters do not age throughout the strip's life, but in some strips, like Lynn Johnston's award-winning \"For Better or For Worse\", the characters age as the years pass. The first strip to feature aging characters was \"Gasoline Alley\".\nThe history of comic strips also includes series that are not humorous, but tell an ongoing dramatic story. Examples include \"The Phantom\", \"Prince Valiant\", \"Dick Tracy\", \"Mary Worth\", \"Modesty Blaise\", \"Little Orphan Annie\", \"Flash Gordon\", and \"Tarzan\". Sometimes these are spin-offs from comic books, for example \"Superman\", \"Batman\", and \"The Amazing Spider-Man\".\nA number of strips have featured animals ('funny animals') as main characters. Some are non-verbal (\"Marmaduke\", \"The Angriest Dog in the World\"), some have verbal thoughts but are not understood by humans, (\"Garfield\", Snoopy in \"Peanuts\"), and some can converse with humans (\"Bloom County\", \"Calvin and Hobbes\", \"Mutts\", \"Citizen Dog\", \"Buckles\", \"Get Fuzzy\", \"Pearls Before Swine\", and \"Pooch Cafe\"). Other strips are centered entirely on animals, as in \"Pogo\" and \"Donald Duck\". Gary Larson's \"The Far Side\" was unusual, as there were no central characters. Instead \"The Far Side\" used a wide variety of characters including humans, monsters, aliens, chickens, cows, worms, amoebas, and more. John McPherson's \"Close to Home\" also uses this theme, though the characters are mostly restricted to humans and real-life situations. Wiley Miller not only mixes human, animal, and fantasy characters, but also does several different comic strip continuities under one umbrella title, \"Non Sequitur\". Bob Thaves's \"Frank &amp; Ernest\" began in 1972 and paved the way for some of these strips, as its human characters were manifest in diverse forms \u2014 as animals, vegetables, and minerals.\nSocial and political influence.\nThe comics have long held a distorted mirror to contemporary society, and almost from the beginning have been used for political or social commentary. This ranged from the conservative slant of Harold Gray's \"Little Orphan Annie\" to the unabashed liberalism of Garry Trudeau's \"Doonesbury\". Al Capp's \"Li'l Abner\" espoused liberal opinions for most of its run, but by the late 1960s, it became a mouthpiece for Capp's repudiation of the counterculture.\n\"Pogo\" used animals to particularly devastating effect, caricaturing many prominent politicians of the day as animal denizens of Pogo's Okeefenokee Swamp. In a fearless move, Pogo's creator Walt Kelly took on Joseph McCarthy in the 1950s, caricaturing him as a bobcat named Simple J. Malarkey, a megalomaniac who was bent on taking over the characters' birdwatching club and rooting out all undesirables. Kelly also defended the medium against possible government regulation in the McCarthy era. At a time when comic books were coming under fire for supposed sexual, violent, and subversive content, Kelly feared the same would happen to comic strips. Going before the Congressional subcommittee, he proceeded to charm the members with his drawings and the force of his personality. The comic strip was safe for satire.\nDuring the early 20th century, comic strips were widely associated with publisher William Randolph Hearst, whose papers had the largest circulation of strips in the United States. Hearst was notorious for his practice of yellow journalism, and he was frowned on by readers of \"The New York Times\" and other newspapers which featured few or no comic strips. Hearst's critics often assumed that all the strips in his papers were fronts for his own political and social views. Hearst did occasionally work with or pitch ideas to cartoonists, most notably his continued support of George Herriman's \"Krazy Kat\". An inspiration for Bill Watterson and other cartoonists, \"Krazy Kat\" gained a considerable following among intellectuals during the 1920s and 1930s.\nSome comic strips, such as \"Doonesbury\" and \"Mallard Fillmore\", may be printed on the editorial or op-ed page rather than the comics page because of their regular political commentary. For example, the August 12, 1974 \"Doonesbury\" strip was awarded a 1975 Pulitzer Prize for its depiction of the Watergate scandal. \"Dilbert\" is sometimes found in the business section of a newspaper instead of the comics page because of the strip's commentary about office politics, and Tank McNamara often appears on the sports page because of its subject matter. Lynn Johnston's \"For Better or for Worse\" created an uproar when Lawrence, one of the strip's supporting characters, came out of the closet.\nPublicity and recognition.\nThe world's longest comic strip is long and on display at Trafalgar Square as part of the London Comedy Festival. The London Cartoon Strip was created by 15 of Britain's best known cartoonists and depicts the history of London.\nThe Reuben, named for cartoonist Rube Goldberg, is the most prestigious award for U.S. comic strip artists. Reuben awards are presented annually by the National Cartoonists Society (NCS).\nIn 1995, the United States Postal Service issued a series of commemorative stamps, Comic Strip Classics, marking the comic-strip centennial.\nToday's strip artists, with the help of the NCS, enthusiastically promote the medium, which since the 1970s (and particularly the 1990s) has been considered to be in decline due to numerous factors such as changing tastes in humor and entertainment, the waning relevance of newspapers in general and the loss of most foreign markets outside English-speaking countries. One particularly humorous example of such promotional efforts is the Great Comic Strip Switcheroonie, held in 1997 on April Fool's Day, an event in which dozens of prominent artists took over each other's strips. \"Garfield\"\u2019s Jim Davis, for example, switched with \"Blondie\"\u2019s Stan Drake, while Scott Adams (\"Dilbert\") traded strips with Bil Keane (\"The Family Circus\").\nWhile the 1997 Switcheroonie was a one-time publicity stunt, an artist taking over a feature from its originator is an old tradition in newspaper cartooning (as it is in the comic book industry). In fact, the practice has made possible the longevity of the genre's more popular strips. Examples include \"Little Orphan Annie\" (drawn and plotted by Harold Gray from 1924 to 1944 and thereafter by a succession of artists including Leonard Starr and Andrew Pepoy), and \"Terry and the Pirates\", started by Milton Caniff in 1934 and picked up by George Wunder.\nA business-driven variation has sometimes led to the same feature continuing under a different name. In one case, in the early 1940s, Don Flowers' \"Modest Maidens\" was so admired by William Randolph Hearst that he lured Flowers away from the Associated Press and to King Features Syndicate by doubling the cartoonist's salary, and renamed the feature \"Glamor Girls\" to avoid legal action by the AP. The latter continued to publish \"Modest Maidens\", drawn by Jay Allen in Flowers' style.\nIssues in U.S. newspaper comic strips.\nAs newspapers have declined, the changes have affected comic strips. Jeff Reece, lifestyle editor of \"The Florida Times-Union\", wrote, \"Comics are sort of the 'third rail' of the newspaper.\"\nSize.\nIn the early decades of the 20th century, all Sunday comics received a full page, and daily strips were generally the width of the page. The competition between papers for having more cartoons than the rest from the mid-1920s, the growth of large-scale newspaper advertising during most of the thirties, paper rationing during World War II, the decline on news readership (as television newscasts began to be more common) and inflation (which has caused higher printing costs) beginning during the fifties and sixties led to Sunday strips being published on smaller and more diverse formats. As newspapers have reduced the page count of Sunday comic sections since the late 1990s (by the 2010s, most sections have only four pages, with the back page not always being destined for comics) has also led to further downsizes.\nDaily strips have suffered as well. Before the mid-1910s, there wasn't a \"standard\" size\", with strips running the entire width of a page or having more than one tier. By the 1920s, strips often covered six of the eight columns occupied by a traditional broadsheet paper. During the 1940s, strips were reduced to four columns wide (with a \"transition\" width of five columns). As newspapers became narrower beginning in the 1970s, strips have gotten even smaller, often being just three columns wide, a similar width to the one most daily panels occupied before the 1940s.\nIn an issue related to size limitations, Sunday comics are often bound to rigid formats that allow their panels to be rearranged in several different ways while remaining readable. Such formats usually include throwaway panels at the beginning, which some newspapers will omit for space. As a result, cartoonists have less incentive to put great efforts into these panels. \"Garfield\" and \"Mutts\" were known during the mid-to-late 80s and 1990s respectively for their throwaways on their Sunday strips, however both strips now run \"generic\" title panels.\nSome cartoonists have complained about this, with Walt Kelly, creator of \"Pogo,\" openly voicing his discontent about being forced to draw his Sunday strips in such rigid formats from the beginning. Kelly's heirs opted to end the strip in 1975 as a form of protest against the practice. Since then, \"Calvin and Hobbes\" creator Bill Watterson has written extensively on the issue, arguing that size reduction and dropped panels reduce both the potential and freedom of a cartoonist. After a lengthy battle with his syndicate, Watterson won the privilege of making half page-sized Sunday strips where he could arrange the panels any way he liked. Many newspaper publishers and a few cartoonists objected to this, and some papers continued to print \"Calvin and Hobbes\" at small sizes. \"Opus\" won that same privilege years after \"Calvin and Hobbes\" ended, while Wiley Miller circumvented further downsizes by making his \"Non Sequitur\" Sunday strip available only in a vertical arrangement. Actually, most strips created since 1990 are drawn in the unbroken \"third-page\" format. Few newspapers still run half-page strips, as with \"Prince Valiant\" and \"H\u00e4gar the Horrible\" in the front page of the \"Reading Eagle\" Sunday comics section until the mid-2010s.\nFormat.\nWith the success of \"The Gumps\" during the 1920s, it became commonplace for strips (comedy- and adventure-laden alike) to have lengthy stories spanning weeks or months. The \"Monarch of Medioka\" story in Floyd Gottfredson's \"Mickey Mouse\" comic strip ran from September 8, 1937 to May 2, 1938. Between the 1960s and the late 1980s, as television news relegated newspaper reading to an occasional basis rather than daily, syndicators were abandoning long stories and urging cartoonists to switch to simple daily gags, or week-long \"storylines\" (with six consecutive (mostly unrelated) strips following a same subject), with longer storylines being used mainly on adventure-based and dramatic strips. Strips begun during the mid-1980s or after (such as \"Get Fuzzy\", \"Over the Hedge\", \"Monty\", and others) are known for their heavy use of storylines, lasting between one and three weeks in most cases.\nThe writing style of comic strips changed as well after World War II. With an increase in the number of college-educated readers, there was a shift away from slapstick comedy and towards more cerebral humor. Slapstick and visual gags became more confined to Sunday strips, because as \"Garfield\" creator Jim Davis put it, \"Children are more likely to read Sunday strips than dailies.\"\nSecond author.\nMany older strips are no longer drawn by the original cartoonist, who has either died or retired. Such strips are known as \"zombie strips\". A cartoonist, paid by the syndicate or sometimes a relative of the original cartoonist, continues writing the strip, a tradition that became commonplace in the early half of the 20th century. \"H\u00e4gar the Horrible\" and \"Frank and Ernest\" are both drawn by the sons of the creators. Some strips which are still in affiliation with the original creator are produced by small teams or entire companies, such as Jim Davis' \"Garfield\", however there is some debate if these strips fall in this category.\nThis act is commonly criticized by modern cartoonists including Watterson and \"Pearls Before Swine\"'s Stephan Pastis. The issue was addressed in six consecutive \"Pearls\" strips in 2005. Charles Schulz, of \"Peanuts\" fame, requested that his strip not be continued by another cartoonist after his death. He also rejected the idea of hiring an inker or letterer, comparing it to a golfer hiring a man to make his putts. Schulz's family has honored his wishes and refused numerous proposals by syndicators to continue \"Peanuts\" with a new author.\nAssistants.\nSince the consolidation of newspaper comics by the first quarter of the 20th century, most cartoonists have used a group of assistants (with usually one of them credited). However, quite a few cartoonists (e.g.: George Herriman and Charles Schulz, among others) have done their strips almost completely by themselves; often criticizing the use of assistants for the same reasons most have about their editors hiring anyone else to continue their work after their retirement.\nRights to the strips.\nHistorically, syndicates owned the creators' work, enabling them to continue publishing the strip after the original creator retired, left the strip, or died. This practice led to the term \"legacy strips,\" or more pejoratively \"zombie strips\"). Most syndicates signed creators to 10- or even 20-year contracts. (There have been exceptions, however, such as Bud Fisher's \"Mutt and Jeff\" being an early\u2014if not the earliest\u2014case in which the creator retained ownership of his work.) Both these practices began to change with the 1970 debut of Universal Press Syndicate, as the company gave cartoonists a 50-percent ownership share of their work. Creators Syndicate, founded in 1987, granted artists full rights to the strips, something that Universal Press did in 1990, followed by King Features in 1995. By 1999 both Tribune Media Services and United Feature had begun granting ownership rights to creators (limited to new and/or hugely popular strips).\nCensorship.\nStarting in the late 1940s, the national syndicates which distributed newspaper comic strips subjected them to very strict censorship. \"Li'l Abner\" was censored in September 1947 and was pulled from the Pittsburgh Press by Scripps-Howard. The controversy, as reported in \"Time\", centered on Capp's portrayal of the U.S. Senate. Said Edward Leech of Scripps, \"We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables.\"\nAs comics are easier for children to access compared to other types of media, they have a significantly more rigid censorship code than other media. Stephan Pastis has lamented that the \"unwritten\" censorship code is still \"stuck somewhere in the 1950s\". Generally, comics are not allowed to include such words as \"damn\", \"sucks\", \"screwed\", and \"hell\", although there have been exceptions such as the September 22, 2010 \"Mother Goose and Grimm\" in which an elderly man says, \"This nursing home food sucks,\" and a pair of \"Pearls Before Swine\" comics from January 11, 2011 with a character named Ned using the word \"crappy\". Naked backsides and shooting guns cannot be shown, according to \"Dilbert\" cartoonist Scott Adams. Such comic strip taboos were detailed in Dave Breger's book \"But That's Unprintable\" (Bantam, 1955).\nMany issues such as sex, narcotics, and terrorism cannot or can very rarely be openly discussed in strips, although there are exceptions, usually for satire, as in \"Bloom County\". This led some cartoonists to resort to double entendre or dialogue children do not understand, as in Greg Evans' \"Luann\". Younger cartoonists have claimed commonplace words, images, and issues should be allowed in the comics, considering that the pressure on \"clean\" humor has been a chief factor for the declining popularity of comic strips since the 1990s (Aaron McGruder, creator of \"The Boondocks\", decided to end his strip partly because of censorship issues, while the \"Popeye\" daily comic strip ended in 1994 after newspapers objected to a storyline they considered to be a satire on abortion). Some of the taboo words and topics are mentioned daily on television and other forms of visual media. Webcomics and comics distributed primarily to college newspapers are much freer in this respect."}
{"id": "5705", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5705", "title": "Continuum hypothesis", "text": "In mathematics, the continuum hypothesis (abbreviated CH) is a hypothesis about the possible sizes of infinite sets. It states:\nIn Zermelo\u2013Fraenkel set theory with the axiom of choice (ZFC), this is equivalent to the following equation in aleph numbers: formula_1.\nThe continuum hypothesis was advanced by Georg Cantor in 1878, and establishing its truth or falsehood is the first of Hilbert's 23\u00a0problems presented in 1900. The answer to this problem is independent of ZFC, so that either the continuum hypothesis or its negation can be added as an axiom to ZFC set theory, with the resulting theory being consistent if and only if ZFC is consistent. This independence was proved in 1963 by Paul Cohen, complementing earlier work by Kurt G\u00f6del in 1940.\nThe name of the hypothesis comes from the term \"the continuum\" for the real numbers.\nHistory.\nCantor believed the continuum hypothesis to be true and for many years tried in vain to prove it . It became the first on David Hilbert's list of important open questions that was presented at the International Congress of Mathematicians in the year 1900 in Paris. Axiomatic set theory was at that point not yet formulated. \nKurt G\u00f6del proved in 1940 that the negation of the continuum hypothesis, i.e., the existence of a set with intermediate cardinality, could not be proved in standard set theory. The second half of the independence of the continuum hypothesis \u2013 i.e., unprovability of the nonexistence of an intermediate-sized set \u2013 was proved in 1963 by Paul Cohen.\nCardinality of infinite sets.\nTwo sets are said to have the same \"cardinality\" or \"cardinal number\" if there exists a bijection (a one-to-one correspondence) between them. Intuitively, for two sets \"S\" and \"T\" to have the same cardinality means that it is possible to \"pair off\" elements of \"S\" with elements of \"T\" in such a fashion that every element of \"S\" is paired off with exactly one element of \"T\" and vice versa. Hence, the set {banana, apple, pear} has the same cardinality as {yellow, red, green}.\nWith infinite sets such as the set of integers or rational numbers, the existence of a bijection between two sets becomes more difficult to demonstrate. The rational numbers seemingly form a counterexample to the continuum hypothesis: the integers form a proper subset of the rationals, which themselves form a proper subset of the reals, so intuitively, there are more rational numbers than integers and more real numbers than rational numbers. However, this intuitive analysis is flawed; it does not take proper account of the fact that all three sets are infinite. It turns out the rational numbers can actually be placed in one-to-one correspondence with the integers, and therefore the set of rational numbers is the same size (\"cardinality\") as the set of integers: they are both countable sets.\nCantor gave two proofs that the cardinality of the set of integers is strictly smaller than that of the set of real numbers (see Cantor's first uncountability proof and Cantor's diagonal argument). His proofs, however, give no indication of the extent to which the cardinality of the integers is less than that of the real numbers. Cantor proposed the continuum hypothesis as a possible solution to this question.\nThe continuum hypothesis states that the set of real numbers has minimal possible cardinality which is greater than the cardinality of the set of integers. That is, every set, \"S\", of real numbers can either be mapped one-to-one into the integers or the real numbers can be mapped one-to-one into \"S\". As the real numbers are equinumerous with the powerset of the integers, formula_2 and the continuum hypothesis says that there is no set formula_3 for which formula_4.\nAssuming the axiom of choice, there is a smallest cardinal number formula_5 greater than formula_6, and the continuum hypothesis is in turn equivalent to the equality formula_7 .\nIndependence from ZFC.\nThe independence of the continuum hypothesis (CH) from Zermelo\u2013Fraenkel set theory (ZF) follows from combined work of Kurt G\u00f6del and Paul Cohen.\n showed that CH cannot be disproved from ZF, even if the axiom of choice (AC) is adopted (making ZFC). G\u00f6del's proof shows that CH and AC both hold in the constructible universe L, an inner model of ZF set theory, assuming only the axioms of ZF. The existence of an inner model of ZF in which additional axioms hold shows that the additional axioms are consistent with ZF, provided ZF itself is consistent. The latter condition cannot be proved in ZF itself, due to G\u00f6del's incompleteness theorems, but is widely believed to be true and can be proved in stronger set theories.\nCohen (1963, 1964) showed that CH cannot be proven from the ZFC axioms, completing the overall independence proof. To prove his result, Cohen developed the method of forcing, which has become a standard tool in set theory. Essentially, this method begins with a model of ZF in which CH holds, and constructs another model which contains more sets than the original, in a way that CH does not hold in the new model. Cohen was awarded the Fields Medal in 1966 for his proof.\nThe independence proof just described shows that CH is independent of ZFC. Further research has shown that CH is independent of all known \"large cardinal axioms\" in the context of ZFC. () Moreover, it has been shown that the cardinality of the continuum can be any cardinal consistent with K\u00f6nig's theorem. A result of Solovay, proved shortly after Cohen's result on the independence of the continuum hypothesis, shows that in any model of ZFC, if formula_8 is a cardinal of uncountable cofinality, then there is a forcing extension in which formula_9. However, per K\u00f6nig's theorem, it is not consistent to assume formula_10 is formula_11 or formula_12 or any cardinal with cofinality formula_13.\nThe continuum hypothesis is closely related to many statements in analysis, point set topology and measure theory. As a result of its independence, many substantial conjectures in those fields have subsequently been shown to be independent as well.\nThe independence from ZFC means that proving or disproving the CH within ZFC is impossible. However, G\u00f6del and Cohen's negative results are not universally accepted as disposing of all interest in the continuum hypothesis. Hilbert's problem remains an active topic of research; see and for an overview of the current research status.\nThe continuum hypothesis was not the first statement shown to be independent of ZFC. An immediate consequence of G\u00f6del's incompleteness theorem, which was published in 1931, is that there is a formal statement (one for each appropriate G\u00f6del numbering scheme) expressing the consistency of ZFC that is independent of ZFC, assuming that ZFC is consistent. The continuum hypothesis and the axiom of choice were among the first mathematical statements shown to be independent of ZF set theory.\nArguments for and against the continuum hypothesis.\nG\u00f6del believed that CH is false, and that his proof that CH is consistent with ZFC only shows that the Zermelo\u2013Fraenkel axioms do not adequately characterize the universe of sets. G\u00f6del was a platonist and therefore had no problems with asserting the truth and falsehood of statements independent of their provability. Cohen, though a formalist , also tended towards rejecting CH.\nHistorically, mathematicians who favored a \"rich\" and \"large\" universe of sets were against CH, while those favoring a \"neat\" and \"controllable\" universe favored CH. Parallel arguments were made for and against the axiom of constructibility, which implies CH. More recently, Matthew Foreman has pointed out that ontological maximalism can actually be used to argue in favor of CH, because among models that have the same reals, models with \"more\" sets of reals have a better chance of satisfying CH .\nAnother viewpoint is that the conception of set is not specific enough to determine whether CH is true or false. This viewpoint was advanced as early as 1923 by Skolem, even before G\u00f6del's first incompleteness theorem. Skolem argued on the basis of what is now known as Skolem's paradox, and it was later supported by the independence of CH from the axioms of ZFC since these axioms are enough to establish the elementary properties of sets and cardinalities. In order to argue against this viewpoint, it would be sufficient to demonstrate new axioms that are supported by intuition and resolve CH in one direction or another. Although the axiom of constructibility does resolve CH, it is not generally considered to be intuitively true any more than CH is generally considered to be false .\nAt least two other axioms have been proposed that have implications for the continuum hypothesis, although these axioms have not currently found wide acceptance in the mathematical community. In 1986, Chris Freiling presented an argument against CH by showing that the negation of CH is equivalent to Freiling's axiom of symmetry, a statement derived by arguing from particular intuitions about probabilities. Freiling believes this axiom is \"intuitively true\" but others have disagreed. A difficult argument against CH developed by W. Hugh Woodin has attracted considerable attention since the year 2000 . Foreman (2003) does not reject Woodin's argument outright but urges caution.\nSolomon Feferman (2011) has argued that CH is not a definite mathematical problem. He proposes a theory of \"definiteness\" using a semi-intuitionistic subsystem of ZF that accepts classical logic for bounded quantifiers but uses intuitionistic logic for unbounded ones, and suggests that a proposition formula_14 is mathematically \"definite\" if the semi-intuitionistic theory can prove formula_15. He conjectures that CH is not definite according to this notion, and proposes that CH should, therefore, be considered not to have a truth value. Peter Koellner (2011b) wrote a critical commentary on Feferman's article.\nJoel David Hamkins proposes a multiverse approach to set theory and argues that \"the continuum hypothesis is settled on the multiverse view by our extensive knowledge about how it behaves in the multiverse, and, as a result, it can no longer be settled in the manner formerly hoped for.\" . In a related vein, Saharon Shelah wrote that he does \"not agree with the pure Platonic view that the interesting problems in set theory can be decided, that we just have to discover the additional axiom. My mental picture is that we have many possible set theories, all conforming to ZFC.\" .\nThe generalized continuum hypothesis.\nThe generalized continuum hypothesis (GCH) states that if an infinite set's cardinality lies between that of an infinite set \"S\" and that of the power set formula_16 of \"S\", then it has the same cardinality as either \"S\" or formula_16. That is, for any infinite cardinal formula_18 there is no cardinal formula_8 such that formula_20. GCH is equivalent to:\nThe beth numbers provide an alternate notation for this condition: formula_23 for every ordinal formula_22. The continuum hypothesis is the special case for the cardinal formula_25. GCH was first suggested by . (For the early history of GCH, see ).\nLike CH, GCH is also independent of ZFC, but Sierpi\u0144ski proved that ZF + GCH implies the axiom of choice (AC) (and therefore the negation of the axiom of determinacy, AD), so choice and GCH are not independent in ZF; there are no models of ZF in which GCH holds and AC fails. To prove this, Sierpi\u0144ski showed GCH implies that every cardinality n is smaller than some aleph number, and thus can be ordered. This is done by showing that n is smaller than formula_26 which is smaller than its own Hartogs number\u2014this uses the equality formula_27; for the full proof, see .\nKurt G\u00f6del showed that GCH is a consequence of ZF + V=L (the axiom that every set is constructible relative to the ordinals), and is therefore consistent with ZFC. As GCH implies CH, Cohen's model in which CH fails is a model in which GCH fails, and thus GCH is not provable from ZFC. W.\u00a0B.\u00a0Easton used the method of forcing developed by Cohen to prove Easton's theorem, which shows it is consistent with ZFC for arbitrarily large cardinals formula_28 to fail to satisfy formula_29. Much later, Foreman and Woodin proved that (assuming the consistency of very large cardinals) it is consistent that formula_30 holds for every infinite cardinal formula_8. Later Woodin extended this by showing the consistency of formula_32 for every formula_8. showed that, for each \"n\"\u00a0\u2265\u00a01, it is consistent with ZFC that for each \u03ba, 2\u03ba is the \"n\"th successor of \u03ba. On the other hand, proved, that if \u03b3 is an ordinal and for each infinite cardinal \u03ba, 2\u03ba is the \u03b3th successor of \u03ba, then \u03b3 is finite.\nFor any infinite sets A and B, if there is an injection from A to B then there is an injection from subsets of A to subsets of B. Thus for any infinite cardinals A and B, formula_34 . If A and B are finite, the stronger inequality formula_35 holds. GCH implies that this strict, stronger inequality holds for infinite cardinals as well as finite cardinals.\nImplications of GCH for cardinal exponentiation.\nAlthough the generalized continuum hypothesis refers directly only to cardinal exponentiation with 2 as the base, one can deduce from it the values of cardinal exponentiation formula_36 in all cases. GCH implies that :\nThe first equality (when \"\u03b1\" \u2264 \"\u03b2\"+1) follows from:\nThe third equality (when \"\u03b2\"+1 &lt; \"\u03b1\" and formula_44) follows from:\nWhere, for every \u03b3, GCH is used for equating formula_47 and formula_48; formula_49 is used as it is equivalent to the axiom of choice."}
{"id": "5706", "revid": "1189543", "url": "https://en.wikipedia.org/wiki?curid=5706", "title": "\u00c7evik Bir", "text": "\u00c7evik Bir (born 1939) is a retired Turkish army general. He was a member of the Turkish General Staff in the 1990s. He took a major part in several important international missions in the Middle East and North Africa. He was born in Buca, Izmir Province, in 1939 and is married with one child. \u00c7evik Bir is of Albanian origin.\nHe graduated from the Turkish Military Academy as an engineer officer in 1958, from the Army Staff College in 1970 and from the Armed Forces College in 1971. He graduated from NATO Defense College, Rome, Italy in 1973.\nFrom 1973 to 1985, he served at SHAPE, NATO's headquarters in Belgium. He was promoted to brigadier general and commanded an armed brigade and division in Turkey. From 1987 to 1991, he served as major general, and then was promoted to lieutenant general.\nAfter the dictator Siad Barre\u2019s ousting, conflicts between the General Mohammed Farah Aidid party and other clans in Somalia had led to famine and lawlessness throughout the country. An estimated 300,000 people had died from starvation. A combined military force of United States and United Nations (under the name \"UNOSOM\") were deployed to Mogadishu, to monitor the ceasefire and deliver food and supplies to the starving people of Somali. \u00c7evik Bir, who was then a lieutenant-general of Turkey, became the force commander of UNOSOM II in April 1993. Despite the retreat of US and UN forces after several deaths due to local hostilities mainly led by Aidid, the introduction of a powerful military force opened the transportation routes, enabling the provision of supplies and ended the famine quickly. He was succeeded as Force Commander by a Malaysian general in January 1994.\nHe became a four-star general and served three years as vice chairman of the Turkish Armed Forces, then appointed commander of the Turkish First Army, in Istanbul. While he was vice chairman of the TAF, he signed the Turkish-Israeli Military Coordination agreement in 1996.\n\u00c7evik Bir became the Turkish army's deputy chief of general staff shortly after the Somali operation and played a vital role in establishing a Turkish-Israeli entente.\n\u00c7evik Bir retired from the army on August 30, 1999. He is a former member of the Association for the Study of the Middle East and Africa (ASMEA).\nOn April 12, 2012, Bir and 30 other officers were taken in custody for their role in the 1997 military memorandum that forced the then Turkish government, led by the Refah Partisi (Welfare Party), to step down.\n\u00c7evik Bir, one of the generals who planned the process, said \"In Turkey we have a marriage of Islam and democracy. (\u2026) The child of this marriage is secularism. Now this child gets sick from time to time. The Turkish Armed Forces is the doctor which saves the child. Depending on how sick the kid is, we administer the necessary medicine to make sure the child recuperates\".\nReferences.\n "}
{"id": "5708", "revid": "1002965925", "url": "https://en.wikipedia.org/wiki?curid=5708", "title": "Collectivism", "text": "Collectivism is a value that is characterized by emphasis on cohesiveness among individuals and prioritization of the group over the self. Individuals or groups that subscribe to a collectivist worldview tend to find common values and goals as particularly salient and demonstrate greater orientation toward in-group than toward out-group. The term \"in-group\" is thought to be more diffusely defined for collectivist individuals to include societal units ranging from the nuclear family to a religious or racial/ethnic group. \nOrigins and historical perspectives.\nThe German sociologist Ferdinand T\u00f6nnies described an early model of collectivism and individualism using the terms \"Gemeinschaft\" (community) and \"Gesellschaft\" (society). \"Gemeinschaft\" relationships, in which communalism is prioritized, were thought to be characteristic of small, rural village communities. An anthropologist, Redfield (1941) echoed this notion in work contrasting folk society with urban society.\nMax Weber (1930) contrasted collectivism and individualism through the lens of religion, believing that Protestants were more individualistic and self-reliant compared to Catholics, who endorsed hierarchical, interdependent relationships among people. Geert Hofstede (1980) was highly influential in ushering in an era of cross-cultural research making comparisons along the dimension of collectivism versus individualism. Hofstede conceptualized collectivism and individualism as part of a single continuum, with each cultural construct representing an opposite pole. The author characterized individuals that endorsed a high degree of collectivism as being embedded in their social contexts and prioritizing communal goals over individual goals.\nMarxism\u2013Leninism.\nCollectivism was an important part of Marxist\u2013Leninist ideology in the Soviet Union, where it played a key part in forming the New Soviet man, willingly sacrificing his or her life for the good of the collective. Terms such as \"collective\" and \"the masses\" were frequently used in the official language and praised in agitprop literature, for example by Vladimir Mayakovsky (\"Who needs a \"1\"\") and Bertolt Brecht (The Decision, Man Equals Man).\nAnarcho-collectivism.\nAnarcho-collectivism deals with collectivism in a decentralized anarchistic system, in which people are paid off their surplus labor. Collectivist anarchism is contrasted with anarcho-communism, where wages would be abolished and where individuals would take freely from a storehouse of goods \"to each according to his need\". It is most commonly associated with Mikhail Bakunin, the anti-authoritarian sections of the International Workingmen's Association and the early Spanish anarchist movement.\nTerminology and measurement.\nThe construct of collectivism is represented in empirical literature under several different names. Most commonly, the term interdependent self-construal is used. Other phrases used to describe the concept of collectivism-individualism include allocentrism-idiocentrism, collective-private self, as well as subtypes of collectivism-individualism (meaning, vertical and horizontal subtypes). Inconsistent terminology is thought to account for some of the difficulty in effectively synthesizing the empirical literature on collectivism.\nTheoretical models.\nIn one critical model of collectivism, Markus and Kitayama describe the interdependent (i.e., collectivistic) self as fundamentally connected to the social context. As such, one's sense of self depends on and is defined in part by those around them and is primarily manifested in public, overt behavior. As such, the organization of the self is guided by using others as a reference. That is, an interdependent individual uses the unexpressed thoughts, feelings, and beliefs of another person with whom they have a relationship, as well as the other person's behaviors, to make decisions about their own internal attributes and actions.\nMarkus and Kitayama also contributed to the literature by challenging Hofstede's unidimensional model of collectivism-individualism. The authors conceptualized these two constructs bidimensionally, such that both collectivism and individualism can be endorsed independently and potentially to the same degree. This notion has been echoed by other prominent theorists in the field.\nSome researchers have expanded the collectivism-individualism framework to include a more comprehensive view. Specifically, Triandis and colleagues introduced a theoretical model that incorporates the notion of relational contexts. The authors argue that the domains of collectivism and individualism can be further described by horizontal and vertical relationships. Horizontal relationships are believed to be status-equal whereas vertical relationships are characterized as hierarchical and status-unequal. As such, horizontal collectivism is manifested as an orientation in which group harmony is highly valued and in-group members are perceived to experience equal standing. Vertical collectivism involves the prioritization of group goals over individual goals, implying a hierarchical positioning of the self in relation to the overarching in-group. The horizontal-vertical individualism-collectivism model has received empirical support and has been used to explore patterns within cultures. Subsequent work by other researchers suggests that as many as seven dimensions might be necessary to describe independent vs. interdependent models of selfhood.\nOriginated by W. E. B. DuBois, some researchers have adopted a historical perspective on the emergence of collectivism among some cultural groups. DuBois and others argued that oppressed minority groups contend with internal division, meaning that the development of self-identity for individuals from these groups involves the integration of one's own perceptions of their group as well as typically negative, societal views of their group. This division is thought to impact goal formation such that people from marginalized groups tend to emphasize collectivistic over individualistic values.\nSome organizational research has found different variations of collectivism. These include institutional collectivism and in-group collectivism. Institutional collectivism is the idea that a work environment creates a sense of collectivist nature due to similar statuses and similar rewards, such as earning the same salary. In-group collectivism is the idea that an individual's chosen group of people, such as family or friend groups, create a sense of collectivist nature. In-group collectivism can be referred to as family collectivism.\nMacro-level effects.\nCultural views are believed to have a reciprocal relationship with macro-level processes such as economics, social change, and politics. Societal changes in the People's Republic of China exemplifies this well. Beginning in the early 1980s, China experienced dramatic expansion of economic and social structures, resulting in greater income inequality between families, less involvement of the government in social welfare programs, and increased competition for employment. Corresponding with these changes was a shift in ideology among Chinese citizens, especially among those who were younger, away from collectivism (the prevailing cultural ideology) toward individualism. China also saw this shift reflected in educational policies, such that teachers were encouraged to promote the development of their students\u2019 individual opinions and self-efficacy, which prior to the aforementioned economic changes, was not emphasized in Chinese culture.\nAn example of the impact of cultural beliefs on macro-level economic, political, legal and social constructs are the Maghrebi, eleventh-century Jewish traders who emerged from the Muslim world. The Maghrebis\u2019 collectivist cultural beliefs created a societal organisation premised on the group\u2019s ability to sanction \u2018deviants\u2019 economically and socially. This system created efficient intragroup relations without need for formal justice institutions such as courts, but it restricted intergroup economic efficiency.\nAttempts to study the association of collectivism and political views and behaviors has largely occurred at the aggregate national level. However, more isolated political movements have also adopted a collectivistic framework. For example, collectivist anarchism is a revolutionary anarchist doctrine that advocates the abolition of both the state and private ownership of the means of production. It instead envisions the means of production being owned collectively and controlled and managed by the producers themselves."}
{"id": "5711", "revid": "158038", "url": "https://en.wikipedia.org/wiki?curid=5711", "title": "Nepeta", "text": "Nepeta is a genus of flowering plants in the family Lamiaceae. The genus name is reportedly in reference to Nepete, an ancient Etruscan city. There are about 250 species.\nThe genus is native to Europe, Asia, and Africa, and has also naturalized in North America.\nSome members of this group are known as catnip or catmint because of their effect on house cats \u2013 the nepetalactone contained in some \"Nepeta\" species binds to the olfactory receptors of cats, typically resulting in temporary euphoria.\nDescription.\nMost of the species are herbaceous perennial plants, but some are annuals. They have sturdy stems with opposite heart-shaped, green to gray-green leaves. \"Nepeta\" plants are usually aromatic in foliage and flowers.\nThe tubular flowers can be lavender, blue, white, pink, or lilac, and spotted with tiny lavender-purple dots. The flowers are located in verticillasters grouped on spikes; or the verticillasters are arranged in opposite cymes, racemes, or panicles \u2013 toward the tip of the stems.\nThe calyx is tubular or campanulate, they are slightly curved or straight, and the limbs are often 2-lipped with five teeth. The lower lip is larger, with 3-lobes, and the middle lobe is the largest. The flowers have 4 hairless stamens that are nearly parallel, and they ascend under the upper lip of the corolla. Two stamen are longer and stamens of pistillate flowers are rudimentary. The style protrudes outside of the mouth of the flowers.\nThe fruits are nutlets, which are oblong-ovoid, ellipsoid, ovoid, or obovoid in shape. The surfaces of the nutlets can be slightly ribbed, smooth or warty.\nSelected species.\nSpecies include:\nUses.\nCultivation.\nSome \"Nepeta\" species are cultivated as ornamental plants. They can be drought tolerant \u2013 water conserving, often deer repellent, with long bloom periods from late spring to autumn. Some species also have repellent properties to insect pests, including aphids and squash bugs, when planted in a garden.\n\"Nepeta\" species are used as food plants by the larvae of some Lepidoptera (butterfly and moth) species including \"Coleophora albitarsella\", and as nectar sources for pollinators, such as honey bees and hummingbirds."}
{"id": "5714", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=5714", "title": "Cornish Nationalist Party", "text": "The Cornish Nationalist Party (CNP; ) is a political party, founded by Dr James Whetter, who campaigned for independence for Cornwall. It was formed by people who left Cornwall's main nationalist party Mebyon Kernow on 28 May 1975, but it is no longer for independence.\nA separate party with a similar name (Cornish National Party) existed from 1969.\nThe split with Mebyon Kernow was based on the same debate that was occurring in most of the other political parties campaigning for autonomy from the United Kingdom at the time (such as the Scottish National Party and Plaid Cymru): whether to be a centre-left party, appealing to the electorate on a social democratic line, or whether to appeal emotionally on a centre-right cultural line. Originally, another subject of the split was whether to embrace devolution as a first step to full independence (or as the sole step if this was what the electorate wished) or for it to be \"all or nothing\".\nThe CNP essentially represented a more right-wing outlook from those who disagree that economic arguments were more likely to win votes than cultural. The CNP worked to preserve the Celtic identity of Cornwall and improve its economy, and encouraged links with Cornish people overseas and with other regions with distinct identities. It also gave support to the Cornish language and commemorated Thomas Flamank, a leader of the Cornish Rebellion in 1497, at an annual ceremony at Bodmin on 27 June each year.\nThe Policy Statement and Programme of the CNP were published in 1975 and included the following points:\nWhile the CNP is not a racist organisation, there was a perceived image problem from the similarly styled BNP and NF (the nativist British National Party and National Front), and during the 1970s letters were published in the party magazine \"The Cornish Banner\" (\"An Baner Kernewek\") sympathetic to the NF and critical of \"Zionist\" politicians. The CNP also formed a controversial uniformed wing known as the \"Greenshirts\" led by the CNP Youth Movement leader and Public Relations Officer, Wallace Simmons who also founded the pro-NF \"Cornish Front\". (Although the CNP and CF were sympathetic to Irish republicanism while the NF was supportive of Ulster loyalism, with the exception of leading NF figures like Patrick Harrington, who refused to condemn the IRA during an interview for the Channel 4 TV documentary \"Disciples of Chaos\").\nThe CNP polled 227 (0.4) votes in Truro during the 1979 UK General Election, 364 (0.67) in North Cornwall in the 1983 UK General Election, and 1,892 (1.0) at the European Parliament elections in the Cornwall and Plymouth constituency in 1984. The candidate on all three occasions was the founder and first leader of the CNP, Dr James Whetter.\nThe CNP was for some time seen as more of a pressure group, as it did not put up candidates for any elections, although its visibility and influence within Cornwall is negligible. , it is now registered on the UK political parties register, and so Mebyon Kernow is no longer the only registered political party based in Cornwall. In April 2009, a news story reported that the CNP had re-formed following a conference in Bodmin; however, it did not contest any elections that year.\nDr Whetter was the founder and editor of the CNP quarterly journal, \"The Cornish Banner\" (\"An Baner Kernewek\"), within the actions of the Roseland Institute. Since his death in 2018 the CNP has been led by Androw Hawke.\nA newspaper article and a revamp of the party website in October 2014 state that the party is now to contest elections once more.\nJohn Le Bretton, vice-chairman of the party, said: \"The CNP supports the retention of Cornwall Council as a Cornwall-wide authority running Cornish affairs and we call for the British government in Westminster to devolve powers to the council so that decisions affecting Cornwall can be made in Cornwall\".\nThe party's policies include the following:\nThe CNP has one parish councillor, CNP leader Androw Hawke who was elected to Polperro Community Council for the second time on 4 May 2017.\nThe reformed party was registered with the Electoral Commission in 2014, but ceased to be registered in 2017.\nThe CNP now operates as more of a pressure group standing candidates as Independents in local elections in Cornwall."}
{"id": "5715", "revid": "18054835", "url": "https://en.wikipedia.org/wiki?curid=5715", "title": "Cryptanalysis", "text": "Cryptanalysis (from the Greek \"krypt\u00f3s\", \"hidden\", and \"anal\u00fdein\", \"to analyze\") is the study of analyzing information systems in order to study the hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.\nIn addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.\nEven though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.\nOverview.\nGiven some encrypted data (\"ciphertext\"), the goal of the \"cryptanalyst\" is to gain as much information as possible about the original, unencrypted data (\"plaintext\").\nAmount of information available to the attacker.\nAttacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon's Maxim \"the enemy knows the system\" \u2014 in its turn, equivalent to Kerckhoffs' principle. This is a reasonable assumption in practice \u2014 throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been broken through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes):\nComputational resources required.\nAttacks can also be characterised by the resources they require. Those resources include:\nIt's sometimes difficult to predict these quantities precisely, especially when the attack isn't practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated \"order of magnitude\" of their attacks' difficulty, saying, for example, \"SHA-1 collisions now 252.\"\nBruce Schneier notes that even computationally impractical attacks can be considered breaks: \"Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2128 encryptions; an attack requiring 2110 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised.\"\nPartial breaks.\nThe results of cryptanalysis can also vary in usefulness. For example, cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:\nAcademic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it's possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.\nIn academic cryptography, a \"weakness\" or a \"break\" in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can't: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking of the full system.\nHistory.\nCryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography\u2014new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.\nClassical ciphers.\nAlthough the actual word \"cryptanalysis\" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. David Kahn notes in \"The Codebreakers\" that Arab scholars were the first people to systematically document cryptanalytic methods.\nThe first known recorded explanation of cryptanalysis was given by Al-Kindi (c. 801\u2013873, also known as \"Alkindus\" in Europe), a 9th-century Arab polymath, in \"Risalah fi Istikhraj al-Mu'amma\" (\"A Manuscript on Deciphering Cryptographic Messages\"). This treatise contains the first description of the method of frequency analysis. Al-Kindi is thus regarded as the first codebreaker in history. His breakthrough work was influenced by Al-Khalil (717\u2013786), who wrote the \"Book of Cryptographic Messages\", which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.\nFrequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more often than others; in English, \"E\" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph \"TH\" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for \"E\". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.\nAl-Kindi's invention of the frequency analysis technique for breaking monoalphabetic substitution ciphers was the most significant cryptanalytic advance until World War II. Al-Kindi's \"Risalah fi Istikhraj al-Mu'amma\" described the first cryptanalytic techniques, including some for polyalphabetic ciphers, cipher classification, Arabic phonetics and syntax, and most importantly, gave the first descriptions on frequency analysis. He also covered methods of encipherments, cryptanalysis of certain encipherments, and statistical analysis of letters and letter combinations in Arabic. An important contribution of Ibn Adlan (1187\u20131268) was on sample size for use of frequency analysis.\nIn Europe, Italian scholar Giambattista della Porta (1535-1615) was the author of a seminal work on cryptanalysis, \"De Furtivis Literarum Notis\".\nSuccessful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.\nIn Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigen\u00e8re (1523\u201396). For some three centuries, the Vigen\u00e8re cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure (\"le chiffre ind\u00e9chiffrable\"\u2014\"the indecipherable cipher\"). Nevertheless, Charles Babbage (1791\u20131871) and later, independently, Friedrich Kasiski (1805\u201381) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigen\u00e8re system.\nCiphers from World War I and World War II.\nIn World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers \u2014 including the Enigma machine and the Lorenz cipher \u2014 and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence.\nCryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.\nIn practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers \u2014 the first electronic digital computers to be controlled by a program.\nIndicator.\nWith reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the \"indicator\", as it indicates to the receiving operator how to set his machine to decipher the message.\nPoorly designed and implemented indicator systems allowed first Polish cryptographers and then the British cryptographers at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify \"depths\" that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.\nDepth.\nSending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be \"in depth.\" This may be detected by the messages having the same \"indicator\" by which the sending operator informs the receiving operator about the key generator initial settings for the message.\nGenerally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example, the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the \"exclusive or\" operator, which is also known as \"modulo-2 addition\" (symbolized by \u2295 ):\nDeciphering combines the same key bits with the ciphertext to reconstruct the plaintext:\n(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts:\nThe individual plaintexts can then be worked out linguistically by trying \"probable words\" (or phrases), also known as \"cribs,\" at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:\nThe recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed:\nKnowledge of a key of course allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.\nDevelopment of modern cryptography.\nGovernments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today.\nEven though computation was used to great effect in the cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes:\nKahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are \"moving very slowly forward in a mature field.\"\nHowever, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:\nThus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.\nAsymmetric ciphers.\nAsymmetric cryptography (or public key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on \"hard\" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.\nAsymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie\u2013Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization \u2014 a breakthrough in factoring would impact the security of RSA.\nIn 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.\nAnother distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.\nQuantum computing applications for cryptanalysis.\nQuantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.\nBy using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length."}
{"id": "5716", "revid": "39075319", "url": "https://en.wikipedia.org/wiki?curid=5716", "title": "Chicano", "text": "Chicano or Chicana is a chosen identity for many Mexican Americans in the United States. The label Chicano is sometimes used interchangeably with \"Mexican American\", although the terms have different meanings. While \"Mexican American\" identity emerged to encourage assimilation into white American society and separate the community from African-American political struggle, Chicano/a identity emerged among anti-assimilationist youth, some of whom belonged to the Pachuco/a subculture, who reclaimed the term (which had previously been a classist and racist slur). \"Chicano/a\" was widely reclaimed in the 1960s and 1970s to express political empowerment, ethnic solidarity, and pride in being of Indigenous descent (with many using the Nahuatl language as a symbol), diverging from the more assimilationist \"Mexican American\" identity\".\" Chicano Movement leaders were influenced by and collaborated with Black Power leaders and activists. Chicano youth in \"barrios\" rejected cultural assimilation into whiteness and embraced their identity and worldview as a form of empowerment and resistance.\nThe Chicano Movement faltered by the mid-1970s as a result of state surveillance, infiltration, and repression by U.S. government agencies, informants, and agent provocateurs, such as through COINTELPRO, a hyper-fixation on masculine pride and machismo which excluded Chicanas and queer Chicanas/os from the movement, and a growing disinterest in Chicano nationalist constructs such as Aztl\u00e1n. The identity experienced a further decline by the late 1970s and 1980s as assimilation and economic mobility became a goal of many Mexican Americans in an era of conservatism, who instead identified as \"Hispanic\". \"Hispanic\" emerged out of a collaboration between the U.S. government and Mexican American political elites in the Hispanic Caucus who wanted to encourage assimilation into 'mainstream' American society by departing from the radical politics of Chicano/a identity and separate themselves from what they saw as the 'militant' Black Caucus.\n\"Chicano\" had \"lost its fire\", as summarized by Earl Shorris. However, Chicanas/os continued to participate in building the foundations of the feminist, gay and lesbian, and anti-apartheid movements of the 1980s, which maintained its relevance at the grassroots level. After a decade of \"Hispanic\" dominance, Chicana/o student activism amidst the early 1990s recession and the anti-Gulf War movement provoked a revival of Chicana/o identity and a demand for the expansion of Chicana/o studies programs. Chicanas, rather than Chicanos, were now largely at the forefront of Chicana/o activist movements and were critical in elevating Chicana/o identity. Though they faced critiques from \"movement loyalists,\" Chicana feminists worked to address social problems of employment discrimination, environmental racism, healthcare, sexual violence, and capitalist exploitation in their communities and in solidarity with the Third World. While there had previously been widespread repression of the non-masculine and non-heteronormative Chicana/o subject in the Chicano Movement, Chicana feminists critiqued Chicano patriarchal authority as a legacy of colonization, informed by a desire \"to liberate her \"entire people\"\"; not to oppress men, but to be equal partners in the movement.\n\"Xicanisma\", coined by Ana Castillo in 1994, gained some recognition among Chicana feminists, scholars and artists by the early 2000s and indicate efforts to shift away from the patriarchal overtones of \"Chicanismo\". The \"X\" was also a symbolic gesture towards acknowledging one's Indigenous roots while also recognizing the need to support Indigenous sovereignty. Building solidarity with undocumented immigrants also became important, despite legal status and economic competitiveness at times working to maintain distance. In the 2000s, the Chicana/o worldview increasingly became transnational, informed by and expanding upon earlier traditions of anti-imperialism and Third World solidarity in the Chicano Movement. Chicanas/os connected U.S. foreign interventions abroad with domestic racial politics, and committed themselves to \"the struggle for social justice of citizens and non-citizens.\" They emphasized that, while their struggles were not identical, they were \"equally rooted in power imbalances between the First World and the Third World.\" In the 2010s, the identity experienced a resurgence centered on ethnic pride, Indigenous consciousness, cultural expression, defense of immigrants, and the rights of women and queer people; some even referred to it as a 'renaissance'. In the late 2010s, Xicanx identity emerged, indicating a shift in the Chicano Movement. The term has been described as openly inclusive to people beyond Mexican origin and representative of a connection to Indigeneity, decolonial consciousness, deconstructing the gender binary, and transnational solidarity.\nEtymology.\nThe etymology of the term \"Chicano\" is not definitive and has been debated by historians, scholars, and activists. Although there has been controversy over the origins of \"Chicano\", community conscience reportedly remains strong among those who claim the identity.\n\"Chicano\" is believed by some scholars to be a Spanish language derivative of an older Nahuatl word Mexitli (\"Meh-shee-tlee\"). Mexitli formed part of the expression \"Huitzilopochtlil Mexitli\"\u2014a reference to the historic migration of the Mexica people from their homeland of Aztl\u00e1n to the Oaxaca Valley. Mexitli is the linguistic progenitor or root of the word \"Mexica,\" referring to the Mexica people, and its singular form \"Mexihcatl\" (). The \"x\" in Mexihcatl represents an /\u0283/ or \"sh\" sound in both Nahuatl and early modern Spanish, while the glottal stop in the middle of the Nahuatl word disappeared.\nThe word \"Chicano\" therefore more directly derives from the loss of the initial syllable of \"Mexicano\" (Mexican). According to Villanueva, \"given that the velar (x) is a palatal phoneme (S) with the spelling (sh),\" in accordance with the Indigenous phonological system of the Mexicas (\"Meshicas\"), it would become \"Meshicano\" or \"Mechicano.\" Some Chicanos further replace the \"ch\" with the letter \"x\", forming \"Xicano\", as a means of reclaiming and reverting to the Nahuatl use of the \"x\" sound. The first two syllables of \"Xicano\" are therefore in Nahuatl while the last syllable is Castillian.\nIn Mexico's Indigenous regions, the non-indigenous majority are referred to as \"mexicanos\", referring to the modern nation, rather than the ' (village or tribal) identification of the speaker, be it Mayan, Zapotec, Mixtec, Huasteco, or any of hundreds of other indigenous groups. Thus, a newly emigrated Nahuatl speaker in an urban center might have referred to his cultural relatives in this country, different from himself, as ', shortened to \"Chicanos\".\nUsage of terms.\nEarly recorded usage.\nThe town of \"Chicana\" was shown on the Guti\u00e9rrez 1562 New World map near the mouth of the Colorado River, and is probably pre-Columbian in origin. The town was again included on , a 1566 French map by Paolo Forlani. Scholar Roberto Cintli Rodr\u00edguez places the location of \"Chicana\" at the mouth of the Colorado River, near present-day Yuma, Arizona. An 18th century map of the Nayarit Missions used the name \"Xicana\" for a town near the same location of \"Chicana\", which is considered to be the oldest recorded usage of the term.\nA gunboat, the \"Chicana\", was sold in 1857 to Jose Maria Carvajal to ship arms on the Rio Grande. The King and Kenedy firm submitted a voucher to the Joint Claims Commission of the United States in 1870 to cover the costs of this gunboat's conversion from a passenger steamer. No explanation for the boat's name is known.\nThe Chicano poet and writer Tino Villanueva traced the first documented use of the term as an ethnonym to 1911, as referenced in a then-unpublished essay by University of Texas anthropologist Jos\u00e9 Lim\u00f3n. Linguists Edward R. Simmen and Richard F. Bauerle report the use of the term in an essay by Mexican-American writer, Mario Su\u00e1rez, published in the \"Arizona Quarterly\" in 1947. There is ample literary evidence to substantiate that \"Chicano\" is a long-standing endonym, as a large body of Chicano literature pre-dates the 1950s.\nReclamation.\nIn the 1940s and 1950s, \"Chicano/a\" was reclaimed by pachucos as an expression of defiance to Anglo-American society. \"Chicano/a\" at this time was still widely used among English and Spanish speakers as a classist and racial slur to refer to working class Mexican American people in Spanish-speaking neighborhoods. In Mexico, the term was used interchangeably with \"Pocho\" \"to deride Mexicans living in the United States, and especially their U.S.-born children, for losing their culture, customs, and language.\" The Mexican archeologist and anthropologist Manuel Gamio reported in 1930 that the term \"Chicamo\" (with an \"m\") was used as a derogatory term by Hispanic Texans for recently arrived Mexican immigrants displaced during the Mexican Revolution in the beginning of the early 20th century.\nBy the mid-20th century, \"Chicano\" began to be used to reference those who resisted total assimilation, while \"Pocho\" referred (often pejoratively) to those who strongly advocated for assimilation. In his essay \"Chicanismo\" in \"The Oxford Encyclopedia of Mesoamerican Cultures\" (2002), Jos\u00e9 Cu\u00e9llar, dates the transition from derisive to positive to the late 1950s, with increasing usage by young Mexican-American high school students. These younger, politically aware, Mexican Americans adopted the term \"as an act of political defiance and ethnic pride,\" similar to the reclamation of \"Black\" by African Americans. The Chicano Movement of the 1960s and early 1970s furthered the reclamation process of \"Chicana/o\", challenging those who used as a term of derision on both sides of the Mexico-U.S. border.\nDemographic differences in the adoption of Chicano/a identity occurred; because of the prior vulgar connotations, it was more likely to be used by males than females, and less likely to be used among those of higher socioeconomic status. Usage was also generational, with the more assimilated third-generation members (again, more likely male) likely to adopt the usage. This group was also younger, of more radical persuasion, and less connected to a Mexican cultural heritage. Ana Castillo notes an example of how \"Chicana\" has been used as a classist term of derision to refer to \"[a] marginalized, brown woman who is treated as a foreigner and is expected to do menial labor and ask nothing of the society in which she lives.\" Castillo herself considers \"Chicano/a\" to be a positive identity of self-determination and political solidarity. Some identify that \"Chicano\" is widely known and used in Mexico and may still be associated with a Mexican American person of low importance, class, and poor morals (similar to the terms \"Cholo\", \"Chulo\" and \"Majo\").\n\"Chicano/a\".\nChicano identity was widely reclaimed in the 1960s and 1970s by Mexican Americans as a means of asserting their own ethnic, political, and cultural identity while rejecting and resisting assimilation into whiteness, systematic racism and stereotypes, colonialism, and the American nation-state. Chicano identity was also founded on the need to create alliances with other oppressed ethnic and Third World peoples while protesting U.S. imperialism. Chicano identity was organized around seven objectives: unity, economy, education, institutions, self-defense, culture, and political liberation, in an effort to bridge regional and class divisions among people of Mexican descent. The notion of Aztl\u00e1n, a mythical homeland claimed to be located in the southwestern United States, mobilized Mexican Americans to take social and political action. Chicanos/as originally espoused the belief in \"Chicano/a\" as a unifying \"mestizo\" identity and also centered their platform in the masculine body.\nIn the 1970s, Chicano identity became further defined under a reverence for machismo while also maintaining the values of their original platform, exemplified via the language employed in court cases such as Montez v. Superior Court, 1970, which defined the Chicano community as unified under \"a commonality of ideals and \"costumbres\" with respect to masculinity (machismo), family roles, child discipline, [and] religious values.\" Oscar Zeta Acosta defined machismo as the source of Chicano identity, claiming that this \"instinctual and mystical source of manhood, honor and pride... alone justifies all behavior.\" Armando Rend\u00f3n wrote in \"Chicano Manifesto\" (1971) that machismo was \"in fact an underlying drive of the gathering identification of Mexican Americans... the essence of \"machismo\", of being \"macho\", is as much a symbolic principle for the Chicano revolt as it is a guideline for family life.\"\nFrom the beginning of the Chicano Movement, Chicana activists and scholars have \"criticized the conflation of revolutionary commitment with manliness or machismo\" and questioned \"whether machismo is indeed a genuinely Mexican cultural value or a kind of distorted view of masculinity generated by the psychological need to compensate for the indignities suffered by Chicanos in a white supremacist society,\" as noted by Jos\u00e9-Antonio Orosco. Academic Angie Chabram-Dernersesian indicates in her study of literary texts formative to the Chicano Movement that most of the stories focused on men and boys and none focused on Chicanas. The omission of Chicanas and the masculine-focused foundations of Chicano identity eventually created a shift in consciousness among some Chicanas/os by the 1990s.\n\"Xicana/o/x\".\n\"Xicanisma\" was coined by Chicana Feminist writer Ana Castillo in \"Massacre of the Dreamers: Essays on Xicanisma\" (1994) as a recognition of the shift in consciousness since the Chicano Movement. In the 1990s and early 2000s, Xicana/o activists and scholars, including Guillermo G\u00f3mez-Pe\u00f1a, were beginning to form a new ideological notion of \"Xicanisma\": \"a call for a return to the Amerindian roots of most Latinos as well as a call for a strategic alliance to give agency to Native American groups,\" reasserting the need to form coalitions with other oppressed ethnic groups, which was foundational in the formation of Chicano identity. Juan Velasco states that \"implicit in the 'X' of more recent configurations of 'Xicano' and 'Xicanisma' is a criticism not only of the term 'Hispanic' but of the racial poetics of the 'multiracial' within Mexican and American culture.\" While still recognizing many of the foundational elements of Chicano identity, some Xicana feminists have preferred to identify as Xicana because of the masculine-focused foundations of Chicano identity and the patriarchal biases inherent in the Spanish language.\nScholar Francesca A. L\u00f3pez notes that \"\"Chicanismo\" has evolved into Xicanismo and even Xicanisma and other variations, but however it is spelled, it is based on the idea that to be Xican@ means to be proud of your Mexican Indigenous roots and committed to the struggle for liberation of all oppressed people.\" While adopting Chicano identity was a means of rejecting conformity to the dominant system as well as Hispanic identity, Xicano identity was adopted to emphasize a diasporic Indigenous American identity through being ancestrally connected to the land. Dylan Miner has noted how the emergence of Xicano identity emphasizes an \"Indigenous and indigenist turn\" which recognizes the Indigenous roots of Xicana/o/x people by explicitly referencing Nahuatl language and using an 'x' to signify a \"lost or colonized history.\" While \"Chicano\" has been noted by scholars such as Francisco Rios as being limited by its focus on \"race and ethnicity with strong male overtones,\" \"Xicanismo\" has been referred to as elastic enough to recognize the \"intersecting nature of identities\" (race/ethnicity and gender, class and sexual orientation) as well as roots \"from Mexico as well as those with roots centered in Central and South America.\"\nAs poet and writer Luis J. Rodriguez states, both Xicanx and Chicano \"mean the same thing\"; referring to \"Xicanx\" as \"the most recent incarnation of a word that describes people that are neither totally Mexican nor totally what is conceived as American.\" As Rodriguez remarks on the term's inclusivity, \"Xicanx are all genders and gender non-conforming ... And even though most US Mexicans may not use this term, there is, nonetheless, in the Xicanx areas of the country, a third culture with its own dialect, food, and ethnic stamp.\" \"Xicanx\" has been used when referring to the need to destabilize \"the principle of putting cisgender masculinity at the center of life\" within the community. Artist Roy Martinez describes \"Xicanx\" as \"not being bound to the feminine or masculine aspects,\" stating that \"it's not a set thing\" that people should feel enclosed in, but that it is a fluid identity that extends beyond fitting within the gender binary. Martinez also suggests the identity should extend beyond borders: \"A lot of people are like 'Oh you weren't born in Mexico, so these identifiers exclude you... ' I feel like Xicanx is inclusive to anyone who identifies with it.\"\nDistinction from other terms.\n\"Mexican American\".\nLegal scholar Ian Haney L\u00f3pez records that, in the 1930s, \"community leaders promoted the term \"Mexican American\" to convey an assimilationist ideology stressing white identity.\" Academic Lisa Y. Ramos notes that \"this phenomenon demonstrates why no Black-Brown civil rights effort emerged prior to the 1960s.\" As a precursor to the Chicano Movement, anti-assimilationist Mexican American youth rejected the previous generation's racial aspirations to assimilate into Anglo-American society and developed an \"alienated \"pachuco\" culture that fashioned itself neither as Mexican nor American.\" Pachucos themselves adopted Chicano identity to emphasize their opposition to assimilation in the 1940s.\nThe rise of Chicano/a identity during the Chicano Movement opened new possibilities for Black-Brown unity through rejecting assimilation: \"Chicanos defined themselves as proud members of a brown race, thereby rejecting, not only the previous generation's assimilationist orientation, but their racial pretensions as well.\" Chicano/a leaders, organizations, and demonstrations learned from and collaborated with Black Power movement leaders and activists. As a result, \"Mexican American\" became used by those who insisted that Mexicans were white and wanted to assimilate, while \"Chicano\" became used by those who embraced a non-white and non-assimilationist worldview.\n\"Mexican American\" continued to be used by a more assimilationist faction within the community who wanted to define Mexican Americans \"as a white ethnic group that had little in common with African Americans.\" The desire of this assimilationist \"Mexican American\" faction of the community to separate themselves from Black people and political struggle was rooted in an attempt to minimize \"the existence of racism toward their own people, [believing] they could 'deflect' anti-Mexican sentiment in society\" through embracing whiteness.\n\"Hispanic\".\n\"Hispanic\" was first promoted in the late 1970s and was first used on the 1980 U.S. Census. \"Hispanic\" was first defined by the U.S. Federal Office of Management and Budget's (OMB) Directive No. 15 in 1977, which defined a Hispanic as \"a person of Mexican, Puerto Rican, Cuban, Central or South America or other Spanish culture or origin, regardless of race.\" The term was formed out of a collaboration with Mexican American political elites to encourage cultural assimilation into American society among all \"Hispanic/Latino\" peoples and move away from the anti-assimilationist politics of Chicano identity, which had gained prominence in the preceding decades through the Chicano Movement. The rise of Hispanic identity paralleled an emerging era of conservatism during the 1980s.\nLegal scholar Laura E. G\u00f3mez notes that key members of the Mexican American political elite, all of whom were middle-aged men, helped popularize the term \"Hispanic\" among the Mexican American community, which in turn fueled both electronic and print media to use the term when referring to Mexican Americans in the 1980s. G\u00f3mez conducted a series of interviews with Mexican American political elites on their role in promoting \"Hispanic\" and found that one of the main reasons was because it stood in contrast to Chicano identity: \"The Chicano label reflected the more radical political agenda of Mexican-Americans in the 1960s and 1970s, and the politicians who call themselves Hispanic today are the harbringers of a more conservative, more accomadationist politics.\" Some of these elites sought to encourage cultural assimilation through \"Hispanic\" within their community and not be seen as \"militant\" in order to appeal to white American sensibilities, particularly in regard to separating themselves from Black political consciousness. G\u00f3mez records:Another respondent agreed with this position, contrasting his white colleagues' perceptions of the Congressional Hispanic Caucus with their perception of the Congressional Black Caucus. 'We certainly haven't been militant like the Black Caucus. We're seen as a power bloc\u2014an ethnic power bloc striving to deal with mainstream issues.'Since then, \"Hispanic\" has widely been used by politicians and the media. For this reason, many Chicanos reject the term \"Hispanic\".\nOther terms.\nInstead of identifying as Chicano/a or any of its variations, some may prefer:\nand more recently, Latinx\nIdentity.\nChicana/o identity embodies elements of ethnic, political, cultural and Indigenous hybridity. These qualities of what constitutes Chicano/a identity may be expressed Chicanos/as differently, although they are still Chicano/a. As Armando Rend\u00f3n wrote in the \"Chicano Manifesto\" (1971), \"I am Chicano. What it means to me may be different than what it means to you.\" Similarly, writer Benjamin Alire S\u00e1enz wrote \"There is no such thing as the Chicano voice: there are only Chicano and Chicana \"voices\".\" The identity thus may be understood as somewhat ambiguous (e.g. in the 1991 Culture Clash play \"A Bowl of Beings\", in response to Che Guevara's demand for a definition of \"Chicano,\" an \"armchair activist\" cries out, \"I still don't know!\").\nHowever, as substantiated by Chicanas/os since the Chicano Movement, many Chicanos/as understand themselves as being \"neither from here, nor from there,\" in reference to the United States and Mexico. Juan Bruce-Novoa, a professor of Spanish and Portuguese at University of California, Irvine, wrote in 1990: \"A Chicano lives in the space between the hyphen in Mexican-American.\" Being Chicano represents the struggle of being institutionally acculturated to assimilate into the Anglo-dominated society of the United States, while maintaining the cultural sense developed as a Latin-American cultured, U.S.-born Mexican child. As described by Rafael P\u00e8rez-Torres, \"one can no longer assert the wholeness of a Chicano subject ... It is illusory to deny the nomadic quality of the Chicano communtiy, a community in flux that yet survives and, through survival, affirms its self.\"\nEthnic identity.\nFrom a popular perspective, the term \"Chicano\" became widely visible outside of Chicano communities during the American civil rights movement. It was commonly used during the mid-1960s by Mexican-American activists such as Rodolfo \"Corky\" Gonzales, who was one of the first to reclaim the term, in an attempt to assert their civil rights and rid the word of its polarizing negative connotations. \"Chicano\" soon became an identity for Mexican Americans to assert their ethnic pride, proudly identifying themselves as \"Chicanos/as\" while also asserting a notion of \"Brown Pride\", drawing on the \"Black is Beautiful\" movement, inverting phrases of insult into forms of ethnic empowerment. As journalist Rub\u00e9n Salazar described in a 1970 \"Los Angeles Times\" piece entitled \"Who is a Chicano? And what is it the Chicanos want?\": \"A Chicano is a Mexican-American with a non-Anglo image of himself.\"\nAfter it was reclaimed, Chicano/a identity became a celebration of being non-white and non-European and worked against the state-sanctioned census categories of \"Whites with Spanish Surnames,\" originally promulgated on the 1950 U.S. census, and \"Mexican-American,\" which Chicanas/os felt encouraged assimilation into European American society. Chicanos/as asserted ethnic pride during a time when Mexican assimilation into whiteness was being actively promoted by the U.S. government in order to \"serve Anglo self-interest,\" who tried to claim Chicano/as were white in order to deny racism against them, as noted by Ian Haney L\u00f3pez.\nThe U.S. Census Bureau provided no clear way for Mexican Americans or Latinos to officially identify as a racial/ethnic category prior to 1980, when the broader-than-Mexican term \"Hispanic\" was first available as a self-identification in census forms. While \"Chicano\" also appeared on the 1980 census, indicating the success of the Chicano Movement in gaining some federal recognition, it was only permitted to be selected as a subcategory underneath Spanish/Hispanic descent, which erased Afro-Chicanos/as and the visibility of Amerindian and African ancestries among Chicanas/os and populations throughout Latin America and the Caribbean.\nChicana/o ethnic identity is born out of colonial encounters between Europe, Africa, and the Americas. Alfred Arteaga writes how the Chicana/o arose as a result of the violence of colonialism, emerging as a hybrid ethnicity or race. Arteaga acknowledges how this ethnic and racial hybridity among Chicanos is highly complex and extends beyond a previously generalized \"Aztec\" ancestry, as originally asserted during the formative years of the Chicano Movement. Chicano ethnic identity may involve more than just Spanish ancestry and may include African ancestry (as a result of Spanish slavery or runaway slaves from Anglo-Americans). Arteaga concludes that \"the physical manifestation of the Chicano, is itself a product of hybridity.\"\nAfro-Chicanos/as, most of whom have origins in working class community interactions, have faced erasure from Chicano/a identity until recently. \"Because so many people uncritically apply the 'one drop rule' in the U.S., our popular language ignores the complexity of racial hybridity,\" as described by Afro-Chicano poet Robert Quintana Hopkins. Black and Chicano/a communities have engaged in close political interactions \"around civil rights struggles, union activism, and demographic changes,\" especially during the Black Power and Chicano Movement struggles for liberation in the 1960s and 1970s. There have also been tensions between Black and Chicano/a communities because of \"increased competition for scarce resources,\" which has \"too often positioned workers of different races in opposition to each other.\" Afro-Chicano photographer Walter Thompson-Hernandez reflected on how there were difficulties in his personal life because of racial conflicts between Black and Latino communities, yet stated how \"being able to connect with other Blaxicans [Black-Mexicans] has allowed me to see that in all of my conclusions and struggles, I was never alone.\" Similarly, Afro-Chicano rapper Choosey stated \"there's a stigma that Black and Mexican cultures don't get along, but I wanted to show the beauty in being a product of both.\u201d\nPolitical identity.\nChicano/a political identity developed from a reverence of pachuco resistance to assimilation in the 1940s and 1950s. Luis Valdez records that \"pachuco determination and pride grew through the 1950s and gave impetus to the Chicano Movement of the 1960s ... By then the political consciousness stirred by the 1943 Zoot Suit Riots had developed into a movement that would soon issue the Chicano Manifesto\u2013a detailed platform of political activism.\" By the 1960s, according to Catherine S. Ram\u00edrez, the pachuco figure \"emerged as an icon of resistance in Chicano cultural production.\" However, the pachuca figure was not regarded with the same status as the pachuco, which Ram\u00edrez credits with the pachuca's embodiment of \"dissident femininity, female masculinity, and, in some instances, lesbian sexuality.\"\nBy the 1960s, Chicano/a identity was consolidating around several key political positions: rejecting assimilation into Anglo-American society, resisting systemic racism and the American nation-state, and affirming the need to create alliances with other oppressed ethnic groups and Third World peoples. Political liberation was a founding principle of Chicano nationalism, which called for the creation of a Chicano/a subject whose political identity was separate from the U.S. nation-state, which Chicanos recognized had impoverished, oppressed, and destroyed their people and communities. Alberto Varon writes that, while Chicano nationalism \"created enduring social improvement for the lives of Mexican Americans and others\" through political action, this brand of Chicano nationalism privileged the machismo subject in its calls for political resistance, which has since been critiqued by Chicana feminism.\nSeveral Chicana/o writers state that Chicano hypermasculinity inhibited and stifled the Chicano Movement. Chicana writer Cherr\u00ede Moraga identifies homophobia and sexism as obstacles to the Movement which deprived Chicanas of critical knowledge about a \"grassroots feminist movement where women of color, including lesbians of color, [had] been actively involved in reproductive rights, especially sterilization abuse, battered women's shelters, rape crisis centers, welfare advocacy, Third World women's conferences, cultural events, health and self-help clinics and more.\" Sonia Sald\u00edvar-Hull writes that crucial texts such as \"Essays on La Mujer\" (1977), \"Mexican Women in the United States\" (1980), and \"This Bridge Called My Back\" (1981) have been relatively ignored, even in Chicana/o Studies, while \"a failure to address women's issues and women's historical participation in the political arena continues.\" Sald\u00edvar-Hull notes that when Chicanas have challenged sexism, their identities have been invalidated.\nChicano political activist groups such as the Brown Berets (1967-1972; 1992\u2013Present), founded by David S\u00e1nchez in East L.A. as the Young Chicanos for Community Action, gained support for their political objectives of protesting educational inequalities and demanding an end to police brutality. They collaborated with the Black Panthers and Young Lords, which were founded in 1966 and 1968 respectively. Membership in the Brown Berets was estimated to have reached five thousand in over 80 chapters (mostly centered in California and Texas). The Brown Berets helped organize the Chicano Blowouts of 1968 and the national Chicano Moratorium, which protested the high rate of Chicano casualties in the Vietnam War. Continued police harassment, infiltration by federal agents provacateur via COINTELPRO, and internal disputes led to the decline and disbandment of the Berets in 1972. S\u00e1nchez, then a professor at East Los Angeles College, revived the Brown Berets in 1992 after being prompted by the high number of Chicano homicides is Los Angeles County, seeking to supplant the structure of the gang as family with the Brown Berets.\nAt certain points in the 1970s, \"Chicano\" was the preferred term for reference to Mexican Americans, particularly in scholarly literature. \"Chicano/a\" fell out of favor as a way of referring to the entire population in the 1980s following the decline of the Chicano Movement. This indicated a political shift among Mexican Americans, many of whom shifted to identifying as \"Hispanic\" in an era of American conservatism. Hispanic itself emerged from an assimilationist politics rooted in anti-Black sentiments. The term was forged out of a collaboration between Mexican American political elites in the emerging Hispanic Caucus and the U.S. government, who wanted to use the term to encourage a shift away from Chicana/o identity in order to appear more 'mainstream' or respectable to white Americans. The Hispanic Caucus also sought to separate themselves from the radical politics of Chicanismo and what they perceived as the 'militancy' of Chicana/o and Black political consciousness. Reies Tijerina, who was a vocal claimant to the rights of Latin Americans and Mexican Americans and a major figure of the early Chicano Movement, wrote: \"The Anglo press degradized the word 'Chicano'. They use it to divide us. We use it to unify ourselves with our people and with Latin America.\"\nCultural identity.\nSince the Chicano Movement, \"Chicano\" has been reclaimed by Mexican-Americans to denote an identity that is in opposition to Anglo-American culture while being neither fully \"American\" or \"Mexican.\" Chicano culture embodies the \"in-between\" nature of cultural hybridity. Central aspects of Chicano culture include lowriding, hip hop, rock, graffiti art, theater, muralism, visual art, literature, poetry, and more. Notable subscultures include the chola/o, pachuca, pachuco, and pinta/o subcultures. Chicano culture has had international influence in the form of lowrider car clubs in Brazil and England, music and youth culture in Japan, M\u0101ori youth enhancing lowrider bicycles and taking on cholo style, and intellectuals in France \"embracing the deterritorializing qualities of Chicano subjectivity.\" Former president of the Modern Language Association Mary Louise Pratt stated that Chicano cultural practices constitute a space \"of ongoing critical and inventive interaction with the dominant culture, as contact zones across which significations move in many directions.\"\nAs early as the 1930s, the precursors to Chicano cultural identity were developing in Los Angeles, California and the Southwestern United States. Former zoot suiter Salvador \"El Chava\" reflects on how racism and poverty forged a hostile social environment for Chicanos/as which led to the development of gangs: \"we had to protect ourselves.\" \"Barrios\" and \"colonias\" (rural \"barrios\") emerged throughout southern California and elsewhere in neglected districts of cities and outlying areas with little infrastructure. Alienation from public institutions made some Chicano youth susceptible to gang channels, who became drawn to their rigid hierarchical structure and assigned social roles in a world of government-sanctioned disorder. \nPachuco/a culture developed in the borderland areas of California and Texas as \"Pachuquismo\", which would eventually evolve into \"Chicanismo\". Chicano zoot suiters on the west coast were influenced by Black zoot suiters in the jazz and swing music scene on the East Coast. Chicano zoot suiters developed a unique cultural identity, as noted by Charles \"Chaz\" Boj\u00f3rquez, \"with their hair done in big pompadours, and 'draped' in tailor-made suits, they were swinging to their own styles. They spoke \"C\u00e1lo\", their own language, a cool jive of half-English, half-Spanish rhythms. [...] Out of the zootsuiter experience came lowrider cars and culture, clothes, music, tag names, and, again, its own graffiti language.\" As described by artist Carlos Jackson, \"Pachuco culture remains a prominent theme in Chicano art because the contemporary urban \"cholo\" culture\" is seen as its heir.\nMany aspects of Chicano culture, such as lowriding cars and bicycles, have been stigmatized and policed by Anglo Americans who perceive Chicanos as \"juvenile delinquents or gang members\" for their embrace of nonwhite style and cultures, much as they did Pachucos. These negative societal perceptions of Chicanos were amplified by media outlets such as the \"Los Angeles Times\". Luis Alvarez remarks how negative portrayals in the media served as a tool to increase policing of Black and Brown male bodies in particular: \"Popular discourse characterizing nonwhite youth as animal-like, hypersexual, and criminal marked their bodies as 'other' and, when coming from city officials and the press, served to help construct for the public a social meaning of African Americans and Mexican American youth. In these ways, the physical and discursive bodies of nonwhite youth were the sites upon which their dignity was denied.\"\nChicano rave culture in southern California provided a space for Chicanos to partially escape criminalization in the 1990s. Artist and archivist Guadalupe Rosales states that \"a lot of teenagers were being criminalised or profiled as criminals or gangsters, so the party scene gave access for people to escape that.\" Numerous party crews, such as Aztek Nation, organized events and parties would frequently take place in neighborhood backyards, particularly in East and South Los Angeles, the surrounding valleys, and Orange County. By 1995, it was estimated that over 500 party crews were in existence. They laid the foundations for \"an influential but oft-overlooked Latin dance subculture that offered community for Chicano ravers, queer folk, and other marginalized youth.\" Ravers used map points techniques to derail police raids. Rosales states that a shift occurred around the late 1990s and increasing violence effected the Chicano party scene. \nIndigenous identity.\nChicano/a identity functions as a way to reclaim one's Indigenous American, and often Indigenous Mexican, ancestry\u2014to form an identity distinct from European identity, despite some Chicanos/as being of partial European descent\u2014as a way to resist and subvert colonial domination. Rather than a \"subculture\" of European American culture, Alicia Gasper de Alba refers to \"Chicanismo\" as an \"\"alter-Native\" culture, an Other American culture Indigenous to the land base now known as the West and Southwest of the United States.\" While influenced by settler-imposed systems and structures, Alba refers to Chicana/o culture as \"not immigrant but native, not foreign but colonized, not alien but different from the overarching hegemony of white America.\"\nThe Plan Espiritual de Aztl\u00e1n (1969) drew from Frantz Fanon's \"The Wretched of the Earth\" (1961). In \"Wretched\", Fanon stated: \"the past existence of an Aztec civilization does not change anything very much in the diet of the Mexican peasant today,\" elaborating that \"this passionate search for a national culture which existed before the colonial era finds its legitimate reason in the anxiety shared by native intellectuals to shrink away from that of Western culture in which they all risk being swamped ... the native intellectuals, since they could not stand wonderstruck before the history of today's barbarity, decided to go back further and to delve deeper down; and, let us make no mistake, it was with the greatest delight that they discovered that there was nothing to be ashamed of in the past, but rather dignity, glory, and solemnity.\" \nThe Chicano Movement adopted this perspective through the notion of Aztl\u00e1n\u2014a mythic Aztec homeland which Chicano/as used as a way to connect themselves to a precolonial past, before the time of the \"'gringo' invasion of our lands.\" Chicano/a scholars describe how this reclamation functioned as a way for Chicano/as to reclaim a diverse or imprecise Indigenous past; while recognizing how Aztl\u00e1n promoted divisive forms of Chicano nationalism that \"did little to shake the walls and bring down the structures of power as its rhetoric so firmly proclaimed.\" As stated by Chicano historian Juan G\u00f3mez-Qui\u00f1ones, the Plan Espiritual de Aztl\u00e1n was \"stripped of what radical element it possessed by stressing its alleged romantic idealism, reducing the concept of Aztl\u00e1n to a psychological ploy ... all of which became possible because of the Plan's incomplete analysis which, in turn, allowed it ... to degenerate into reformism.\"\nWhile acknowledging its romanticized and exclusionary foundations, Chicano/a scholars like Rafael P\u00e8rez-Torres state that Aztl\u00e1n opened a subjectivity which stressed a connection to Indigenous peoples and cultures at a critical historical moment in which \"Mexican-Americans\" and Mexicans were \"under pressure to assimilate particular standards\u2014of beauty, of identity, of aspiration. In a Mexican context, the pressure was to urbanize and Europeanize ... 'Mexican-Americans' were expected to accept anti-indigenous discourses as their own.\" As P\u00e8rez-Torres concludes, Aztl\u00e1n allowed \"for another way of aligning one's interests and concerns with community and with history ... though hazy as to the precise means in which agency would emerge, Aztl\u00e1n valorized a Chicanismo that rewove into the present previously devalued lines of descent.\" Romanticized notions of \"Aztl\u00e1n\" have declined among some Chicano/as, who argue for a need to reconstruct the place of Indigeneity in relation to Chicano/a identity.\nDanza Azteca grew popular in the U.S. with the rise of the Chicano Movement, which inspired some \"Latinos to embrace their ethnic heritage and question the Eurocentric norms forced upon them.\" The appropriation of pre-contact Aztec cultural elements has been critiqued by some Chicana/os who argue for a need to affirm the diversity of Indigenous ancestry among Chicana/os. Patrisia Gonzales portrays Chicano people as descendants of the Indigenous peoples of Mexico who have been displaced because of colonial violence, positioning them among \"detribalized Indigenous peoples and communities.\" Roberto Cintli Rodr\u00edguez describes Chicano/as as \"de-Indigenized,\" which he remarks occurred \"in part due to religious indoctrination and a violent uprooting from the land,\" detaching them from ma\u00edz-based cultures throughout the greater Mesoamerican region. Rodr\u00edguez examines how and why \"peoples who are clearly red or brown and undeniably Indigenous to this continent have allowed ourselves, historically, to be framed by bureaucrats and the courts, by politicians, scholars, and the media as alien, illegal, and less than human.\"\nGloria E. Anzald\u00faa has addressed detribalization, stating \"In the case of Chicanos, being 'Mexican' is not a tribe. So in a sense Chicanos and Mexicans are 'detribalized'. We don't have tribal affiliations but neither do we have to carry ID cards establishing tribal affiliation.\" Anzald\u00faa also recognizes that \"Chicanos, people of color, and 'whites',\" have often chosen \"to ignore the struggles of Native people even when it's right in our \"caras\" (faces),\" expressing disdain for this \"willful ignorance.\" She concludes that \"though both 'detribalized urban mixed bloods' and Chicanas/os are recovering and reclaiming, this society is killing off urban mixed bloods through cultural genocide, by not allowing them equal opportunities for better jobs, schooling, and health care.\" In\u00e9s Hern\u00e1ndez-\u00c1vila emphasizes how Chicano/as should recognize and reconnect with their roots \"respectfully and humbly\" while also validating \"those peoples who still maintain their identity as original peoples of this continent\" in order to create radical change capable of \"transforming our world, our universe, and our lives.\"\nPolitical aspects.\nAnti-imperialism and international solidarity.\nDuring World War II, Chicano youth were targeted by white servicemen, who despised their \"cool, measured indifference to the war, as well as an increasingly defiant posture toward whites in general.\" Historian Robin Kelley states that this \"annoyed white servicemen to no end.\" During the Zoot Suit Riots (1943), white rage erupted in Los Angeles, which \"became the site of racist attacks on Black and Chicano youth, during which white soldiers engaged in what amounted to a ritualized stripping of the zoot.\" Zoot suits were a symbol of collective resistance among Chicano and Black youth against city segregation and fighting in the war. Many Chicano and Black zoot-suiters engaged in draft evasion because they felt it was hypocritical for them to be expected to \"fight for democracy\" abroad yet face racism and oppression daily in the U.S.\nThis galvanized Chicano youth to focus on anti-war activism, \"especially influenced by the Third World movements of liberation in Asia, Africa, and Latin America.\" Historian Mario T. Garc\u00eda reflects that \"these anti-colonial and anti-Western movements for national liberation and self-awareness touched a historical nerve among Chicanos/as as they began to learn that they shared some similarities with these Third World struggles.\" Chicano poet Alurista argued that \"Chicanas/os cannot be truly free until they recognize that the struggle in the United States is intricately bound with the anti-imperialist struggle in other countries.\" The Cuban Revolution (1953\u201359) led by Fidel Castro and Che Guevara was particularly influential to Chicanos, as noted by Garc\u00eda, who notes that Chicanas/os viewed the revolution as \"a nationalist revolt against \"Yankee imperialism\" and neo-colonialism.\"\nIn the 1960s, the Chicano Movement brought \"attention and commitment to local struggles with an analysis and understanding of international struggles.\" Chicano youth organized with Black, Latin American, and Filipino activists to form the Third World Liberation Front (TWLF), which fought for the creation of a Third World college. During the Third World Liberation Front strikes of 1968, Chicano artists created posters to express solidarity. Chicano poster artist Rupert Garc\u00eda referred to the place of artists in the movement: \"I was critical of the police, of capitalist exploitation. I did posters of Che, of Zapata, of other Third World leaders. As artists, we climbed down from the ivory tower.\" Learning from Cuban poster makers of the post-revolutionary period, Chicano artists \"incorporated international struggles for freedom and self-determination, such as those of Angola, Chile, and South Africa,\" while also promoting the struggles of Indigenous people and other civil rights movements through Black-brown unity. Chicanas organized with women of color activists to create the Third World Women's Alliance (1968-1980), representing \"visions of liberation in third world solidarity that inspired political projects among racially and economically marginalized communities\" against U.S. capitalism and imperialism.\nThe Chicano Moratorium (1969\u201371) against the Vietnam War was one of the largest demonstrations of Mexican-Americans in history, drawing over 30,000 supporters in East L.A. Draft evasion was a form of resistance for Chicano anti-war activists such as Rosalio Mu\u00f1oz, Ernesto Vigil, and Salomon Baldengro. They faced a felony charge\u2014a minimum of five years prison time, $10,000, or both. In response, Munoz wrote \"I declare my independence of the Selective Service System. I accuse the government of the United States of America of genocide against the Mexican people. Specifically, I accuse the draft, the entire social, political, and economic system of the United States of America, of creating a funnel which shoots Mexican youth into Vietnam to be killed and to kill innocent men, women, and children...\" Rodolfo Corky Gonzales expressed a similar stance: \u201cMy feelings and emotions are aroused by the complete disregard of our present society for the rights, dignity, and lives of not only people of other nations but of our own unfortunate young men who die for an abstract cause in a war that cannot be honestly justified by any of our present leaders.\u201d\nAnthologies such as \"This Bridge Called My Back\": \"Writings by Radical Women of Color\" (1981) were produced in the late 1970s and early 80s by lesbian of color writers Cherr\u00ede Moraga, Pat Parker, Toni Cade Bambara, Chrystos, Audre Lorde, Gloria E. Anzald\u00faa, Cheryl Clarke, Jewelle Gomez, Kitty Tsui, and Hattie Gossett, who developed a poetics of liberation. and Third Woman Press, founded in 1979 by Chicana feminist Norma Alarc\u00f3n, provided sites for the production of women of color and Chicana literatures and critical essays. While first world feminists focused \"on the liberal agenda of political rights,\" Third World feminists \"linked their agenda for women's rights with economic and cultural rights\" and unified together \"under the banner of Third World solidarity.\" Maylei Blackwell identifies that this internationalist critique of capitalism and imperialism forged by women of color has yet to be fully historicized and is \"usually dropped out of the false historical narrative.\"\nIn the 1980s and 90s, Central American activists influenced Chicano leaders. The Mexican American Legislative Caucus (MALC) supported the Esquipulas Peace Agreement in 1987, standing in opposition to Contra aid. Al Luna criticized Reagan and American involvement while defending Nicaragua's Sandinista-led government: \"President Reagan cannot credibly make public speeches for peace in Central America while at the same time advocating for a three-fold increase in funding to the Contras.\" The Southwest Voter Research Initiative (SVRI), launched by Chicano leader Willie Vel\u00e1squez, intended to educate Chicano youth about Central and Latin American political issues. In 1988, \"there was no significant urban center in the Southwest where Chicano leaders and activists had not become involved in lobbying or organizing to change U.S. policy in Nicaragua.\" In the early 90s, Cherr\u00ede Moraga urged Chicano activists to recognize that \"the Anglo invasion of Latin America [had] extended well beyond the Mexican/American border\" while Gloria E. Anzald\u00faa positioned Central America as the primary target of a U.S. interventionism that had murdered and displaced thousands. However, Chicano solidarity narratives of Central Americans in the 1990s tended to center themselves, stereotype Central Americans, and filter their struggles \"through Chicana/o struggles, histories, and imaginaries.\"\nChicano activists organized against the Gulf War (1990\u201391). Raul Ruiz of the Chicano Mexican Committee against the Gulf War stated that U.S. intervention was \"to support U.S. oil interests in the region.\" Ruiz expressed, \"we were the only Chicano group against the war. We did a lot of protesting in L.A. even though it was difficult because of the strong support for the war and the anti-Arab reaction that followed ... we experienced racist attacks [but] we held our ground.\" The end of the Gulf War, along with the Rodney King Riots, were crucial in inspiring a new wave of Chicano political activism. In 1994, one of the largest demonstrations of Mexican Americans in the history of the United States occurred when 70,000 people, largely Chicano/a and Latino/a marched in Los Angeles and other cities to protest Proposition 187, which aimed to cut educational and welfare benefits for undocumented immigrants.\nIn 2004, Mujeres against Militarism and the Raza Unida Coalition sponsored a Day of the Dead vigil against militarism within the Latino community, addressing the War in Afghanistan (2001-) and the Iraq War (2003\u201311) They held photos of the dead and chanted \"no blood for oil.\" The procession ended with a five-hour vigil at Tia Chucha's Centro Cultural. They condemned \"the Junior Reserve Officers Training Corps (JROTC) and other military recruitment programs that concentrate heavily in Latino and African American communities, noting that JROTC is rarely found in upper-income Anglo communities.\" Rub\u00e9n Funkahuatl Guevara organized a benefit concert for Latin@s Against the War in Iraq and \"Mexam\u00e9rica por la Paz\" at Self-Help Graphics against the Iraq War. Although the events were well-attended, Guevara stated that \"the Feds know how to manipulate fear to reach their ends: world military dominance and maintaining a foothold in a oil-rich region were their real goals.\"\nLabor organizing against capitalist exploitation.\nChicano/a and Mexican labor organizers played an active role in notable labor strikes since the early 20th century including the Oxnard strike of 1903, Pacific Electric Railway strike of 1903, 1919 Streetcar Strike of Los Angeles, Cantaloupe strike of 1928, California agricultural strikes (1931\u201341), and the Ventura County agricultural strike of 1941, endured mass deportations as a form of strikebreaking in the Bisbee Deportation of 1917 and Mexican Repatriation (1929\u201336), and experienced tensions with one another during the Bracero program (1942\u201364). Although organizing laborers were harassed, sabotaged, and repressed, sometimes through war-like tactics from capitalist owners who engaged in coervice labor relations and collaborated with and received support from local police and community organizations, Chicano/a and Mexican workers, particularly in agriculture, have been engaged in widespread unionization activities since the 1930s.\nPrior to unionization, agricultural workers, many of whom were undocumented, worked in dismal conditions. Historian F. Arturo Rosales recorded a Federal Project Writer of the period, who stated: \"It is sad, yet true, commentary that to the average landowner and grower in California the Mexican was to be placed in much the same category with ranch cattle, with this exception\u2013the cattle were for the most part provided with comparatively better food and water and immeasurably better living accommodations.\" Growers used cheap Mexican labor to reap bigger profits and, until the 1930s, perceived Mexicans as docile and compliant with their subjugated status because they \"did not organize troublesome labor unions, and it was held that he was not educated to the level of unionism.\" As one grower described, \"We want the Mexican because we can treat them as we cannot treat any other living man ... We can control them by keeping them at night behind bolted gates, within a stockade eight feet high, surrounded by barbed wire ... We can make them work under armed guards in the fields.\"\nUnionization efforts were initiated by the Confederaci\u00f3n de Uniones Obreras (Federation of Labor Unions) in Los Angeles, with twenty-one chapters quickly extending throughout southern California, and La Uni\u00f3n de Trabajadores del Valle Imperial (Imperial Valley Workers' Union). The latter organized the Cantaloupe strike of 1928, in which workers demanded better working conditions and higher wages, but \"the growers refused to budge and, as became a pattern, local authorities sided with the farmers and through harassment broke the strike.\" Communist-led organizations such as the Cannery and Agricultural Workers' Industrial Union (CAWIU) supported Mexican workers, renting spaces for cotton pickers during the cotton strikes of 1933 after they were thrown out of company housing by growers. Capitalist owners used \"red-baiting\" techniques to discredit the strikes through associating them with communists. Chicana and Mexican working women showed the greatest tendency to organize, particularly in the Los Angeles garment industry with the International Ladies' Garment Workers' Union, led by anarchist Rose Pesotta.\nDuring World War II, the government-funded Bracero program (1942\u201364) hindered unionization efforts. In response to the California agricultural strikes and the 1941 Ventura County strike of Chicano/a and Mexican, as well as Filipino, lemon pickers/packers, growers organized the Ventura County Citrus Growers Committee (VCCGC) and launched a lobbying campaign to pressure the U.S. government to pass laws to prohibit labor organizing. VCCGC joined with other grower associations, forming a powerful lobbying bloc in Congress, and worked to legislate for (1) a Mexican guest workers program, which would become the Bracero program, (2) laws prohibiting strike activity, and (3) military deferments for pickers. Their lobbying efforts were successful: unionization among farmworkers was made illegal, farmworkers were excluded from minimum wage laws, and the usage of child labor by growers was ignored. In formerly active areas, such as Santa Paula, union activity stopped for over thirty years as a result.\nWhen World War II ended, the Bracero program continued. Legal anthropologist Martha Menchaca states that this was \"regardless of the fact that massive quantities of crops were no longer needed for the war effort ... after the war, the braceros were used for the benefit of the large-scale growers and not for the nation's interest.\" The program was extended for an indefinite period in 1951. In the mid-1940s, labor organizer Ernesto Galarza founded the National Farm Workers Union (NFWU) in opposition to the Bracero Program, organizing a large-scale 1947 strike against the Di Giorgio Fruit Company in Arvin, California. Hundreds of Mexican, Filipino, and white workers walked out and demanded higher wages. The strike was broken by the usual tactics, with law enforcement on the side of the owners, evicting strikers and bringing in undocumented workers as strikebreakers. The NFWU folded, but served as a precursor to the United Farm Workers Union led by C\u00e9sar Ch\u00e1vez. By the 1950s, opposition to the Bracero program had grown considerably, as unions, churches, and Mexican-American political activists raised awareness about the effects it had on American labor standards. On December 31, 1964, the U.S. government conceded and terminated the program.\nFollowing the closure of the Bracero program, domestic farmworkers began to organize again because \"growers could not longer maintain the peonage system\" with the end of imported laborers from Mexico. Labor organizing formed part of the Chicano Movement via the struggle of farmworkers against depressed wages and working conditions. C\u00e9sar Ch\u00e1vez began organizing Chicano farmworkers in the early 1960s, first through the National Farm Workers Association (NFWA) and then merging the association with the Agricultural Workers Organizing Committee (AWOC), an organization of mainly Filipino workers, to form the United Farm Workers. The labor organizing of Ch\u00e1vez was central to the expansion of unionization throughout the United States and inspired the Farm Labor Organizing Committee (FLOC), under the leadership of Baldemar Vel\u00e1squez, which continues today. Farmworkers collaborated with local Chicano organizations, such as in Santa Paula, California, where farmworkers attended Brown Berets meetings in the 1970s and Chicano youth organized to improve working conditions and initiate an urban renewal project on the eastside of the city.\nAlthough Mexican and Chicano/a workers, organizers, and activists organized for decades to improve working conditions and increase wages, some scholars characterize these gains as minimal. As described by Ronald Mize and Alicia Swords, \"piecemeal gains in the interests of workers have had very little impact on the capitalist agricultural labor process, so picking grapes, strawberries, and oranges in 1948 is not so different from picking those same crops in 2008.\" U.S. agriculture today remains totally reliant on Mexican labor, with Mexican-born individuals now constituting about 90% of the labor force.\nStruggles in the education system.\nChicana/o students often endure struggles in the U.S. education system, which has been identified as a colonial institution exercising control over colonized students. Chicana/o communities have engaged in numerous forms of protest and direct action against the colonial education system, such as walkouts. On March 5, 1968, the Chicano Blowouts at East Los Angeles High School occurred as a response to the racist treatment of Chicana/o students, an unresponsive school board, and a high dropout rate. It became known as \"the first major mass protest against racism undertaken by Mexican-Americans in the history of the United States.\" Sal Castro, a Chicano social science teacher at the school was arrested and fired for inspiring the walkouts, led by student activists such as Harry Gamboa Jr., who was named \"one of the hundred most dangerous and violent subversives in the United States\" for organizing the student walkouts. The day before the walkouts, FBI director J. Edgar Hoover sent out a memo to law enforcement to place top priority on \"political intelligence work to prevent the development of nationalist movements in minority communities.\" Chicana activist Alicia Escalante protested Castro's dismissal: \"We in the Movement will at least be able to hold our heads up and say that we haven't submitted to the gringo or to the pressures of the system. We are brown and we are proud. I am at least raising my children to be proud of their heritage, to demand their rights, and as they become parents they too will pass this on until justice is done.\"\nIn 1969, Plan de Santa B\u00e1rbara was drafted as a 155-page document that outlined the foundation of Chicana/o Studies programs in higher education. It called for students, faculty, employees and the community to come together as \"central and decisive designers and administrators of these programs.\" Chicana/o students and activists asserted that universities should exist to serve the community. However, by the mid-1970s, much of the radicalism of earlier Chicana/o studies became deflated by the colonial academy, which aimed \"to change the objective and purpose\" of Chicana/o Studies programs from within. As stated by historian Mario Garc\u00eda, problems arose when Chicanas/os became part of the academic institution; one \"encountered a deradicalization of the radicals.\" Some opportunistic faculty avoided their political responsibilities to the community while university administrators co-opted oppositional forces within Chicana/o Studies programs and encouraged tendencies that led \"to the loss of autonomy of Chicano Studies programs.\" At the same time, \"a domesticated Chicano Studies provided the university with the facade of being tolerant, liberal, and progressive.\"\nSome Chicanas/os argued that the solution was \"to strengthen Chicano Studies institutionally\" by creating \"publishing outlets that would challenge Anglo control of academic print culture with its rules on peer review and thereby publish alternative research,\" arguing that by creating a Chicano space in the colonial academy that Chicanas/os could \"avoid colonization in higher education.\" They worked with institutions like the Ford Foundation in an attempt to establish educational autonomy, but quickly found that \"these organizations presented a paradox.\" As described by Rodolfo Acu\u00f1a, while these organizations may have initially \"formed part of the Chicano/a challenge to higher education and the transformation of the community, they quickly became content to only acquire funding for research and thereby determine the success or failure of faculty.\" As a result, Chicana/o Studies had soon become \"much closer the mainstream than its practitioners wanted to acknowledge.\" For example, the Chicano Studies Center at UCLA, shifted away from its earlier interests in serving the Chicana/o community to gaining status within the colonial institution through a focus on academic publishing. Because of the historical and contemporary struggles of Chicanas/os in the colonial education system, many doubt its potential for transformative change; as Rodolfo Acu\u00f1a states, \"revolutions are made in the streets, not on college campuses.\"\nChicanas/os continue to acknowledge the US educational system as an institution upholding Anglo colonial dominance. In 2012, the Mexican American Studies Department Programs (MAS) in Tucson Unified School District were banned after a campaign led by Anglo-American politician Tom Horne accused it of working to \"promote the overthrow of the U.S. government, promote resentment toward a race or class of people, are designed primarily for pupils of a particular ethnic group or advocate ethnic solidarity instead of the treatment of pupils as individuals.\" Classes on Latino literature, American history/Mexican-American perspectives, Chicano art, and an American government/social justice education project course were banned. Readings of In Lak'ech from Luis Valdez's poem \"Pensamiento Serpentino\" were also banned. Seven books, including Paulo Friere's \"Pedagogy of the Oppressed\" and works covering Chicano history and critical race theory, were banned, taken from students, and stored away. The ban was overturned in 2017 by Judge A. Wallace Tashima, who ruled that it was unconstitutional and motivated by racism by depriving Chicana/o students of knowledge, thereby violating their Fourteenth Amendment right. The Xicanx Institute for Teaching &amp; Organizing (XITO) emerged to carry on the legacy of the MAS programs.\nRejection of borders.\nChicanas/os often reject the concept of borders through the concept of \"sin fronteras\", the idea of no borders. The 1848 Treaty of Guadalupe Hidalgo transformed the Rio Grande region from a rich cultural center to a rigid border poorly enforced by the United States government. At the end of the Mexican-American War, 80,000 Spanish-Mexican-Indian people were forced into sudden U.S. habitation. Some Chicanas/os identified with the idea of Aztl\u00e1n as a result, which celebrated a time preceding land division and rejected the \"immigrant/foreigner\" categorization by Anglo society. Chicana/o activists have called for unionism between both Mexicans and Chicanas/os on both sides of the border.\nIn the early 20th century, the border crossing had become a site of brutality and dehumanization for Mexicans. Protests in 1910 arose along the Santa Fe Bridge due to abuses committed against Mexican workers while crossing the border. The 1917 Bath riots erupted after Mexicans crossing the border were required to strip naked and be disinfected with various chemical agents, including gasoline, kerosene, sulfuric acid, and Zyklon B, the latter of which was the fumigation of choice and would later notoriously be used in the gas chambers of Nazi Germany. During the early 20th century, Chicanos used \"corridos\" \"to counter Anglocentric hegemony.\" As described by Ram\u00f3n Saldivar, \"\"corridos\" served the symbolic function of empirical events and for creating counterfactual worlds of lived experience (functioning as a substitute for fiction writing).\"\nNewspaper \"Sin Fronteras\" (1976\u201379) openly rejected the Mexico-United States border. The newspaper considered it \"to be only an artificial creation that in time would be destroyed by the struggles of Mexicans on both sides of the border\" and recognized that \"Yankee political, economic, and cultural colonialism victimized all Mexicans, whether in the U.S. or in Mexico.\" Similarly, the General Brotherhood of Workers (CASA), important to the development of young Chicano intellectuals and activists, identified that, as \"victims of oppression, \"Mexicanos\" could achieve liberation and self-determination only by engaging in a borderless struggle to defeat American international capitalism.\"\nChicana theorist Gloria E. Anzald\u00faa notably emphasized the border as a \"1,950 mile-long wound that does not heal.\" In referring to the border as a wound, writer Catherine Leen suggests that Anzald\u00faa recognizes \"the trauma and indeed physical violence very often associated with crossing the border from Mexico to the US, but also underlies the fact that the cyclical nature of this immigration means that this process will continue and find little resolution.\" Anzald\u00faa writes that \"la frontera\" signals \"the coming together of two self-consistent but habitually incompatible frames of reference [which] cause \"un choque\", a cultural collision\" because \"the U.S.-Mexican border \"es una herida abierta\" where the Third World grates against the first and bleeds.\" Chicana/o and Mexican artists and filmmakers continue to address \"the contentious issues of exploitation, exclusion, and conflict at the border and attempt to overturn border stereotypes\" through their work. Luis Alberto Urrea writes \"the border runs down the middle of me. I have a barbed wire fence neatly bisecting my heart.\"\nSociological aspects.\nCriminalization.\nNot aspiring to assimilate in Anglo-American society, Chicano/a youth were criminalized for their defiance to cultural assimilation: \"When many of the same youth began wearing what the larger society considered outlandish clothing, sporting distinctive hairstyles, speaking in their own language (\"Cal\u00f3\"), and dripping with attitude, law enforcement redoubled their efforts to rid the streets of this emerging predatory class.\" Chicano sociologist and lawyer Alfredo Mirand\u00e9, who developed the sociohistorical theory of gringo justice to explain the double standard applied to Chicanos and Latinos (in comparison to Anglo-Americans) in the US criminal justice system, states that \"the criminalization of the Chicano resulted not from their being more criminal or violent but from a clash between conflicting and competing cultures, world views, and economic, political, and judicial systems.\" The criminalization of Chicanos in Anglo-American society historically led to the rise of Chicano gang culture, initially as a way to resist Euro-American racism. \nThe historical image of the Mexican in the Southwest was \"that of the greasy Mexican bandit or \"bandito\",\" who was perceived as criminal because of Mestizo ancestry and \"Indian blood.\" As stated by Walter Prescott in 1935, \"there is a cruel streak in the Mexican nature ... this cruelty may be a heritage from the Spanish and of the Inquisition; it may, and doubtless should be, attributed partly to Indian blood.\" The \"greasy bandito\" stereotype of the old West evolved into images of \"crazed Zoot-Suiters and pachuco killers in the 1940s, to contemporary \"cholos\", gangsters, and gang members.\" Pachucos were portrayed as violent criminals in American mainstream media which fueled the Zoot Suit Riots; initiated by off-duty policemen conducting a vigilante-hunt, the riots targeted Chicano youth who wore the zoot suit as a symbol of empowerment. On-duty police supported the violence against Chicano zoot suiters; they \"escorted the servicemen to safety and arrested their Chicano victims.\" Arrest rates of Chicano youth rose during these decades, fueled by the \"criminal\" image portrayed in the media, by politicians, and by the police.\nThe Zoot Suit Riots and the Sleepy Lagoon case served as an origins point for \"the beginning of the hyper-criminalization of Chicana/o youth.\" In the 1970s, there was a wave of police killings of Chicanos. One of the most prominent cases was Luis \"Tato\" Rivera, who was a 20-year-old Chicano shot in the back by officer Craig Short in 1975. 2,000 Chicano/a demonstrators showed up to the city hall of National City, California in protest. Short was indicted for manslaughter by district attorney Ed Miller and was acquitted of all charges. Short was later appointed acting chief of police of National City in 2003. Another high-profile case was the murder of Ricardo Falc\u00f3n, a student at the University of Colorado and leader of the United Latin American Students (UMAS), by Perry Brunson, a member of the far-right American Independent Party, at a gas station. Bruson was tried for manslaughter and was \"acquitted by an all-White jury.\" Falc\u00f3n became a martyr for the Chicano Movement as police violence increased in the subsequent decades. This led sociologist Alfredo Mirand\u00e9 to refer to the criminal justice system as \"gringo justice\", because \"it reflected one standard for Anglos and another for Chicanos.\"\nThe criminalization of Chicano/a youth in the \"barrio\" remains omnipresent. Chicano/a youth who adopt a \"cholo\" or \"chola\" identity endure hyper-criminalization in what has been described by Victor Rios as the youth control complex. While older residents initially \"embraced the idea of a \"chola\" or \"cholo\" as a larger subculture not necessarily associated with crime and violence (but rather with a youthful temporary identity), law enforcement agents, ignorant or disdainful of \"barrio\" life, labeled youth who wore clean white tennis shoes, shaved their heads, or long socks, as deviant.\" Community members were convinced by the police of cholo/a criminality, which led to criminalization and surveillance \"reminiscent of the criminalization of Chicana and Chicano youth during the Zoot-Suit era in the 1940s.\" Sociologist Jos\u00e9 S. Plascencia-Castillo refers to the \"barrio\" as a panopticon, a space which leads to intense self-regulation, as Cholo/a youth are both scrutinized by law enforcement to \"stay in their side of town\" and by the community who in some cases \"call the police to have the youngsters removed from the premises.\" The intense governance of Chicana/o youth, especially those who adopt the chola/o identity, has deep implications on youth experience, affecting their physical and mental health as well as their outlook on the future. Some youth feel they \"can either comply with the demands of authority figures, and become obedient and compliant, and suffer the accompanying loss of identity and self-esteem, or, adopt a resistant stance and contest social invisibility to command respect in the public sphere.\"\nGender and sexuality.\nChicana women and girls often confront objectification in Anglo society, being perceived as \"exotic,\" \"lascivious,\" and \"hot\" at a very young age while also facing denigration as \"barefoot,\" \"pregnant,\" \"dark,\" and \"low-class.\" These perceptions in society engender numerous negative sociological and psychological effects, such as excessive dieting and eating disorders. Social media may enhance these stereotypes of Chicana women and girls. Numerous studies have found that Chicanas experience elevated levels of stress as a result of sexual expectations by their parents and families. Although many Chicana youth desire open conversation of these gendered and sexual expectations, as well as mental health, these issues are often not discussed openly in Chicano families, which perpetuates unsafe and destructive practices. While young Chicana women are objectified, middle-aged Chicanas discuss feelings of being invisible, saying they feel trapped in balancing family obligations to their parents and children while attempting to create a space for their own sexual desires. The expectation that Chicana women should be \"protected\" by Chicano men may also constrict the agency and mobility of Chicana women.\nChicano men develop their identity within a context of marginalization in Anglo society. Some writers argue that \"Mexican men and their Chicano brothers suffer from an inferiority complex due to the conquest and genocide inflicted upon their Indigenous ancestors,\" which leaves Chicano men feeling trapped between identifying with the so-called \"superior\" European and the so-called \"inferior\" Indigenous sense of self. This conflict is said to manifest itself in the form of hypermasculinity or machismo, in which a \"quest for power and control over others in order to feel better\" about oneself is undertaken. This may result in abusive behavior, the development of an impenetrable \"cold\" persona, alcohol abuse, and other destructive and self-isolating behaviors. The lack of discussion of sexuality between Chicano men and their fathers or their mothers means that Chicano men tend to learn about sex from their peers as well as older male family members who perpetuate the idea that as men they have \"a right to engage in sexual activity without commitment.\" The looming threat of being labeled a \"joto\" (gay) for not engaging in sexual activity also conditions many Chicano men to \"use\" women for their own sexual desires.\nHeteronormative gender roles are often enforced in Chicano families. Any deviation from gender and sexual conformity is perceived as a weakening or attack of \"la familia\". However, certain Chicano men who retain a masculine gender identity are afforded some mobility to secretly engage in homosexual behaviors because of their gender performance, as long as it remains on the fringes. Effeminacy in Chicano men, Chicana lesbianism, and any other deviation which challenges patriarchal gender and sexuality is highly policed and understood as an attack on the family by Chicano men. Chicana women in the normative Chicano family are relegated to a secondary and subordinate status. Cherrie Moraga argues that this issue of patriarchal ideology in Chicano and Latino communities runs deep, as the great majority of Chicano and Latino men believe in and uphold male supremacy. Moraga also points to how this ideology is upheld in Chicano families by mothers in their relationship to their children: \"the daughter must constantly earn the mother's love, prove her fidelity to her. The son\u2013he gets her love for free.\"\nQueer Chicanas/os may seek refuge in their families, if possible, because it is difficult for them to find spaces where they feel safe in the dominant and hostile Anglo culture which surrounds them while also feeling excluded because of the hypermasculinity, and subsequent homophobia, that frequently exists in Chicano familial and communal spaces. Gabriel S. Estrada describes how \"the overarching structures of capitalist white (hetero)sexism,\" including higher levels of criminalization directed toward Chicanos, has proliferated \"further homophobia\" among Chicano boys and men who may adopt \"hypermasculine personas that can include sexual violence directed at others.\" Estrada notes that not only does this constrict \"the formation of a balanced Indigenous sexuality for anyone, but especially ... for those who do identify\" as part of the queer community to reject the \"Judeo-Christian mandates against homosexuality that are not native to their own ways,\" recognizing that many Indigenous societies in Mexico and elsewhere accepted homosexuality openly prior to arrival of European colonizers.\nMental health.\nChicana/os may seek out both Western biomedical healthcare and Indigenous health practices when dealing with trauma or illness. The effects of colonization are proven to produce psychological distress among Indigenous communities. Intergenerational trauma, along with racism and institutionalized systems of oppression, have been shown to adversely impact the mental health of Chicana/os and Latina/os. Mexican Americans are three times more likely than European Americans to live in poverty. Chicano/a adolescent youth experience high rates of depression and anxiety. Chicana adolescents have higher rates of depression and suicidal ideation than their European-American and African-American peers. Chicano adolescents experience high rates of homicide, and suicide. Chicana/os ages ten to seventeen are at a greater risk for mood and anxiety disorders than their European-American and African-American peers. Scholars have determined that the reasons for this are unclear due to the scarcity of studies on Chicana/o youth, but that intergenerational trauma, acculturative stress, and family factors are believed to contribute.\nAmong Mexican immigrants who have lived in the United States for less than thirteen years, lower rates of mental health disorders were found in comparison to Mexican-Americans and Chicanos born in the United States. Scholar Yvette G. Flores concludes that these studies demonstrate that \"factors associated with living in the United States are related to an increased risk of mental disorders.\" Risk factors for negative mental health include historical and contemporary trauma stemming from colonization, marginalization, discrimination, and devaluation. The disconnection of Chicanos from their Indigeneity has been cited as a cause of trauma and negative mental health:Loss of language, cultural rituals, and spiritual practices creates shame and despair. The loss of culture and language often goes unmourned, because it is silenced and denied by those who occupy, conquer, or dominate. Such losses and their psychological and spiritual impact are passed down across generations, resulting in depression, disconnection, and spiritual distress in subsequent generations, which are manifestations of historical or intergenerational trauma.Psychological distress may emerge from Chicanos being \"othered\" in society since childhood and is linked to psychiatric disorders and symptoms which are culturally bound \u2013 \"susto\" (fright), \"nervios\" (nerves), \"mal de ojo\" (evil eye), and \"ataque de nervios\" (an attack of nerves resembling a panic attack). Dr. Manuel X. Zamarripa discusses how mental health and spirituality are often seen as disconnected subjects in Western perspectives. Zamarripa states \"in our community, spirituality is key for many of us in our overall wellbeing and in restoring and giving balance to our lives.\" For Chicana/os, Zamarripa recognizes that identity, community, and spirituality are three core aspects which are essential to maintaining good mental health.\nSpirituality.\nChicano/a spirituality has been described as a process of engaging in a journey to unite one's consciousness for the purposes of cultural unity and social justice. It brings together many elements and is therefore hybrid in nature. Scholar Regina M Marchi states that Chicano/a spirituality \"emphasizes elements of struggle, process, and politics, with the goal of creating a unity of consciousness to aid social development and political action.\" Lara Medina and Martha R. Gonzales explain that \"reclaiming and reconstructing our spirituality based on non-Western epistemologies is central to our process of decolonization, particularly in these most troubling times of incessant Eurocentric, heteronormative patriarchy, misogyny, racial injustice, global capitalist greed, and disastrous global climate change.\" As a result, some scholars state that Chicana/o spirituality must involve a study of Indigenous Ways of Knowing (IWOK). The \"Circulo de Hombres\" group in San Diego, California spiritually heals Chicano, Latino, and Indigenous men \"by exposing them to Indigenous-based frameworks, men of this cultural group heal and rehumanize themselves through Maya-Nahua Indigenous-based concepts and teachings,\" helping them process integenerational trauma and dehumanization that has resulted from colonization. A study on the group reported that reconnecting with Indigenous worldviews was overwhelmingly successful in helping Chicano, Latino, and Indigenous men heal. As stated by Jesus Mendoza, \"our bodies remember our indigenous roots and demand that we open our mind, hearts, and souls to our reality.\" \nChicano/a spirituality is a way for Chicana/os to listen, reclaim, and survive while disrupting coloniality. While historically Catholicism was the primary way for Chicana/os to express their spirituality, this is changing rapidly. According to a Pew Research Center report in 2015, \"the primary role of Catholicism as a conduit to spirituality has declined and some Chicana/os have changed their affiliation to other Christian religions and many more have stopped attending church altogether.\" Increasingly, Chicana/os are considering themselves spiritual rather than religious or part of an organized religion. A study on spirituality and Chicano men in 2020 found that many Chicanos indicated the benefits of spirituality through connecting with Indigenous spiritual beliefs and worldviews instead of Christian or Catholic organized religion in their lives. Dr. Lara Medina defines spirituality as (1) Knowledge of oneself - one's gifts and one's challenges, (2) Co-creation or a relationship with communities (others), and (3) A relationship with sacred sources of life and death 'the Great Mystery' or Creator. Jesus Mendoza writes that, for Chicana/os, \"spirituality is our connection to the earth, our pre-Hispanic history, our ancestors, the mixture of pre-Hispanic religion with Christianity ... a return to a non-Western worldview that understands all life as sacred.\" In her writing on Gloria Anzaldua's idea of \"spiritual activism\", AnaLouise Keating states that spirituality is distinct from organized religion and New Age thinking. Leela Fernandes defines spirituality as follows:When I speak of spirituality, at the most basic level I am referring to an understanding of the self as encompassing body and mind, as well as spirit. I am also referring to a transcendent sense of interconnection that moves beyond the knowable, visible material world. This sense of interconnection has been described variously as divinity, the sacred, spirit, or simply the universe. My understanding is also grounded in a form of lived spirituality, which is directly accessible to all and which does not need to be mediated by religious experts, institutions or theological texts; this is what is often referred to as the mystical side of spirituality... Spirituality can be as much about practices of compassion, love, ethics, and truth defined in nonreligious terms as it can be related to the mystical reinterpretations of existing religious traditions.\nDavid Carrasco states that Mesoamerican spiritual or religious beliefs have historically always been evolving in response to the conditions of the world around them: \"These ritual and mythic traditions were not mere repetitions of ancient ways. New rituals and mythic stories were produced to respond to ecological, social, and economic changes and crises.\" This was represented through the art of the Olmecs, Maya, and Mexica. European colonizers sought and worked to destroy Mesoamerican worldviews regarding spirituality and replace these with a Christian model. The colonizers used syncreticism in art and culture, exemplified through practices such as the idea presented in the Testerian Codices that \"Jesus ate tortillas with his disciples at the last supper\" or the creation of the Virgen de Guadalupe (mirroring the Christian Mary) in order to force Christianity into Mesoamerican cosmology.\nChicana/os can create new spiritual traditions by recognizing this history or \"by observing the past and creating a new reality.\" Gloria Anzaldua states that this can be achieved through nepantla spirituality or a space where, as stated by Jesus Mendoza, \"all religious knowledge can coexist and create a new spirituality ... where no one is above the other ... a place where all is useful and none is rejected.\" Anzaldua and other scholars acknowledge that this is a difficult process that involves navigating many internal contradictions in order to find a path towards spiritual liberation. Cherrie Moraga calls for a deeper self-exploration of who Chicana/os are in order to reach \"a place of deeper inquiry into ourselves as a people ... possibly, we must turn our eyes away from racist America and take stock at the damages done to us. Possibly, the greatest risks yet to be taken are entre nosotros, where we write, paint, dance, and draw the wound for one another to build a stronger pueblo. The women artist seemed disposed to do this, their work often mediating the delicate area between cultural affirmation and criticism.\" Laura E. P\u00e9rez states in her study of Chicana art that \"the artwork itself [is] altar-like, a site where the disembodied - divine, emotional, or social - [is] acknowledged, invoked, meditated upon, and released as a shared offering.\" \nCultural aspects.\nThe term \"Chicanismo\" describes the cultural, cinematic, literary, musical, and artistic movements that emerged with the Chicano Movement. While the Chicano Movement tended to focus and prioritize the masculine subject, the diversity of Chicano cultural production is vast. As noted by artist Guillermo G\u00f3mez-Pe\u00f1a, \"the actual diversity and complexity\" of the Chicana/o community, which includes influences from Central American, Caribbean, Asian, and African Americans who have moved into Chicana/o communities as well as queer people of color, has been consistently overlooked. Many Chicanx artists therefore continue to challenge and question \"conventional, static notions of \"Chicanismo\",\" while others conform to more conventional cultural traditions.\nWith mass media, Chicana/o culture has become popularized internationally. Lowrider car clubs have emerged, most notably in S\u00e3o Paulo, Brazil, M\u0101ori youth enhancing lowrider bicycles and taking on cholo style, and elements of Chicano culture including music, lowriders, and the arts being adopted in Japan. Chicano culture took hold in Japan in the 1980s and continues to grow with contributions from Shin Miyata, Junichi Shimodaira, Miki Style, Night Tha Funksta, and MoNa (Sad Girl). Miyata owns a record label, Gold Barrio Records, that re-releases Chicano music. Chicana/o fashion and other cultural aspects have also been adopted in Japan. There has been debate over whether this should be termed cultural appropriation, with some arguing that it is appreciation rather than appropriation.\nFilm.\nChicana/o film is rooted in economic, social, and political oppression and has therefore been marginalized since its inception. Scholar Charles Ram\u00edrez Berg has suggested that Chicana/o cinema has progressed through three fundamental stages since its establishment in the 1960s. The first wave occurred from 1969 to 1976 and was characterized by the creation of radical documentaries which chronicled \"the cinematic expression of a cultural nationalist movement, it was politically contestational and formally oppositional.\" Some films of this era include El Teatro Campesino's \"Yo Soy Joaqu\u00edn\" (1969) and Luis Valdez's \"El Corrido\" (1976). These films were focused on documenting the systematic oppression of Chicanas/os in the United States.\nThe second wave of Chicana/o film, according to Ram\u00edrez Berg, developed out of portraying anger against oppression faced in society, highlighting immigration issues, and re-centering the Chicana/o experience, yet channeling this in more accessible forms which were not as outright separatist as the first wave of films. Docudramas like Esperanza Vasquez's \"\" (1977), Jes\u00fas Salvador Trevi\u00f1o's \"Ra\u00edces de Sangre\" (1977), and Robert M. Young's \"\u00a1Alambrista!\" (1977) served as transitional works which would inspire full-length narrative films. Early narrative films of the second wave include Valdez's \"Zoot Suit\" (1981), Young's \"The Ballad of Gregorio Cortez\" (1982), Gregory Nava's, \"My Family/Mi familia\" (1995) and \"Selena\" (1997), and Josefina L\u00f3pez's \"Real Women Have Curves\", originally a play which premiered in 1990 and was later released as a film in 2002.\nThe second wave of Chicana/o film is still ongoing and overlaps with the third wave, the latter of which gained noticeable momentum in the 1990s and does not emphasize oppression, exploitation, or resistance as central themes. According to Ram\u00edrez Berg, third wave films \"do not accentuate Chicano oppression or resistance; ethnicity in these films exists as one fact of several that shape characters' lives and stamps their personalities.\"\nLiterature.\nChicana/o literature tends to incorporate themes of identity, discrimination, and culture, with an emphasis on validating Mexican American and Chicana/o culture in the United States. Chicana/o writers also focus on challenging the dominant colonial narrative, \"not only to critique the uncritically accepted 'historical' past, but more importantly to reconfigure it in order to envision and prepare for a future in which native peoples can find their appropriate place in the world and forge their individual, hybrid sense of self.\" Notable Chicana/o writers include Norma Elia Cant\u00fa, Gary Soto, Sergio Troncoso, Rigoberto Gonz\u00e1lez, Raul Salinas, Daniel Olivas, Benjamin Alire S\u00e1enz, Lu\u00eds Alberto Urrea, Dagoberto Gilb, Alicia Gaspar de Alba, Luis J. Rodriguez and Pat Mora.\nRodolfo \"Corky\" Gonzales's \"Yo Soy Joaquin\" is one of the first examples of explicitly Chicano poetry, while Jos\u00e9 Antonio Villarreal's \"Pocho\" (1959) is widely recognized as the first major Chicano novel. The novel \"Chicano\", by Richard Vasquez, was the first novel about Mexican Americans to be released by a major publisher (Doubleday, 1970). It was widely read in high schools and universities during the 1970s and is now recognized as a breakthrough novel. Vasquez's social themes have been compared with those found in the work of Upton Sinclair and John Steinbeck.\nChicana writers have tended to focus on themes of identity, questioning how identity is constructed, who constructs it, and for what purpose in a racist, classist, and patriarchal structure. Characters in books such as Victuum (1976) by Isabella R\u00edos, \"The House on Mango Street\" (1983) by Sandra Cisneros, \"Loving in the War Years: lo que nunca pas\u00f3 por sus labios\" (1983) by Cherr\u00ede Moraga, \"The Last of the Menu Girls\" (1986) by Denise Ch\u00e1vez, \"Margins\" (1992) by Terri de la Pe\u00f1a, and \"Gulf Dreams\" (1996) by Emma P\u00e9rez have also been read regarding how they intersect with themes of gender and sexuality. Academic Catri\u00f3na Rueda Esquibel performs a queer reading of Chicana literature in her work \"With Her Machete in Her Hand: Reading Chicana Lesbians\" (2006), demonstrating how some of the intimate relationships between girls and women in these works contributes to a discourse on homoeroticism and nonnormative sexuality in Chicana/o literature.\nChicano writers have tended to gravitate toward themes of cultural, racial, and political tensions in their work, while not explicitly focusing on issues of identity or gender and sexuality, in comparison to the work of Chicana writers. Chicanos who were marked as overtly gay in early Chicana/o literature, from 1959 to 1972, tended to be removed from the Mexican-American \"barrio\" and were typically portrayed with negative attributes, as examined by Daniel Enrique P\u00e9rez, such as the character of \"Joe Pete\" in \"Pocho\" and the unnamed protagonist of John Rechy's \"City of Night\" (1963). However, other characters in the Chicano canon may also be read as queer, such as the unnamed protagonist of Tom\u00e1s Rivera's \"...y no se lo trag\u00f3 la tierra\" (1971), and \"Antonio M\u00e1rez\" in Rudolfo Anaya's Bless Me, Ultima (1972), since, according to P\u00e9rez, \"these characters diverge from heteronormative paradigms and their identities are very much linked to the rejection of heteronormativity.\"\nAs noted by scholar Juan Bruce-Novoa, Chicano novels allowed for androgynous and complex characters \"to emerge and facilitate a dialogue on nonnormative sexuality\" and that homosexuality was \"far from being ignored during the 1960s and 1970s\" in Chicano literature, although homophobia may have curtailed portrayals of openly gay characters during this era. Given this representation in early Chicano literature, Bruce-Novoa concludes, \"we can say our community is less sexually repressive than we might expect.\"\nMusic.\nLalo Guerrero has been lauded as the \"father of Chicano music.\" Beginning in the 1930s, he wrote songs in the big band and swing genres that were popular at the time. He expanded his repertoire to include songs written in traditional genres of Mexican music, and during the farmworkers' rights campaign, wrote music in support of C\u00e9sar Ch\u00e1vez and the United Farm Workers. Jeffrey Lee Pierce of The Gun Club often spoke about being half-Mexican and growing up with the Chicano culture.\nOther Chicano/Mexican-American singers include Selena, who sang a mixture of Mexican, Tejano, and American popular music, and died in 1995 at the age of 23; Zack de la Rocha, social activist and lead vocalist of Rage Against the Machine; and Los Lonely Boys, a Texas-style country rock band who have not ignored their Mexican-American roots in their music. In recent years, a growing Tex-Mex polka band trend influenced by the ' and ' music of Mexican immigrants, has in turn influenced much new Chicano folk music, especially on large-market Spanish language radio stations and on television music video programs in the U.S. Some of these artists, like the band Quetzal, are known for the political content of political songs.\nElectronic.\nChicano electronic artists DJ Rolando, Santiago Salazar, DJ Tranzo, and Esteban Adame have released music through independent labels like Underground Resistance, Planet E, Krown Entertainment, and Rush Hour. In the 1990s, artists such as DJ Juanito (Johnny Loopz), Rudy \"Rude Dog\" Gonzalez, and Juan V. released numerous tracks through Los Angeles-based house labels Groove Daddy Records and Bust A Groove.\nDJ Rolando's \"Knights of the Jaguar,\" released on the UR label in 1999, became the most well-known Chicano techno track after charting at #43 in the UK in 2000 and being named one of the \"20 best US rave anthems of the '90s\" by Mixmag: \"after it was released, it spread like wildfire all over the world. It's one of those rare tracks that feels like it can play for an eternity without anyone batting an eyelash.\" In 2013, it was voted the 26th best house track of all time by Mixmag. \nSalazar and Adame are also affiliated with UR and have collaborated with DJ Dex (Nomadico). Salazar founded music labels Major People, Ican (as in \"Mex-Ican\", with Esteban Adame) and Historia y Violencia (with Juan Mendez a.k.a. Silent Servant) and released his debut album \"Chicanismo\" in 2015 to positive reviews. Nomadico's label Yaxteq, founded in 2015, has released tracks by veteran Los Angeles techno producer Xavier De Enciso and Honduran producer Ritmos.\nHip hop.\nHip hop culture, which is cited as having formed in the 1980s street culture of African American, West Indian (especially Jamaican), and Puerto Rican New York City Bronx youth and characterized by DJing, rap music, graffiti, and breakdancing, was adopted by many Chicano youth by the 1980s as its influence moved westward across the United States. Chicano artists were beginning to develop their own style of hip hop. Rappers such as Ice-T and Easy-E shared their music and commercial insights with Chicano rappers in the late 1980s. Chicano rapper Kid Frost, who is often cited as \"the godfather of Chicano rap\" was highly influenced by Ice-T and was even cited as his prot\u00e9g\u00e9.\nChicano rap is a unique style of hip hop music which started with Kid Frost, who saw some mainstream exposure in the early 1990s. While Mellow Man Ace was the first mainstream rapper to use Spanglish, Frost's song \"La Raza\" paved the way for its use in American hip hop. Chicano rap tends to discuss themes of importance to young urban Chicanos. Some of today's Chicano artists include A.L.T., Lil Rob, Psycho Realm, Baby Bash, Serio, A Lighter Shade of Brown, and Funky Aztecs Sir Dyno, and Choosey.\nChicano R&amp;B artists include Paula DeAnda, Frankie J, and Victor Ivan Santos (early member of the Kumbia Kings and associated with Baby Bash).\nJazz.\nAlthough Latin jazz is most popularly associated with artists from the Caribbean (particularly Cuba) and Brazil, young Mexican Americans have played a role in its development over the years, going back to the 1930s and early 1940s, the era of the zoot suit, when young Mexican-American musicians in Los Angeles and San Jose, such as Jenni Rivera, began to experiment with \"\", a jazz-like fusion genre that has grown recently in popularity among Mexican Americans\nRock.\nIn the 1950s, 1960s and 1970s, a wave of Chicano pop music surfaced through innovative musicians Carlos Santana, Johnny Rodriguez, Ritchie Valens and Linda Ronstadt. Joan Baez, who is also of Mexican-American descent, included Hispanic themes in some of her protest folk songs. Chicano rock is rock music performed by Chicano groups or music with themes derived from Chicano culture.\nThere are two undercurrents in Chicano rock. One is a devotion to the original rhythm and blues roots of Rock and roll including Ritchie Valens, Sunny and the Sunglows, and ? and the Mysterians. Groups inspired by this include Sir Douglas Quintet, Thee Midniters, Los Lobos, War, Tierra, and El Chicano, and, of course, the Chicano Blues Man himself, the late Randy Garribay. The second theme is the openness to Latin American sounds and influences. Trini Lopez, Santana, Malo, Azteca, Toro, Ozomatli and other Chicano Latin rock groups follow this approach. Chicano rock crossed paths of other Latin rock genres (Rock en espa\u00f1ol) by Cubans, Puerto Ricans, such as Joe Bataan and Ralphi Pagan and South America (Nueva canci\u00f3n). Rock band The Mars Volta combines elements of progressive rock with traditional Mexican folk music and Latin rhythms along with Cedric Bixler-Zavala's Spanglish lyrics.\nChicano punk is a branch of Chicano rock. There were many bands that emerged from the California punk scene, including The Zeros, Bags, Los Illegals, The Brat, The Plugz, Manic Hispanic, and the Cruzados; as well as others from outside of California including Mydolls from Houston, Texas and Los Crudos from Chicago, Illinois. Some music historians argue that Chicanos of Los Angeles in the late 1970s might have independently co-founded punk rock along with the already-acknowledged founders from European sources when introduced to the US in major cities. The rock band ? and the Mysterians, which was composed primarily of Mexican-American musicians, was the first band to be described as punk rock. The term was reportedly coined in 1971 by rock critic Dave Marsh in a review of their show for \"Creem\" magazine.\nPerformance arts.\nEl Teatro Campesino (The Farmworkers' Theater) was founded by Luis Valdez and Agustin Lira in 1965 as the cultural wing of the United Farm Workers (UFW) as a result of the Great Delano Grape Strike in 1965. All of the actors were farmworkers and involved in organizing for farmworkers' rights. Its first performances sought to recruit members for the UFW and dissuade strikebreakers. Many early performances were not scripted and were rather conceived through the direction of Valdez and others through \"actos\", in which a scenario would be proposed for a scene and then dialogue would simply be improvised.\nChicano/a performance art continued with the work of Los Angeles' comedy troupe Culture Clash, Guillermo G\u00f3mez-Pe\u00f1a, and Nao Bustamante, known internationally for her conceptual art pieces and as a participant in \"\". Chicano performance art became popular in the 1970s, blending humor and pathos for tragicomic effect. Groups such as Asco and the Royal Chicano Air Force illustrated this aspect of performance art through their work. Asco (Spanish for \"naseau\" or \"disgust\"), composed of Willie Her\u00f3n, Gronk, Harry Gamboa Jr., and Patssi Valdez, created performance pieces such as the \"Walking Mural\", walking down Whittier Boulevard dressed as \"a multifaceted mural, a Christmas tree, and the Virgin of Guadalupe. Asco continued its conceptual performance piece until 1987.\nIn the 1990s, San Diego-based artist cooperative of David Avalos, Louis Hock, and Elizabeth Sisco used their National Endowment for the Arts $5,000 fellowship subversively, deciding to circulate money back to the community: \"handing 10-dollar bills to undocumented workers to spend as they please.\" Their piece, entitled Arte Reembolsa (Art Rebate) created controversy among the art establishment, with the documentation of the piece featuring \"footage of U.S. House and Senate members questioning whether the project was, in fact, art.\"\nVisual arts.\nThe Chicano visual art tradition, like the identity, is grounded in community empowerment and resisting assimilation and oppression. Prior to the introduction of spray cans, paint brushes were used by Chicano \"shoeshine boys [who] marked their names on the walls with their daubers to stake out their spots on the sidewalk\" in the early 20th century. Pachuco graffiti culture in Los Angeles was already \"in full bloom\" by the 1930s and 1940s, pachucos developed their \"placa\", \"a distinctive calligraphic writing style\" which went on to influence contemporary graffiti tagging. Pa\u00f1o, a form of \"pinto arte\" (a \"cal\u00f3\" term for male prisoner) using pen and pencil, developed in the 1930s, first using bed sheets and pillowcases as canvases. Pa\u00f1o has been described as \"rasquachismo\", a Chicano worldview and artmaking method which makes the most from the least.\nGraffiti artists, such as Charles \"Chaz\" Boj\u00f3rquez, developed an original style of graffiti art known as West Coast Cholo style influenced by Mexican murals and pachuco \"placas\" (tags which indicate territorial boundaries) in the mid-20th century. In the 1960s, Chicano graffiti artists from San Antonio to L.A., especially in East LA, Whittier, and Boyle Heights, used the artform to challenge authority, tagging police cars, buildings, and subways as \"a demonstration of their bravado and anger,\" understanding their work as \"individual acts of pride or protest, gang declarations of territory or challenge, and weapons in a class war.\" Chicano graffiti artists wrote \"con safos\" (loosely translated to expressing a \"so what\" or \"the same to you\" attitude)\u2014a common expression among Chicanos on the eastside of Los Angeles.\nThe Chicano Movement and political identity had heavily influenced Chicano/a artists by the 1970s. Alongside the Black arts movement, this led to the development of institutions such as Self-Help Graphics, Los Angeles Contemporary Exhibitions, and Plaza de la Raza. Artists such as Harry Gamboa Jr., Gronk, and Judith Baca created art which \"stood in opposition to the commercial galleries, museums, and civic institutional mainstream.\" This was exemplified with Asco's tagging of LACMA after \"a curator refused to even entertain the idea of a Chicano art show within its walls\" in 1972.\nChicano muralism, which began in the 1960s, became a state-sanctioned artform in the 1970s as an attempt by outsiders to \"prevent gang violence and dissuade graffiti practices.\" This led to the creation of murals at Estrada Courts and other sites throughout Chicano/a communities. In some instances, these murals were covered with the \"placas\" they were instituted by the state to prevent. Marcos Sanchez-Tranquilino states that \"rather than vandalism, the tagging of one's own murals points toward a complex sense of wall ownership and a social tension created by the uncomfortable yet approving attentions of official cultural authority.\" This created a division between established Chicano/a artists who celebrated inclusion and acceptance by the dominant culture and younger Chicano artists who \"saw greater power in renegade muralism and \"barrio\" calligraphy than in state-sanctioned pieces.\" Chicano poster art became prominent in the 1970s as a way to challenge political authority, with pieces such as Rupert Garc\u00eda's \"Save Our Sister\" (1972), depicting Angela Davis, and Yolanda M. L\u00f3pez's \"Who's the Illegal Alien, Pilgrim?\" (1978) addressing settler colonialism.\nThe oppositional current of Chicano art was bolstered in the 1980s by a rising hip hop culture. The Olympic freeway murals, including Frank Romero's \"Going to the Olympics\", created for the 1984 Olympic Games in Los Angeles became another site of contestation, as Chicano and other graffiti artists tagged the state-sanctioned public artwork. Government officials, muralists, and some residents were unable to understand the motivations for this, described it \"as \u2018mindless', \u2018animalistic' vandalism perpetrated by \u2018kids' who simply lack respect.\" L.A. had developed a distinct graffiti culture by the 1990s and, with the rise of drugs and violence, Chicano youth culture gravitated towards graffiti to express themselves and to mark their territory amidst state-sanctioned disorder. Following the Rodney King riots and the murder of Latasha Harlins, which exemplified an explosion of racial tensions bubbling under in American society, racialized youth in L.A., \"feeling forgotten, angry, or marginalized, [embraced] graffiti\u2019s expressive power [as] a tool to push back.\"\n Chicano art, although accepted into some institutional art spaces in shows like , was still largely excluded from many mainstream art institutions in the 1990s. By the 2000s, attitudes towards graffiti by white hipster culture were changing, as it became known as \"street art.\" In academic circles, \"street art\" was termed \"post-graffiti.\" By the 2000s, where the LAPD once deployed CRASH (Community Resources Against Street Hoodlums) units in traditionally Chicano neighborhoods like Echo Park and \"often brutalized suspected taggers and gang members,\" \"street art\" was now being mainstreamed by the white art world in those same neighborhoods.\nDespite this shift, Chicana/o artists continued to challenge what was acceptable to both insiders and outsiders of their communities. Controversy surrounding Chicana artist Alma L\u00f3pez's \"Our Lady\" at the Museum of International Folk Art in 2001 erupted when \"local demonstrators demanded the image be removed from the state-run museum.\" Previously, L\u00f3pez's digital mural \"Heaven\" (2000), which depicted two Latina women embracing, had been vandalized. L\u00f3pez received homophobic slurs, threats of physical violence, and over 800 hate mail inquiries for \"Our Lady.\" Santa Fe Archbishop Michael J Sheehan referred to the woman in L\u00f3pez's piece as \"as a tart or a street woman.\" L\u00f3pez stated that the response came from the conservative Catholic Church, \"which finds women's bodies inherently sinful, and thereby promot[es] hatred of women's bodies.\" The art was again protested in 2011.\nManuel Paul's mural \"Por Vida\" (2015) at Galeria de la Raza in Mission District, San Francisco, which depicted queer and trans Chicanos/as, was targeted multiple times after its unveiling. Paul, a queer DJ and artist of the Maric\u00f3n Collective, received online threats for the work. Ani Rivera, director of Galeria de la Raza, attributed the anger towards the mural to gentrification, which has led \"some people [to] associate LGBT people with non-Latino communities.\" The mural was meant to challenge \"long-held assumptions regarding the traditional exclusivity of heterosexuality in lowrider culture.\" Some credited the negative response to the mural's direct challenging of machismo and heteronormativity in the community.\nXandra Ibarra's video art \"Spictacle II: La Tortillera\" (2004) was censored by San Antonio's Department of Arts and Culture in 2020 from \"XicanX: New Visions,\" a show which aimed to challenge \"previous and existing surveys of Chicano and Latino identity-based exhibitions\" through highlighting \u201cthe womxn, queer, immigrant, indigenous and activist artists who are at the forefront of the movement.\u201d Ibarra stated \"the video is designed to challenge normative ideals of Mexican womanhood and is in alignment with the historical lineage of LGBTQAI+ artists\u2019 strategies to intervene in homophobic and sexist violence.\""}
{"id": "5717", "revid": "1012622717", "url": "https://en.wikipedia.org/wiki?curid=5717", "title": "Canary Islands", "text": "The Canary Islands (; , ), also known informally as \"the Canaries\", is a Spanish archipelago in the Atlantic Ocean, in a region known as Macaronesia. At their closest point to the African mainland, they are west of Morocco. They are the southernmost of the autonomous communities of Spain, and are located in the African Tectonic Plate. The archipelago is economically and politically European, and is part of the European Union.\nThe eight main islands are (from largest to smallest in area) Tenerife, Fuerteventura, Gran Canaria, Lanzarote, La Palma, La Gomera, El Hierro and La Graciosa. The archipelago includes many smaller islands and islets, including Alegranza, Isla de Lobos, Monta\u00f1a Clara, Roque del Oeste, and Roque del Este. It also includes a number of rocks, including those of Salmor, Fasnia, Bonanza, Garachico, and Anaga. In ancient times, the island chain was often referred to as \"the Fortunate Isles\". The Canary Islands are the southernmost region of Spain, and the largest and most populous archipelago of Macaronesia. Because of their location, the Canary Islands have historically been considered a bridge between the four continents of Africa, North America, South America, and Europe.\nIn 2019, the Canary Islands had a population of 2,153,389 with a density of 287.39 inhabitants per km2, making it the eighth most populous autonomous community. The population is mostly concentrated in the two capital islands: around 43% on the island of Tenerife and 40% on the island of Gran Canaria.\nThe Canary Islands, especially Tenerife, Gran Canaria, Fuerteventura, and Lanzarote, are a major tourist destination, with over 12\u00a0million visitors per year. This is due to their beaches, subtropical climate, and important natural attractions, especially Maspalomas in Gran Canaria, Teide National Park, and Mount Teide (a World Heritage Site) in Tenerife. Mount Teide is the third tallest volcano in the world, measured from its base on the ocean floor. Because of the islands\u2019 subtropical climate, it has long, hot summers, and moderately warm winters. The amount of precipitation and the level of maritime moderation vary depending on location and elevation. The archipelago includes green areas as well as desert areas. The islands\u2019 high mountains are ideal for astronomical observation, because they lie above the temperature inversion layer. As a result, the archipelago boasts two professional observatories: Teide Observatory on the island of Tenerife, and Roque de los Muchachos Observatory on the island of La Palma.\nIn 1927, the Province of Canary Islands was split into two provinces. In 1982, the autonomous community of the Canary Islands was established. The cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria are, jointly, the capital of the islands. Those cities are also, respectively, the capitals of the provinces of Santa Cruz de Tenerife and Las Palmas. Las Palmas de Gran Canaria has been the largest city in the Canaries since 1768, except for a brief period in the 1910s. Between the 1833 territorial division of Spain and 1927, Santa Cruz de Tenerife was the sole capital of the Canary Islands. In 1927, it was ordered by decree that the capital of the Canary Islands would be shared between two cities, and this arrangement persists to the present day. The third largest city in the Canary Islands is San Crist\u00f3bal de La Laguna (a World Heritage Site) on Tenerife. This city is also home to the \"Consejo Consultivo de Canarias\", which is the supreme consultative body of the Canary Islands.\nDuring the era of the Spanish Empire, the Canaries were the main stopover for Spanish galleons on their way to the Americas, which sailed that far south in order to catch the prevailing northeasterly trade winds.\nEtymology.\nThe name \"Islas Canarias\" is likely derived from the Latin name \"Canariae Insulae\", meaning \"Islands of the Dogs\", a name that was evidently generalized from the ancient name of one of these islands, \"Canaria\" \u2013 presumably Gran Canaria. According to the historian Pliny the Elder, the island \"Canaria\" contained \"vast multitudes of dogs of very large size\".\nAlternatively, it is said that the original inhabitants of the island, Guanches, used to worship dogs, mummified them and generally treated them as holy animals. Some hypothesize that the Canary Islands' dog-worship and the ancient Egyptian cult of the dog-headed god Anubis are closely connected.\nOther theories speculate that the name comes from the Nukkari Berber tribe living in the Moroccan Atlas, named in Roman sources as \"Canarii\", though Pliny again mentions the relation of this term with dogs.\nThe connection to dogs is retained in their depiction on the islands' coat-of-arms.\nIt is considered that the aborigines of Gran Canaria called themselves \"Canarios\". It is possible that after being conquered, this name was used in plural in Spanish, i.e., as to refer to all of the islands as the Canarii-as.\nWhat is certain is that the name of the islands does not derive from the canary bird; rather, the birds are named after the islands.\nPhysical geography.\nTenerife is the largest and most populous island of the archipelago. Gran Canaria, with 865,070 inhabitants, is both the Canary Islands' second most populous island, and the third most populous one in Spain after Majorca and Tenerife. The island of Fuerteventura is the second largest in the archipelago and located from the African coast.\nThe islands form the Macaronesia ecoregion with the Azores, Cape Verde, Madeira, and the Savage Isles. The Canary Islands is the largest and most populated archipelago of the Macaronesia region. The archipelago consists of seven large and several smaller islands, all of which are volcanic in origin. The antipodes of the Canary Islands are found in the Pacific Ocean, between New Zealand, New Caledonia, Australia and the ocean.\nAccording to the position of the islands with respect to the north-east trade winds, the climate can be mild and wet or very dry. Several native species form laurisilva forests.\nAs a consequence, the individual islands in the Canary archipelago tend to have distinct microclimates. Those islands such as El Hierro, La Palma and La Gomera lying to the west of the archipelago have a climate which is influenced by the moist Canary Current. They are well vegetated even at low levels and have extensive tracts of sub-tropical laurisilva forest. As one travels east toward the African coast, the influence of the current diminishes, and the islands become increasingly arid. Fuerteventura and Lanzarote, the islands which are closest to the African mainland, are effectively desert or semi desert. Gran Canaria is known as a \"continent in miniature\" for its diverse landscapes like Maspalomas and Roque Nublo. In terms of its climate Tenerife is particularly interesting. The north of the island lies under the influence of the moist Atlantic winds and is well vegetated, while the south of the island around the tourist resorts of Playa de las Americas and Los Cristianos is arid. The island rises to almost above sea level, and at altitude, in the cool relatively wet climate, forests of the endemic pine \"Pinus canariensis\" thrive. Many of the plant species in the Canary Islands, like the Canary Island pine and the dragon tree, \"Dracaena draco\" are endemic, as noted by Sabin Berthelot and Philip Barker Webb in their work, \"L'Histoire Naturelle des \u00celes Canaries\" (1835\u201350).\nClimate.\nThe climate is tropical and desertic, moderated by the sea and in summer by the trade winds. There are a number of microclimates and the classifications range mainly from semi-arid to desert. According to the K\u00f6ppen climate classification, the majority of the Canary Islands have a hot desert climate represented as BWh, caused partly due to the cool Canary Current. There also exists a subtropical humid climate which is very influenced by the ocean in the middle of the islands of La Gomera, Tenerife and La Palma, where laurisilva forests grow.\nGeology.\nThe seven major islands, one minor island, and several small islets were originally volcanic islands, formed by the Canary hotspot. The Canary Islands is the only place in Spain where volcanic eruptions have been recorded during the Modern Era, with some volcanoes still active (El Hierro, 2011).\nVolcanic islands such as those in the Canary chain often have steep ocean cliffs caused by catastrophic debris avalanches and landslides.\nThe Teide volcano on Tenerife is the highest mountain in Spain, and the third tallest volcano on Earth on a volcanic ocean island. All the islands except La Gomera have been active in the last million years; four of them (Lanzarote, Tenerife, La Palma and El Hierro) have historical records of eruptions since European discovery. The islands rise from Jurassic oceanic crust associated with the opening of the Atlantic. Underwater magmatism commenced during the Cretaceous, and continued to the present day. The current islands reached the ocean's surface during the Miocene. The islands were once considered as a distinct physiographic section of the Atlas Mountains province, which in turn is part of the larger African Alpine System division, but are nowadays recognized as being related to a magmatic hot spot.\nIn the summer of 2011 a series of low-magnitude earthquakes occurred beneath El Hierro. These had a linear trend of northeast\u2013southwest. In October a submarine eruption occurred about south of Restinga. This eruption produced gases and pumice, but no explosive activity was reported.\nThe following table shows the highest mountains in each of the islands:\nNatural symbols.\nThe official natural symbols associated with Canary Islands are the bird \"Serinus canaria\" (canary) and the \"Phoenix canariensis\" palm.\nNational parks.\nFour of Spain's thirteen national parks are located in the Canary Islands, more than any other autonomous community. Two of these have been declared UNESCO World Heritage Sites and the other two are part of Biosphere Reserves. The parks are:\nTeide National Park is the oldest and largest national park in the Canary Islands and one of the oldest in Spain. Located in the geographic centre of the island of Tenerife, it is the most visited national park in Spain. In 2010, it became the most visited national park in Europe and second worldwide. The park's highlight is the Teide volcano; standing at an altitude of , it is the highest elevation of the country and the third largest volcano on Earth from its base. In 2007, the Teide National Park was declared one of the 12 Treasures of Spain.\nPolitics.\nGovernance.\nThe regional executive body, the Parliament of the Canary Islands, is presided over by \u00c1ngel V\u00edctor Torres (PSOE), the current President of the Canary Islands. The latter is invested by the members of the regional legislature, the Parliament of the Canary Islands, that consists of 70 elected legislators. The last regional election took place in May 2019.\nThe islands have 14 seats in the Spanish Senate. Of these, 11 seats are directly elected (3 for Gran Canaria, 3 for Tenerife, and 1 each for Lanzarote (including La Graciosa), Fuerteventura, La Palma, La Gomera and El Hierro) while the other 3 are appointed by the regional legislature.\nPolitical geography.\nThe Autonomous Community of the Canary Islands consists of two provinces (\"provincias\"), Las Palmas and Santa Cruz de Tenerife, whose capitals (Las Palmas de Gran Canaria and Santa Cruz de Tenerife) are capitals of the autonomous community. Each of the seven major islands is ruled by an island council named \"Cabildo Insular\". Each island is subdivided into smaller municipalities (\"municipios\"); Las Palmas is divided into 34 municipalities, and Santa Cruz de Tenerife is divided into 54 municipalities.\nThe international boundary of the Canaries is the subject of dispute between Spain and Morocco. Morocco's official position is that international laws regarding territorial limits do not authorise Spain to claim seabed boundaries based on the territory of the Canaries, since the Canary Islands enjoy a large degree of autonomy. In fact, the islands do not enjoy any special degree of autonomy as each one of the Spanish regions is considered an autonomous community with equal status to the European ones. \nThe boundary determines the ownership of seabed oil deposits and other ocean resources. Morocco and Spain have been unable to agree on a compromise regarding the territorial boundary, since neither nation wants to cede its claimed right to the vast resources whose ownership depends upon the boundary. In 2002, for example, Morocco rejected a unilateral Spanish proposal.\nCanarian nationalism.\nThere are some pro-independence political parties, like the National Congress of the Canaries (CNC) and the Popular Front of the Canary Islands, but their popular support is almost insignificant, with no presence in either the autonomous parliament or the \"cabildos insulares\".\nAccording to a 2012 study by the Centro de Investigaciones Sociol\u00f3gicas, when asked about national identity, the majority of respondents from the Canary Islands (49.3%) consider themselves Spanish and Canarian in equal measures, followed by 37.1% who consider themselves more Canarian than Spanish. Only 6.1% of the respondents consider themselves only Canarian.\nHistory.\nAncient and pre-colonial times.\nBefore the arrival of humans, the Canaries were inhabited by prehistoric animals; for example, the giant lizard (\"Gallotia goliath\"), the Tenerife and Gran Canaria giant rats, and giant prehistoric tortoises, \"Geochelone burchardi\" and \"Geochelone vulcanica\".\nThe islands may have been visited by the Phoenicians, the Greeks, and the Carthaginians. King Juba II, Caesar Augustus's Numidian prot\u00e9g\u00e9, is credited with discovering the islands for the Western world. According to Pliny the Elder, Juba found the islands uninhabited, but found \"a small temple of stone\" and \"some traces of buildings\". Juba dispatched a naval contingent to re-open the dye production facility at Mogador in what is now western Morocco in the early first century\u00a0AD. That same naval force was subsequently sent on an exploration of the Canary Islands, using Mogador as their mission base.\nThe names given by Romans to the individual islads islands were \"Ninguaria\" or \"Nivaria\" (Tenerife), \"Canaria\" (Gran Canaria), \"Pluvialia\" or \"Invale\" (Lanzarote), \"Ombrion\" (La Palma), \"Planasia\" (Fuerteventura), \"Iunonia\" or \"Junonia\" (El Hierro) and \"Capraria\" (La Gomera).\nWhen the Europeans began to explore the islands in the late Middle Ages, they encountered several indigenous peoples living at a Neolithic level of technology. Although the prehistory of the settlement of the Canary Islands is still unclear, linguistic and genetic analyses seem to indicate that at least some of these inhabitants shared a common origin with the Berbers on the nearby North African coast. The precolonial inhabitants came to be known collectively as the Guanches, although \"Guanches\" had been the name for only the indigenous inhabitants of Tenerife. From the 14th\u00a0century onward, numerous visits were made by sailors from Majorca, Portugal and Genoa. Lancelotto Malocello settled on Lanzarote in 1312. The Majorcans established a mission with a bishop in the islands that lasted from 1350 to 1400.\nCastilian conquest.\nIn 1402, the Castilian conquest of the islands began, with the expedition of the French explorers Jean de B\u00e9thencourt and Gadifer de la Salle, nobles and vassals of Henry III of Castile, to Lanzarote. From there, they went on to conquer Fuerteventura (1405) and El Hierro. B\u00e9thencourt received the title King of the Canary Islands, but still recognised King Henry III as his overlord. It was not a simple military enterprise, given the aboriginal resistance on some islands. Neither was it politically, since the particular interests of the nobility (determined to strengthen their economic and political power through the acquisition of the islands) conflicted with those of the states, particularly Castile, which were in the midst of territorial expansion and in a process of strengthening of the Crown against the nobility.\nHistorians distinguish two periods in the conquest of the Canary Islands:\nAristocratic conquest (\"Conquista se\u00f1orial\"). This refers to the early conquests carried out by the nobility, for their own benefit and without the direct participation of the Crown of Castile, which merely granted rights of conquest in exchange for pacts of vassalage between the noble conqueror and the Crown. One can identify within this period an early phase known as the Betancurian or Norman Conquest, carried out by Jean de Bethencourt (who was originally from Normandy) and Gadifer de la Salle between 1402 and 1405, which involved the islands of Lanzarote, El Hierro and Fuerteventura. The subsequent phase is known as the Castilian Conquest, carried out by Castilian nobles who acquired, through purchases, assignments and marriages, the previously conquered islands and also incorporated the island of La Gomera around 1450.\nRoyal conquest (\"Conquista realenga\"). This defines the conquest between 1478 and 1496, carried out directly by the Crown of Castile, during the reign of the Catholic Monarchs, who armed and partly financed the conquest of those islands which were still unconquered: Gran Canaria, La Palma and Tenerife. This phase of the conquest came to an end in the year 1496, with the dominion of the island of Tenerife, bringing the entire Canarian Archipelago under the control of the Crown of Castile.\nB\u00e9thencourt also established a base on the island of La Gomera, but it would be many years before the island was fully conquered. The natives of La Gomera, and of Gran Canaria, Tenerife, and La Palma, resisted the Castilian invaders for almost a century. In 1448 Maciot de B\u00e9thencourt sold the lordship of Lanzarote to Portugal's Prince Henry the Navigator, an action that was accepted by neither the natives nor the Castilians. Despite Pope Nicholas V ruling that the Canary Islands were under Portuguese control, the crisis swelled to a revolt which lasted until 1459 with the final expulsion of the Portuguese. In 1479, Portugal and Castile signed the Treaty of Alc\u00e1\u00e7ovas, which settled disputes between Castile and Portugal over the control of the Atlantic. This treaty recognized Castilian control of the Canary Islands but also confirmed Portuguese possession of the Azores, Madeira, and the Cape Verde islands, and gave the Portuguese rights to any further islands or lands in the Atlantic that might be discovered.\nThe Castilians continued to dominate the islands, but due to the topography and the resistance of the native Guanches, they did not achieve complete control until 1496, when Tenerife and La Palma were finally subdued by Alonso Fern\u00e1ndez de Lugo. After that, the Canaries were incorporated into the Kingdom of Castile.\nAfter the conquest.\nAfter the conquest, the Castilians imposed a new economic model, based on single-crop cultivation: first sugarcane; then wine, an important item of trade with England. In this era, the first institutions of colonial government were founded. Gran Canaria, a colony of the Crown of Castile since 6 March 1480 (from 1556, of Spain), and Tenerife, a Spanish colony since 1496, each had its own governor. There has been speculation that the abundance of roccella tinctoria on the Canary Islands offered a profit motive for Jean de B\u00e9thencourt during his conquest of the islands. Lichen has been used for centuries to make dyes. This includes royal purple colors derived from roccella tinctoria, also known as orseille.\nThe cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria became a stopping point for the Spanish conquistadors, traders, and missionaries on their way to the New World. This trade route brought great prosperity to some of the social sectors of the islands. The islands became quite wealthy and soon were attracting merchants and adventurers from all over Europe. Magnificent palaces and churches were built on La Palma during this busy, prosperous period. The Church of El Salvador survives as one of the island's finest examples of the architecture of the 16th century. Civilian architecture survives in forms such as Casas de los S\u00e1nchez-Ochando or Casa Quintana.\nThe Canaries' wealth invited attacks by pirates and privateers. Ottoman Turkish admiral and privateer Kemal Reis ventured into the Canaries in 1501, while Murat Reis the Elder captured Lanzarote in 1585.\nThe most severe attack took place in 1599, during the Dutch Revolt. A Dutch fleet of 74 ships and 12,000 men, commanded by Pieter van der Does, attacked the capital Las Palmas de Gran Canaria (the city had 3,500 of Gran Canaria's 8,545 inhabitants). The Dutch attacked the Castillo de la Luz, which guarded the harbor. The Canarians evacuated civilians from the city, and the Castillo surrendered (but not the city). The Dutch moved inland, but Canarian cavalry drove them back to Tamaraceite, near the city.\nThe Dutch then laid siege to the city, demanding the surrender of all its wealth. They received 12 sheep and 3 calves. Furious, the Dutch sent 4,000 soldiers to attack the Council of the Canaries, who were sheltering in the village of Santa Br\u00edgida. 300 Canarian soldiers ambushed the Dutch in the village of Monte Lentiscal, killing 150 and forcing the rest to retreat. The Dutch concentrated on Las Palmas de Gran Canaria, attempting to burn it down. The Dutch pillaged Maspalomas, on the southern coast of Gran Canaria, San Sebasti\u00e1n on La Gomera, and Santa Cruz on La Palma, but eventually gave up the siege of Las Palmas and withdrew.\nIn 1618 the Barbary pirates attacked Lanzarote and La Gomera taking 1000 captives to be sold as slaves. Another noteworthy attack occurred in 1797, when Santa Cruz de Tenerife was attacked by a British fleet under Horatio Nelson on 25 July. The British were repulsed, losing almost 400 men. It was during this battle that Nelson lost his right arm.\n18th to 19th century.\nThe sugar-based economy of the islands faced stiff competition from Spain's Caribbean colonies. Low sugar prices in the 19th century caused severe recessions on the islands. A new cash crop, cochineal (\"cochinilla\"), came into cultivation during this time, reinvigorating the islands' economy. During this time the Canarian-American trade was developed, in which Canarian products such as cochineal, sugarcane and rum were sold in American ports such as Veracruz, Campeche, La Guaira and Havana, among others.\nBy the end of the 18th century, Canary Islanders had already emigrated to Spanish American territories, such as Havana, Veracruz, and Santo Domingo, San Antonio, Texas and St. Bernard Parish, Louisiana. These economic difficulties spurred mass emigration during the 19th and first half of the 20th century, primarily to the Americas. Between 1840 and 1890 as many as 40,000 Canary Islanders emigrated to Venezuela. Also, thousands of Canarians moved to Puerto Rico where the Spanish monarchy felt that Canarians would adapt to island life better than other immigrants from the mainland of Spain. Deeply entrenched traditions, such as the Mascaras Festival in the town of Hatillo, Puerto Rico, are an example of Canarian culture still preserved in Puerto Rico. Similarly, many thousands of Canarians emigrated to the shores of Cuba. During the Spanish\u2013American War of 1898, the Spanish fortified the islands against a possible American attack, but no such event took place.\nRomantic period and scientific expeditions.\nSirera and Renn (2004) distinguish two different types of expeditions, or voyages, during the period 1770\u20131830, which they term \"the Romantic period\":\nFirst are \"expeditions financed by the States, closely related with the official scientific Institutions. characterised by having strict scientific objectives (and inspired by) the spirit of Illustration and progress\". In this type of expedition, Sirera and Renn include the following travellers:\nThe second type of expedition identified by Sirera and Renn is one that took place starting from more or less private initiatives. Among these, the key exponents were the following:\nSirera and Renn identify the period 1770\u20131830 as one in which \"In a panorama dominated until that moment by France and England enters with strength and brio Germany of the Romantic period whose presence in the islands will increase\".\nEarly 20th century.\nAt the beginning of the 20th century, the British introduced a new cash-crop, the banana, the export of which was controlled by companies such as Fyffes.\n30 November 1833 the Province of Canary Islands had been created with the capital being declared as Santa Cruz de Tenerife. The rivalry between the cities of Las Palmas de Gran Canaria and Santa Cruz de Tenerife for the capital of the islands led to the division of the archipelago into two provinces on 23 September 1927.\nDuring the time of the Second Spanish Republic, Marxist and anarchist workers' movements began to develop, led by figures such as Jose Miguel Perez and Guillermo Ascanio. However, outside of a few municipalities, these organisations were a minority and fell easily to Nationalist forces during the Spanish Civil War.\nFranco regime.\nIn 1936, Francisco Franco was appointed General Commandant of the Canaries. He joined the military revolt of 17 July which began the Spanish Civil War. Franco quickly took control of the archipelago, except for a few points of resistance on La Palma and in the town of Vallehermoso, on La Gomera. Though there was never a war in the islands, the post-war suppression of political dissent on the Canaries was most severe.\nDuring the Second World War, Winston Churchill prepared plans for the British seizure of the Canary Islands as a naval base, in the event of Gibraltar being invaded from the Spanish mainland.\nOpposition to Franco's regime did not begin to organise until the late 1950s, which experienced an upheaval of parties such as the Communist Party of Spain and the formation of various nationalist, leftist parties.\nSelf-governance.\nAfter the death of Franco, there was a pro-independence armed movement based in Algeria, the Movement for the Independence and Self-determination of the Canaries Archipelago (MAIAC). In 1968, the Organisation of African Unity recognized the MAIAC as a legitimate African independence movement, and declared the Canary Islands as an African territory still under foreign rule.\nAfter the establishment of a democratic constitutional monarchy in Spain, autonomy was granted to the Canaries via a law passed in 1982, with a newly established autonomous devolved government and parliament. In 1983, the first autonomous elections were held. The Spanish Socialist Workers' Party (PSOE) won. In the 2007 elections, the PSOE gained a plurality of seats, but the nationalist Canarian Coalition and the conservative Partido Popular (PP) formed a ruling coalition government.\nCapitals.\nAt present, the Canary Islands is the only autonomous community in Spain that has two capitals: Santa Cruz de Tenerife and Las Palmas de Gran Canaria, since the was created in 1982.\nThe political capital of the archipelago did not exist as such until the nineteenth century. The first cities founded by the Europeans at the time of the conquest of the Canary Islands in the 15th century were: Telde (in Gran Canaria), San Marcial del Rubic\u00f3n (in Lanzarote) and Betancuria (in Fuerteventura). These cities boasted the first European institutions present in the archipelago, including Catholic bishoprics. Although, because the period of splendor of these cities developed before the total conquest of the archipelago and its incorporation into the Crown of Castile never had a political and real control of the entire Canary archipelago.\nThe function of a Canarian city with full jurisdiction for the entire archipelago only exists after the conquest of the Canary Islands, although originally \"de facto\", that is, without legal and real meaning and linked to the headquarters of the Canary Islands General Captaincy.\nLas Palmas de Gran Canaria was the first city that exercised this function. This is because the residence of the Captain General of the Canary Islands was in this city during part of the sixteenth and seventeenth centuries. In May 1661, the Captain General of the Canary Islands, Jer\u00f3nimo de Benavente y Qui\u00f1ones, moved the headquarters of the captaincy to the city of San Crist\u00f3bal de La Laguna on the island of Tenerife. This was due to the fact that this island since the conquest was the most populated, productive and with the highest economic expectations. La Laguna would be considered the \"de facto\" capital of the archipelago until the official status of the capital of Canary Islands in the city of Santa Cruz de Tenerife was confirmed in the 19th century, due in part to the constant controversies and rivalries between the bourgeoisies of San Crist\u00f3bal de La Laguna and Las Palmas de Gran Canaria for the economic, political and institutional hegemony of the archipelago.\nAlready in 1723, the Captain General of the Canary Islands Lorenzo Fernandez de Villavicencio had moved the headquarters of the General Captaincy of the Canary Islands from San Crist\u00f3bal de La Laguna to Santa Cruz de Tenerife. This decision continued without pleasing the society of the island of Gran Canaria. It would be after the creation of the Province of Canary Islands in November 1833 in which Santa Cruz would become the first fully official capital of the Canary Islands (\"De jure\" and not of \"de facto\" as happened previously). Santa Cruz de Tenerife would be the capital of the Canary archipelago until during the Government of General Primo de Rivera in 1927 the Province of Canary Islands was split in two provinces: Las Palmas with capital in Las Palmas de Gran Canaria, and Santa Cruz de Tenerife with capital in the homonymous city.\nFinally, with the Statute of Autonomy of the Canary Islands in 1982 and the creation of the Autonomous Community of the Canary Islands, the capital of the archipelago between Las Palmas de Gran Canaria and Santa Cruz de Tenerife is fixed, which is how it remains today.\nDemographics.\nThe Canary Islands have a population of 2,153,389 inhabitants (2019), making it the eighth most populous of Spain's autonomous communities. The total area of the archipelago is , resulting in a population density of 287.4 inhabitants per square kilometre.\nThe Canarian population includes long-tenured residents and new waves of mainland Spanish immigrants, as well as foreign-born populations. In 2019 the total population was 2,153,389, of which 72.1% were native Canary Islanders. A total of 80.6%, or 1,735,457, were born in Spain and 19.4%, or 417,932, were born outside the country. Of these, the majority are from the Americas, mainly from Venezuela (66,593), Cuba (41,807) and Colombia (31,368). There are also almost 40,000 people from Africa, the majority from Morocco (24,281).\nPopulation of the individual islands.\nThe population of the islands according to the 2019 data are:\nReligion.\nThe Roman Catholic branch of Christianity has been the majority religion in the archipelago for more than five centuries, ever since the Conquest of the Canary Islands. There are also several other religious communities.\nRoman Catholic Church.\nThe overwhelming majority of native Canarians are Roman Catholic (76.7%) with various smaller foreign-born populations of other Christian beliefs such as Protestants.\nThe appearance of the Virgin of Candelaria (Patron of Canary Islands) was credited with moving the Canary Islands toward Christianity. Two Catholic saints were born in the Canary Islands: Peter of Saint Joseph de Betancur and Jos\u00e9 de Anchieta. Both born on the island of Tenerife, they were respectively missionaries in Guatemala and Brazil.\nThe Canary Islands are divided into two Catholic dioceses, each governed by a bishop:\nOther religions.\nSeparate from the overwhelming Christian majority are a minority of Muslims. Among the followers of Islam, the Islamic Federation of the Canary Islands exists to represent the Islamic community in the Canary Islands as well as to provide practical support to members of the Islamic community.\nOther religious faiths represented include Jehovah's Witnesses, The Church of Jesus Christ of Latter-day Saints as well as Hinduism. Minority religions are also present such as the Church of the Guanche People which is classified as a neo-pagan native religion. Also present are Buddhism, Judaism, Bah\u00e1\u02bc\u00ed, African religion, and Chinese religions.\nStatistics.\nThe distribution of beliefs in 2012 according to the CIS Barometer Autonomy was as follows:\nIslands.\nOrdered from west to east, the Canary Islands are El Hierro, La Palma, La Gomera, Tenerife, Gran Canaria, Fuerteventura, Lanzarote and La Graciosa. In addition, north of Lanzarote are the islets of Monta\u00f1a Clara, Alegranza, Roque del Este and Roque del Oeste, belonging to the Chinijo Archipelago, and northeast of Fuerteventura is the islet of Lobos. There are also a series of small adjacent rocks in the Canary Islands: the Roques de Anaga, Garachico and Fasnia in Tenerife, and those of Salmor and Bonanza in El Hierro.\nEl Hierro.\nEl Hierro, the westernmost island, covers , making it the second smallest of the major islands, and the least populous with 10,798 inhabitants. The whole island was declared Reserve of the Biosphere in 2000. Its capital is Valverde. Also known as Ferro, it was once believed to be the westernmost land in the world.\nFuerteventura.\nFuerteventura, with a surface of , is the second-most extensive island of the archipelago. It has been declared a Biosphere reserve by Unesco. It has a population of 113,275. Being also the most ancient of the islands, it is the one that is more eroded: its highest point is the Peak of the Bramble, at a height of . Its capital is Puerto del Rosario.\nGran Canaria.\nGran Canaria has 846,717 inhabitants. The capital, Las Palmas de Gran Canaria (377,203 inhabitants), is the most populous city and shares the status of capital of the Canaries with Santa Cruz de Tenerife. Gran Canaria's surface area is . In center of the island lie the Roque Nublo and Pico de las Nieves (\"Peak of Snow\") . In the south of island are the Maspalomas Dunes (Gran Canaria), these are the biggest tourist attractions.\nLa Gomera.\nLa Gomera has an area of and is the second least populous island with 21,136 inhabitants. Geologically it is one of the oldest of the archipelago. The insular capital is San Sebastian de La Gomera. Garajonay's National Park is located on the island.\nLanzarote.\nLanzarote is the easternmost island and one of the most ancient of the archipelago, and it has shown evidence of recent volcanic activity. It has a surface of , and a population of 149,183 inhabitants, including the adjacent islets of the Chinijo Archipelago. The capital is Arrecife, with 56,834 inhabitants.\nChinijo Archipelago.\nThe Chinijo Archipelago includes the islands La Graciosa, Alegranza, Monta\u00f1a Clara, Roque del Este and Roque del Oeste. It has a surface of , and a population of 658 inhabitants all of them on La Graciosa. With , La Graciosa, is the smallest inhabited island of the Canaries, and the major island of the Chinijo Archipelago.\nLa Palma.\nLa Palma, with 81,863 inhabitants covering an area of , is in its entirety a biosphere reserve. It shows no recent signs of volcanic activity, even though the volcano Tenegu\u00eda entered into eruption last in 1971. In addition, it is the second-highest island of the Canaries, with the Roque de los Muchachos as highest point. Santa Cruz de La Palma (known to those on the island as simply \"Santa Cruz\") is its capital.\nTenerife.\nTenerife is, with its area of , the most extensive island of the Canary Islands. In addition, with 904,713 inhabitants it is the most populated island of the archipelago and Spain. Two of the islands' principal cities are located on it: The capital, Santa Cruz de Tenerife and San Crist\u00f3bal de La Laguna (a World Heritage Site). San Crist\u00f3bal de La Laguna, the second city of the island is home to the oldest university in the Canary Islands, the University of La Laguna. Teide, with its is the highest peak of Spain and also a World Heritage Site. Tenerife is the site of the worst air disaster in the history of aviation, in which 583 people were killed in the collision of two Boeing 747s on 27 March 1977.\nLa Graciosa.\nGraciosa Island or commonly La Graciosa is a volcanic island in the Canary Islands of Spain, located 2\u00a0km (1.2\u00a0mi) north of the island of Lanzarote across the Strait of El R\u00edo. It was formed by the Canary hotspot. The island is part of the Chinijo Archipelago and the Chinijo Archipelago Natural Park (Parque Natural del Archipi\u00e9lago Chinijo). It is administrated by the municipality of Teguise. In 2018 La Graciosa officially became the eighth Canary Island. Before then, La Graciosa had the status of an islet, administratively dependent on the island of Lanzarote. It is the smallest and least populated of the main islands, with a population of about 700 people.\nEconomy and environment.\nThe economy is based primarily on tourism, which makes up 32% of the GDP. The Canaries receive about 12\u00a0million tourists per year. Construction makes up nearly 20% of the GDP and tropical agriculture, primarily bananas and tobacco, are grown for export to Europe and the Americas. Ecologists are concerned that the resources, especially in the more arid islands, are being overexploited but there are still many agricultural resources like tomatoes, potatoes, onions, cochineal, sugarcane, grapes, vines, dates, oranges, lemons, figs, wheat, barley, maize, apricots, peaches and almonds.\nWater resources are also being overexploited, due to the high water usage by tourists. Also, some islands (such as Gran Canaria and Tenerife) overexploit the ground water. This is done in such degree that, according to European and Spanish legal regulations, the current situation is not acceptable. To address the problems, good governance and a change in the water use paradigm have been proposed. These solutions depend largely on controlling water use and on demand management. As this is administratively difficult and politically unpalatable, most action is currently directed at increasing the public offer of water through import from outside; a decision which is economically, politically and environmentally questionable.\nTo bring in revenue for environmental protection, innovation, training and water sanitation a tourist tax was considered in 2018, along with a doubling of the ecotax and restrictions on holiday rents in the zones with the greatest pressure of demand.\nThe economy is \u20ac 25\u00a0billion (2001 GDP figures). The islands experienced continuous growth during a 20-year period, up until 2001, at a rate of approximately 5% annually. This growth was fueled mainly by huge amounts of foreign direct investment, mostly to develop tourism real estate (hotels and apartments), and European Funds (near \u20ac11\u00a0billion in the period from 2000 to 2007), since the Canary Islands are labelled Region Objective 1 (eligible for euro structural funds). Additionally, the EU allows the Canary Islands Government to offer special tax concessions for investors who incorporate under the Zona Especial Canaria (ZEC) regime and create more than five jobs.\nSpain gave permission in August 2014 for Repsol and its partners to explore oil and natural gas prospects off the Canary Islands, involving an investment of \u20ac7.5\u00a0billion over four years, to commence at the end of 2016. Repsol at the time said the area could ultimately produce 100,000 barrels of oil a day, which would meet 10 percent of Spain's energy needs. However, the analysis of samples obtained did not show the necessary volume nor quality to consider future extraction, and the project was scrapped.\nDespite currently having very high dependence on fossil fuels, research on the renewable energy potential concluded that a high potential for renewable energy technologies exists on the archipelago. This, in such extent even that a scenario pathway to 100% renewable energy supply by 2050 has been put forward.\nThe Canary Islands have great natural attractions, climate and beaches make the islands a major tourist destination, being visited each year by about 12\u00a0million people (11,986,059 in 2007, noting 29% of Britons, 22% of Spanish, not residents of the Canaries, and 21% of Germans). Among the islands, Tenerife has the largest number of tourists received annually, followed by Gran Canaria and Lanzarote. The archipelago's principal tourist attraction is the Teide National Park (in Tenerife) where the highest mountain in Spain and third largest volcano in the world (Mount Teide), receives over 2.8\u00a0million visitors annually.\nThe combination of high mountains, proximity to Europe, and clean air has made the Roque de los Muchachos peak (on La Palma island) a leading location for telescopes like the Grantecan.\nThe islands, as an autonomous region of Spain, are in the European Union and the Schengen Area. They are in the European Union Customs Union but outside the VAT area, Instead of VAT there is a local Sales Tax (IGIC) which has a general rate of 7%, an increased tax rate of 13.5%, a reduced tax rate of 3% and a zero tax rate for certain basic need products and services. Consequently, some products are subject to additional VAT if being exported from the islands into mainland Spain or the rest of the EU.\nCanarian time is Western European Time (WET) (or GMT; in summer one hour ahead of GMT). So Canarian time is one hour behind that of mainland Spain and the same as that of the UK, Ireland and mainland Portugal all year round.\nTourism statistics.\nThe number of tourists who visited the Canary Islands had been in 2018 16,150,054 and in the year 2019 15,589,290.\nTransport.\nThe Canary Islands have eight airports altogether, two of the main ports of Spain, and an extensive network of autopistas (highways) and other roads. For a road map see multimap. Traffic congestion is sometimes a problem in Tenerife and on Grand Canaria.\nThere are large ferry boats that link islands as well as fast ferries linking most of the islands. Both types can transport large numbers of passengers and cargo (including vehicles). Fast ferries are made of aluminium and powered by modern and efficient diesel engines, while conventional ferries have a steel hull and are powered by heavy oil. Fast ferries travel relatively quickly (in excess of ) and are a faster method of transportation than the conventional ferry (some ). A typical ferry ride between La Palma and Tenerife may take up to eight hours or more while a fast ferry takes about two and a half hours and between Tenerife and Gran Canaria can be about one hour.\nThe largest airport is the Gran Canaria Airport. Tenerife has two airports, Tenerife North Airport and Tenerife South Airport. The island of Tenerife gathers the highest passenger movement of all the Canary Islands through its two airports. The two main islands (Tenerife and Gran Canaria) receive the greatest number of passengers. Tenerife 6,204,499 passengers and Gran Canaria 5,011,176 passengers.\nThe port of Las Palmas is first in freight traffic in the islands, while the port of Santa Cruz de Tenerife is the first fishing port with approximately 7,500 tons of fish caught, according to the Spanish government publication Statistical Yearbook of State Ports. Similarly, it is the second port in Spain as regards ship traffic, only surpassed by the Port of Algeciras Bay. The port's facilities include a border inspection post (BIP) approved by the European Union, which is responsible for inspecting all types of imports from third countries or exports to countries outside the European Economic Area. The port of Los Cristianos (Tenerife) has the greatest number of passengers recorded in the Canary Islands, followed by the port of Santa Cruz de Tenerife. The Port of Las Palmas is the third port in the islands in passengers and first in number of vehicles transported.\nThe SS America was beached at the Canary islands on 18 January 1994. However, the ocean liner broke apart after the course of several years and eventually sank beneath the surface.\nRail transport.\nThe Tenerife Tram opened in 2007 and is currently the only one in the Canary Islands, travelling between the cities of Santa Cruz de Tenerife and San Crist\u00f3bal de La Laguna.\nThree more railway lines are being planned for the Canary Islands:\nHealth.\nThe \"Servicio Canario de Salud\" is an autonomous body of administrative nature attached to the Ministry responsible for Health of the Government of the Canary Islands. The majority of the archipelago's hospitals belong to this organization:\nWildlife.\nPrehistoric fauna.\nBefore the arrival of the Aborigines, the Canary Islands was inhabited by endemic animals, such as some extinct; giant lizards (\"Gallotia goliath\"), giant rats (\"Canariomys bravoi\" and \"Canariomys tamarani\") and giant tortoises (\"Geochelone burchardi\" and \"Geochelone vulcanica\"), among others.\nTerrestrial wildlife.\nWith a range of habitats, the Canary Islands exhibit diverse plant species. The bird life includes European and African species, such as the black-bellied sandgrouse; and a rich variety of endemic (local) taxa including the:\nTerrestrial fauna includes geckos, wall lizards, and three endemic species of recently rediscovered and critically endangered giant lizard: the El Hierro giant lizard (or Roque Chico de Salmor giant lizard), La Gomera giant lizard, and La Palma giant lizard. Mammals include the Canarian shrew, Canary big-eared bat, the Algerian hedgehog (which may have been introduced) and the more recently introduced mouflon. Some endemic mammals, the lava mouse, Tenerife giant rat and Gran Canaria giant rat, are extinct, as are the Canary Islands quail, long-legged bunting, the eastern Canary Islands chiffchaff and the giant prehistoric tortoises; Geochelone burchardi and Geochelone vulcanica.\nMarine life.\nThe marine life found in the Canary Islands is also varied, being a combination of North Atlantic, Mediterranean and endemic species. In recent years, the increasing popularity of both scuba diving and underwater photography have provided biologists with much new information on the marine life of the islands.\nFish species found in the islands include many species of shark, ray, moray eel, bream, jack, grunt, scorpionfish, triggerfish, grouper, goby, and blenny. In addition, there are many invertebrate species, including sponge, jellyfish, anemone, crab, mollusc, sea urchin, starfish, sea cucumber and coral.\nThere are a total of five different species of marine turtle that are sighted periodically in the islands, the most common of these being the endangered loggerhead sea turtle. The other four are the green sea turtle, hawksbill sea turtle, leatherback sea turtle and Kemp's ridley sea turtle. Currently, there are no signs that any of these species breed in the islands, and so those seen in the water are usually migrating. However, it is believed that some of these species may have bred in the islands in the past, and there are records of several sightings of leatherback sea turtle on beaches in Fuerteventura, adding credibility to the theory.\nMarine mammals include the large varieties of cetaceans including rare and not well-known species (see more details in the \"Marine life of the Canary Islands\"). Hooded seals have also been known to be vagrant in the Canary Islands every now and then. The Canary Islands were also formerly home to a population of the rarest pinniped in the world, the Mediterranean monk seal.\nHolidays.\nSome holidays of those celebrated in the Canary Islands are international and national, others are regional holidays and others are of insular character. The official day of the autonomous community is Canary Islands Day on 30 May. The anniversary of the first session of the Parliament of the Canary Islands, based in the city of Santa Cruz de Tenerife, held on 30 May 1983, is commemorated with this day.\nThe common festive calendar throughout the Canary Islands is as follows:\nIn addition, each of the islands has an island festival in which it is a holiday only on that island in question. These are the festivities of island patrons saints of each island. Organized chronologically are:\nThe most famous festivals of the Canary Islands is the carnival. It is the most famous and international festival of the archipelago. The carnival is celebrated in all the islands and all its municipalities, perhaps the two busiest are those of the two Canarian capitals; the Carnival of Santa Cruz de Tenerife (\"Tourist Festival of International Interest\") and the Carnival of Las Palmas de Gran Canaria. It is celebrated on the streets between the months of February and March. But the rest of the islands of the archipelago have their carnivals with their own traditions among which stand out: The Festival of the Carneros of El Hierro, the Festival of the Diabletes of Teguise in Lanzarote, Los Indianos de La Palma, the Carnival of San Sebasti\u00e1n de La Gomera and the Carnival of Puerto del Rosario in Fuerteventura.\nScience and technology.\nIn the 1960s, Gran Canaria was selected as the location for one of the 14 ground stations in the Manned Space Flight Network (MSFN) to support the NASA space program. Maspalomas Station, located in the south of the island, took part in a number of space missions including the Apollo 11 Moon landings and Skylab. Today it continues to support satellite communications as part of the ESA network.\nBecause of the remote location, a number of astronomical observatories are located in the archipelago, including the Teide Observatory on Tenerife, the Roque de los Muchachos Observatory on La Palma, and the Temisas Astronomical Observatory on Gran Canaria.\nSports.\nA unique form of wrestling known as Canarian wrestling (\"lucha canaria\") has opponents stand in a special area called a \"terrero\" and try to throw each other to the ground using strength and quick movements.\nAnother sport is the \"game of the sticks\" where opponents fence with long sticks. This may have come about from the shepherds of the islands who would challenge each other using their long walking sticks.\nFurthermore, there is the shepherd's jump (\"salto del pastor\"). This involves using a long stick to vault over an open area. This sport possibly evolved from the shepherd's need to occasionally get over an open area in the hills as they were tending their sheep.\nThe two main football teams in the archipelago are: the CD Tenerife (founded in 1912) and UD Las Palmas (founded in 1949). As of the 2018/2019 season, both Tenerife and Las Palmas play in Liga Adelante. When in the same division, the clubs contest the Canary Islands derby. There are smaller clubs also playing in the mainland Spanish football league system, most notably UD Lanzarote and CD Laguna, although no other Canarian clubs have played in the top flight.\nThe mountainous terrain of the Canary Islands also caters to the growing popularity of ultra running and ultramarathons as host of annual competitive long-distance events including CajaMar Tenerife Bluetrail on Tenerife, Transvulcania on La Palma, Transgrancanaria on Gran Canaria, and the Half Marathon des Sables on Fuerteventura. A yearly Ironman Triathlon has been taking place on Lanzarote since 1992."}
{"id": "5718", "revid": "1009177917", "url": "https://en.wikipedia.org/wiki?curid=5718", "title": "Chuck D", "text": "Carlton Douglas Ridenhour (born August 1, 1960), known professionally as Chuck D, is an American rapper, author, and producer. As the leader of the rap group Public Enemy, which he co-founded in 1985 with Flavor Flav, Chuck D helped create politically and socially conscious hip hop music in the mid-1980s. \"The Source\" ranked him at No. 12 on their list of the Top 50 Hip-Hop Lyricists of All Time.\nEarly life.\nRidenhour was born in Queens, New York. He began writing rhymes after the New York City blackout of 1977. After attending W. Tresper Clarke High School, he went to Adelphi University on Long Island to study graphic design, where he met William Drayton (Flavor Flav). He received a B.F.A. from Adelphi in 1984 and later received an honorary doctorate from Adelphi in 2013.\nWhile at Adelphi, Ridenhour co-hosted hip hop radio show the \"Super Spectrum Mix Hour\" as Chuck D on Saturday nights at Long Island rock radio station WLIR, designed flyers for local hip-hop events, and drew a cartoon called \"Tales of the Skind\" for Adelphi student newspaper \"The Delphian\".\nCareer.\nRidenhour (using the nickname Chuck D) formed Public Enemy in 1985 with Flavor Flav. Upon hearing Ridenhour's demo track \"Public Enemy Number One\", fledgling producer/upcoming music-mogul Rick Rubin insisted on signing him to his Def Jam label. Their major label releases were \"Yo! Bum Rush the Show\" (1987), \"It Takes a Nation of Millions to Hold Us Back\" (1988), \"Fear of a Black Planet\" (1990), \"Apocalypse 91... The Enemy Strikes Black\" (1991), the compilation album \"Greatest Misses\" (1992), and \"Muse Sick-n-Hour Mess Age\" (1994). They also released a full-length album soundtrack for the film \"He Got Game\" in 1998.\nRidenhour also contributed (as Chuck D) to several episodes of the PBS documentary series \"The Blues\". He has appeared as a featured artist on many other songs and albums, having collaborated with artists such as Janet Jackson, Kool Moe Dee, The Dope Poet Society, Run\u2013D.M.C., Ice Cube, Boom Boom Satellites, Rage Against the Machine, Anthrax, John Mellencamp and many others. In 1990, he appeared on \"Kool Thing\", a song by the alternative rock band Sonic Youth, and along with Flavor Flav, he sang on George Clinton's song \"Tweakin'\", which appears on his 1989 album \"The Cinderella Theory\". In 1993, he executive produced \"Got 'Em Running Scared\", an album by Ichiban Records group Chief Groovy Loo and the Chosen Tribe.\nLater career.\nIn 1996, Ridenhour released \"Autobiography of Mistachuck\" on Mercury Records. Chuck D made a rare appearance at the 1998 MTV Video Music Awards, presenting the Video Vanguard Award to the Beastie Boys, whilst commending their musicianship. In November 1998, he settled out of court with Christopher \"The Notorious B.I.G.\" Wallace's estate over the latter's sampling of his voice in the song \"Ten Crack Commandments\". The specific sampling is Ridenhour counting off the numbers one to nine on the track \"Shut 'Em Down\". He later described the decision to sue as \"stupid\".\nIn September 1999, he launched a multi-format \"supersite\" on the web site Rapstation.com. The site includes a TV and radio station with original programming, prominent hip hop DJs, celebrity interviews, free MP3 downloads (the first was contributed by multi-platinum rapper Coolio), downloadable ringtones by ToneThis, social commentary, current events, and regular features on turning rap careers into a viable living. Since 2000, he has been one of the most vocal supporters of peer-to-peer file sharing in the music industry.\nHe loaned his voice to ' as DJ Forth Right MC for the radio station Playback FM. In 2000, he collaborated with Public Enemy's Gary G-Whiz and MC Lyte on the theme music to the television show \"Dark Angel\". He appeared with Henry Rollins in a cover of Black Flag's \"Rise Above\" for the album '. In 2003, he was featured in the PBS documentary in which he recorded a version of Muddy Waters' song \"Mannish Boy\" with Common, Electrik Mud Cats, and Kyle Jason. He was also featured on Z-Trip's album \"Shifting Gears\" on a track called \"Shock and Awe\"; a 12-inch of the track was released featuring artwork by Shepard Fairey. In 2008 he contributed a chapter to \"Sound Unbound: Sampling Digital Music and Culture\" (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky, and also turned up on The Go! Team's album \"Proof of Youth\" on the track \"Flashlight Fight.\" He also fulfilled his childhood dreams of being a sports announcer by performing the play-by-play commentary in the video game \"\" on Xbox 360 and PlayStation 3.\nIn 2009, Ridenhour wrote the foreword to the book \"The Love Ethic: The Reason Why You Can't Find and Keep Beautiful Black Love\" by Kamau and Akilah Butler. He also appeared on Brother Ali's album, \"Us\".\nIn March 2011, Chuck D re-recorded vocals with The Dillinger Escape Plan for a cover of \"Fight the Power\".\nChuck D duetted with Rock singer Meat Loaf on his 2011 album \"Hell in a Handbasket\" on the song \"Mad Mad World/The Good God Is a Woman and She Don't Like Ugly\".\nIn 2016 Chuck D joined the band Prophets of Rage along with B-Real and former members of Rage Against the Machine.\nIn July 2019, Ridenhour sued Terrordome Music Publishing and Reach Music Publishing for $1 million for withholding royalties.\nRapping technique and creative process.\nChuck D is known for his powerful rapping. \"How to Rap\" says he \"has a powerful, resonant voice that is often acclaimed as one of the most distinct and impressive in hip-hop\". Chuck says this was based on listening to Melle Mel and sportscasters such as Marv Albert.\nChuck often comes up with a title for a song first. He writes on paper, though sometimes edits using a computer. He prefers to not punch in or overdub vocals.\nChuck listed his favourite rap albums in \"Hip Hop Connection\": 10. N.W.A, \"Straight Outta Compton\" 9. Boogie Down Productions, \"Criminal Minded\" 8. Run-DMC, \"Tougher Than Leather\" 7. Big Daddy Kane, \"Looks Like a Job For...\" 6. Stetsasonic, \"In Full Gear\" 5. Ice Cube, \"AmeriKKKa's Most Wanted\" 4. Dr. Dre, \"The Chronic\" 3. De La Soul, \"3 Feet High and Rising\" 2. Eric B. &amp; Rakim, \"Follow the Leader\" 1. Run-DMC, \"Raising Hell\" (\"It was the first record that made me realise this was an album-oriented genre\")\nPolitics.\nChuck D identifies as Black, as opposed to African or African-American. In a 1993 issue of DIRT Magazine covering a taping of \"In the Mix\" hosted by Alimi Ballard at the Apollo, Dan Field writes, At one point, Chuck bristles a bit at the term \"African-American.\" He thinks of himself as Black and sees nothing wrong with the term. Besides, he says, having been born in the United States and lived his whole life here, he doesn't consider himself African. Being in Public Enemy has given him the chance to travel around the world, an experience that really opened his eyes and his mind. He says visiting Africa and experiencing life on a continent where the majority of people are Black gave him a new perspective and helped him get in touch with his own history. He also credits a trip to the ancient Egyptian pyramids at Giza with helping him appreciate the relative smallness of man.\nRidenhour is politically active; he co-hosted \"Unfiltered\" on Air America Radio, testified before Congress in support of peer-to-peer MP3 sharing, and was involved in a 2004 rap political convention. He has continued to be an activist, publisher, lecturer, and producer.\nAddressing the negative views associated with rap music, he co-wrote the essay book \"Fight the Power: Rap, Race, and Reality\" with Yusuf Jah. He argues that \"music and art and culture is escapism, and escapism sometimes is healthy for people to get away from reality\", but sometimes the distinction is blurred and that's when \"things could lead a young mind in a direction.\" He also founded the record company Slam Jamz and acted as narrator in Kareem Adouard's short film \"Bling: Consequences and Repercussions\", which examines the role of conflict diamonds in bling fashion. Despite Chuck D and Public Enemy's success, Chuck D claims that popularity or public approval was never a driving motivation behind their work. He is admittedly skeptical of celebrity status, revealing in a 1999 interview with \"BOMB Magazine\" that, \"The key for the record companies is to just keep making more and more stars, and make the ones who actually challenge our way of life irrelevant. The creation of celebrity has clouded the minds of most people in America, Europe and Asia. It gets people off the path they need to be on as individuals.\"\nIn an interview with \"Le Monde\" published January 29, 2008, Chuck D stated that rap is devolving so much into a commercial enterprise, that the relationship between the rapper and the record label is that of slave to a master. He believes that nothing has changed for African-Americans since the debut of Public Enemy and, although he thinks that an Obama-Clinton alliance is great, he does not feel that the establishment will allow anything of substance to be accomplished. He stated that French President Nicolas Sarkozy is like any other European elite: he has profited through the murder, rape, and pillaging of those less fortunate and he refuses to allow equal opportunity for those men and women from Africa. In this article, he defended a comment made by Professor Griff in the past that he says was taken out of context by the media. The real statement was a critique of the Israeli government and its treatment of the Palestinian people. Chuck D stated that it is Public Enemy's belief that all human beings are equal.\nIn an interview with the magazine \"N'Digo\" published in June 2008, he spoke of today's mainstream urban music seemingly relishing the addictive euphoria of materialism and sexism, perhaps being the primary cause of many people harboring resentment towards the genre and its future. However, he has expressed hope for its resurrection, saying \"It's only going to be dead if it doesn't talk about the messages of life as much as the messages of death and non-movement\", citing artists such as NYOil, M.I.A. and The Roots as socially conscious artists who push the envelope creatively. \"A lot of cats are out there doing it, on the Web and all over. They're just not placing their career in the hands of some major corporation.\"\nIn 2010, Chuck D released a track, \"Tear Down That Wall.\" He said, \"I talked about the wall not only just dividing the U.S. and Mexico but the states of California, New Mexico and Texas. But Arizona, it's like, come on. Now they're going to enforce a law that talks about basically racial profiling.\"\nHe is on the board of the TransAfrica Forum, a Pan African organization that is focused on African, Caribbean and Latin American issues.\nHe has been an activist with projects of The Revcoms, such as Refuse Fascism and Stop Mass Incarceration Network. Carl Dix interviewed Chuck D on The Revcoms' YouTube program \"The RNL \u2013 Revolution, Nothing Less! \u2013 Show\".\nPersonal life.\nChuck D lives in California, and lost his home in the Thomas Fire of December 2017-January 2018.\nDiscography.\nwith Public Enemy.\nStudio albums\nwith Confrontation Camp.\nStudio albums\nwith Prophets of Rage.\nStudio albums\nStudio EPs\nSolo.\nStudio albums\nCompilation albums"}
{"id": "5719", "revid": "1544984", "url": "https://en.wikipedia.org/wiki?curid=5719", "title": "Cutaway (filmmaking)", "text": "In film and video, a cutaway is the interruption of a continuously filmed action by inserting a view of something else. It is usually followed by a cut back to the first shot. A cutaway scene is the interruption of a scene with the insertion of another scene, generally unrelated or only peripherally related to the original scene. The interruption is usually quick, and is usually, although not always, ended by a return to the original scene. The effect is of commentary to the original scene, frequently comic in nature.\nUsage.\nThe most common use of cutaway shots in dramatic films is to adjust the pace of the main action, to conceal the deletion of some unwanted part of the main shot, or to allow the joining of parts of two versions of that shot. For example, a scene may be improved by cutting a few frames out of an actor's pause; a brief view of a listener can help conceal the break. Or the actor may fumble some of his lines in a group shot; rather than discarding a good version of the shot, the director may just have the actor repeat the lines for a new shot, and cut to that alternate view when necessary.\nCutaways are also used often in older horror films in place of special effects. For example, a shot of a zombie getting its head cut off may, for instance, start with a view of an axe being swung through the air, followed by a close-up of the actor swinging it, then followed by a cut back to the now severed head. George A. Romero, creator of the Dead Series, and Tom Savini pioneered effects that removed the need for cutaways in horror films. \"30 Rock\" would often use cutaway scenes to create visual humor, the \"Werewolf Bar Mitzvah\" scene taking three days to create for only five seconds of screen time. \nIn news broadcasting and documentary work, the cutaway is used much as it would be in fiction. On location, there is usually just one camera to film an interview, and it's usually trained on the interviewee. Often there is also only one microphone. After the interview, the interviewer will usually repeat his questions while he himself is being filmed, with pauses as they act as if to listen to the answers. These shots can be used as cutaways. Cutaways to the interviewer, called noddies, can also be used to cover cuts.\nThe cutaway does not necessarily contribute any dramatic content of its own, but is used to help the editor assemble a longer sequence. For this reason, editors choose cutaways related to the main action, such as another action or object in the same location. For example, if the main shot is of a man walking down an alley, possible cutaways may include a shot of a cat on a nearby dumpster or a shot of a person watching from a window overhead."}
{"id": "5721", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5721", "title": "Coma", "text": "A coma is a deep state of prolonged unconsciousness in which a person cannot be awakened, fails to respond normally to painful stimuli, light, or sound, lacks a normal wake-sleep cycle and does not initiate voluntary actions. Coma patients exhibit a complete absence of wakefulness and are unable to consciously feel, speak or move. Comas can be derived by natural causes, or can be medically induced.\nClinically, a coma can be defined as the inability consistently to follow a one-step command. It can also be defined as a score of \u2264 8 on the Glasgow Coma Scale (GCS) lasting \u2265 6 hours. For a patient to maintain consciousness, the components of \"wakefulness\" and \"awareness\" must be maintained. Wakefulness describes the quantitative degree of consciousness, whereas awareness relates to the qualitative aspects of the functions mediated by the cortex, including cognitive abilities such as attention, sensory perception, explicit memory, language, the execution of tasks, temporal and spatial orientation and reality judgment. From a neurological perspective, consciousness is maintained by the activation of the cerebral cortex\u2014the gray matter that forms the outer layer of the brain and by the reticular activating system (RAS), a structure located within the brainstem.\nEtymology.\nThe term 'coma', from the Greek \"koma\", meaning deep sleep, had already been used in the Hippocratic corpus (\"Epidemica\") and later by Galen (second century AD). Subsequently, it was hardly used in the known literature up to the middle of the 17th century. The term is found again in Thomas Willis' (1621\u20131675) influential \"De anima brutorum\" (1672), where lethargy (pathological sleep), 'coma' (heavy sleeping), \"carus\" (deprivation of the senses) and apoplexy (into which \"carus\" could turn and which he localized in the white matter) are mentioned. The term \"carus\" is also derived from Greek, where it can be found in the roots of several words meaning soporific or sleepy. It can still be found in the root of the term 'carotid'. Thomas Sydenham (1624\u201389) mentioned the term 'coma' in several cases of fever (Sydenham, 1685).\nSigns and symptoms.\nGeneral symptoms of a person in a comatose state are:\nCauses.\nMany types of problems can cause a coma. Forty percent of comatose states result from drug poisoning. Certain drug use under certain conditions can damage or weaken the synaptic functioning in the ascending reticular activating system (ARAS) and keep the system from properly functioning to arouse the brain. Secondary effects of drugs, which include abnormal heart rate and blood pressure, as well as abnormal breathing and sweating, may also indirectly harm the functioning of the ARAS and lead to a coma. Given that drug poisoning is the cause for a large portion of patients in a coma, hospitals first test all comatose patients by observing pupil size and eye movement, through the vestibular-ocular reflex. (See \"Diagnosis\" below.)\nThe second most common cause of coma, which makes up about 25% of cases, is lack of oxygen, generally resulting from cardiac arrest. The Central Nervous System (CNS) requires a great deal of oxygen for its neurons. Oxygen deprivation in the brain, also known as hypoxia, causes sodium and calcium from outside of the neurons to decrease and intracellular calcium to increase, which harms neuron communication. Lack of oxygen in the brain also causes ATP exhaustion and cellular breakdown from cytoskeleton damage and nitric oxide production.\nTwenty percent of comatose states result from the side effects of a stroke. During a stroke, blood flow to part of the brain is restricted or blocked. An ischemic stroke, brain hemorrhage, or tumor may cause restriction of blood flow. Lack of blood to cells in the brain prevents oxygen from getting to the neurons, and consequently causes cells to become disrupted and die. As brain cells die, brain tissue continues to deteriorate, which may affect the functioning of the ARAS.\nThe remaining 15% of comatose cases result from trauma, excessive blood loss, malnutrition, hypothermia, hyperthermia, abnormal glucose levels, and many other biological disorders. Furthermore, studies show that 1 out of 8 patients with traumatic brain injury experience a comatose state.\nPathophysiology.\nInjury to either or both of the cerebral cortex or the reticular activating system (RAS) is sufficient to cause a person to enter coma.\nThe cerebral cortex is the outer layer of neural tissue of the cerebrum of the brain. The cerebral cortex is composed of gray matter which consists of the nuclei of neurons, whereas the inner portion of the cerebrum is composed of white matter and is composed of the axons of neuron. White matter is responsible for perception, relay of the sensory input via the thalamic pathway, and many other neurological functions, including complex thinking.\nThe RAS, on the other hand, is a more primitive structure in the brainstem which includes the reticular formation (RF). The RAS has two tracts, the ascending and descending tract. The ascending track, or ascending reticular activating system (ARAS), is made up of a system of acetylcholine-producing neurons, and works to arouse and wake up the brain. Arousal of the brain begins from the RF, through the thalamus, and then finally to the cerebral cortex. Any impairment in ARAS functioning, a neuronal dysfunction, along the arousal pathway stated directly above, prevents the body from being aware of its surroundings. Without the arousal and consciousness centers, the body cannot awaken, remaining in a comatose state.\nThe severity and mode of onset of coma depends on the underlying cause. There are two main subdivisions of a coma: structural and diffuse neuronal. A structural cause, for example, is brought upon by a mechanical force that brings about cellular damage, such as physical pressure or a blockage in neural transmission. While a diffuse cause is limited to aberrations of cellular function, that fall under a metabolic or toxic subgroup. Toxin-induced comas are caused by extrinsic substances, whereas metabolic-induced comas are caused by intrinsic processes, such as body thermoregulation or ionic imbalances(e.g. sodium). For instance, severe hypoglycemia (low blood sugar) or hypercapnia (increased carbon dioxide levels in the blood) are examples of a metabolic diffuse neuronal dysfunction. Hypoglycemia or hypercapnia initially cause mild agitation and confusion, but progress to obtundation, stupor, and finally, complete unconsciousness. In contrast, coma resulting from a severe traumatic brain injury or subarachnoid hemorrhage can be instantaneous. The mode of onset may therefore be indicative of the underlying cause.\nStructural and diffuse causes of coma are not isolated from one another, as one can lead to the other in some situations. For instance, coma induced by a diffuse metabolic process, such as hypoglycemia, can result in a structural coma if it is not resolved. Another example is if cerebral edema, a diffuse dysfunction, leads to ischemia of the brainstem, a structural issue, due to the blockage of the circulation in the brain.\nDiagnosis.\nAlthough diagnosis of coma is simple, investigating the underlying cause of onset can be rather challenging. As such, after gaining stabilization of the patient's airways, breathing and circulation (the basic ABCs) various diagnostic tests, such as physical examinations and imaging tools (CT scan, MRI, etc.) are employed to access the underlying cause of the coma.\nWhen an unconscious person enters a hospital, the hospital utilizes a series of diagnostic steps to identify the cause of unconsciousness. According to Young, the following steps should be taken when dealing with a patient possibly in a coma:\nInitial evaluation.\nIn the initial assessment of coma, it is common to gauge the level of consciousness on the AVPU (alert, vocal stimuli, painful stimuli, unresponsive) scale by spontaneously exhibiting actions and, assessing the patient's response to vocal and painful stimuli. More elaborate scales, such as the Glasgow Coma Scale, quantify an individual's reactions such as eye opening, movement and verbal response in order to indicate their extent of brain injury. The patient's score can vary from a score of 3 (indicating severe brain injury and death) to 15 (indicating mild or no brain injury).\nIn those with deep unconsciousness, there is a risk of asphyxiation as the control over the muscles in the face and throat is diminished. As a result, those presenting to a hospital with coma are typically assessed for this risk (\"airway management\"). If the risk of asphyxiation is deemed high, doctors may use various devices (such as an oropharyngeal airway, nasopharyngeal airway or endotracheal tube) to safeguard the airway.\nImaging and testing.\nImaging basically encompasses computed tomography (CAT or CT) scan of the brain, or MRI for example, and is performed to identify specific causes of the coma, such as hemorrhage in the brain or herniation of the brain structures. Special tests such as an EEG can also show a lot about the activity level of the cortex such as semantic processing, presence of seizures, and are important available tools not only for the assessment of the cortical activity but also for predicting the likelihood of the patient's awakening. The autonomous responses such as the skin conductance response may also provide further insight on the patient's emotional processing.\nIn the treatment of traumatic brain injury (TBI), there are 4 examination methods that have proved useful: skull x-ray, angiography, computed tomography (CT), and magnetic resonance imaging (MRI). The skull x-ray can detect linear fractures, impression fractures (expression fractures) and burst fractures. Angiography is used on rare occasions for TBIs i.e. when there is suspicion of an aneurysm, carotid sinus fistula, traumatic vascular occlusion, and vascular dissection. A CT can detect changes in density between the brain tissue and hemorrhages like subdural and intracerebral hemorrhages. MRIs are not the first choice in emergencies because of the long scanning times and because fractures cannot be detected as well as CT. MRIs are used for the imaging of soft tissues and lesions in the posterior fossa which cannot be found with the use of CT.\nBody movements.\nAssessment of the brainstem and cortical function through special reflex tests such as the oculocephalic reflex test (doll's eyes test), oculovestibular reflex test (cold caloric test), corneal reflex, and the gag reflex. Reflexes are a good indicator of what cranial nerves are still intact and functioning and is an important part of the physical exam. Due to the unconscious status of the patient, only a limited number of the nerves can be assessed. These include the cranial nerves number 2 (CN II), number 3 (CN III), number 5 (CN V), number 7 (CN VII), and cranial nerves 9 and 10 (CN IX, CN X).\nAssessment of posture and physique is the next step. It involves general observation about the patient's positioning. There are often two stereotypical postures seen in comatose patients. Decorticate posturing is a stereotypical posturing in which the patient has arms flexed at the elbow, and arms adducted toward the body, with both legs extended. Decerebrate posturing is a stereotypical posturing in which the legs are similarly extended (stretched), but the arms are also stretched (extended at the elbow). The posturing is critical since it indicates where the damage is in the central nervous system. A decorticate posturing indicates a lesion (a point of damage) at or above the red nucleus, whereas a decerebrate posturing indicates a lesion at or below the red nucleus. In other words, a decorticate lesion is closer to the cortex, as opposed to a decerebrate posturing which indicates that the lesion is closer to the brainstem.\nPupil size.\nPupil assessment is often a critical portion of a comatose examination, as it can give information as to the cause of the coma; the following table is a technical, medical guideline for common pupil findings and their possible interpretations:\nSeverity.\nA coma can be classified as (1) supratentorial (above Tentorium cerebelli), (2) infratentorial (below Tentorium cerebelli), (3) metabolic or (4) diffused. This classification is merely dependent on the position of the original damage that caused the coma, and does not correlate with severity or the prognosis. The severity of coma impairment however is categorized into several levels. Patients may or may not progress through these levels. In the first level, the brain responsiveness lessens, normal reflexes are lost, the patient no longer responds to pain and cannot hear.\nThe Rancho Los Amigos Scale is a complex scale that has eight separate levels, and is often used in the first few weeks or months of coma while the patient is under closer observation, and when shifts between levels are more frequent.\nTreatment.\nTreatment for people in a coma will depend on the severity and cause of the comatose state. Upon admittance to an emergency department, coma patients will usually be placed in an Intensive Care Unit (ICU) immediately, where maintenance of the patient's respiration and circulation become a first priority. Stability of their respiration and circulation is sustained through the use of intubation, ventilation, administration of intravenous fluids or blood and other supportive care as needed.\nContinued care.\nOnce a patient is stable and no longer in immediate danger, there may be a shift of priority from stabilizing the patient to maintaining the state of their physical wellbeing. Moving patients every 2\u20133 hours by turning them side to side is crucial to avoiding bed sores as a result of being confined to a bed. Moving patients through the use of physical therapy also aids in preventing atelectasis, contractures or other orthopedic deformities which would interfere with a coma patient's recovery.\nPneumonia is also common in coma patients due to their inability to swallow which can then lead to aspiration. A coma patient's lack of a gag reflex and use of a feeding tube can result in food, drink or other solid organic matter being lodged within their lower respiratory tract (from the trachea to the lungs). This trapping of matter in their lower respiratory tract can ultimately lead to infection, resulting in aspiration pneumonia.\nComa patients may also deal with restlessness or seizures. As such, soft cloth restraints may be used to prevent them from pulling on tubes or dressings and side rails on the bed should be kept up to prevent patients from falling.\nCaregivers.\nComa has a wide variety of emotional reactions from the family members of the affected patients, as well as the primary care givers taking care of the patients. Research has shown that the severity of injury causing coma was found to have no significant impact compared to how much time has passed since the injury occurred. Common reactions, such as desperation, anger, frustration, and denial are possible. The focus of the patient care should be on creating an amicable relationship with the family members or dependents of a comatose patient as well as creating a rapport with the medical staff. Although there is heavy importance of a primary care taker, secondary care takers can play a supporting role to temporarily relieve the primary care taker's burden of tasks.\nPrognosis.\nComas can last from several days to several weeks. In more severe cases a coma may last for over five weeks, while some have lasted as long as several years. After this time, some patients gradually come out of the coma, some progress to a vegetative state, and others die. Some patients who have entered a vegetative state go on to regain a degree of awareness and in some cases, may remain in vegetative state for years or even decades (the longest recorded period being 42 years).\nPredicted chances of recovery will differ depending on which techniques were used to measure the patient's severity of neurological damage. Predictions of recovery are based on statistical rates, expressed as the level of chance the person has of recovering. Time is the best general predictor of a chance of recovery. For example, after four months of coma caused by brain damage, the chance of partial recovery is less than 15%, and the chance of full recovery is very low.\nThe outcome for coma and vegetative state depends on the cause, location, severity and extent of neurological damage. A deeper coma alone does not necessarily mean a slimmer chance of recovery, similarly, milder comas do not ensure higher chances of recovery. The most common cause of death for a person in a vegetative state is secondary infection such as pneumonia, which can occur in patients who lie still for extended periods.\nRecovery.\nPeople may emerge from a coma with a combination of physical, intellectual, and psychological difficulties that need special attention. It is common for coma patients to awaken in a profound state of confusion and suffer from dysarthria, the inability to articulate any speech. Recovery usually occurs gradually. In the first days, patients may only awaken for a few minutes, with increased duration of wakefulness as their recovery progresses and may eventually recover full awareness. That said, some patients may never progress beyond very basic responses.\nThere are reports of people coming out of a coma after long periods of time. After 19 years in a minimally conscious state, Terry Wallis spontaneously began speaking and regained awareness of his surroundings.\nA brain-damaged man, trapped in a coma-like state for six years, was brought back to consciousness in 2003 by doctors who planted electrodes deep inside his brain. The method, called deep brain stimulation (DBS) successfully roused communication, complex movement and eating ability in the 38-year-old American man who suffered a traumatic brain injury. His injuries left him in a minimally conscious state (MCS), a condition akin to a coma but characterized by occasional, but brief, evidence of environmental and self-awareness that coma patients lack.\nSociety and culture.\nResearch by Dr. Eelco Wijdicks on the depiction of comas in movies was published in Neurology in May 2006. Dr. Wijdicks studied 30 films (made between 1970 and 2004) that portrayed actors in prolonged comas, and he concluded that only two films accurately depicted the state of a coma victim and the agony of waiting for a patient to awaken: \"Reversal of Fortune\" (1990) and \"The Dreamlife of Angels\" (1998). The remaining 28 were criticized for portraying miraculous awakenings with no lasting side effects, unrealistic depictions of treatments and equipment required, and comatose patients remaining muscular and tanned.\nBioethics.\nA person in a coma is said to be in an unconscious state. Perspectives on personhood, identity and consciousness come into play when discussing the metaphysical and bioethical views on comas.\nIt has been argued that unawareness should be just as ethically relevant and important as a state of awareness and that there should be metaphysical support of unawareness as a state.\nIn the ethical discussions about disorders of consciousness (DOCs), two abilities are usually considered as central: \"experiencing well-being and having interests.\" Well-being can broadly be understood as the positive effect related to what makes life good (according to specific standards) for the individual in question. The only condition for well-being broadly considered is the ability to experience its \u2018positiveness\u2019. That said, because experiencing positiveness is a basic emotional process with phylogenetic roots, it is likely to occur at a completely unaware level and therefore, introduces the idea of an unconscious well-being. As such, the ability of having interests, is crucial for describing two abilities which those with comas are deficient in. Having an interest in a certain domain can be understood as having a stake in something that can affect what makes our life good in that domain. An interest is what directly and immediately improves life from a certain point of view or within a particular domain, or greatly increases the likelihood of life improvement enabling the subject to realize some good. That said, sensitivity to reward signals is a fundamental element in the learning process, both consciously and unconsciously. Moreover, the unconscious brain is able to interact with its surroundings in a meaningful way and to produce meaningful information processing of stimuli coming from the external environment, including other people.\nAccording to Hawkins, \"1. A life is good if the subject is able to value, or more basically if the subject is able to care. Importantly, Hawkins stresses that caring has no need for cognitive commitment, i.e. for high-level cognitive activities: it requires being able to distinguish something, track it for a while, recognize it over time, and have certain emotional dispositions \"vis-\u00e0-vis\" something. 2. A life is good if the subject has the capacity for relationship with others, i.e. for meaningfully interacting with other people.\" This suggests that unawareness may (at least partly) fulfill both conditions identified by Hawkins for life to be good for a subject, thus making the unconscious ethically relevant."}
{"id": "5722", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5722", "title": "Call of Cthulhu (role-playing game)", "text": "Call of Cthulhu is a horror fiction role-playing game based on H. P. Lovecraft's story of the same name and the associated Cthulhu Mythos. The game, often abbreviated as \"CoC\", is published by Chaosium; it was first released in 1981 and is currently in its seventh edition, with licensed foreign language editions available as well. Its game system is based on Chaosium's Basic Role-Playing (BRP), with additions for the horror genre. These include special rules for sanity and luck among others.\nGameplay.\nSetting.\nThe setting of \"Call of Cthulhu\" is a darker version of our world, based on H. P. Lovecraft's observation (from his essay, \"Supernatural Horror in Literature\") that \"The oldest and strongest emotion of mankind is fear, and the strongest kind of fear is fear of the unknown.\" The original game, first published in 1981, uses Basic Role-Playing as its basis, and is set in the 1920s, the setting of many of Lovecraft's stories. Additional settings were developed in the \"Cthulhu by Gaslight\" supplement, a blend of occult and Holmesian mystery and mostly set in England during the 1890s, and modern/1980s conspiracy with \"Cthulhu Now\" and \"Delta Green\". More recent additions include 1000 AD (\"Cthulhu: Dark Ages\"), 23rd century (\"Cthulhu Rising\") and Ancient Roman times (\"Cthulhu Invictus\"). The protagonists may also travel to places that are not of this earth, represented in the Dreamlands (which can be accessed through dreams as well as being physically connected to the earth), to other planets, or into the voids of space. In keeping with the Lovecraftian theme, the gamemaster is called the Keeper of Arcane Lore, or simply the keeper, while player characters are called \"investigators\".\nWhile predominantly focused on Lovecraftian fiction and horror, playing in the Cthulhu Mythos is not required. The system also includes ideas for non-Lovecraft games, such as utilizing folk horror or the settings of other authors and horror movies, or with entirely custom settings and creatures by the gamemaster and/or players. Additionally, though the early 1900s serve as the default era of play, rules and statistics are given for setting games in modern or futuristic time periods, Lovecraftian or otherwise.\nMechanics.\n\"CoC\" uses the Basic Role-Playing system first developed for \"RuneQuest\" and used in other Chaosium games. It is skill-based, with player characters getting better with their skills by succeeding at using them for as long as they stay functionally healthy and sane. They do not, however, gain hit points and do not become significantly harder to kill. The game does not use levels.\n\"CoC\" uses percentile dice (with a results ranging from 1 to 100) to determine success or failure. Every player statistic is intended to be compatible with the notion that there is a probability of success for a particular action given what the player is capable of doing. For example, an artist may have a 75% chance of being able to draw something (represented by having 75 in Art skill), and thus rolling a number under 75 would yield a success. Rolling or less of the skill level (1-15 in the example) would be a \"special success\" (or an \"impale\" for combat skills) and would yield some extra bonus to be determined by the keeper. For example, the artist character might draw especially well or especially fast, or catch some unapparent detail in the drawing.\nThe players take the roles of ordinary people drawn into the realm of the mysterious: detectives, criminals, scholars, artists, war veterans, etc. Often, happenings begin innocently enough, until more and more of the workings behind the scenes are revealed. As the characters learn more of the true horrors of the world and the irrelevance of humanity, their sanity (represented by \"Sanity Points\", abbreviated SAN) inevitably withers away. The game includes a mechanism for determining how damaged a character's sanity is at any given point; encountering the horrific beings usually triggers a loss of SAN points. To gain the tools they need to defeat the horrors \u2013 mystic knowledge and magic \u2013 the characters may end up losing some of their sanity, though other means such as pure firepower or simply outsmarting one's opponents also exist. \"CoC\" has a reputation as a game in which it is quite common for a player character to die in gruesome circumstances or end up in a mental institution. Eventual triumph of the players is not guaranteed.\nHistory.\nThe original conception of \"Call of Cthulhu\" was \"Dark Worlds\", a game commissioned by the publisher Chaosium but never published. Sandy Petersen contacted them regarding writing a supplement for their popular fantasy game \"RuneQuest\" set in Lovecraft's Dreamlands. He took over the writing of \"Call of Cthulhu\", and the game was released in 1981. Petersen oversaw the first four editions with only minor changes to the system. Once he left, development was continued by Lynn Willis, who was credited as co-author in the fifth and sixth editions. After the death of Willis, Mike Mason became \"Call of Cthulhu\" line editor in 2013, continuing its development with Paul Fricker. Together they made the most significant rules alterations than in any previous edition, culminating in the release of the 7th edition in 2014.\nEarly releases.\nFor those grounded in the RPG tradition, the very first release of \"Call of Cthulhu\" created a brand new framework for table-top gaming. Rather than the traditional format established by \"Dungeons &amp; Dragons\", which often involved the characters wandering through caves or tunnels and fighting different types of monsters, Sandy Petersen introduced the concept of the \"Onion Skin\": Interlocking layers of information and nested clues that lead the player characters from seemingly minor investigations into a missing person to discovering mind-numbingly awful, global conspiracies to destroy the world. Unlike its predecessor games, \"CoC\" assumed that most investigators would not survive, alive or sane, and that the only safe way to deal with the vast majority of nasty things described in the rule books was to run away. A well-run \"CoC\" campaign should engender a sense of foreboding and inevitable doom in its players. The style and setting of the game, in a relatively modern time period, created an emphasis on real-life settings, character research, and thinking one's way around trouble.\nThe first book of \"Call of Cthulhu\" adventures was \"Shadows of Yog-Sothoth\". In this work, the characters come upon a secret society's foul plot to destroy mankind, and pursue it first near to home and then in a series of exotic locations. This template was to be followed in many subsequent campaigns, including \"Fungi from Yuggoth\" (later known as \"Curse of Cthulhu\" and \"Day of the Beast\"), \"Spawn of Azathoth\", and possibly the most highly acclaimed, \"Masks of Nyarlathotep\".\n\"Shadows of Yog-Sothoth\" is important not only because it represents the first published addition to the boxed first edition of \"Call of Cthulhu\", but because its format defined a new way of approaching a campaign of linked RPG scenarios involving actual clues for the would-be detectives amongst the players to follow and link in order to uncover the dastardly plots afoot. Its format has been used by every other campaign-length \"Call of Cthulhu\" publication. The standard of \"CoC\" scenarios was well received by independent reviewers. \"The Asylum and Other Tales\", a series of stand alone articles released in 1983, rated an overall 9/10 in Issue 47 of \"White Dwarf\" magazine.\nThe standard of the included 'clue' material varies from scenario to scenario, but reached its zenith in the original boxed versions of the \"Masks of Nyarlathotep\" and \"Horror on the Orient Express\" campaigns. Inside these one could find matchbooks and business cards apparently defaced by non-player characters, newspaper cuttings and (in the case of \"Orient Express\") period passports to which players could attach their photographs, increasing the sense of immersion. Indeed, during the period that these supplements were produced, third party campaign publishers strove to emulate the quality of the additional materials, often offering separately-priced 'deluxe' clue packages for their campaigns.\nAdditional milieux were provided by Chaosium with the release of \"Dreamlands\", a boxed supplement containing additional rules needed for playing within the Lovecraft Dreamlands, a large map and a scenario booklet, and \"Cthulhu By Gaslight\", another boxed set which moved the action from the 1920s to the 1890s.\n\"Cthulhu Now\".\nIn 1987, Chaosium issued the supplement titled \"Cthulhu Now\", a collection of rules, supplemental source materials and scenarios for playing \"Call of Cthulhu\" in the present day. This proved to be a very popular alternative milieu, so much so that much of the supplemental material is now included in the core rule book.\n\"Lovecraft Country\".\n\"Lovecraft Country\" was a line of supplements for \"Call of Cthulhu\" released in 1990. These supplements were overseen by Keith Herber and provided backgrounds and adventures set in Lovecraft's fictional towns of Arkham, Kingsport, Innsmouth, Dunwich, and their environs. The intent was to give investigators a common base, as well as to center the action on well-drawn characters with clear motivations.\n\"Terror Australis\".\nIn 1987, \"Terror Australis: Call of Cthulhu in the Land Down Under\" was published. In 2018, a revised and updated version of the 1987 game was reissued, with about triple the content and two new games. It requires the \"Call of Cthulhu Keeper's Rulebook\" (7th Edition) and is usable with \"Pulp Cthulhu\".\nRecent history.\nIn the years since the collapse of the \"Mythos\" collectible card game (production ceased in 1997), the release of \"CoC\" books has been very sporadic, with up to a year between releases. Chaosium struggled with near bankruptcy for many years before finally starting their upward climb again.\n2005 was Chaosium's busiest year for many years, with 10 releases for the game. Chaosium took to marketing \"monographs\"\u2014short books by individual writers with editing and layout provided out-of-house\u2014directly to the consumer, allowing the company to gauge market response to possible new works. The range of times and places in which the horrors of the Mythos can be encountered was also expanded in late 2005 onward with the addition of \"Cthulhu Dark Ages\" by St\u00e9phane Gesbert, which gives a framework for playing games set in 11th century Europe, \"Secrets of Japan\" by Michael Dziesinski for gaming in modern-day Japan, and \"Secrets of Kenya\" by David Conyers for gaming in interwar period Africa.\nIn July 2011, Chaosium announced it would re-release a 30th anniversary edition of the \"CoC\" 6th edition role-playing game. This 320-page book features thick (3\u00a0mm) leatherette hardcovers with the front cover and spine stamped with gold foil. The interior pages are printed in black ink, on 90 gsm matte art paper. The binding is thread sewn, square backed. Chaosium offered a one-time printing of this Collector's Edition.\nOn May 28, 2013, a crowdfunding campaign on Kickstarter for the 7th edition of \"Call of Cthulhu\" was launched with a goal of $40,000; it ended on June 29 of the same year having collected $561,836. It included many more major revisions than any previous edition, and also split the core rules into two books, a Player's Guide and Keeper's Guide. Problems and delays fulfilling the Kickstarters for the 7th edition of \"Call of Cthulhu\" led Greg Stafford and Sandy Petersen (who had both left in 1998) to return to an active role at Chaosium in June 2015.\nThe available milieux were also expanded with the release of Cthulhu Through the Ages, a supplement containing additional rules needed for playing within the Roman Empire, Mythic Iceland, a futuristic micro-setting, and the End Times, where the monsters of the mythos attempt to subjugate or destroy the world.\nLicenses.\nChaosium has licensed other publishers to create supplements, video, card and board games using the setting and the \"Call of Cthulhu\" brand. Many, such as \"Delta Green\" by Pagan Publishing and Arkham Horror by Fantasy Flight, have moved away completely from \"Call of Cthulhu\". Other licensees have included Infogrames, Miskatonic River Press, Theater of the Mind Enterprises, Triad Entertainment, Games Workshop, RAFM, Goodman Games, Grenadier Models Inc. and Yog-Sothoth.com. These supplements may be set in different time frames or even different game universes from the original game.\nVideo Games.\n\"Shadow of the Comet\".\n\"Shadow of the Comet\" (later repackaged as \"Call of Cthulhu: Shadow of the Comet\") is an adventure game developed and released by Infogrames in 1993. The game is based on H. P. Lovecraft's Cthulhu Mythos and uses many elements from Lovecraft's \"The Dunwich Horror\" and \"The Shadow Over Innsmouth\". A follow-up game, \"Prisoner of Ice\", is not a direct sequel.\n\"Prisoner of Ice\".\n\"Prisoner of Ice\" (also \"Call of Cthulhu: Prisoner of Ice\") is an adventure game developed and released by Infogrames for the PC and Macintosh computers in 1995 in America and Europe. It is based on H. P. Lovecraft's Cthulhu Mythos, particularly \"At the Mountains of Madness\", and is a follow-up to Infogrames' earlier \"Shadow of the Comet\". In 1997, the game was ported to the Sega Saturn and PlayStation exclusively in Japan.\n\"Dark Corners of the Earth\".\nA licensed first-person shooter adventure game by Headfirst Productions, based on \"Call of Cthulhu\" campaign \"Escape from Innsmouth\" and released by Bethesda Softworks in 2005/2006 for the PC and Xbox.\n\"The Wasted Land\".\nIn April 2011, Chaosium and new developer Red Wasp Design announced a joint project to produce a mobile video game based on the \"Call of Cthulhu\" RPG, entitled \"Call of Cthulhu: The Wasted Land\". The game was released on January 30, 2012.\n\"Cthulhu Chronicles\".\nIn 2018, Metarcade produced \"Cthulhu Chronicles\", a game for iOS with a campaign of nine mobile interactive fiction stories set in 1920s England based on \"Call of Cthulhu\". The first five stories were released on July 10, 2018.\n\"Call of Cthulhu\".\n\"Call of Cthulhu\" is a survival horror role-playing video game developed by Cyanide and published by Focus Home Interactive for PlayStation 4, Xbox One and Windows. The game features a semi-open world environment and incorporates themes of Lovecraftian and psychological horror into a story which includes elements of investigation and stealth. It is inspired by H. P. Lovecraft's short story \"The Call of Cthulhu\".\n\"Trail of Cthulhu\".\nIn February 2008, Pelgrane Press published \"Trail of Cthulhu\", a stand-alone game created by Kenneth Hite using the GUMSHOE System developed by Robin Laws. GUMSHOE is specifically designed to be used in investigative games.\n\"Shadows of Cthulhu\".\nIn September 2008, Reality Deviant Publications published \"Shadows of Cthulhu\", a supplement that brings Lovecraftian gaming to Green Ronin's True20 system.\n\"Realms of Cthulhu\".\nIn October 2009, Reality Blurs published \"Realms of Cthulhu\", a supplement for Pinnacle Entertainment's Savage Worlds system.\n\"Delta Green\".\nPagan Publishing published \"Delta Green\", a series of supplements originally set in the 1990s, although later supplements add support for playing closer to the present day. In these, player characters are agents of a secret agency known as Delta Green, which fights against creatures from the Mythos and conspiracies related to them. Arc Dream Publishing released a new version of \"Delta Green\" in 2016 as a standalone game, partially using the mechanics from \"Call of Cthulhu\".\n\"d20 Call of Cthulhu\".\nIn 2001, a stand-alone version of \"Call of Cthulhu\" was released by Wizards of the Coast, for the d20 system. Intended to preserve the feeling of the original game, the d20 conversion of the game rules were supposed to make the game more accessible to the large \"D&amp;D\" player base. The d20 system also made it possible to use \"Dungeons &amp; Dragons\" characters in \"Call of Cthulhu\", as well as to introduce the Cthulhu Mythos into \"Dungeons &amp; Dragons\" games. The d20 version of the game is no longer supported by Wizards as per their contract with Chaosium. Chaosium included d20 stats as an appendix in three releases (see Lovecraft Country), but have since dropped the \"dual stat\" idea.\nCard games.\n\"Mythos\" was a collectible card game (CCG) based on the Cthulhu Mythos that Chaosium produced and marketed during the mid-1990s. While generally praised for its fast gameplay and unique mechanics, it ultimately failed to gain a very large market presence. It bears mention because its eventual failure brought the company to hard times that affected its ability to produce material for \"Call of Cthulhu\". \"Call of Cthulhu: The Card Game\" is a second collectible card game, produced by Fantasy Flight Games.\nMiniatures.\nThe first licensed \"Call of Cthulhu\" gaming miniatures were sculpted by Andrew Chernack and released by Grenadier Models in boxed sets and blister packs in 1983. The license was later transferred to RAFM. As of 2011, RAFM still produce licensed C\"all of Cthulhu\" models sculpted by Bob Murch. Both lines include investigator player character models and the iconic monsters of the Cthulhu mythos.\nAs of July 2015, Reaper Miniatures started its third \"Bones Kickstarter\", a Kickstarter intended to help the company migrate some miniatures from metal to plastic, and introducing some new ones. Among the stretch goals was the second $50 expansion, devoted to the Mythos, with miniatures such as Cultists, Deep Ones, Mi'Go, and an extra $15 Shub-Niggurath \"miniature\" (it is, at least, 6x4 squares). It is expected for those miniatures to remain in the Reaper Miniatures catalogue after the Kickstarter project finishes. In 2020 Chaosium announced a license agreement with Ardacious for \"Call of Cthulhu\" virtual miniatures to be released on their augmented reality app Ardent Roleplay.\nReception.\nSeveral reviews of various editions appeared in \"Space Gamer/Fantasy Gamer\". \nSeveral reviews of various editions appeared in \"White Dwarf\".\nSeveral reviews of various editions and supplements also appeared in \"Dragon\". \nIn a 1996 reader poll by \"Arcane\" magazine to determine the 50 most popular roleplaying games of all time, \"Call of Cthulhu\" was ranked 1st. Editor Paul Pettengale commented: \"\"Call of Cthulhu\" is fully deserved of the title as the most popular roleplaying system ever - it's a game that doesn't age, is eminently playable, and which hangs together perfectly. The system, even though it's over ten years old, it still one of the very best you'll find in any roleplaying game. Also, there's not a referee in the land who could say they've read every Lovecraft inspired book or story going, so there's a pretty-well endless supply of scenario ideas. It's simply marvellous.\"\nAwards.\nThe game has won several major awards:"}
{"id": "5723", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=5723", "title": "Constellations (journal)", "text": "Constellations: An International Journal of Critical and Democratic Theory is a quarterly peer-reviewed academic journal of critical and democratic theory and successor of \"Praxis International\". It is currently edited by Jean L. Cohen, Cristina Lafont, and Hubertus Buchstein. Ertug Tombus is the managing editor of the journal since 2010. Seyla Benhabib, Nancy Fraser and Andrew Arato are the co-founding former editors. With an international editorial contribution, it is based at the New School in New York. \nNadia Urbinati, Amy Allen, and Andreas Kalyvas are former co-editors."}
{"id": "5724", "revid": "41199313", "url": "https://en.wikipedia.org/wiki?curid=5724", "title": "Cape Breton Island", "text": "Cape Breton Island (, formerly '; or '; ) is an island on the Atlantic coast of North America and part of the province of Nova Scotia, Canada. \nThe island accounts for 18.7% of Nova Scotia's total area. Although the island is physically separated from the Nova Scotia peninsula by the Strait of Canso, the long rock-fill Canso Causeway connects it to mainland Nova Scotia. The island is east-northeast of the mainland with its northern and western coasts fronting on the Gulf of Saint Lawrence; its western coast also forms the eastern limits of the Northumberland Strait. The eastern and southern coasts front the Atlantic Ocean; its eastern coast also forms the western limits of the Cabot Strait. Its landmass slopes upward from south to north, culminating in the highlands of its northern cape. One of the world's larger saltwater lakes, (\"Arm of Gold\" in French), dominates the island's centre.\nThe island is divided into four of Nova Scotia's eighteen counties: Cape Breton, Inverness, Richmond, and Victoria. Their total population at the 2016 census numbered 132,010 Cape Bretoners; this is approximately 15% of the provincial population. Cape Breton Island has experienced a decline in population of approximately 2.9% since the 2011 census. Approximately 75% of the island's population is in the Cape Breton Regional Municipality (CBRM) which includes all of Cape Breton County and is often referred to as Industrial Cape Breton, given the history of coal mining and steel manufacturing in this area, which was Nova Scotia's industrial heartland throughout the 20th century.\nThe island has five reserves of the Mi\ua78ckmaq Nation: , , , , and /Chapel Island. is the largest in both population and land area.\nToponymy.\nCape Breton Island takes its name from its easternmost point, Cape Breton. This may have been named after the Basque fishing port of Capbreton, but more probably takes its name from the Bretons of northwestern France. A Portuguese mappa mundi of 1516\u201320 includes the label \"Terra q(ue) foy descuberta por Bertomes\" in the vicinity of the Gulf of St Lawrence, which means \"Land discovered by Bretons\". The name \"Cape Breton\" first appears on a map of 1516, as \"C(abo) dos Bretoes\", and became the general name for both the island and the cape toward the end of the 16th century. The Breton origin of the name is not universally accepted: William Francis Ganong argued that the Portuguese term \"Bertomes\" referred to Englishmen or Britons, and that the name should be interpreted as \"Cape of the English\".\nHistory.\nCape Breton Island's first residents were likely Archaic maritime natives, ancestors of the Mi'kmaq. These peoples and their progeny inhabited the island (known as Unama'ki) for several thousand years and continue to live there to this day. Their traditional lifestyle centred around hunting and fishing because of the unfavourable agricultural conditions of their maritime home. This ocean-centric lifestyle did, however, make them among the first indigenous peoples to discover European explorers and sailors fishing in the St Lawrence Estuary. John Cabot reportedly visited the island in 1497. However, European histories and maps of the period are of too poor quality to be sure whether Cabot first visited Newfoundland or Cape Breton Island. This discovery is commemorated by Cape Breton's Cabot Trail, and by the Cabot's Landing Historic Site &amp; Provincial Park, near the village of Dingwall.\nThe local Mi'kmaq peoples began trading with European fishermen when the fishermen began landing in their territories as early as the 1520s. In about 1521\u201322, the Portuguese under Jo\u00e3o \u00c1lvares Fagundes established a fishing colony on the island. As many as two hundred settlers lived in a village, the name of which is not known, located according to some historians at what is now Ingonish on the island's northeastern peninsula. These fishermen traded with the local population but did not maintain a permanent settlement. This Portuguese colony's fate is unknown, but it is mentioned as late as 1570.\nDuring the Anglo-French War of 1627 to 1629, under Charles I, the Kirkes took Quebec City; Sir James Stewart of Killeith, Lord Ochiltree planted a colony on Unama'ki at Baleine, Nova Scotia; and Alexander's son, William Alexander, 1st Earl of Stirling, established the first incarnation of \"New Scotland\" at Port Royal. These claims, and larger European ideals of native conquest were the first time the island was incorporated as European territory, though it would be several decades later that treaties would actually be signed (no copies of these treaties exist).\nThese Scottish triumphs, which left Cape Sable as the only major French holding in North America, did not last. Charles I's haste to make peace with France on the terms most beneficial to him meant the new North American gains would be bargained away in the Treaty of Saint-Germain-en-Laye (1632), which established which European power had claim over the territories, but did not in fact establish that Europeans had any claim to begin with.\nThe French quickly defeated the Scots at Baleine, and established the first European settlements on \u00cele Royale: present day Englishtown (1629) and St. Peter's (1630). These settlements lasted only one generation, until Nicolas Denys left in 1659. The island did not have any European settlers for another fifty years before those communities along with Louisbourg were re-established in 1713, after which point European settlement was permanently established on the island.\n\u00cele Royale.\nKnown as \"\u00cele Royale\" (\"Royal Island\") to the French, the island also saw active settlement by France. After the French ceded their claims to Newfoundland and the Acadian mainland to the British by the Treaty of Utrecht in 1713, the French relocated the population of Plaisance, Newfoundland, to \u00cele Royale and the French garrison was established in the central eastern part at Sainte Anne. As the harbour at Sainte Anne experienced icing problems, it was decided to build a much larger fortification at Louisbourg to improve defences at the entrance to the Gulf of Saint Lawrence and to defend France's fishing fleet on the Grand Banks. The French also built the Louisbourg Lighthouse in 1734, the first lighthouse in Canada and one of the first in North America. In addition to Cape Breton Island, the French colony of \u00cele Royale also included \u00cele Saint-Jean, today called Prince Edward Island, and Les \u00celes-de-la-Madeleine.\nFrench and Indian War.\nLouisbourg itself was one of the most important commercial and military centres in New France. Louisbourg was captured by New Englanders with British naval assistance in 1745 and by British forces in 1758. The French population of \u00cele Royale was deported to France after each siege. While French settlers returned to their homes in \u00cele Royale after the Treaty of Aix-la-Chapelle was signed in 1748, the fortress was demolished after the second siege. \u00cele Royale remained formally part of New France until it was ceded to Great Britain by the Treaty of Paris in 1763. It was then merged with the adjacent, British colony of Nova Scotia (present day peninsular Nova Scotia and New Brunswick). Acadians who had been expelled from Nova Scotia and \u00cele Royale were permitted to settle in Cape Breton beginning in 1764, and established communities in north-western Cape Breton, near Cheticamp, and southern Cape Breton, on and near Isle Madame.\nSome of the first British-sanctioned settlers on the island following the Seven Years' War were Irish, although upon settlement they merged with local French communities to form a culture rich in music and tradition. From 1763 to 1784, the island was administratively part of the colony of Nova Scotia and was governed from Halifax.\nThe first permanently settled Scottish community on Cape Breton Island was Judique, settled in 1775 by Michael Mor MacDonald. He spent his first winter using his upside-down boat for shelter, which is reflected in the architecture of the village's Community Centre. He composed a song about the area called \"O 's \u00e0lainn an t-\u00e0ite\", or \"O, Fair is the Place.\"\nAmerican Revolution.\nDuring the American Revolution, on 1 November 1776, John Paul Jones \u2013 the father of the American Navy \u2013 set sail in command of \"Alfred\" to free hundreds of American prisoners working in the area's coal mines. Although winter conditions prevented the freeing of the prisoners, the mission did result in the capture of \"Mellish\", a vessel carrying a vital supply of winter clothing intended for John Burgoyne's troops in Canada.\nMajor Timothy Hierlihy and his regiment on board HMS \"Hope\" worked in and protected from privateer attacks on the coal mines at Sydney Cape Breton. Sydney Cape Breton provided a vital supply of coal for Halifax throughout the war. The British began developing the mining site at Sydney Mines in 1777. On 14 May 1778, Major Hierlihy arrived at Cape Breton. While there, Hierlihy reported that he \"beat off many piratical attacks, killed some and took other prisoners.\"\nA few years into the war there was also a naval engagement between French ships and a British convoy off Sydney, Nova Scotia, near Spanish River (1781), Cape Breton. French ships (fighting with the Americans) were re-coaling and defeated a British convoy. Six French sailors were killed and 17 British, with many more wounded.\nColony of Cape Breton.\nIn 1784, Britain split the colony of Nova Scotia into three separate colonies: New Brunswick, Cape Breton Island, and present-day peninsular Nova Scotia, in addition to the adjacent colonies of St. John's Island (renamed Prince Edward Island in 1798) and Newfoundland. The colony of Cape Breton Island had its capital at Sydney on its namesake harbour fronting on Spanish Bay and the Cabot Strait. Its first Lieutenant-Governor was Joseph Frederick Wallet DesBarres (1784\u20131787) and his successor was William Macarmick (1787).\nA number of United Empire Loyalists emigrated to the Canadian colonies, including Cape Breton. David Mathews, the former Mayor of New York City during the American Revolution, emigrated with his family to Cape Breton in 1783. He succeeded Macarmick as head of the colony and served from 1795 to 1798.\nFrom 1799 to 1807, the military commandant was John Despard, brother of Edward.\nAn order forbidding the granting of land in Cape Breton, issued in 1763, was removed in 1784. The mineral rights to the island were given over to the Duke of York by an order-in-council. The British government had intended that the Crown take over the operation of the mines when Cape Breton was made a colony, but this was never done, probably because of the rehabilitation cost of the mines. The mines were in a neglected state, caused by careless operations dating back at least to the time of the final fall of Louisbourg in 1758.\nLarge-scale shipbuilding began in the 1790s, beginning with schooners for local trade moving in the 1820s to larger brigs and brigantines, mostly built for British shipowners. Shipbuilding peaked in the 1850s, marked in 1851 by the full-rigged ship \"Lord Clarendon\", the largest wooden ship ever built in Cape Breton.\nMerger with Nova Scotia.\nIn 1820, the colony of Cape Breton Island was merged for the second time with Nova Scotia. This development is one of the factors which led to large-scale industrial development in the Sydney Coal Field of eastern Cape Breton County. By the late 19th century, as a result of the faster shipping, expanding fishery and industrialization of the island, exchanges of people between the island of Newfoundland and Cape Breton increased, beginning a cultural exchange that continues to this day.\nThe 1920s were some of the most violent times in Cape Breton. They were marked by several severe labour disputes. The famous murder of William Davis by strike breakers, and the seizing of the New Waterford power plant by striking miners led to a major union sentiment that persists to this day in some circles. William Davis Miners' Memorial Day is celebrated in coal mining towns to commemorate the deaths of miners at the hands of the coal companies.\n20th century.\nThe turn of the 20th century saw Cape Breton Island at the forefront of scientific achievement with the now-famous activities launched by inventors Alexander Graham Bell and Guglielmo Marconi.\nFollowing his successful invention of the telephone and being relatively wealthy, Bell acquired land near Baddeck in 1885, largely due to surroundings reminiscent of his early years in Scotland. He established a summer estate complete with research laboratories, working with deaf people\u2014including Helen Keller\u2014and continued to invent. Baddeck would be the site of his experiments with hydrofoil technologies as well as the Aerial Experiment Association, financed by his wife, which saw the first powered flight in Canada when the AEA \"Silver Dart\" took off from the ice-covered waters of Bras d'Or Lake. Bell also built the forerunner to the iron lung and experimented with breeding sheep.\nMarconi's contributions to Cape Breton Island were also quite significant, as he used the island's geography to his advantage in transmitting the first North American trans-Atlantic radio message from a station constructed at Table Head in Glace Bay to a receiving station at Poldhu in Cornwall, England. Marconi's pioneering work in Cape Breton marked the beginning of modern radio technology. Marconi's station at Marconi Towers, on the outskirts of Glace Bay, became the chief communication centre for the Royal Canadian Navy in World War I through to the early years of World War II.\nPromotions for tourism beginning in the 1950s recognized the importance of the Scottish culture to the province, and the provincial government started encouraging the use of Gaelic once again. The establishment of funding for the Gaelic College of Celtic Arts and Crafts and formal Gaelic language courses in public schools are intended to address the near-loss of this culture to English assimilation.\nIn the 1960s, the Fortress of Louisbourg was partially reconstructed by Parks Canada. Since 2009, this National Historic Site of Canada has attracted an average of 90\u00a0000 visitors per year.\nGaelic speakers.\nGaelic speakers in Cape Breton, as elsewhere in Nova Scotia, furnished a large proportion of the local population from the 18th century on. They brought with them a common culture of poetry, traditional songs and tales, music and dance, and used this to develop distinctive local traditions.\nMost Gaelic settlement in Nova Scotia happened between 1770 and 1840, with probably over 50,000 Gaelic speakers emigrating from the Scottish Highlands and the Hebrides to Nova Scotia and Prince Edward Island. Such emigration was facilitated by changes in Gaelic society and the economy, with sharp increases in rents, confiscation of land and disruption of local customs and rights. In Nova Scotia, poetry and song in Gaelic flourished. George Emmerson argues that an \"ancient and rich\" tradition of storytelling, song, and Gaelic poetry emerged during the 18th century and was transplanted from the Highlands of Scotland to Nova Scotia, where the language similarly took root there. The majority of those settling in Nova Scotia from the end of the 18th century through to middle of the next were from the Scottish Highlands, rather than the Lowlands, making the Highland tradition's impact more profound on the region. Gaelic settlement in Cape Breton began in earnest in the early nineteenth century.\nThe Gaelic language became dominant from Colchester County in the west of Nova Scotia into Cape Breton County in the east. It was reinforced in Cape Breton in the first half of the 19th century with an influx of Highland Scots numbering approximately 50,000 as a result of the Highland Clearances.\nGaelic speakers, however, tended to be poor; they were largely illiterate and had little access to education. This situation still obtained in the early twentieth century. In 1921 Gaelic was approved as an optional subject in the curriculum of Nova Scotia, but few teachers could be found and children were discouraged from using the language in schools. By 1931 the number of Gaelic speakers in Nova Scotia had fallen to approximately 25,000, mostly in discrete pockets. In Cape Breton it was still a majority language, but the proportion was falling. Children were no longer being raised with Gaelic.\nFrom 1939 on, attempts were made to strengthen its position in the public school system in Nova Scotia, but funding, official commitment and the availability of teachers continued to be a problem. By the 1950s the number of speakers was less than 7,000. The advent of multiculturalism in Canada in the 1960s meant that new educational opportunities became available, with a gradual strengthening of the language at secondary and tertiary level. At present several schools in Cape Breton offer Gaelic Studies and Gaelic language programs, and the language is taught at University College of Cape Breton.\nThe 2016 Canadian Census shows that there are only 40 reported speakers of Gaelic as a mother tongue in Cape Breton. On the other hand, there are families and individuals who have recommenced intergenerational transmission. They include fluent speakers from Gaelic-speaking areas of Scotland and speakers who became fluent in Nova Scotia and who in some cases studied in Scotland. Other revitalization activities include adult education, community cultural events and publishing.\nEnvironment.\nGeography.\nThe island measures in area, making it the 77th largest island in the world and Canada's 18th largest island. Cape Breton Island is composed mainly of rocky shores, rolling farmland, glacial valleys, barren headlands, mountains, woods and plateaus. Geological evidence suggests at least part of the island was joined with present-day Scotland and Norway, now separated by millions of years of plate tectonics.\nCape Breton Island's northern portion is dominated by the Cape Breton Highlands, commonly shortened to simply the \"Highlands\", which are an extension of the Appalachian mountain chain. The Highlands comprise the northern portions of Inverness and Victoria counties. In 1936, the federal government established the Cape Breton Highlands National Park covering across the northern third of the Highlands. The Cabot Trail scenic highway also encircles the plateau's coastal perimeter.\nCape Breton Island's hydrological features include the Bras d'Or Lake system, a salt-water fjord at the heart of the island, and freshwater features including Lake Ainslie, the Margaree River system, and the Mira River. Innumerable smaller rivers and streams drain into the Bras d'Or Lake estuary and on to the Gulf of St. Lawrence and Atlantic coasts.\nCape Breton Island is joined to the mainland by the Canso Causeway, which was completed in 1955, enabling direct road and rail traffic to and from the island, but requiring marine traffic to pass through the Canso Canal at the eastern end of the causeway.\nCape Breton Island is divided into four counties: Cape Breton, Inverness, Richmond, and Victoria.\nThe climate is one of mild, often pleasantly warm summers and cold winters, although the proximity to the Atlantic Ocean and Gulf Stream moderates the extreme winter cold found on the mainland, especially on the east side that faces the Atlantic. Precipitation is abundant year round, with annual totals up to 60 inches on the eastern side facing the Atlantic storms. Considerable snowfall occurs in winter, especially in the highlands.\nDemographics.\nThe island's residents can be grouped into five main cultures: Scottish, Mi'kmaq, Acadian, Irish, English, with respective languages Scottish Gaelic, Mi'kmaq, French, and English. English is now the primary language, including a locally distinctive Cape Breton accent, while Mi'kmaq, Scottish Gaelic and Acadian French are still spoken in some communities.\nLater migrations of Black Loyalists, Italians, and Eastern Europeans mostly settled in the island's eastern part around the industrial Cape Breton region. Cape Breton Island's population has been in decline two decades with an increasing exodus in recent years due to economic conditions.\nAccording to the Census of Canada, the population of Cape Breton [Economic region] in 2016 / 2011 / 2006 / 1996 was 132,010 / 135,974 / 142,298 / 158,260.\nReligious groups.\nStatistics Canada in 2001 reported a \"religion\" total of 145,525 for Cape Breton, including 5,245 with \"no religious affiliation.\" Major categories included:\nEconomy.\nMuch of the recent economic history of Cape Breton Island can be tied to the coal industry.\nThe island has two major coal deposits:\nSydney has traditionally been the main port, with facilities in a large, sheltered, natural harbour. It is the island's largest commercial centre and home to the \"Cape Breton Post\" daily newspaper, as well as one television station, CJCB-TV (CTV), and several radio stations. The Marine Atlantic terminal at North Sydney is the terminal for large ferries traveling to Channel-Port aux Basques and seasonally to Argentia, both on the island of Newfoundland.\nPoint Edward on the west side of Sydney Harbour is the location of Sydport, a former navy base () now converted to commercial use. The Canadian Coast Guard College is nearby at Westmount. Petroleum, bulk coal, and cruise ship facilities are also in Sydney Harbour.\nGlace Bay, the second largest urban community in population, was the island's main coal mining centre until its last mine closed in the 1980s. Glace Bay was the hub of the Sydney &amp; Louisburg Railway and a major fishing port. At one time, Glace Bay was known as the largest town in Nova Scotia, based on population.\nPort Hawkesbury has risen to prominence since the completion of the Canso Causeway and Canso Canal created an artificial deep-water port, allowing extensive petrochemical, pulp and paper, and gypsum handling facilities to be established. The Strait of Canso is completely navigable to Seawaymax vessels, and Port Hawkesbury is open to the deepest-draught vessels on the world's oceans. Large marine vessels may also enter Bras d'Or Lake through the Great Bras d'Or channel, and small craft can use the Little Bras d'Or channel or St. Peters Canal. While commercial shipping no longer uses the St. Peters Canal, it remains an important waterway for recreational vessels.\nThe industrial Cape Breton area faced several challenges with the closure of the Cape Breton Development Corporation's (DEVCO) coal mines and the Sydney Steel Corporation's (SYSCO) steel mill. In recent years, the Island's residents have tried to diversify the area economy by investing in tourism developments, call centres, and small businesses, as well as manufacturing ventures in fields such as auto parts, pharmaceuticals, and window glazings.\nWhile the Cape Breton Regional Municipality is in transition from an industrial to a service-based economy, the rest of Cape Breton Island outside the industrial area surrounding Sydney-Glace Bay has been more stable, with a mixture of fishing, forestry, small-scale agriculture, and tourism.\nTourism in particular has grown throughout the post-Second World War era, especially the growth in vehicle-based touring, which was furthered by the creation of the Cabot Trail scenic drive. The scenery of the island is rivalled in northeastern North America by only Newfoundland; and Cape Breton Island tourism marketing places a heavy emphasis on its Scottish Gaelic heritage through events such as the Celtic Colours Festival, held each October, as well as promotions through the Gaelic College of Celtic Arts and Crafts.\nWhale-watching is a popular attraction for tourists. Whale-watching cruises are operated by vendors from Baddeck to Cheticamp. The most popular species of whale found in Cape Breton's waters is the Pilot whale.\nThe island's primary east\u2013west road is Highway 105, the Trans-Canada Highway, although Trunk 4 is also heavily used. Highway 125 is an important arterial route around Sydney Harbour in the Cape Breton Regional Municipality. The Cabot Trail, circling the Cape Breton Highlands, and Trunk 19, along the island's western coast, are important secondary roads. The Cape Breton and Central Nova Scotia Railway maintains railway connections between the port of Sydney to the Canadian National Railway in Truro\nThe Cabot Trail is a scenic road circuit around and over the Cape Breton Highlands with spectacular coastal vistas; over 400,000 visitors drive the Cabot Trail each summer and fall. Coupled with the Fortress of Louisbourg, it has driven the growth of the tourism industry on the island in recent decades. The \"Cond\u00e9 Nast\" travel guide has rated Cape Breton Island as one of the world's best island destinations.\nTraditional music.\nCape Breton is well known for its traditional fiddle music, which was brought to North America by Scottish immigrants during the Highland Clearances. The traditional style has been well preserved in Cape Breton, and c\u00e9ilidhs have become a popular attraction for tourists. Inverness County in particular has a heavy concentration of musical activity, with regular performances in communities such as Mabou and Judique. Judique is recognized as 'Baile nam Fonn', (literally: Village of Tunes) or the 'Home of Celtic Music', featuring the Celtic Music Interpretive Centre. Performers who have received significant recognition outside of Cape Breton include Angus Chisholm, Buddy MacMaster, Joseph Cormier, first Cape Breton fiddler to record an album made available in Europe (1974), Lee Cremo, Bruce Guthro, Natalie MacMaster, Ashley MacIsaac, The Rankin Family, Aselin Debison, Gordie Sampson, Dawn and Margie Beaton, also known as \"The Beaton Sisters\", and the Barra MacNeils. The Margaree's of Cape Breton also serve as a large contributor of fiddle music celebrated throughout the island. This traditional fiddle music of Cape Breton is studied by musicians around the world, where its global recognition continues to rise.\nThe Men of the Deeps are a male choral group of current and former miners from the industrial Cape Breton area.\nNotable people.\nCape Breton artists who have been recognized with major national or international awards include actor Harold Russell of North Sydney, who won an Academy Award in 1946 for his portrayal of Homer Parrish in \"The Best Years of Our Lives\", and Lynn Coady and Linden MacIntyre of Inverness County, who are both past winners of the Giller Prize for Canadian literature. The Rankin Family and Rita MacNeil have recorded multiple albums certified as Double Platinum by Music Canada.\nPeople from Cape Breton have also achieved a number of firsts in Canadian politics and governance. These include Mayann Francis of Whitney Pier, the first Black Lieutenant Governor of Nova Scotia, Isaac Phills of Sydney, Nova Scotia, the first person of African descent to be awarded the Order of Canada, and Elizabeth May of Margaree Harbour, the first member of the Green Party of Canada elected to the House of Commons of Canada.\nCape Breton Island is also home to YouTube weather forecaster Frankie MacDonald, who has over 200,000 subscribers. He accurately predicted a magnitude 7 earthquake in New Zealand in November 2016.\nAmerican artists like sculptor Richard Serra, composer Philip Glass and abstract painter John Beardman spent part of the year on Cape Breton Island.\nSteve Arbuckle is a Canadian-born actor born in Cape Breton Island.\nDirector Ashley McKenzie's 2016 film \"Werewolf\" is set on the island and features local actors; McKenzie grew up on the island.\nBruce Guthro; Lead singer and guitarist of the former Scottish Celtic band Runrig, which disbanded in 2018 after 46 years. Guthro resides in Hammonds Plains, Nova Scotia.\nDylan Guthro; musician. Son of Bruce Guthro."}
{"id": "5725", "revid": "15135172", "url": "https://en.wikipedia.org/wiki?curid=5725", "title": "Cthulhu Mythos", "text": "The Cthulhu Mythos is a shared fictional universe, originating in the works of American horror writer H. P. Lovecraft. The term was coined by August Derleth, a contemporary correspondent and prot\u00e9g\u00e9 of Lovecraft, to identify the settings, tropes, and lore that were employed by Lovecraft and his literary successors. The name \"Cthulhu\" derives from the central creature in Lovecraft's seminal short story \"The Call of Cthulhu\", first published in the pulp magazine \"Weird Tales\" in 1928.\nRichard L. Tierney, a writer who also wrote Mythos tales, later applied the term \"Derleth Mythos\" to distinguish Lovecraft's works from Derleth's later stories, which modify key tenets of the Mythos. Authors of Lovecraftian horror in particular frequently use elements of the Cthulhu Mythos.\nHistory.\nIn his essay \"H. P. Lovecraft and the Cthulhu Mythos\", Robert M. Price described two stages in the development of the Cthulhu Mythos. Price called the first stage the \"Cthulhu Mythos proper.\" This stage was formulated during Lovecraft's lifetime and was subject to his guidance. The second stage was guided by August Derleth who, in addition to publishing Lovecraft's stories after his death, attempted to categorize and expand the Mythos.\nFirst stage.\nAn ongoing theme in Lovecraft's work is the complete irrelevance of mankind in the face of the cosmic horrors that apparently exist in the universe. Lovecraft made frequent references to the \"Great Old Ones\", a loose pantheon of ancient, powerful deities from space who once ruled the Earth and have since fallen into a deathlike sleep. While these monstrous deities were present in almost all of Lovecraft's published work (his second short story \"Dagon\", published in 1919, is considered the start of the mythos), the first story to really expand the pantheon of Great Old Ones and its themes is \"The Call of Cthulhu\", which was published in 1928.\nLovecraft broke with other pulp writers of the time by having his main characters' minds deteriorate when afforded a glimpse of what exists outside their perceived reality. He emphasized the point by stating in the opening sentence of the story that \"The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\"\nWriter Dirk W. Mosig notes that Lovecraft was a \"mechanistic materialist\" who embraced the philosophy of \"cosmic indifference\" (Cosmicism). Lovecraft believed in a purposeless, mechanical, and uncaring universe. Human beings, with their limited faculties, can never fully understand this universe, and the cognitive dissonance caused by this revelation leads to insanity, in his view.\nThere have been attempts at categorizing this fictional group of beings. Phillip A. Schreffler argues that by carefully scrutinizing Lovecraft's writings, a workable framework emerges that outlines the entire \"pantheon\"from the unreachable \"Outer Ones\" (e.g., Azathoth, who occupies the centre of the universe) and \"Great Old Ones\" (e.g., Cthulhu, imprisoned on Earth in the sunken city of R'lyeh) to the lesser castes (the lowly slave shoggoths and the Mi-go).\nDavid E. Schultz, however, believes that Lovecraft never meant to create a canonical Mythos but rather intended his imaginary pantheon to serve merely as a background element. Lovecraft himself humorously referred to his Mythos as \"Yog Sothothery\" (Dirk W. Mosig coincidentally suggested the term \"Yog-Sothoth Cycle of Myth\" be substituted for \"Cthulhu Mythos\"). At times, Lovecraft even had to remind his readers that his Mythos creations were entirely fictional.\nThe view that there was no rigid structure is expounded upon by S. T. Joshi, who said\nPrice, however, believed that Lovecraft's writings could at least be divided into categories and identified three distinct themes: the \"Dunsanian\" (written in a similar style as Lord Dunsany), \"Arkham\" (occurring in Lovecraft's fictionalized New England setting), and \"Cthulhu\" (the cosmic tales) cycles. Writer Will Murray noted that while Lovecraft often used his fictional pantheon in the stories he ghostwrote for other authors, he reserved Arkham and its environs exclusively for those tales he wrote under his own name.\nAlthough the Mythos was not formalized or acknowledged between them, Lovecraft did correspond and share story elements with other contemporary writers including Clark Ashton Smith, Robert E. Howard, Robert Bloch, Frank Belknap Long, Henry Kuttner, Henry S. Whitehead, and Fritz Leibera group referred to as the \"Lovecraft Circle.\"\nFor example, Robert E. Howard's character Friedrich Von Junzt reads Lovecraft's \"Necronomicon\" in the short story \"The Children of the Night\" (1931), and in turn Lovecraft mentions Howard's \"Unaussprechlichen Kulten\" in the stories \"Out of the Aeons\" (1935) and \"The Shadow Out of Time\" (1936). Many of Howard's original unedited \"Conan\" stories also involve parts of the Cthulhu Mythos.\nSecond stage.\nPrice denotes the second stage's commencement with August Derleth, with the principal difference between Lovecraft and Derleth being Derleth's use of hope and development of the idea that the Cthulhu mythos essentially represented a struggle between good and evil. Derleth is credited with creating the \"Elder Gods.\" He stated:\nPrice believes that the basis for Derleth's system is found in Lovecraft: \"Was Derleth's use of the rubric 'Elder Gods' so alien to Lovecraft's in \"At the Mountains of Madness\"? Perhaps not. In fact, this very story, along with some hints from \"The Shadow over Innsmouth\", provides the key to the origin of the 'Derleth Mythos'. For in \"At the Mountains of Madness\" is shown the history of a conflict between interstellar races, first among them the Elder Ones and the Cthulhu-spawn.\nDerleth himself believed that Lovecraft wished for other authors to actively write about the Mythos as opposed to it being a discrete plot device within Lovecraft's own stories. Derleth expanded the boundaries of the Mythos by including any passing reference to another author's story elements by Lovecraft as part of the genre. Just as Lovecraft made passing reference to Clark Ashton Smith's \"Book of Eibon\", Derleth in turn added Smith's Ubbo-Sathla to the Mythos.\nDerleth also attempted to connect the deities of the Mythos to the four elements (\"air\", \"earth\", \"fire\", and \"water\"), creating new beings representative of certain elements in order to legitimize his system of classification. Derleth created \"Cthugha\" as a sort of fire elemental when a fan, Francis Towner Laney, complained that he had neglected to include the element in his schema. Laney, the editor of \"The Acolyte\", had categorized the Mythos in an essay that first appeared in the Winter 1942 issue of the magazine.\nImpressed by the glossary, Derleth asked Laney to rewrite it for publication in the Arkham House collection \"Beyond the Wall of Sleep\" (1943). Laney's essay (\"The Cthulhu Mythos\") was later republished in \"Crypt of Cthulhu #32\" (1985). In applying the elemental theory to beings that function on a cosmic scale (e.g., Yog-Sothoth) some authors created a fifth element that they termed \"aethyr\"."}
{"id": "5726", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=5726", "title": "Crane shot", "text": "In filmmaking and video production, a crane shot is a shot taken by a camera on a moving crane or jib. Most cranes accommodate both the camera and an operator, but some can be moved by remote control. Camera cranes go back to the dawn of movie-making, and were frequently used in silent films to enhance the epic nature of large sets and massive crowds. Another use is to move up and away from the actors, a common way of ending a movie. Crane shots are often found in what are supposed to be emotional or suspenseful scenes. One example of this technique is the shots taken by remote cranes in the car-chase sequence of the 1985 film \"To Live and Die in L.A.\". Some filmmakers place the camera on a boom arm simply to make it easier to move around between ordinary set-ups.\nTechnique.\nThe major supplier of cranes in the cinema of the United States throughout the 1940s, 1950s, and 1960s was the Chapman Company (later Chapman-Leonard of North Hollywood), supplanted by dozens of similar manufacturers around the world. The traditional design provided seats for both the director and the camera operator, and sometimes a third seat for the cinematographer as well. Large weights on the back of the crane compensate for the weight of the people riding the crane and must be adjusted carefully to avoid the possibility of accidents. During the 1960s, the tallest crane was the Chapman Titan crane, a massive design over 20 feet high that won an Academy Scientific &amp; Engineering award. Most such cranes were manually operated, requiring an experienced boom operator who knew how to vertically raise, lower, and \"crab\" the camera alongside actors while the crane platform rolled on separate tracks. The crane operator and camera operator had to precisely coordinate their moves so that focus, pan, and camera position all started and stopped at the same time, requiring great skill and rehearsal.\nTypes.\nCamera cranes may be small, medium, or large, depending on the load capacity and length of the loading arm. Historically, the first camera crane provided for lifting the chamber together with the operator, and sometimes an assistant. The range of motion of the boom was restricted because of the high load capacity and the need to ensure operator safety. In recent years a camera crane boom tripod with a remote control has become popular. It carries on the boom only a movie or television camera without an operator and allows shooting from difficult positions as a small load capacity makes it possible to achieve a long reach of the crane boom and relative freedom of movement. The operator controls the camera from the ground through a motorized panoramic head, using remote control and video surveillance by watching the image on the monitor. A separate category consists of telescopic camera cranes. These devices allow setting an arbitrary trajectory of the camera, eliminating the characteristic jib crane radial displacement that comes with traditional spanning shots. \nLarge camera cranes are almost indistinguishable from the usual boom-type cranes, with the exception of special equipment for smoothly moving the boom and controlling noise. Small camera cranes and crane-trucks have a lightweight construction, often without a mechanical drive. The valves are controlled manually by balancing the load-specific counterweight, facilitating manipulation. To improve usability and repeatability of movement of the crane in different takes, the axis of rotation arrows are provided with limbs and a pointer. In some cases, the camera crane is mounted on a dolly for even greater camera mobility. Such devices are called crane trolleys. In modern films robotic cranes allow use of multiple actuators for high-accuracy repeated movement of the camera in trick photography. These devices are called tap-robots; some sources use the term motion control.\nManufacturers.\nDuring the last few years, camera cranes have been miniaturized and costs have dropped so dramatically that most aspiring film makers have access to these tools. What was once a \"Hollywood\" effect is now available for under $400. Manufacturers of camera cranes include ABC-Products, Cambo, Filmotechnic, Polecam, Panther and Matthews Studio Equipment and Newton Nordic."}
{"id": "5729", "revid": "33548114", "url": "https://en.wikipedia.org/wiki?curid=5729", "title": "Chariots of Fire", "text": "Chariots of Fire is a 1981 British historical drama film. It is based on the true story of two British athletes in the 1924 Olympics: Eric Liddell: a devout Scottish Christian who runs for the glory of God, and Harold Abrahams, an English Jew who runs to overcome prejudice.\nThe film was conceived and produced by David Puttnam, written by Colin Welland, and directed by Hugh Hudson. Ben Cross and Ian Charleson starred as Abrahams and Liddell, alongside Nigel Havers, Ian Holm, Lindsay Anderson, John Gielgud, Cheryl Campbell, and Alice Krige in supporting roles. It was nominated for seven Academy Awards and won four, including Best Picture and Best Original Screenplay. It is ranked 19th in the British Film Institute's list of Top 100 British films. The film is also notable for its memorable electronic theme tune by Vangelis, who won the Academy Award for Best Original Score.\nThe film's title was inspired by the line \"Bring me my Chariot of fire!\" from the William Blake poem adapted into the British hymn \"Jerusalem\"; the hymn is heard at the end of the film. The original phrase \"chariot(s) of fire\" is from 2 Kings and in the Bible.\nPlot.\nIn 1919, Harold Abrahams (Ben Cross) enters the University of Cambridge, where he experiences anti-Semitism from the staff, but enjoys participating in the Gilbert and Sullivan club. He becomes the first person ever to complete the Trinity Great Court Run, running around the college courtyard in the time it takes for the clock to strike 12, and achieves an undefeated string of victories in various national running competitions. Although focused on his running, he falls in love with Sybil (Alice Krige), a leading Gilbert and Sullivan soprano.\nEric Liddell (Ian Charleson), born in China of Scottish missionary parents, is in Scotland. His devout sister Jennie (Cheryl Campbell) disapproves of Liddell's plans to pursue competitive running, but Liddell sees running as a way of glorifying God before returning to China to work as a missionary.\nWhen they first race against each other, Liddell beats Abrahams. Abrahams takes it poorly, but Sam Mussabini (Ian Holm), a professional trainer whom he had approached earlier, offers to take him on to improve his technique. This attracts criticism from the Cambridge college masters (John Gielgud and Lindsay Anderson), who allege it is not gentlemanly for an amateur to \"play the tradesman\" by employing a professional coach. Abrahams dismisses this concern, interpreting it as cover for anti-Semitic and class-based prejudice.\nWhen Liddell accidentally misses a church prayer meeting because of his running, his sister Jennie upbraids him and accuses him of no longer caring about God. Eric tells her that though he intends to return eventually to the China mission, he feels divinely inspired when running, and that not to run would be to dishonour God, saying \"I believe that God made me for a purpose. But He also made me fast, and when I run, I feel His pleasure.\"\nThe two athletes, after years of training and racing, are accepted to represent Great Britain in the 1924 Olympics in Paris. Also accepted are Abrahams' Cambridge friends, Lord Andrew Lindsay (Nigel Havers), Aubrey Montague (Nicholas Farrell), and Henry Stallard (Daniel Gerroll).\nWhile boarding the boat to France for the Olympics, Liddell discovers the heats for his 100-metre race will be on a Sunday. He refuses to run the race, despite strong pressure from the Prince of Wales and the British Olympic Committee, because his Christian convictions prevent him from running on the Sabbath.\nA solution is found thanks to Liddell's teammate Lindsay, who, having already won a silver medal in the 400 metres hurdles, offers to give his place in the 400-metre race on the following Thursday to Liddell, who gratefully acccepts. Liddell's religious convictions in the face of national athletic pride make headlines around the world.\nLiddell delivers a sermon at the Paris Church of Scotland that Sunday, and quotes from , ending with \"But they that wait upon the Lord shall renew their strength; they shall mount up with wings as eagles; they shall run, and not be weary; and they shall walk, and not faint.\"\nAbrahams is badly beaten by the heavily favoured United States runners in the 200 metre race. He knows his last chance for a medal will be the 100 metres. He competes in the race, and wins. His coach Sam Mussabini, who was barred from the stadium, is overcome that the years of dedication and training have paid off with an Olympic gold medal. Now Abrahams can get on with his life and reunite with his girlfriend Sybil, whom he had neglected for the sake of running.\nBefore Liddell's race, the American coach remarks dismissively to his runners that Liddell has little chance of doing well in his now, far longer, 400 metre race. But one of the American runners, Jackson Scholz, hands Liddell a note of support, quoting 1 Samuel 2:30 \"He that honors Me I will honor\". Liddell defeats the American favourites and wins the gold medal.\nThe British team returns home triumphant. As the film ends, onscreen text explains that Abrahams married Sybil and became the elder statesman of British athletics. Liddell went on to missionary work in China. All of Scotland mourned his death in 1945 in Japanese-occupied China.\nProduction.\nScreenplay.\nProducer David Puttnam was looking for a story in the mold of \"A Man for All Seasons\" (1966), regarding someone who follows his conscience, and felt sports provided clear situations in this sense. He discovered Eric Liddell's story by accident in 1977, when he happened upon a reference book on the Olympics while housebound from the flu in a rented house in Los Angeles.\nScreenwriter Colin Welland, commissioned by Puttnam, did an enormous amount of research for his Academy Award-winning script. Among other things, he took out advertisements in London newspapers seeking memories of the 1924 Olympics, went to the National Film Archives for pictures and footage of the 1924 Olympics, and interviewed everyone involved who was still alive. Welland just missed Abrahams, who died on 14 January 1978, but he did attend Abrahams' February 1978 memorial service, which inspired the present-day framing device of the film. Aubrey Montague's son saw Welland's newspaper ad and sent him copies of the letters his father had sent home \u2013 which gave Welland something to use as a narrative bridge in the film. Except for changes in the greetings of the letters from \"Darling Mummy\" to \"Dear Mum\" and the change from Oxford to Cambridge, all of the readings from Montague's letters are from the originals.\nWelland's original script also featured, in addition to Eric Liddell and Harold Abrahams, a third protagonist, 1924 Olympic gold medallist Douglas Lowe, who was presented as a privileged aristocratic athlete. However, Lowe refused to have anything to do with the film, and his character was written out and replaced by the fictional character of Lord Andrew Lindsay.\nInitial financing towards development costs was provided by Goldcrest Films, who then sold the project to Allied, but kept a percentage of the profits.\nIan Charleson wrote Eric Liddell's speech to the post-race workingmen's crowd at the Scotland v. Ireland races. Charleson, who had studied the Bible intensively in preparation for the role, told director Hugh Hudson that he didn't feel the portentous and sanctimonious scripted speech was either authentic or inspiring. Hudson and Welland allowed him to write words he personally found inspirational instead.\nPuttnam chose Hugh Hudson, a multiple award-winning advertising and documentary filmmaker who had never helmed a feature film, to direct \"Chariots of Fire\". Hudson and Puttnam had known each other since the 1960s, when Puttnam was an advertising executive and Hudson was making films for ad agencies. In 1977, Hudson had also been second-unit director on the Puttnam-produced film \"Midnight Express\".\nCasting.\nDirector Hugh Hudson was determined to cast young, unknown actors in all the major roles of the film, and to back them up by using veterans like John Gielgud, Lindsay Anderson, and Ian Holm as their supporting cast. Hudson and producer David Puttnam did months of fruitless searching for the perfect actor to play Eric Liddell. They then saw Scottish stage actor Ian Charleson performing the role of Pierre in the Royal Shakespeare Company's production of \"Piaf\", and knew immediately they had found their man. Unbeknownst to them, Charleson had heard about the film from his father, and desperately wanted to play the part, feeling it would \"fit like a kid glove\".\nBen Cross, who plays Harold Abrahams, was discovered while playing Billy Flynn in \"Chicago\". In addition to having a natural pugnaciousness, he had the desired ability to sing and play the piano. Cross was thrilled to be cast, and said he was moved to tears by the film's script.\n20th Century Fox, which put up half of the production budget in exchange for distribution rights outside of North America, insisted on having a couple of notable American names in the cast. Thus the small parts of the two American champion runners, Jackson Scholz and Charlie Paddock, were cast with recent headliners: Brad Davis had recently starred in \"Midnight Express\" (also produced by Puttnam), and Dennis Christopher had recently starred, as a young bicycle racer, in the popular indie film \"Breaking Away\".\nAll of the actors portraying runners underwent a gruelling three-month training intensive with renowned running coach Tom McNab. This training and isolation of the actors also created a strong bond and sense of camaraderie among them.\nFilming.\nThe beach scenes showing the athletes running towards the Carlton Hotel at Broadstairs, Kent, were shot in Scotland on West Sands, St Andrews next to the 18th hole of the Old Course at St Andrews Links. A plaque now commemorates the filming. The lasting impact of these iconic scenes (as the athletes run in slow motion to Vangelis's music) prompted Broadstairs town council to commemorate them with their own seafront plaque.\nAll of the Cambridge scenes were actually filmed at Hugh Hudson's alma mater Eton College, because Cambridge refused filming rights, fearing depictions of anti-Semitism. The Cambridge administration greatly regretted the decision after the film's enormous success.\nLiverpool Town Hall was the setting for the scenes depicting the British Embassy in Paris. The Colombes Olympic Stadium in Paris was represented by the Oval Sports Centre, Bebington, Merseyside. The nearby Woodside ferry terminal was used to represent the embarkation scenes set in Dover. The railway station scenes were filmed in York, using locomotives from the National Railway Museum. The scene depicting a performance of \"The Mikado\" was filmed in the Royal Court Theatre, Liverpool, with members of the D'Oyly Carte Opera Company who were on tour.\nEditing.\nThe film was slightly altered for the U.S. audience. A brief scene depicting a pre-Olympics cricket game between Abrahams, Liddell, Montague, and the rest of the British track team appears shortly after the beginning of the original film. For the American audience, this brief scene was deleted. In the U.S., to avoid the initial G rating, which had been strongly associated with children's films and might have hindered box office sales, a different scene was used \u2013 one depicting Abrahams and Montague arriving at a Cambridge railway station and encountering two First World War veterans who use an obscenity \u2013 in order to be given a PG rating.\nSoundtrack.\nAlthough the film is a period piece, set in the 1920s, the Academy Award-winning original soundtrack composed by Vangelis uses a modern 1980s electronic sound, with a strong use of synthesizer and piano among other instruments. This was a bold and significant departure from earlier period films, which employed sweeping orchestral instrumentals. The title theme of the film has become iconic, and has been used in subsequent films and television shows during slow-motion segments.\nVangelis, a Greek-born electronic composer who moved to Paris in the late 1960s, had been living in London since 1974. Director Hugh Hudson had collaborated with him on documentaries and commercials, and was also particularly impressed with his 1979 albums \"Opera Sauvage\" and \"China\". David Puttnam also greatly admired Vangelis's body of work, having originally selected his compositions for his previous film \"Midnight Express\". Hudson made the choice for Vangelis and for a modern score: \"I knew we needed a piece which was anachronistic to the period to give it a feel of modernity. It was a risky idea but we went with it rather than have a period symphonic score.\" The soundtrack had a personal significance to Vangelis: After composing the iconic theme tune he told Puttnam, \"My father is a runner, and this is an anthem to him.\"\nHudson originally wanted Vangelis's 1977 tune \"L'Enfant\", from his \"Opera Sauvage\" album, to be the title theme of the film, and the beach running sequence was actually filmed with \"L'Enfant\" playing on loudspeakers for the runners to pace to. Vangelis finally convinced Hudson he could create a new and better piece for the film's main theme \u2013 and when he played the now-iconic \"Chariots of Fire\" theme for Hudson, it was agreed the new tune was unquestionably better. The \"L'Enfant\" melody still made it into the film: when the athletes reach Paris and enter the stadium, a brass band marches through the field, and first plays a modified, acoustic performance of the piece. Vangelis's electronic \"L'Enfant\" track eventually was used prominently in the 1982 film \"The Year of Living Dangerously\".\nSome pieces of Vangelis's music in the film did not end up on the film's soundtrack album. One of them is the background music to the race Eric Liddell runs in the Scottish highlands. This piece is a version of \"Hymne\", the original version of which appears on Vangelis's 1979 album, \"Op\u00e9ra sauvage\". Various versions are also included on Vangelis's compilation albums \"Themes\", \"Portraits\", and \"\", though none of these include the version used in the film.\nFive lively Gilbert and Sullivan tunes also appear in the soundtrack, and serve as jaunty period music which counterpoints Vangelis's modern electronic score. These are: \"He is an Englishman\" from \"H.M.S. Pinafore\", \"Three Little Maids from School Are We\" from \"The Mikado\", \"With Catlike Tread\" from \"The Pirates of Penzance\", \"The Soldiers of Our Queen\" from \"Patience\", and \"There Lived a King\" from \"The Gondoliers\".\nThe film also incorporates a major traditional work: \"Jerusalem\", sung by a British choir at the 1978 funeral of Harold Abrahams. The words, written by William Blake in 1804\u201308, were set to music by Parry in 1916 as a celebration of England. This hymn has been described as \"England's unofficial national anthem\", concludes the film and inspired its title. A handful of other traditional anthems and hymns and period-appropriate instrumental ballroom-dance music round out the film's soundtrack.\nReception.\nSince its release, \"Chariots of Fire\" has received generally positive reviews from critics. , the film holds an 82% \"Certified Fresh\" rating on the review aggregator website Rotten Tomatoes, based on 73 reviews, with a weighted average of 7.53/10. The site's consensus reads: \"Decidedly slower and less limber than the Olympic runners at the center of its story, the film nevertheless manages to make effectively stirring use of its spiritual and patriotic themes.\" On Metacritic, the film has a score of 78 out of 100 based on 19 critics' reviews, indicating \"generally favorable reviews\".\nFor its 2012 re-release, Kate Muir of \"The Times\" gave the film five stars, writing: \"In a time when drug tests and synthetic fibres have replaced gumption and moral fibre, the tale of two runners competing against each other in the 1924 Olympics has a simple, undiminished power. From the opening scene of pale young men racing barefoot along the beach, full of hope and elation, backed by Vangelis's now famous anthem, the film is utterly compelling.\"\nThe film was the highest-grossing British film for the year with theatrical rentals of \u00a31,859,480.\nAccolades.\n\"Chariots of Fire\" was very successful at the 54th Academy Awards, winning four of seven nominations. When accepting his Oscar for Best Original Screenplay, Colin Welland famously announced \"The British are coming\". It was the first film released by Warner Bros. to win Best Picture since the 1959 version of Ben-Hur over two decades earlier. At the 1981 Cannes Film Festival the film won two awards and competed for the Palme d'Or.\nAmerican Film Institute recognition\nHistorical differences.\n\"Chariots of Fire\" is a film about achieving victory through self sacrifice and moral courage. While the producers' intent was to make a cinematic work that was historically authentic, the film was not intended to be historically accurate. Numerous liberties were taken with the actual historical chronology, the inclusion and exclusion of notable people, and the creation of fictional scenes for dramatic purpose, plot pacing and exposition.\nCharacters.\nThe film depicts Abrahams as attending Gonville and Caius College, Cambridge with three other Olympic athletes: Henry Stallard, Aubrey Montague, and Lord Andrew Lindsay. Abrahams and Stallard were in fact students there and competed in the 1924 Olympics. Montague also competed in the Olympics as depicted, but he attended Oxford, not Cambridge. Aubrey Montague sent daily letters to his mother about his time at Oxford and the Olympics; these letters were the basis of Montague's narration in the film.\nThe character of Lindsay was based partially on Lord Burghley, a significant figure in the history of British athletics. Although Burghley did attend Cambridge, he was not a contemporary of Harold Abrahams, as Abrahams was an undergraduate from 1919 to 1923 and Burghley was at Cambridge from 1923 to 1927. One scene in the film depicts the Burghley-based \"Lindsay\" as practising hurdles on his estate with full champagne glasses placed on each hurdle \u2013 this was something the wealthy Burghley did, although he used matchboxes instead of champagne glasses. The fictional character of Lindsay was created when Douglas Lowe, who was Britain's third athletics gold medallist in the 1924 Olympics, was not willing to be involved with the film.\nAnother scene in the film recreates the Great Court Run, in which the runners attempt to run around the perimeter of the Great Court at Trinity College, Cambridge in the time it takes the clock to strike 12 at midday. The film shows Abrahams performing the feat for the first time in history. In fact, Abrahams never attempted this race, and at the time of filming the only person on record known to have succeeded was Lord Burghley, in 1927. In \"Chariots of Fire\", Lindsay, who is based on Lord Burghley, runs the Great Court Run with Abrahams in order to spur him on, and crosses the finish line just a moment too late. Since the film's release, the Great Court Run has also been successfully run by Trinity undergraduate Sam Dobin, in October 2007.\nIn the film, Eric Liddell is tripped up by a Frenchman in the 400-metre event of a Scotland\u2013France international athletic meeting. He recovers, makes up a 20-metre deficit, and wins. This was based on fact; the actual race was the 440 yards at a Triangular Contest meet between Scotland, England, and Ireland at Stoke-on-Trent in England in July 1923. His achievement was remarkable as he had already won the 100- and 220-yard events that day. Also unmentioned with regard to Liddell is that it was he who introduced Abrahams to Sam Mussabini. This is alluded to: In the film Abrahams first encounters Mussabini while he is watching Liddell race.\nAbrahams and Liddell did race against each other once, but not quite as depicted in the film, which shows Liddell winning the final of the 100 yards against a shattered Abrahams at the 1923 AAA Championship at Stamford Bridge. In fact, they raced only in a heat of the 220 yards, which Liddell won, five yards ahead of Abrahams, who did not progress to the final. In the 100 yards, Abrahams was eliminated in the heats and never raced against Liddell, who won the finals of both races the next day.\nAbrahams' fianc\u00e9e is misidentified as Sybil Gordon, a soprano at the D'Oyly Carte Opera Company. In fact, in 1936, Abrahams married Sybil Evers, who sang at the D'Oyly Carte, but they did not meet until 1934. Also, in the film, Sybil is depicted as singing the role of Yum-Yum in \"The Mikado\", but neither Sybil Gordon nor Sybil Evers ever sang that role with D'Oyly Carte, although Evers was known for her charm in singing Peep-Bo, one of the two other \"little maids from school\". Harold Abrahams' love of and heavy involvement with Gilbert and Sullivan, as depicted in the film, is factual.\nLiddell's sister was several years younger than she was portrayed in the film. Her disapproval of Liddell's track career was creative licence; she actually fully supported his sporting work. Jenny Liddell Somerville cooperated fully with the making of the film and has a brief cameo in the Paris Church of Scotland during Liddell's sermon.\nAt the memorial service for Harold Abrahams, which opens the film, Lord Lindsay mentions that he and Aubrey Montague are the only members of the 1924 Olympic team still alive. However, Montague died in 1948, 30 years before Abrahams' death.\nParis Olympics 1924.\nIn the film, the 100m bronze medallist is a character called \"Tom Watson\"; the real medallist was Arthur Porritt of New Zealand, who refused permission for his name to be used in the film, allegedly out of modesty, and his wish was accepted by the film's producers, even though his permission was not necessary. However, the brief back-story given for Watson, who is called up to the New Zealand team from the University of Oxford, substantially matches Porritt's history. With the exception of Porritt, all the runners in the 100m final are identified correctly when they line up for inspection by the Prince of Wales.\nJackson Scholz is depicted as handing Liddell an inspirational Bible-quotation message before the 400 metres final: \"It says in the Old Book, 'He that honors me, I will honor.' Good luck.\" In reality, the note was from members of the British team, and was handed to Liddell before the race by his attending masseur at the team's Paris hotel. For dramatic purposes, screenwriter Welland asked Scholz if he could be depicted handing the note, and Scholz readily agreed, saying \"Yes, great, as long as it makes me look good.\"\nThe events surrounding Liddell's refusal to race on a Sunday are fictional. In the film, he does not learn that the 100-metre heat is to be held on the Christian Sabbath until he is boarding the boat to Paris. In fact, the schedule was made public several months in advance; Liddell did however face immense pressure to run on that Sunday and to compete in the 100 metres, getting called before a grilling by the British Olympic Committee, the Prince of Wales, and other grandees, and his refusal to run made headlines around the world.\nThe decision to change races was, even so, made well before embarking to Paris, and Liddell spent the intervening months training for the 400 metres, an event in which he had previously excelled. It is true, nonetheless, that Liddell's success in the Olympic 400m was largely unexpected.\nThe film depicts Lindsay, having already won a medal in the 400-metre hurdles, giving up his place in the 400-metre race for Liddell. In fact Burghley, on whom Lindsay is loosely based, was eliminated in the heats of the 110 hurdles (he would go on to win a gold medal in the 400 hurdles at the 1928 Olympics), and was not entered for the 400 metres.\nThe film reverses the order of Abrahams' 100m and 200m races at the Olympics. In reality, after winning the 100 metres race, Abrahams ran the 200 metres but finished last, Jackson Scholz taking the gold medal. In the film, before his triumph in the 100m, Abrahams is shown losing the 200m and being scolded by Mussabini. And during the following scene in which Abrahams speaks with his friend Montague while receiving a massage from Mussabini, there is a French newspaper clipping showing Scholz and Charley Paddock with a headline which states that the 200 metres was a triumph for the United States. In the same conversation, Abrahams laments getting \"beaten out of sight\" in the 200. The film thus has Abrahams overcoming the disappointment of losing the 200 by going on to win the 100, a reversal of the real order.\nEric Liddell actually also ran in the 200m race, and finished third, behind Paddock and Scholz. This was the only time in reality that Liddell and Abrahams competed in the same race. While their meeting in the 1923 AAA Championship in the film was fictitious, Liddell's record win in that race did spur Abrahams to train even harder.\nAbrahams also won a silver medal as an opening runner for the 4 x 100 metres relay team, not shown in the film, and Aubrey Montague placed sixth in the steeplechase, as depicted.\nLondon Olympics' 2012 revival.\n\"Chariots of Fire\" became a recurring theme in promotions for the 2012 Summer Olympics in London. The film's theme tune was featured at the opening of the 2012 London New Years fireworks celebrating the Olympics, and the film's iconic beach-running scene and theme tune were used in \"The Sun\"'s \"Let's Make It Great, Britain\" Olympic ads. The runners who first tested the new Olympic Park were spurred on by the \"Chariots of Fire\" theme tune, and the iconic music was also used to fanfare the carriers of the Olympic flame on parts of its route through the UK. The beach-running sequence was also recreated at St. Andrews and filmed as part of the Olympic torch relay.\nThe film's theme was also performed by the London Symphony Orchestra, conducted by Simon Rattle, during the Opening Ceremony of the games; the performance was accompanied by a comedy skit by Rowan Atkinson (as Mr Bean) which included the opening beach-running footage from the film. The film's theme tune was also played during each medal ceremony of the 2012 Olympics.\nAs an official part of the London 2012 Festival celebrations, a new digitally re-mastered version of the film screened in 150 cinemas throughout the UK. The re-release began 13 July 2012, two weeks before the opening ceremony of the London Olympics.\nA Blu-ray of the film was released on 10 July 2012 in North America, and was released 16 July 2012 in the UK. The release includes nearly an hour of special features, a CD sampler, and a 32-page \"digibook\".\nStage adaptation.\nA stage adaptation of \"Chariots of Fire\" was mounted in honour of the 2012 Olympics. The play, \"Chariots of Fire\", which was adapted by playwright Mike Bartlett and included the iconic Vangelis score, ran from 9 May to 16 June 2012 at London's Hampstead Theatre, and transferred to the Gielgud Theatre in the West End on 23 June, where it ran until 5 January 2013. It starred Jack Lowden as Eric Liddell and James McArdle as Harold Abrahams, and Edward Hall directed. Stage designer Miriam Buether transformed each theatre into an Olympic stadium, and composer Jason Carr wrote additional music. Vangelis also created several new pieces of music for the production. The stage version for the London Olympic year was the idea of the film's director, Hugh Hudson, who co-produced the play; he stated, \"Issues of faith, of refusal to compromise, standing up for one's beliefs, achieving something for the sake of it, with passion, and not just for fame or financial gain, are even more vital today.\"\nAnother play, \"Running for Glory\", written by Philip Dart, based on the 1924 Olympics, and focusing on Abrahams and Liddell, toured parts of Britain from 25 February to 1 April 2012. It starred Nicholas Jacobs as Harold Abrahams, and Tom Micklem as Eric Liddell."}
{"id": "5731", "revid": "236191", "url": "https://en.wikipedia.org/wiki?curid=5731", "title": "Capitalist", "text": ""}
{"id": "5734", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5734", "title": "Consequentialism", "text": "Consequentialism is a class of normative, teleological ethical theories that holds that the consequences of one's conduct are the ultimate basis for any judgment about the rightness or wrongness of that conduct. Thus, from a consequentialist standpoint, a morally right act (or omission from acting) is one that will produce a good outcome. Consequentialism, along with eudaimonism, falls under the broader category of teleological ethics, a group of views which claim that the moral value of any act consists in its tendency to produce things of intrinsic value. Consequentialists hold in general that an act is right \"if and only if\" the act (or on some views, the rule under which it falls) will produce, will probably produce, or is intended to produce, a greater balance of good over evil than any available alternative. Different consequentialist theories differ in how they define moral goods, with chief candidates including pleasure, the absence of pain, the satisfaction of one's preferences, and broader notions of the \"general good\".\nConsequentialism is usually contrasted with deontological ethics (or \"deontology\"), in that deontology, in which rules and moral duty are central, derives the rightness or wrongness of one's conduct from the character of the behaviour itself rather than the outcomes of the conduct. It is also contrasted with virtue ethics, which focuses on the character of the agent rather than on the nature or consequences of the act (or omission) itself, and pragmatic ethics which treats morality like science: advancing socially over the course of many lifetimes, such that any moral criterion is subject to revision.\nSome argue that consequentialist theories (such as utilitarianism) and deontological theories (such as Kantian ethics) are not necessarily mutually exclusive. For example, T. M. Scanlon advances the idea that human rights, which are commonly considered a \"deontological\" concept, can only be justified with reference to the consequences of having those rights. Similarly, Robert Nozick argued for a theory that is mostly consequentialist, but incorporates inviolable \"side-constraints\" which restrict the sort of actions agents are permitted to do. Derek Parfit argued that in practice, when understood properly, rule consequentialism, Kantian deontology and contractualism would all end up prescribing the same behavior.\nForms of consequentialism.\nUtilitarianism.\nIn summary, Jeremy Bentham states that people are driven by their interests and their fears, but their interests take precedence over their fears; their interests are carried out in accordance with how people view the consequences that might be involved with their interests. \"Happiness\", in this account, is defined as the maximization of pleasure and the minimization of pain. It can be argued that the existence of phenomenal consciousness and \"qualia\" is required for the experience of pleasure or pain to have an ethical significance.\nHistorically, \"hedonistic utilitarianism\" is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that what matters is the aggregate happiness; the happiness of everyone, and not the happiness of any particular person. John Stuart Mill, in his exposition of hedonistic utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. However, some contemporary utilitarians, such as Peter Singer, are concerned with maximizing the satisfaction of preferences, hence \"preference utilitarianism\". Other contemporary forms of utilitarianism mirror the forms of consequentialism outlined below.\nRule consequentialism.\nIn general, consequentialist theories focus on actions. However, this need not be the case. Rule consequentialism is a theory that is sometimes seen as an attempt to reconcile consequentialism with deontology, or rules-based ethics\u2014and in some cases, this is stated as a criticism of rule consequentialism. Like deontology, rule consequentialism holds that moral behavior involves following certain rules. However, rule consequentialism chooses rules based on the consequences that the selection of those rules has. Rule consequentialism exists in the forms of rule utilitarianism and rule egoism.\nVarious theorists are split as to whether the rules are the only determinant of moral behavior or not. For example, Robert Nozick held that a certain set of minimal rules, which he calls \"side-constraints,\" are necessary to ensure appropriate actions. There are also differences as to how absolute these moral rules are. Thus, while Nozick's side-constraints are absolute restrictions on behavior, Amartya Sen proposes a theory that recognizes the importance of certain rules, but these rules are not absolute. That is, they may be violated if strict adherence to the rule would lead to much more undesirable consequences.\nOne of the most common objections to rule-consequentialism is that it is incoherent, because it is based on the consequentialist principle that what we should be concerned with is maximizing the good, but then it tells us not to act to maximize the good, but to follow rules (even in cases where we know that breaking the rule could produce better results).\nIn \"Ideal Code, Real World\", Brad Hooker avoids this objection by not basing his form of rule-consequentialism on the ideal of maximizing the good. He writes:\n[T]he best argument for rule-consequentialism is not that it derives from an overarching commitment to maximise the good. The best argument for rule-consequentialism is that it does a better job than its rivals of matching and tying together our moral convictions, as well as offering us help with our moral disagreements and uncertainties.\nDerek Parfit described Hooker's book as the \"best statement and defence, so far, of one of the most important moral theories.\"\nState consequentialism.\n\"State consequentialism\", also known as \"Mohist consequentialism\", is an ethical theory that evaluates the moral worth of an action based on how much it contributes to the welfare of a state. According to the \"Stanford Encyclopedia of Philosophy\", Mohist consequentialism, dating back to the 5th century BCE, is the \"world's earliest form of consequentialism, a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare.\"\nUnlike utilitarianism, which views utility as the sole moral good, \"the basic goods in Mohist consequentialist thinking are...order, material wealth, and increase in population.\" During the time of Mozi, war and famine were common, and population growth was seen as a moral necessity for a harmonious society. The \"material wealth\" of Mohist consequentialism refers to basic needs, like shelter and clothing; and \"order\" refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability. In \"The Cambridge History of Ancient China\", Stanford sinologist David Shepherd Nivison writes that the moral goods of Mohism \"are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth...if people have plenty, they would be good, filial, kind, and so on unproblematically.\"\nThe Mohists believed that morality is based on \"promoting the benefit of all under heaven and eliminating harm to all under heaven.\" In contrast to Jeremy Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain. The term \"state consequentialism\" has also been applied to the political philosophy of the Confucian philosopher Xunzi. On the other hand, \"legalist\" Han Fei \"is motivated almost totally from the ruler's point of view.\"\nEthical egoism.\nEthical egoism can be understood as a consequentialist theory according to which the consequences for the individual agent are taken to matter more than any other result. Thus, egoism will prescribe actions that may be beneficial, detrimental, or neutral to the welfare of others. Some, like Henry Sidgwick, argue that a certain degree of egoism \"promotes\" the general welfare of society for two reasons: because individuals know how to please themselves best, and because if everyone were an austere altruist then general welfare would inevitably decrease.\nEthical altruism.\nEthical altruism can be seen as a consequentialist theory which prescribes that an individual take actions that have the best consequences for everyone except for himself. This was advocated by Auguste Comte, who coined the term \"altruism\", and whose ethics can be summed up in the phrase \"Live for others.\"\nTwo-level consequentialism.\nThe two-level approach involves engaging in critical reasoning and considering all the possible ramifications of one's actions before making an ethical decision, but reverting to generally reliable moral rules when one is not in a position to stand back and examine the dilemma as a whole. In practice, this equates to adhering to rule consequentialism when one can only reason on an intuitive level, and to act consequentialism when in a position to stand back and reason on a more critical level.\nThis position can be described as a reconciliation between \"act consequentialism\"\u2014in which the morality of an action is determined by that action's effects\u2014and \"rule consequentialism\"\u2014in which moral behavior is derived from following rules that lead to positive outcomes.\nThe two-level approach to consequentialism is most often associated with R. M. Hare and Peter Singer.\nMotive consequentialism.\nAnother consequentialist version is motive consequentialism, which looks at whether the state of affairs that results from the motive to choose an action is better or at least as good as each of the alternative state of affairs that would have resulted from alternative actions. This version gives relevance to the motive of an act and links it to its consequences. An act can therefore not be wrong if the decision to act was based on a right motive. A possible inference is, that one can not be blamed for mistaken judgments if the motivation was to do good.\nNegative consequentialism.\nMost consequentialist theories focus on \"promoting\" some sort of good consequences. However, negative utilitarianism lays out a consequentialist theory that focuses solely on minimizing bad consequences.\nOne major difference between these two approaches is the agent's responsibility. \"Positive\" consequentialism demands that we bring about good states of affairs, whereas \"negative\" consequentialism requires that we avoid bad ones. Stronger versions of negative consequentialism will require active intervention to prevent bad and ameliorate existing harm. In weaker versions, simple forbearance from acts tending to harm others is sufficient. An example of this is the slippery-slope argument, which encourages others to avoid a specified act on the grounds that it may ultimately lead to undesirable consequences.\nOften \"negative\" consequentialist theories assert that reducing suffering is more important than increasing pleasure. Karl Popper, for example, claimed that \"from the moral point of view, pain cannot be outweighed by pleasure.\" (While Popper is not a consequentialist per se, this is taken as a classic statement of negative utilitarianism.) When considering a theory of justice, negative consequentialists may use a statewide or global-reaching principle: the reduction of suffering (for the disadvantaged) is more valuable than increased pleasure (for the affluent or luxurious).\nActs and omissions.\nSince pure consequentialism holds that an action is to be judged solely by its result, most consequentialist theories hold that a deliberate action is no different from a deliberate decision not to act. This contrasts with the \"acts and omissions doctrine\", which is upheld by some medical ethicists and some religions: it asserts there is a significant moral distinction between acts and deliberate non-actions which lead to the same outcome. This contrast is brought out in issues such as voluntary euthanasia.\nActualism and possibilism.\nThe normative status of an action depends on its consequences according to consequentialism. The consequences of the actions of an agent may include other actions by this agent. Actualism and possibilism disagree on how later possible actions impact the normative status of the current action by the same agent. Actualists assert that it is only relevant what the agent \"would\" actually do later for assessing the value of an alternative. Possibilists, on the other hand, hold that we should also take into account what the agent \"could\" do, even if she wouldn't do it.\nFor example, assume that Gifre has the choice between two alternatives, eating a cookie or not eating anything. Having eaten the first cookie, Gifre could stop eating cookies, which is the best alternative. But after having tasted one cookie, Gifre would freely decide to continue eating cookies until the whole bag is finished, which would result in a terrible stomach ache and would be the worst alternative. Not eating any cookies at all, on the other hand, would be the second-best alternative. Now the question is: should Gifre eat the first cookie or not? Actualists are only concerned with the actual consequences. According to them, Gifre should not eat any cookies at all since it is better than the alternative leading to a stomach ache. Possibilists, however, contend that the best possible course of action involves eating the first cookie and this is therefore what Gifre should do.\nOne counterintuitive consequence of actualism is that agents can avoid moral obligations simply by having an imperfect moral character. For example, a lazy person might justify rejecting a request to help a friend by arguing that, due to her lazy character, she wouldn't have done the work anyway, even if she had accepted the request. By rejecting the offer right away, she managed at least not to waste anyone's time. Actualists might even consider her behavior praiseworthy since she did what, according to actualism, she ought to have done. This seems to be a very easy way to \"get off the hook\" that is avoided by possibilism. But possibilism has to face the objection that in some cases it sanctions and even recommends what actually leads to the worst outcome.\nDouglas W. Portmore has suggested that these and other problems of actualism and possibilism can be avoided by constraining what counts as a genuine alternative for the agent. On his view, it is a requirement that the agent has rational control over the event in question. For example, eating only one cookie and stopping afterward only is an option for Gifre if she has the rational capacity to repress her temptation to continue eating. If the temptation is irrepressible then this course of action is not considered to be an option and is therefore not relevant when assessing what the best alternative is. Portmore suggests that, given this adjustment, we should prefer a view very closely associated with \"possibilism\" called \"maximalism\".\nIssues.\nAction guidance.\nOne important characteristic of many normative moral theories such as consequentialism is the ability to produce practical moral judgements. At the very least, any moral theory needs to define the standpoint from which the goodness of the consequences are to be determined. What is primarily at stake here is the \"responsibility\" of the agent.\nThe ideal observer.\nOne common tactic among consequentialists, particularly those committed to an altruistic (selfless) account of consequentialism, is to employ an ideal, neutral observer from which moral judgements can be made. John Rawls, a critic of utilitarianism, argues that utilitarianism, in common with other forms of consequentialism, relies on the perspective of such an ideal observer. The particular characteristics of this ideal observer can vary from an omniscient observer, who would grasp all the consequences of any action, to an ideally informed observer, who knows as much as could reasonably be expected, but not necessarily all the circumstances or all the possible consequences. Consequentialist theories that adopt this paradigm hold that right action is the action that will bring about the best consequences from this ideal observer's perspective.\nThe real observer.\nIn practice, it is very difficult, and at times arguably impossible, to adopt the point of view of an ideal observer. Individual moral agents do not know everything about their particular situations, and thus do not know all the possible consequences of their potential actions. For this reason, some theorists have argued that consequentialist theories can only require agents to choose the best action in line with what they know about the situation. However, if this approach is na\u00efvely adopted, then moral agents who, for example, recklessly fail to reflect on their situation, and act in a way that brings about terrible results, could be said to be acting in a morally justifiable way. Acting in a situation without first informing oneself of the circumstances of the situation can lead to even the most well-intended actions yielding miserable consequences. As a result, it could be argued that there is a moral imperative for an agent to inform himself as much as possible about a situation before judging the appropriate course of action. This imperative, of course, is derived from consequential thinking: a better-informed agent is able to bring about better consequences.\nConsequences for whom.\nMoral action always has consequences for certain people or things. Varieties of consequentialism can be differentiated by the beneficiary of the good consequences. That is, one might ask \"Consequences for whom?\"\nAgent-focused or agent-neutral.\nA fundamental distinction can be drawn between theories which require that agents act for ends perhaps disconnected from their own interests and drives, and theories which permit that agents act for ends in which they have some personal interest or motivation. These are called \"agent-neutral\" and \"agent-focused\" theories respectively.\nAgent-neutral consequentialism ignores the specific value a state of affairs has for any particular agent. Thus, in an agent-neutral theory, an actor's personal goals do not count any more than anyone else's goals in evaluating what action the actor should take. Agent-focused consequentialism, on the other hand, focuses on the particular needs of the moral agent. Thus, in an agent-focused account, such as one that Peter Railton outlines, the agent might be concerned with the general welfare, but the agent is \"more\" concerned with the immediate welfare of herself and her friends and family.\nThese two approaches could be reconciled by acknowledging the tension between an agent's interests as an individual and as a member of various groups, and seeking to somehow optimize among all of these interests. For example, it may be meaningful to speak of an action as being good for someone as an individual, but bad for them as a citizen of their town.\nHuman-centered?\nMany consequentialist theories may seem primarily concerned with human beings and their relationships with other human beings. However, some philosophers argue that we should not limit our ethical consideration to the interests of human beings alone. Jeremy Bentham, who is regarded as the founder of utilitarianism, argues that animals can experience pleasure and pain, thus demanding that 'non-human animals' should be a serious object of moral concern.\nMore recently, Peter Singer has argued that it is unreasonable that we do not give equal consideration to the interests of animals as to those of human beings when we choose the way we are to treat them. Such equal consideration does not necessarily imply identical treatment of humans and non-humans, any more than it necessarily implies identical treatment of all humans.\nValue of consequences.\nOne way to divide various consequentialisms is by the types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase in pleasure, and the best action is one that results in the most pleasure for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral \"pleasure\". Other theories adopt a package of several goods, all to be promoted equally. As the consequentialist approach contains an inherent assumption that the outcomes of a moral decision can be quantified in terms of \"goodness\" or \"badness,\" or at least put in order of increasing preference, it is an especially suited moral theory for a probabilistic and decision theoretical approach.\nVirtue ethics.\nConsequentialism can also be contrasted with aretaic moral theories such as virtue ethics. Whereas consequentialist theories posit that consequences of action should be the primary focus of our thinking about ethics, virtue ethics insists that it is the character rather than the consequences of actions that should be the focal point. Some virtue ethicists hold that consequentialist theories totally disregard the development and importance of moral character. For example, Philippa Foot argues that consequences in themselves have no ethical content, unless it has been provided by a virtue such as benevolence.\nHowever, consequentialism and virtue ethics need not be entirely antagonistic. Iain King has developed an approach that reconciles the two schools. Other consequentialists consider effects on the character of people involved in an action when assessing consequence. Similarly, a consequentialist theory may aim at the maximization of a particular virtue or set of virtues. Finally, following Foot's lead, one might adopt a sort of consequentialism that argues that virtuous activity ultimately produces the best consequences.\nUltimate end.\nThe \"ultimate end\" is a concept in the moral philosophy of Max Weber, in which individuals act in a faithful, rather than rational, manner.\nTeleological ethics.\nTeleological ethics (Greek: \"telos\", 'end, purpose' + \"logos\", 'science') is a broader class of views in moral philosophy which consequentialism falls under. In general, proponents of teleological ethics argue that the moral value of any act consists in its tendency to produce things of intrinsic value, meaning that an act is right \"if and only if\" it, or the rule under which it falls, produces, will probably produce, or is intended to produce, a greater balance of good over evil than any alternative act. This concept is exemplified by the famous aphorism, \"the end justifies the means,\" i.e. if a goal is morally important enough, any method of achieving it is acceptable.\nTeleological theories differ among themselves on the nature of the particular end that actions ought to promote. The two major families of views in teleological ethics are \"consequentialism\" and \"virtue ethics\". Teleological ethical theories are often discussed in opposition to deontological ethical theories, which hold that acts themselves are \"inherently\" good or bad, rather than good or bad because of extrinsic factors (such as the act's consequences or the moral character of the person who acts).\nEtymology.\nThe term \"consequentialism\" was coined by G. E. M. Anscombe in her essay \"Modern Moral Philosophy\" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick.\nThe phrase and concept of \"the end justifies the means\" are at least as old as the first century BC. Ovid wrote in his \"Heroides\" that \"Exitus acta probat\" (\"The result justifies the deed\").\nCriticisms.\nG. E. M. Anscombe objects to the consequentialism of Sidgwick on the grounds that the moral worth of an action is premised on the predictive capabilities of the individual, relieving them of the responsibility for the \"badness\" of an act should they \"make out a case for not having foreseen\" negative consequences.\nThe future amplification of the effects of small decisions is an important factor that makes it more difficult to predict the ethical value of consequences, even though most would agree that only predictable consequences are charged with a moral responsibility.\nBernard Williams has argued that consequentialism is alienating because it requires moral agents to put too much distance between themselves and their own projects and commitments. Williams argues that consequentialism requires moral agents to take a strictly impersonal view of all actions, since it is only the consequences, and not who produces them, that are said to matter. Williams argues that this demands too much of moral agents\u2014since (he claims) consequentialism demands that they be willing to sacrifice any and all personal projects and commitments in any given circumstance in order to pursue the most beneficent course of action possible. He argues further that consequentialism fails to make sense of intuitions that it can matter whether or not someone is personally the author of a particular consequence. For example, that participating in a crime can matter, even if the crime would have been committed anyway, or would even have been worse, without the agent's participation.\nSome consequentialists\u2014most notably Peter Railton\u2014have attempted to develop a form of consequentialism that acknowledges and avoids the objections raised by Williams. Railton argues that Williams's criticisms can be avoided by adopting a form of consequentialism in which moral decisions are to be determined by the sort of life that they express. On his account, the agent should choose the sort of life that will, on the whole, produce the best overall effects."}
{"id": "5735", "revid": "1012049806", "url": "https://en.wikipedia.org/wiki?curid=5735", "title": "Conscription", "text": "Conscription, sometimes called the draft in the United States, is the mandatory enlistment of people in a national service, most often a military service. Conscription dates back to antiquity and it continues in some countries to the present day under various names. The modern system of near-universal national conscription for young men dates to the French Revolution in the 1790s, where it became the basis of a very large and powerful military. Most European nations later copied the system in peacetime, so that men at a certain age would serve 1\u20138 years on active duty and then transfer to the reserve force.\nConscription is controversial for a range of reasons, including conscientious objection to military engagements on religious or philosophical grounds; political objection, for example to service for a disliked government or unpopular war; sexism, in that historically only men have been subject to the draft; and ideological objection, for example, to a perceived violation of individual rights. Those conscripted may evade service, sometimes by leaving the country, and seeking asylum in another country. Some selection systems accommodate these attitudes by providing alternative service outside combat-operations roles or even outside the military, such as \"Siviilipalvelus\" (alternative civil service) in Finland, \"Zivildienst\" (compulsory community service) in Austria and Switzerland. Several countries conscript male soldiers not only for armed forces, but also for paramilitary agencies, which are dedicated to police-like \"domestic only\" service like internal troops, border guards or \"non-combat\" rescue duties like civil defence.\nAs of the early 21st century, many states no longer conscript soldiers, relying instead upon professional militaries with volunteers. The ability to rely on such an arrangement, however, presupposes some degree of predictability with regard to both war-fighting requirements and the scope of hostilities. Many states that have abolished conscription still, therefore, reserve the power to resume conscription during wartime or times of crisis. States involved in wars or interstate rivalries are most likely to implement conscription, and democracies are less likely than autocracies to implement conscription. With a few exceptions, such as Singapore and Egypt, former British colonies are less likely to have conscription, as they are influenced by British anti-conscription norms that can be traced back to the English Civil War; the United Kingdom abolished conscription in 1960.\nHistory.\nIn pre-modern times.\nIlkum.\nAround the reign of Hammurabi (1791\u20131750 BC), the Babylonian Empire used a system of conscription called \"Ilkum\". Under that system those eligible were required to serve in the royal army in time of war. During times of peace they were instead required to provide labour for other activities of the state. In return for this service, people subject to it gained the right to hold land. It is possible that this right was not to hold land \"per se\" but specific land supplied by the state.\nVarious forms of avoiding military service are recorded. While it was outlawed by the Code of Hammurabi, the hiring of substitutes appears to have been practiced both before and after the creation of the code. Later records show that Ilkum commitments could become regularly traded. In other places, people simply left their towns to avoid their Ilkum service. Another option was to sell Ilkum lands and the commitments along with them. With the exception of a few exempted classes, this was forbidden by the Code of Hammurabi.\nMedieval levies.\nUnder the feudal conditions for holding land in the medieval period, all peasants, freemen commoners and noblemen aged 15 to 60 living in the countryside or in urban centers , were summoned for military duty when required by either the king or the local lord bringing with them weapons and armor according to their wealth. These levies fought as footmen, sergeants, and men at arms under local superiors appointed by the king or the local lord such as the arri\u00e8re-ban in France. Arri\u00e8re-ban denoted a general levy, where all able-bodied males age 15 to 60 living in the Kingdom of France were summoned to go to war by the King (or the constable and the marshals). Men were summoned by the bailiff (or the senechal in the south). Bailiffs were military and political administrators installed by the King to steward and govern a specific area of a province following the king's commands and orders. The men summoned in this way were then summoned by the lieutenant who was the King's representative and military governor over an entire province comprising many bailiwicks, seneschalties and castellanies. All men from the richest noble to the poorest commoner were summoned under the arri\u00e8re-ban and they were supposed to present themselves to the King or his officials carrying arms and armor according to their wealth to fight as footmen, sergeants, and men at arms. A similar system existed in England called commissions of Array. Although the exact laws varied greatly depending on the country and the period, generally these levies were only obliged to fight for one to three months. Many were subsistence farmers, and it was in everyone's interest to send the men home for harvest-time.\nIn medieval Scandinavia the \"lei\u00f0angr\" (Old Norse), \"leidang\" (Norwegian), \"leding\", (Danish), \"ledung\" (Swedish), \"lichting\" (Dutch), \"expeditio\" (Latin) or sometimes \"le\u00feing\" (Old English), was a levy of free farmers conscripted into coastal fleets for seasonal excursions and in defence of the realm.\nThe bulk of the Anglo-Saxon English army, called the \"fyrd\", was composed of part-time English soldiers drawn from the freemen of each county. In the 690s Laws of Ine, three levels of fines are imposed on different social classes for neglecting military service. Some modern writers claim military service was restricted to the landowning minor nobility. These thegns were the land-holding aristocracy of the time and were required to serve with their own armour and weapons for a certain number of days each year. The historian David Sturdy has cautioned about regarding the \"fyrd\" as a precursor to a modern national army composed of all ranks of society, describing it as a \"ridiculous fantasy\":The persistent old belief that peasants and small farmers gathered to form a national army or \"fyrd\" is a strange delusion dreamt up by antiquarians in the late eighteenth or early nineteenth centuries to justify universal military conscription.\nMilitary slavery.\nThe system of military slaves was widely used in the Middle East, beginning with the creation of the corps of Turkish slave-soldiers (\"ghulams\" or \"mamluks\") by the Abbasid caliph al-Mu'tasim in the 820s and 830s. The Turkish troops soon came to dominate the government, establishing a pattern throughout the Islamic world of a ruling military class, often separated by ethnicity, culture and even religion by the mass of the population, a paradigm that found its apogee in the Mamluks of Egypt and the Janissary corps of the Ottoman Empire, institutions that survived until the early 19th century.\nIn the middle of the 14th century, Ottoman Sultan Murad I developed personal troops to be loyal to him, with a slave army called the \"Kap\u0131kulu\". The new force was built by taking Christian children from newly conquered lands, especially from the far areas of his empire, in a system known as the \"dev\u015firme\" (translated \"gathering\" or \"converting\"). The captive children were forced to convert to Islam. The Sultans had the young boys trained over several years. Those who showed special promise in fighting skills were trained in advanced warrior skills, put into the sultan's personal service, and turned into the Janissaries, the elite branch of the \"Kap\u0131kulu\". A number of distinguished military commanders of the Ottomans, and most of the imperial administrators and upper-level officials of the Empire, such as Pargal\u0131 \u0130brahim Pasha and Sokollu Mehmet Pa\u015fa, were recruited in this way. By 1609, the Sultan's \"Kap\u0131kulu\" forces increased to about 100,000.\nIn later years, Sultans turned to the Barbary Pirates to supply their Jannissaries corps. Their attacks on ships off the coast of Africa or in the Mediterranean, and subsequent capture of able-bodied men for ransom or sale provided some captives for the Sultan's system. Starting in the 17th century, Christian families living under the Ottoman rule began to submit their sons into the Kapikulu system willingly, as they saw this as a potentially invaluable career opportunity for their children. Eventually the Sultan turned to foreign volunteers from the warrior clans of Circassians in southern Russia to fill his Janissary armies. As a whole the system began to break down, the loyalty of the Jannissaries became increasingly suspect. Mahmud II forcibly disbanded the Janissary corps in 1826.\nSimilar to the Janissaries in origin and means of development were the Mamluks of Egypt in the Middle Ages. The Mamluks were usually captive non-Muslim Iranian and Turkish children who had been kidnapped or bought as slaves from the Barbary coasts. The Egyptians assimilated and trained the boys and young men to become Islamic soldiers who served the Muslim caliphs and the Ayyubid sultans during the Middle Ages. The first mamluks served the Abbasid caliphs in 9th-century Baghdad. Over time they became a powerful military caste. On more than one occasion, they seized power, for example, ruling Egypt from 1250 to 1517.\nFrom 1250 Egypt had been ruled by the Bahri dynasty of Kipchak origin. Slaves from the Caucasus served in the army and formed an elite corps of troops. They eventually revolted in Egypt to form the Burgi dynasty. The Mamluks' excellent fighting abilities, massed Islamic armies, and overwhelming numbers succeeded in overcoming the Christian Crusader fortresses in the Holy Land. The Mamluks were the most successful defence against the Mongol Ilkhanate of Persia and Iraq from entering Egypt.\nOn the western coast of Africa, Berber Muslims captured non-Muslims to put to work as laborers. They generally converted the younger people to Islam and many became quite assimilated. In Morocco, the Berber looked south rather than north. The Moroccan Sultan Moulay Ismail, called \"the Bloodthirsty\" (1672\u20131727), employed a corps of 150,000 black slaves, called his Black Guard. He used them to coerce the country into submission.\nIn modern times.\nModern conscription, the massed military enlistment of national citizens (Lev\u00e9e en masse), was devised during the French Revolution, to enable the Republic to defend itself from the attacks of European monarchies. Deputy Jean-Baptiste Jourdan gave its name to the 5 September 1798 Act, whose first article stated: \"Any Frenchman is a soldier and owes himself to the defense of the nation.\" It enabled the creation of the \"Grande Arm\u00e9e\", what Napoleon Bonaparte called \"the nation in arms\", which overwhelmed European professional armies that often numbered only into the low tens of thousands. More than 2.6\u00a0million men were inducted into the French military in this way between the years 1800 and 1813.\nThe defeat of the Prussian Army in particular shocked the Prussian establishment, which had believed it was invincible after the victories of Frederick the Great. The Prussians were used to relying on superior organization and tactical factors such as order of battle to focus superior troops against inferior ones. Given approximately equivalent forces, as was generally the case with professional armies, these factors showed considerable importance. However, they became considerably less important when the Prussian armies faced Napoleon's forces that outnumbered their own in some cases by more than ten to one. Scharnhorst advocated adopting the \"lev\u00e9e en masse\", the military conscription used by France. The \"Kr\u00fcmpersystem\" was the beginning of short-term compulsory service in Prussia, as opposed to the long-term conscription previously used.\nIn the Russian Empire, the military service time \"owed\" by serfs was 25 years at the beginning of the 19th century. In 1834 it was decreased to 20 years. The recruits were to be not younger than 17 and not older than 35. In 1874 Russia introduced universal conscription in the modern pattern, an innovation only made possible by the abolition of serfdom in 1861. New military law decreed that all male Russian subjects, when they reached the age of 20, were eligible to serve in the military for six years.\nIn the decades prior to World War I universal conscription along broadly Prussian lines became the norm for European armies, and those modeled on them. By 1914 the only substantial armies still completely dependent on voluntary enlistment were those of Britain and the United States. Some colonial powers such as France reserved their conscript armies for home service while maintaining professional units for overseas duties.\nWorld Wars.\nThe range of eligible ages for conscripting was expanded to meet national demand during the World Wars.\nIn the United States, the Selective Service System drafted men for World War I initially in an age range from 21 to 30 but expanded its eligibility in 1918 to an age range of 18 to 45. In the case of a widespread mobilization of forces where service includes homefront defense, ages of conscripts may range much higher, with the oldest conscripts serving in roles requiring lesser mobility.\nExpanded-age conscription was common during the Second World War: in Britain, it was commonly known as \"call-up\" and extended to age 51. Nazi Germany termed it \"Volkssturm\" (\"People's Storm\") and included children as young as 16 and men as old as 60. During the Second World War, both Britain and the Soviet Union conscripted women. The United States was on the verge of drafting women into the Nurse Corps because it anticipated it would need the extra personnel for its planned invasion of Japan. However, the Japanese surrendered and the idea was abandoned.\nArguments against conscription.\nSexism.\nMen's rights activists, feminists, and opponents of discrimination against men have criticized military conscription, or compulsory military service, as sexist. The National Coalition for Men, a men's rights group, sued the US Selective Service System in 2019, leading to it being declared unconstitutional by a US Federal Judge.\nFeminists have argued that military conscription is sexist because wars serve the interests of what they view as the patriarchy, the military is a sexist institution, conscripts are therefore indoctrinated in sexism, and conscription of men normalizes violence by men as socially acceptable. Feminists have been organizers and participants in resistance to conscription in several countries.\nConscription has also been criticized as, historically, only men have been subjected to conscription. Men who opt out or are deemed unfit for military service must often perform alternative service, such as Zivildienst in Austria and Switzerland, or pay extra taxes, whereas women do not have these obligations. Men who do not sign up for Selective Service in the US, are prohibited from eligibility for citizenship, financial aid, admissions to public colleges or universities, federal grants and loans, federal employment, and in some states, driving licenses.\nInvoluntary servitude.\nAmerican libertarians oppose conscription and call for the abolition of the Selective Service System, believing that impressment of individuals into the armed forces is \"involuntary servitude\". Ron Paul, a former presidential nominee of the U.S. Libertarian Party has said that conscription \"is wrongly associated with patriotism, when it really represents slavery and involuntary servitude\". The philosopher Ayn Rand opposed conscription, suggesting that \"of all the statist violations of individual rights in a mixed economy, the military draft is the worst. It is an abrogation of rights. It negates man's fundamental right\u2014the right to life\u2014and establishes the fundamental principle of statism: that a man's life belongs to the state, and the state may claim it by compelling him to sacrifice it in battle.\"\nIn 1917, a number of radicals and anarchists, including Emma Goldman, challenged the new draft law in federal court arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude. However, the Supreme Court unanimously upheld the constitutionality of the draft act in the case of \"Arver v. United States\" on 7 January 1918. The decision said the Constitution gave Congress the power to declare war and to raise and support armies. The Court emphasized the principle of the reciprocal rights and duties of citizens:\nEconomic.\nIt can be argued that in a cost-to-benefit ratio, conscription during peacetime is not worthwhile. Months or years of service performed by the most fit and capable subtract from the productivity of the economy; add to this the cost of training them, and in some countries paying them. Compared to these extensive costs, some would argue there is very little benefit; if there ever was a war then conscription and basic training could be completed quickly, and in any case there is little threat of a war in most countries with conscription. In the United States, every male resident is required by law to register with the Selective Service System within 30 days following his 18th birthday and be available for a draft; this is often accomplished automatically by a motor vehicle department during licensing or by voter registration.\nAccording to Milton Friedman the cost of conscription can be related to the parable of the broken window in anti-draft arguments. The cost of the work, military service, does not disappear even if no salary is paid. The work effort of the conscripts is effectively wasted, as an unwilling workforce is extremely inefficient. The impact is especially severe in wartime, when civilian professionals are forced to fight as amateur soldiers. Not only is the work effort of the conscripts wasted and productivity lost, but professionally skilled conscripts are also difficult to replace in the civilian workforce. Every soldier conscripted in the army is taken away from his civilian work, and away from contributing to the economy which funds the military. This may be less a problem in an agrarian or pre-industrialized state where the level of education is generally low, and where a worker is easily replaced by another. However, this is potentially more costly in a post-industrial society where educational levels are high and where the workforce is sophisticated and a replacement for a conscripted specialist is difficult to find. Even direr economic consequences result if the professional conscripted as an amateur soldier is killed or maimed for life; his work effort and productivity are lost.\nArguments for conscription.\nPolitical and moral motives.\nJean Jacques Rousseau argued vehemently against professional armies since he believed that it was the right and privilege of every citizen to participate to the defense of the whole society and that it was a mark of moral decline to leave the business to professionals. He based his belief upon the development of the Roman Republic, which came to an end at the same time as the Roman Army changed from a conscript to a professional force. Similarly, Aristotle linked the division of armed service among the populace intimately with the political order of the state. Niccol\u00f2 Machiavelli argued strongly for conscription and saw the professional armies as the cause of the failure of societal unity in Italy.\nOther proponents, such as William James, consider both mandatory military and national service as ways of instilling maturity in young adults. Some proponents, such as Jonathan Alter and Mickey Kaus, support a draft in order to reinforce social equality, create social consciousness, break down class divisions and allow young adults to immerse themselves in public enterprise. Charles Rangel called for the reinstatement of the draft during the Iraq War not because he seriously expected it to be adopted but to stress how the socioeconomic restratification meant that very few children of upper-class Americans served in the all-volunteer American armed forces.\nEconomic and resource efficiency.\nIt is estimated by the British military that in a professional military, a company deployed for active duty in peacekeeping corresponds to three inactive companies at home. Salaries for each are paid from the military budget. In contrast, volunteers from a trained reserve are in their civilian jobs when they are not deployed.\nIt was more financially beneficial for less-educated young Portuguese men born in 1967 to participate in conscription than to participate in the highly-competitive job market with men of the same age who continued to higher education.\nDrafting of women.\nThroughout history, women have only been conscripted to join armed forces in a few countries, in contrast to the universal practice of conscription from among the male population. The traditional view has been that military service is a test of manhood and a rite of passage from boyhood into manhood.\nThere are nations in present day who continue to actively draft women into military service. These are Bolivia,\nChad,\nEritrea,\nIsrael,\nMozambique,\nNorway,\nNorth Korea\nand Sweden.\nNorway introduced female conscription in 2015, making it the first NATO member to have a legally compulsory national service for both men and women. In practice only motivated volunteers are selected to join the army in Norway.\nSweden introduced female conscription in 2010, but it was not activated until 2017. This made Sweden the second nation in Europe to draft women, and the second in the world to draft women on the same formal terms as men.\nIsrael has universal female conscription, although in practice women can avoid service by claiming a religious exemption and over a third of Israeli women do so.\nSudanese law allows for conscription of women, but this is not implemented in practice.\nIn the United Kingdom during World War II, beginning in 1941, women were brought into the scope of conscription but, as all women with dependent children were exempt and many women were informally left in occupations such as nursing or teaching, the number conscripted was relatively few.\nIn the USSR, there was never conscription of women for the armed forces, but the severe disruption of normal life and the high proportion of civilians affected by World War II after the German invasion attracted many volunteers for what was termed \"The Great Patriotic War\". Medical doctors of both sexes could and would be conscripted (as officers). Also, the Soviet university education system required Department of Chemistry students of both sexes to complete an ROTC course in NBC defense, and such female reservist officers could be conscripted in times of war. The United States came close to drafting women into the Nurse Corps in preparation for a planned invasion of Japan.\nIn 1981 in the United States, several men filed lawsuit in the case \"Rostker v. Goldberg\", alleging that the Selective Service Act of 1948 violates the Due Process Clause of the Fifth Amendment by requiring that only men register with the Selective Service System (SSS). The Supreme Court eventually upheld the Act, stating that \"the argument for registering women was based on considerations of equity, but Congress was entitled, in the exercise of its constitutional powers, to focus on the question of military need, rather than 'equity.'\"On October 1, 1999 in Taiwan, the Judicial Yuan of the Republic of China in its Interpretation 490 considered that the physical differences between males and females and the derived role differentiation in their respective social functions and lives would not make drafting only males a violation of the Constitution of the Republic of China. Though women are not conscripted in Taiwan, transsexual persons are exempt.\nConscientious objection.\nA conscientious objector is an individual whose personal beliefs are incompatible with military service, or, more often, with any role in the armed forces. In some countries, conscientious objectors have special legal status, which augments their conscription duties. For example, Sweden used to allow (and once again, with the re-introduction of conscription, allows) conscientious objectors to choose a service in the \"weapons-free\" branch, such as an airport fireman, nurse, or telecommunications technician.\nThe reasons for refusing to serve in the military are varied. Some people are conscientious objectors for religious reasons. In particular, the members of the historic peace churches are pacifist by doctrine, and Jehovah's Witnesses, while not strictly pacifists, refuse to participate in the armed forces on the ground that they believe that Christians should be neutral in international conflicts.\nBy country.\nAustria.\nEvery male citizen of the Republic of Austria up to the age of 35 can be drafted for a six-month long basic military training in the Bundesheer. For men refusing to undergo this training, a nine-month lasting community service is mandatory.\nBelgium.\nBelgium abolished the conscription in 1994. The last conscripts left active service in February 1995. To this day (2019), a small minority of the Belgian citizens supports the idea of reintroducing military conscription, for both men and women.\nBulgaria.\nBulgaria had mandatory military service for males above 18 until conscription was ended in 2008. Due to a shortfall in the army of some 5500 soldiers, parts of the current ruling coalition have expressed their support for the return of mandatory military service, most notably Krasimir Karakachanov. Opposition towards this idea from the main coalition partner, GERB, saw a compromise in 2018, where instead of mandatory military service, Bulgaria could have possibly introduced a voluntary military service by 2019 where young citizens can volunteer for a period of 6 to 9 months, receiving a basic wage. However this has not gone forward.\nChina.\nUniversal conscription in China dates back to the State of Qin, which eventually became the Qin Empire of 221 BC. Following unification, historical records show that a total of 300,000 conscript soldiers and 500,000 conscript labourers constructed the Great Wall of China.\nIn the following dynasties, universal conscription was abolished and reintroduced on numerous occasions.\n, universal military conscription is theoretically mandatory in the People's Republic of China, and reinforced by law. However, due to the large population of China and large pool of candidates available for recruitment, the People's Liberation Army has always had sufficient volunteers, so conscription has not been required in practice at all.\nCyprus.\nMilitary service in Cyprus has a deep rooted history entagled with the Cyprus problem. Military service in the Cypriot National Guard is mandatory for all male citizens of the Republic of Cyprus, as well as any male non-citizens born of a parent of Greek Cypriot descent, lasting from the January 1 of the year in which they turn 18 years of age to December 31, of the year in which they turn 50. (Efthymiou, 2016). All male residents of Cyprus who are of military age (16 and over) are required to obtain an exit visa from the Ministry of Defense. Currently, military conscription in Cyprus lasts 14 months.\nDenmark.\nConscription is known in Denmark since the Viking Age, where one man out of every 10 had to serve the king. Frederick IV of Denmark changed the law in 1710 to every 4th man. The men were chosen by the landowner and it was seen as a penalty.\nSince 12 February 1849, every physically fit man must do military service. According to \u00a781 in the Constitution of Denmark, which was promulgated in 1849: Every male person able to carry arms shall be liable with his person to contribute to the defence of his country under such rules as are laid down by Statute. \u2014 Constitution of DenmarkThe legislation about compulsory military service is articulated in the Danish Law of Conscription. National service takes 4\u201312 months. It is possible to postpone the duty when one is still in full-time education. Every male turning 18 will be drafted to the 'Day of Defence', where they will be introduced to the Danish military and their health will be tested. Physically unfit persons are not required to do military service. It is only compulsory for men, while women are free to choose to join the Danish army. Almost all of the men have been volunteers in recent years, 96.9% of the total number of recruits having been volunteers in the 2015 draft.\nAfter lottery, one can become a conscientious objector. Total objection (refusal from alternative civilian service) results in up to 4 months jailtime according to the law. However, in 2014 a Danish man, who signed up for the service and objected later, got only 14 days of home arrest. In many countries the act of desertion (objection after signing up) is punished harder than objecting the compulsory service.\nFinland.\nConscription in Finland is part of a general compulsion for national military service for all adult males (; ) defined in the 127\u00a7 of the Constitution of Finland.\nConscription can take the form of military or of civilian service. According to Finnish Defence Forces 2011 data slightly under 80% of Finnish males turned 30 had entered and finished the military service. The number of female volunteers to annually enter armed service had stabilised at approximately 300. The service period is 165, 255 or 347 days for the rank and file conscripts and 347 days for conscripts trained as NCOs or reserve officers. The length of civilian service is always twelve months. Those electing to serve unarmed in duties where unarmed service is possible serve either nine or twelve months, depending on their training.\nAny Finnish male citizen who refuses to perform both military and civilian service faces a penalty of 173 days in prison, minus any served days. Such sentences are usually served fully in prison, with no parole. Jehovah's Witnesses are no longer exempted from service as of February 27, 2019. The inhabitants of the demilitarized \u00c5land Islands are exempt from military service. By the Conscription Act of 1951, they are, however, required to serve a time at a local institution, like the coast guard. However, until such service has been arranged, they are freed from service obligation. The non-military service of \u00c5land islands has not been arranged since the introduction of the act, and there are no plans to institute it. The inhabitants of \u00c5land islands can also volunteer for military service on the mainland. As of 1995, women are permitted to serve on a voluntary basis and pursue careers in the military after their initial voluntary military service.\nThe military service takes place in Finnish Defence Forces or in the Finnish Border Guard. All services of the Finnish Defence Forces train conscripts. However, the Border Guard trains conscripts only in land-based units, not in coast guard detachments or in the Border Guard Air Wing. Civilian service may take place in the Civilian Service Center in Lapinj\u00e4rvi or in an accepted non-profit organization of educational, social or medical nature.\nGermany.\nBetween 1956 and 2011 conscription was mandatory for all male citizens in the German federal armed forces (German: \"Bundeswehr\"), as well as for the Federal Border Guard (German: \"Bundesgrenzschutz\") in the 1970s (see Border Guard Service). With the end of the Cold War the German government drastically reduced the size of its armed forces. The low demand for conscripts led to the suspension of compulsory conscription in 2011. Since then, only volunteer professionals serve in the \"Bundeswehr\".\nGreece.\nSince 1914, Greece has had a period of mandatory military service lasting 9 months for men between the ages of 16 and 45. Citizens discharged from active service are normally placed in the reserve and are subject to periodic recalls of 1\u201310 days at irregular intervals.\nUniversal conscription was introduced in Greece during the military reforms of 1909, although various forms of selective conscription had been in place earlier. In more recent years, conscription was associated with the state of general mobilisation declared on July 20, 1974 due to the crisis in Cyprus (the mobilisation was formally ended on December 18, 2002).\nThe period of time that a conscript is required to serve has varied historically, between 12\u201336 months depending on various factors particular to the conscript, and the political situation. Although women are employed by the Greek army as officers and petty officers, they are not required to enlist, as men are. Soldiers receive no health insurance, but they are provided medical support during their army service, including hospitalization costs.\nSince 2009, Greece has mandatory military service of 9 months for male citizens between the ages of 19 and 45. However, as the Armed forces had been gearing towards a completely professional army, the government had announced that the mandatory military service period would be cut to 6 months by 2008 or even abolished completely. However, this timetable was under reconsideration as of April 2006, due to severe manpower shortages. These had been caused by a combination of financial difficulties, meaning that professional soldiers could not be hired at the projected rate, and widespread abuse of the deferment process, resulting in two-thirds of the conscripts deferred service in 2005. In August 2009, the mandatory service period was reduced to 9 months for the army, but has remained at 12 months for the navy and the air force. The number of conscripts affected to the latter two has been greatly reduced, with an aim towards full professionalisation. By the end of 2020 the mandatory service period was increased to 12 months for the army and remained at 12 months for the navy and the air force due to increased provocation of the Turks in the Aegean sea and the increased flows of migrants and refugees in Evros river and Dodecanese islands.\nIsrael.\nThere is a mandatory military service for all men and women in Israel who are fit and 18 years old. Men must serve 30 months while women serve 24 months, with the vast majority of conscripts being Jewish.\nSome Israeli citizens are exempt from mandatory service:\nAll of the exempt above are eligible to volunteer to the Israel Defense Forces (IDF), as long as they declare so.\nMale Druze and male Circassian Israeli citizens are liable for conscription, in accordance with agreement set by their community leaders (their community leaders however signed a clause in which all female Druze and female Circassian are exempt from service).\nA few male Bedouin Israeli citizens choose to enlist to the Israeli military in every draft (despite their Muslim-Arab background that exempt them from conscription).\nLithuania.\nLithuania abolished its conscription in 2008. In May 2015, the Lithuanian parliament voted to reintroduce conscription and the conscripts started their training in August 2015. From 2015 to 2017 there was enough volunteers to avoid drafting civilians.\nLuxembourg.\nLuxembourg practiced military conscription from 1948 until 1967.\nMoldova.\nMoldova, which currently has male conscription, has announced plans to abolish the practice. Moldova's Defense Ministry announced that a plan which stipulates the gradual elimination of military conscription will be implemented starting from the autumn of 2018.\nNetherlands.\nConscription, which was called \"Service Duty\" () in the Netherlands, was first employed in 1810 by French occupying forces. Napoleon's brother Louis Bonaparte, who was King of Holland from 1806 to 1810, had tried to introduce conscription a few years earlier, unsuccessfully. Every man aged 20 years or older had to enlist. By means of drawing lots it was decided who had to undertake service in the French army. It was possible to arrange a substitute against payment.\nLater on, conscription was used for all men over the age of 18. Postponement was possible, due to study, for example. Conscientious objectors could perform an alternative civilian service instead of military service. For various reasons, this forced military service was criticized at the end of the twentieth century. Since the Cold War was over, so was the direct threat of a war. Instead, the Dutch army was employed in more and more peacekeeping operations. The complexity and danger of these missions made the use of conscripts controversial. Furthermore, the conscription system was thought to be unfair as only men were drafted.\nIn the European part of Netherlands, compulsory attendance has been officially suspended since 1 May 1997. Between 1991 and 1996, the Dutch armed forces phased out their conscript personnel and converted to an all-professional force. The last conscript troops were inducted in 1995, and demobilized in 1996. The suspension means that citizens are no longer forced to serve in the armed forces, as long as it is not required for the safety of the country. Since then, the Dutch army has become an all-professional force. However, to this day, every male and \u2013 from January 2020 onward \u2013 female citizen aged 17 gets a letter in which they are told that they have been registered but do not have to present themselves for service.\nNorway.\n, Norway currently employs a weak form of mandatory military service for men and women. In practice recruits are not forced to serve, instead only those who are motivated are selected. About 60,000 Norwegians are available for conscription every year, but only 8,000 to 10,000 are conscripted. Since 1985, women have been able to enlist for voluntary service as regular recruits. On 14 June 2013 the Norwegian Parliament voted to extend conscription to women, making Norway the first NATO member and first European country to make national service compulsory for both sexes. In earlier times, up until at least the early 2000s, all men aged 19\u201344 were subject to mandatory service, with good reasons required to avoid becoming drafted. There is a right of conscientious objection.\nIn addition to the military service, the Norwegian government draft a total of 8,000 men and women between 18 and 55 to non-military Civil defence duty. (Not to be confused with Alternative civilian service.) Former service in the military does not exclude anyone from later being drafted to the Civil defence, but an upper limit of total 19 months of service applies. Neglecting mobilisation orders to training exercises and actual incidents, may impose fines.\nSerbia.\n, Serbia no longer practises mandatory military service. Prior to this, mandatory military service lasted 6 months for men. Conscientious objectors could however opt for 9 months of civil service instead.\nOn 15 December 2010, the Parliament of Serbia voted to suspend mandatory military service. The decision fully came into force on January 1, 2011.\nSweden.\nSweden had conscription () for men between 1901 and 2010. During the last few decades it was selective. Since 1980, women have been allowed to sign up by choice, and, if passing the tests, do military training together with male conscripts. Since 1989 women have been allowed to serve in all military positions and units, including combat.\nIn 2010, conscription was made gender-neutral, meaning both women and men would be conscripted \u2013on equal terms. The conscription system was simultaneously deactivated in peacetime. Seven years later, referencing increased military threat, the Swedish Government reactivated military conscription. Beginning in 2018, both men and women are conscripted.\nUnited Kingdom.\nThe United Kingdom introduced conscription to full-time military service for the first time in January 1916 (the eighteenth month of World War I) and abolished it in 1920. Ireland, then part of the United Kingdom, was exempted from the original 1916 military service legislation, and although further legislation in 1918 gave power for an extension of conscription to Ireland, the power was never put into effect.\nConscription was reintroduced in 1939, in the lead up to World War II, and continued in force until 1963. Northern Ireland was exempted from conscription legislation throughout the whole period.\nIn all, eight million men were conscripted during both World Wars, as well as several hundred thousand younger single women. The introduction of conscription in May 1939, before the war began, was partly due to pressure from the French, who emphasized the need for a large British army to oppose the Germans. From early 1942 unmarried women age 19\u201330 were conscripted. Most were sent to the factories, but they could volunteer for the Auxiliary Territorial Service (ATS) and other women's services. Some women served in the Women's Land Army: initially volunteers but later conscription was introduced. However, women who were already working in a skilled job considered helpful to the war effort, such as a General Post Office telephonist, were told to continue working as before. None was assigned to combat roles unless she volunteered. By 1943 women were liable to some form of directed labour up to age 51. During the Second World War, 1.4\u00a0million British men volunteered for service and 3.2\u00a0million were conscripted. Conscripts comprised 50% of the Royal Air Force, 60% of the Royal Navy and 80% of the British Army.\nThe abolition of conscription in Britain was announced on 4 April 1957, by new prime minister Harold Macmillan, with the last conscripts being recruited three years later.\nUnited States.\nConscription in the United States ended in 1973, but males aged between 18 and 25 are required to register with the Selective Service System to enable a reintroduction of conscription if necessary. President Gerald Ford had suspended mandatory draft registration in 1975; but, President Jimmy Carter reinstated that requirement when the Soviet Union intervened in Afghanistan five years later. Consequently, Selective Service registration is still required of almost all young men. There have been no prosecutions for violations of the draft registration law since 1986. Males between the ages of 17 and 45, and female members of the US National Guard may be conscripted for federal militia service pursuant to 10 U.S. Code \u00a7 246 and the Militia Clauses of the United States Constitution.\nIn February 2019, the United States District Court for the Southern District of Texas ruled that male-only conscription breached the Fourteenth Amendment's equal protection clause. In \"National Coalition for Men v. Selective Service System\", a case brought by non-profit men's rights organisation the National Coalition for Men against the U.S. Selective Service System, judge Gray H. Miller issued a declaratory judgement that the male-only registration requirement is unconstitutional, though did not specify what action the government should take."}
{"id": "5736", "revid": "4323902", "url": "https://en.wikipedia.org/wiki?curid=5736", "title": "Catherine Coleman", "text": "Catherine Grace \"Cady\" Coleman (born 14 December 1960) is an American chemist, a former United States Air Force colonel, and a retired NASA astronaut. She is a veteran of two Space Shuttle missions, and departed the International Space Station on 23 May 2011, as a crew member of Expedition 27 after logging 159 days in space.\nEducation.\nColeman graduated from Wilbert Tucker Woodson High School, Fairfax, Virginia, in 1978; in 1978\u20131979, she was an exchange student at R\u00f8yken Upper Secondary School in Norway with the AFS Intercultural Programs. She received a B.S. degree in chemistry from the Massachusetts Institute of Technology (MIT) in 1983 and was commissioned as graduate of the Air Force Reserve Officer Training Corps (Air Force ROTC)., then received a Ph.D. degree in polymer science and engineering from the University of Massachusetts Amherst in 1991. She was advised by Professor Thomas J. McCarthy on her doctorate. As an undergraduate, she was a member of the intercollegiate rowing crew and was a resident of Baker House.\nMilitary career.\nColeman continued to pursue her PhD at the University of Massachusetts Amherst as a Second lieutenant. In 1988, she entered active duty at Wright-Patterson Air Force Base as a research chemist. During her work, she participated as a surface analysis consultant on the NASA Long Duration Exposure Facility experiment. In 1991, she received her doctorate in polymer science and engineering. She retired from the Air Force in November 2009 as a colonel.\nNASA career.\nColeman was selected by NASA in 1992 to join the NASA Astronaut Corps. In 1995, she was a member of the STS-73 crew on the scientific mission USML-2 with experiments including biotechnology, combustion science, and the physics of fluids. During the flight, she reported to Houston Mission Control that she had spotted an Unidentified flying object (UFO). She also trained for the mission STS-83 to be the backup for Donald A. Thomas; however, as he recovered on time, she did not fly that mission. STS-93 was Coleman's second space flight in 1999. She was mission specialist in charge of deploying the Chandra X-ray Observatory and its Inertial Upper Stage out of the shuttle's cargo bay.\nColeman served as Chief of Robotics for the Astronaut Office, to include robotic arm operations and training for all Space Shuttle and International Space Station missions. In October 2004, Coleman served as an aquanaut during the NEEMO 7 mission aboard the Aquarius underwater laboratory, living and working underwater for eleven days.\nColeman was assigned as a backup U.S. crew member for Expeditions 19, 20 and 21 and served as a backup crew member for Expeditions 24 and 25 as part of her training for Expedition 26.\nColeman launched on 15 December 2010 (16 December 2010 Baikonur time), aboard Soyuz TMA-20 to join the Expedition 26 mission aboard the International Space Station. She retired from NASA on 1 December 2016.\nSpaceflight experience.\nSTS-73 on Space Shuttle \"Columbia\" (20 October to 5 November 1995) was the second United States Microgravity Laboratory (USML-2) mission. The mission focused on materials science, biotechnology, combustion science, the physics of fluids, and numerous scientific experiments housed in the pressurized Spacelab module. In completing her first space flight, Coleman orbited the Earth 256 times, traveled over 6 million miles, and logged a total of 15 days, 21 hours, 52 minutes and 21 seconds in space.\nSTS-93 on \"Columbia\" (22 to 27 July 1999) was a five-day mission during which Coleman was the lead mission specialist for the deployment of the Chandra X-ray Observatory. Designed to conduct comprehensive studies of the universe, the telescope will enable scientists to study exotic phenomena such as exploding stars, quasars, and black holes. Mission duration was 118 hours and 50 minutes.\nSoyuz TMA-20 / Expedition 26/27 (15 December 2010 to 23 May 2011) was an extended duration mission to the International Space Station.\nPersonal.\nColeman is married to glass artist Josh Simpson who lives in Massachusetts. They have one son. She is part of the band Bandella, which also includes fellow NASA astronaut Stephen Robinson, Canadian astronaut Chris Hadfield, and Micki Pettit (wife of the astronaut Donald Pettit). Coleman is a flute player and has taken several flutes with her to the ISS, including a pennywhistle from Paddy Moloney of the The Chieftains, an old Irish flute from Matt Molloy of The Chieftains, and a flute from Ian Anderson of Jethro Tull (band). On 15 February 2011, she played one of the instruments live from orbit on National Public Radio. On 12 April 2011, she played live via video link for the audience of Jethro Tull (band)'s show in Russia in honour of the 50th anniversary of Yuri Gagarin's flight, playing in orbit while Anderson played on the ground. On 13 May of that year, Coleman delivered a taped commencement address to the class of 2011 at the University of Massachusetts Amherst.\nAs do many other astronauts, Coleman holds an amateur radio license (callsign: KC5ZTH).\nAs of 2015, she is also known to be working as a guest speaker at the Baylor College of Medicine, for the children's program 'Saturday Morning Science'.\nIn 2018, she gave a graduation address to Carter Lynch, the sole graduate of Cuttyhunk Elementary School, on Cuttyhunk Island, Massachusetts.\nin 2019 the Irish postal service An Post issued a set of commemorative stamps for the 50th anniversary of the Apollo Moon landings, Catherine Coleman is featured alongside fellow astronauts Neil Armstrong, Michael Collins, and Eileen Collins."}
{"id": "5737", "revid": "72499", "url": "https://en.wikipedia.org/wiki?curid=5737", "title": "Cross cutting", "text": ""}
{"id": "5738", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=5738", "title": "Cervix", "text": "The cervix or cervix uteri (Latin, 'neck of the uterus') is the lower part of the uterus in the human female reproductive system. The cervix is usually 2 to 3\u00a0cm long (~1 inch) and roughly cylindrical in shape, which changes during pregnancy. The narrow, central cervical canal runs along its entire length, connecting the uterine cavity and the lumen of the vagina. The opening into the uterus is called the internal os, and the opening into the vagina is called the external os. The lower part of the cervix, known as the vaginal portion of the cervix (or ectocervix), bulges into the top of the vagina. The cervix has been documented anatomically since at least the time of Hippocrates, over 2,000 years ago.\nThe cervical canal is a passage through which sperm must travel to fertilize an egg cell after sexual intercourse. Several methods of contraception, including cervical caps and cervical diaphragms, aim to block or prevent the passage of sperm through the cervical canal. Cervical mucus is used in several methods of fertility awareness, such as the Creighton model and Billings method, due to its changes in consistency throughout the menstrual period. During vaginal childbirth, the cervix must flatten and dilate to allow the fetus to progress along the birth canal. Midwives and doctors use the extent of the dilation of the cervix to assist decision-making during childbirth.\nThe cervical canal is lined with a single layer of column-shaped cells, while the ectocervix is covered with multiple layers of cells topped with flat cells. The two types of epithelia meet at the squamocolumnar junction. Infection with the human papillomavirus (HPV) can cause changes in the epithelium, which can lead to cancer of the cervix. Cervical cytology tests can often detect cervical cancer and its precursors, and enable early successful treatment. Ways to avoid HPV include avoiding sex, using condoms, and HPV vaccination. HPV vaccines, developed in the early 21st century, reduce the risk of cervical cancer by preventing infections from the main cancer-causing strains of HPV.\nStructure.\nThe cervix is part of the female reproductive system. Around in length, it is the lower narrower part of the uterus continuous above with the broader upper part\u2014or body\u2014of the uterus. The lower end of the cervix bulges through the anterior wall of the vagina, and is referred to as the vaginal portion of cervix (or ectocervix) while the rest of the cervix above the vagina is called the supravaginal portion of cervix. A central canal, known as the cervical canal, runs along its length and connects the cavity of the body of the uterus with the lumen of the vagina. The openings are known as the internal os and external orifice of the uterus (or external os) respectively. The mucosa lining the cervical canal is known as the endocervix, and the mucosa covering the ectocervix is known as the exocervix. The cervix has an inner mucosal layer, a thick layer of smooth muscle, and posteriorly the supravaginal portion has a serosal covering consisting of connective tissue and overlying peritoneum.\nIn front of the upper part of the cervix lies the bladder, separated from it by cellular connective tissue known as parametrium, which also extends over the sides of the cervix. To the rear, the supravaginal cervix is covered by peritoneum, which runs onto the back of the vaginal wall and then turns upwards and onto the rectum, forming the recto-uterine pouch. The cervix is more tightly connected to surrounding structures than the rest of the uterus.\nThe cervical canal varies greatly in length and width between women or over the course of a woman's life, and it can measure 8\u00a0mm (0.3\u00a0inch) at its widest diameter in premenopausal adults. It is wider in the middle and narrower at each end. The anterior and posterior walls of the canal each have a vertical fold, from which ridges run diagonally upwards and laterally. These are known as \"palmate folds\", due to their resemblance to a palm leaf. The anterior and posterior ridges are arranged in such a way that they interlock with each other and close the canal. They are often effaced after pregnancy.\nThe ectocervix (also known as the vaginal portion of the cervix) has a convex, elliptical shape and projects into the cervix between the anterior and posterior vaginal fornices. On the rounded part of the ectocervix is a small, depressed external opening, connecting the cervix with the vagina. The size and shape of the ectocervix and the external opening (external os) can vary according to age, hormonal state, and whether natural or normal childbirth has taken place. In women who have not had a vaginal delivery, the external opening is small and circular, and in women who have had a vaginal delivery, it is slit-like. On average, the ectocervix is long and wide.\nBlood is supplied to the cervix by the descending branch of the uterine artery and drains into the uterine vein. The pelvic splanchnic nerves, emerging as S2\u2013S3, transmit the sensation of pain from the cervix to the brain. These nerves travel along the uterosacral ligaments, which pass from the uterus to the anterior sacrum.\nThree channels facilitate lymphatic drainage from the cervix. The anterior and lateral cervix drains to nodes along the uterine arteries, travelling along the cardinal ligaments at the base of the broad ligament to the external iliac lymph nodes and ultimately the paraaortic lymph nodes. The posterior and lateral cervix drains along the uterine arteries to the internal iliac lymph nodes and ultimately the paraaortic lymph nodes, and the posterior section of the cervix drains to the obturator and presacral lymph nodes. However, there are variations as lymphatic drainage from the cervix travels to different sets of pelvic nodes in some people. This has implications in scanning nodes for involvement in cervical cancer.\nAfter menstruation and directly under the influence of estrogen, the cervix undergoes a series of changes in position and texture. During most of the menstrual cycle, the cervix remains firm, and is positioned low and closed. However, as ovulation approaches, the cervix becomes softer and rises to open in response to the higher levels of estrogen present. These changes are also accompanied by changes in cervical mucus, described below.\nDevelopment.\nAs a component of the female reproductive system, the cervix is derived from the two paramesonephric ducts (also called M\u00fcllerian ducts), which develop around the sixth week of embryogenesis. During development, the outer parts of the two ducts fuse, forming a single urogenital canal that will become the vagina, cervix and uterus. The cervix grows in size at a smaller rate than the body of the uterus, so the relative size of the cervix over time decreases, decreasing from being much larger than the body of the uterus in fetal life, twice as large during childhood, and decreasing to its adult size, smaller than the uterus, after puberty. Previously it was thought that during fetal development, the original squamous epithelium of the cervix is derived from the urogenital sinus and the original columnar epithelium is derived from the paramesonephric duct. The point at which these two original epithelia meet is called the original squamocolumnar junction. New studies show, however, that all the cervical as well as large part of the vaginal epithelium are derived from M\u00fcllerian duct tissue and that phenotypic differences might be due to other causes.\nHistology.\nThe endocervical mucosa is about thick and lined with a single layer of columnar mucous cells. It contains numerous tubular mucous glands, which empty viscous alkaline mucus into the lumen. In contrast, the ectocervix is covered with nonkeratinized stratified squamous epithelium, which resembles the squamous epithelium lining the vagina. The junction between these two types of epithelia is called the squamocolumnar junction. Underlying both types of epithelium is a tough layer of collagen. The mucosa of the endocervix is not shed during menstruation. The cervix has more fibrous tissue, including collagen and elastin, than the rest of the uterus.\nIn prepubertal girls, the functional squamocolumnar junction is present just within the cervical canal. Upon entering puberty, due to hormonal influence, and during pregnancy, the columnar epithelium extends outward over the ectocervix as the cervix everts. Hence, this also causes the squamocolumnar junction to move outwards onto the vaginal portion of the cervix, where it is exposed to the acidic vaginal environment. The exposed columnar epithelium can undergo physiological metaplasia and change to tougher metaplastic squamous epithelium in days or weeks, which is very similar to the original squamous epithelium when mature. The new squamocolumnar junction is therefore internal to the original squamocolumnar junction, and the zone of unstable epithelium between the two junctions is called the \"transformation zone\" of the cervix. Histologically, the transformation zone is generally defined as surface squamous epithelium with surface columnar epithelium or stromal glands/crypts, or both.\nAfter menopause, the uterine structures involute and the functional squamocolumnar junction moves into the cervical canal.\nNabothian cysts (or Nabothian follicles) form in the transformation zone where the lining of metaplastic epithelium has replaced mucous epithelium and caused a strangulation of the outlet of some of the mucous glands. A buildup of mucus in the glands forms Nabothian cysts, usually less than about in diameter, which are considered physiological rather than pathological. Both gland openings and Nabothian cysts are helpful to identify the transformation zone.\nFunction.\nFertility.\nThe cervical canal is a pathway through which sperm enter the uterus after sexual intercourse, and some forms of artificial insemination. Some sperm remains in cervical crypts, infoldings of the endocervix, which act as a reservoir, releasing sperm over several hours and maximising the chances of fertilisation. A theory states the cervical and uterine contractions during orgasm draw semen into the uterus. Although the \"upsuck theory\" has been generally accepted for some years, it has been disputed due to lack of evidence, small sample size, and methodological errors.\nSome methods of fertility awareness, such as the Creighton model and the Billings method involve estimating a woman's periods of fertility and infertility by observing physiological changes in her body. Among these changes are several involving the quality of her cervical mucus: the sensation it causes at the vulva, its elasticity (\"Spinnbarkeit\"), its transparency, and the presence of ferning.\nCervical mucus.\nSeveral hundred glands in the endocervix produce 20\u201360\u00a0mg of cervical mucus a day, increasing to 600\u00a0mg around the time of ovulation. It is viscous because it contains large proteins known as mucins. The viscosity and water content varies during the menstrual cycle; mucus is composed of around 93% water, reaching 98% at midcycle. These changes allow it to function either as a barrier or a transport medium to spermatozoa. It contains electrolytes such as calcium, sodium, and potassium; organic components such as glucose, amino acids, and soluble proteins; trace elements including zinc, copper, iron, manganese, and selenium; free fatty acids; enzymes such as amylase; and prostaglandins. Its consistency is determined by the influence of the hormones estrogen and progesterone. At midcycle around the time of ovulation\u2014a period of high estrogen levels\u2014 the mucus is thin and serous to allow sperm to enter the uterus and is more alkaline and hence more hospitable to sperm. It is also higher in electrolytes, which results in the \"ferning\" pattern that can be observed in drying mucus under low magnification; as the mucus dries, the salts crystallize, resembling the leaves of a fern. The mucus has a stretchy character described as \"Spinnbarkeit\" most prominent around the time of ovulation.\nAt other times in the cycle, the mucus is thick and more acidic due to the effects of progesterone. This \"infertile\" mucus acts as a barrier to keep sperm from entering the uterus. Women taking an oral contraceptive pill also have thick mucus from the effects of progesterone. Thick mucus also prevents pathogens from interfering with a nascent pregnancy.\nA cervical mucus plug, called the operculum, forms inside the cervical canal during pregnancy. This provides a protective seal for the uterus against the entry of pathogens and against leakage of uterine fluids. The mucus plug is also known to have antibacterial properties. This plug is released as the cervix dilates, either during the first stage of childbirth or shortly before. It is visible as a blood-tinged mucous discharge.\nChildbirth.\nThe cervix plays a major role in childbirth. As the fetus descends within the uterus in preparation for birth, the presenting part, usually the head, rests on and is supported by the cervix. As labour progresses, the cervix becomes softer and shorter, begins to dilate, and rotates to face anteriorly. The support the cervix provides to the fetal head starts to give way when the uterus begins its contractions. During childbirth, the cervix must dilate to a diameter of more than to accommodate the head of the fetus as it descends from the uterus to the vagina. In becoming wider, the cervix also becomes shorter, a phenomenon known as effacement.\nAlong with other factors, midwives and doctors use the extent of cervical dilation to assist decision making during childbirth. Generally, the active first stage of labour, when the uterine contractions become strong and regular, begins when the cervical dilation is more than . The second phase of labor begins when the cervix has dilated to , which is regarded as its fullest dilation, and is when active pushing and contractions push the baby along the birth canal leading to the birth of the baby. The number of past vaginal deliveries is a strong factor in influencing how rapidly the cervix is able to dilate in labour. The time taken for the cervix to dilate and efface is one factor used in reporting systems such as the Bishop score, used to recommend whether interventions such as a forceps delivery, induction, or Caesarean section should be used in childbirth.\nCervical incompetence is a condition in which shortening of the cervix due to dilation and thinning occurs, before term pregnancy. Short cervical length is the strongest predictor of preterm birth.\nContraception.\nSeveral methods of contraception involve the cervix. Cervical diaphragms are reusable, firm-rimmed plastic devices inserted by a woman prior to intercourse that cover the cervix. Pressure against the walls of the vagina maintain the position of the diaphragm, and it acts as a physical barrier to prevent the entry of sperm into the uterus, preventing fertilisation. Cervical caps are a similar method, although they are smaller and adhere to the cervix by suction. Diaphragms and caps are often used in conjunction with spermicides. In one year, 12% of women using the diaphragm will undergo an unintended pregnancy, and with optimal use this falls to 6%. Efficacy rates are lower for the cap, with 18% of women undergoing an unintended pregnancy, and 10\u201313% with optimal use. Most types of progestogen-only pills are effective as a contraceptive because they thicken cervical mucus, making it difficult for sperm to pass along the cervical canal. In addition, they may also sometimes prevent ovulation. In contrast, contraceptive pills that contain both oestrogen and progesterone, the combined oral contraceptive pills, work mainly by preventing ovulation. They also thicken cervical mucus and thin the lining of the uterus, enhancing their effectiveness.\nClinical significance.\nCancer.\nIn 2008, cervical cancer was the third-most common cancer in women worldwide, with rates varying geographically from less than one to more than 50 cases per 100,000 women. It is a leading cause of cancer-related death in poor countries, where delayed diagnosis leading to poor outcomes is common. The introduction of routine screening has resulted in fewer cases of (and deaths from) cervical cancer, however this has mainly taken place in developed countries. Most developing countries have limited or no screening, and 85% of the global burden occurring there.\nCervical cancer nearly always involves human papillomavirus (HPV) infection. HPV is a virus with numerous strains, several of which predispose to precancerous changes in the cervical epithelium, particularly in the transformation zone, which is the most common area for cervical cancer to start. HPV vaccines, such as Gardasil and Cervarix, reduce the incidence of cervical cancer, by inoculating against the viral strains involved in cancer development.\nPotentially precancerous changes in the cervix can be detected by cervical screening, using methods including a Pap smear (also called a cervical smear), in which epithelial cells are scraped from the surface of the cervix and examined under a microscope. The colposcope, an instrument used to see a magnified view of the cervix, was invented in 1925. The Pap smear was developed by Georgios Papanikolaou in 1928. A LEEP procedure using a heated loop of platinum to excise a patch of cervical tissue was developed by Aurel Babes in 1927. In some parts of the developed world including the UK, the Pap test has been superseded with liquid-based cytology.\nA cheap, cost-effective and practical alternative in poorer countries is visual inspection with acetic acid (VIA). Instituting and sustaining cytology-based programs in these regions can be difficult, due to the need for trained personnel, equipment and facilities and difficulties in follow-up. With VIA, results and treatment can be available on the same day. As a screening test, VIA is comparable to cervical cytology in accurately identifying precancerous lesions.\nA result of dysplasia is usually further investigated, such as by taking a cone biopsy, which may also remove the cancerous lesion. Cervical intraepithelial neoplasia is a possible result of the biopsy and represents dysplastic changes that may eventually progress to invasive cancer. Most cases of cervical cancer are detected in this way, without having caused any symptoms. When symptoms occur, they may include vaginal bleeding, discharge, or discomfort.\nInflammation.\nInflammation of the cervix is referred to as cervicitis. This inflammation may be of the endocervix or ectocervix. When associated with the endocervix, it is associated with a mucous vaginal discharge and sexually transmitted infections such as chlamydia and gonorrhoea. As many as half of pregnant women having a gonorrheal infection of the cervix are asymptomatic. Other causes include overgrowth of the commensal flora of the vagina. When associated with the ectocervix, inflammation may be caused by the herpes simplex virus. Inflammation is often investigated through directly visualising the cervix using a speculum, which may appear whiteish due to exudate, and by taking a Pap smear and examining for causal bacteria. Special tests may be used to identify particular bacteria. If the inflammation is due to a bacterium, then antibiotics may be given as treatment.\nAnatomical abnormalities.\nCervical stenosis is an abnormally narrow cervical canal, typically associated with trauma caused by removal of tissue for investigation or treatment of cancer, or cervical cancer itself. Diethylstilbestrol, used from 1938 to 1971 to prevent preterm labour and miscarriage, is also strongly associated with the development of cervical stenosis and other abnormalities in the daughters of the exposed women. Other abnormalities include: vaginal adenosis, in which the squamous epithelium of the ectocervix becomes columnar; cancers such as clear cell adenocarcinomas; cervical ridges and hoods; and development of a cockscomb cervix appearance, which is the condition wherein, as the name suggests, the cervix of the uterus is shaped like a cockscomb. About one third of women born to diethylstilbestrol-treated mothers (i.e. in-utero exposure) develop a cockscomb cervix.\nEnlarged folds or ridges of cervical stroma (fibrous tissues) and epithelium constitute a cockscomb cervix. Similarly, cockscomb polyps lining the cervix are usually considered or grouped into the same overarching description. It is in and of itself considered a benign abnormality; its presence, however is usually indicative of DES exposure, and as such women who experience these abnormalities should be aware of their increased risk of associated pathologies.\nCervical agenesis is a rare congenital condition in which the cervix completely fails to develop, often associated with the concurrent failure of the vagina to develop. Other congenital cervical abnormalities exist, often associated with abnormalities of the vagina and uterus. The cervix may be duplicated in situations such as bicornuate uterus and uterine didelphys.\nCervical polyps, which are benign overgrowths of endocervical tissue, if present, may cause bleeding, or a benign overgrowth may be present in the cervical canal. Cervical ectropion refers to the horizontal overgrowth of the endocervical columnar lining in a one-cell-thick layer over the ectocervix.\nOther mammals.\nFemale marsupials have paired uteri and cervices. Most eutherian (placental) mammal species have a single cervix and single, bipartite or bicornuate uterus. Lagomorphs, rodents, aardvarks and hyraxes have a duplex uterus and two cervices. Lagomorphs and rodents share many morphological characteristics and are grouped together in the clade Glires. Anteaters of the family myrmecophagidae are unusual in that they lack a defined cervix; they are thought to have lost the characteristic rather than other mammals developing a cervix on more than one lineage. In domestic pigs, the cervix contains a series of five interdigitating pads that hold the boar's corkscrew-shaped penis during copulation.\nEtymology and pronunciation.\nThe word \"cervix\" () came to English from Latin, where it means \"neck\", and like its Germanic counterpart, it can refer not only to the neck [of the body] but also to an analogous narrowed part of an object. The cervix uteri (neck of the uterus) is thus the uterine cervix, but in English the word \"cervix\" used alone usually refers to it. Thus the adjective \"cervical\" may refer either to the neck (as in \"cervical vertebrae\" or \"cervical lymph nodes\") or to the uterine cervix (as in \"cervical cap\" or \"cervical cancer\").\nLatin \"cervix\" came from the Proto-Indo-European root \"ker-\", referring to a \"structure that projects\". Thus, the word cervix is linguistically related to the English word \"horn\", the Persian word for \"head\" ( \"sar\"), the Greek word for \"head\" ( \"koruphe\"), and the Welsh word for \"deer\" ().\nThe cervix was documented in anatomical literature in at least the time of Hippocrates; cervical cancer was first described more than 2,000 years ago, with descriptions provided by both Hippocrates and Aretaeus. However, there was some variation in word sense among early writers, who used the term to refer to both the cervix and the internal uterine orifice. The first attested use of the word to refer to the cervix of the uterus was in 1702."}
{"id": "5739", "revid": "1012715855", "url": "https://en.wikipedia.org/wiki?curid=5739", "title": "Compiler", "text": "In computing, a compiler is a computer program that translates computer code written in one programming language (the \"source\" language) into another language (the \"target\" language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.\nThere are many different types of compilers which produce output in different useful forms. A compiler that can run on a computer whose CPU or operating system is different from the one on which the code it produces will run is called a \"cross-compiler\". A \"bootstrap compiler\" is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a \"decompiler\". A program that translates between high-level languages is usually called a \"source-to-source compiler\" or \"transcompiler\". A language \"rewriter\" is usually a program that translates the form of expressions without a change of language. The term \"compiler-compiler\" refers to tools used to create parsers that perform syntax analysis.\nA compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.\nCompilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.\nHistory.\nTheoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.\nIt is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:\nThe sentences in a language may be defined by a set of rules called a grammar.\nBackus\u2013Naur form (BNF) describes the syntax of \"sentences\" of a language and was used for the syntax of Algol 60 by John Backus. The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist. \"BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description.\"\nIn the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalk\u00fcl (\"Plan Calculus\"). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s. APL is a language for mathematical computations.\nHigh-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:\nCompiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with the analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.\nSome early milestones in the development of compiler technology:\nEarly operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.\nBCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool. Several compilers have been implemented, Richards' book provides insights to the language and its compiler. BCPL was not only an influential systems programming language that is still used in research but also provided a basis for the design of B and C languages.\nBLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.\nMultics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbat\u00f3 from MIT. Multics was written in the PL/I language developed by IBM and IBM User Group. IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented. For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs. EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.\nBell Labs left the Multics project in 1969: \"Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system.\" Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.\nBell Labs started the development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs. Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.\nObject-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science. At Bell Labs, the development of C++ became interested in OOP. C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983. The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.\nIn many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.\nDARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target. PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.\nPQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure. The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation. Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.\nThe Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with the American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overall effort on Ada development.\nOther Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation. There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.\nHigh-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.\n\"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security.\" The \"Compiler Research: The Next 50 Years\" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.\nCompiler construction.\nA compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.\nIn the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.\nA compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.\nOne-pass versus multi-pass compilers.\nClassifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing much work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.\nThe ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).\nIn some cases, the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.\nThe disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.\nSplitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.\nThree-stage compiler structure.\nRegardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.\nThis front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end. Practical examples of this approach are the GNU Compiler Collection, Clang (LLVM-based C/C++ compiler), and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.\nFront end.\nThe front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.\nWhile the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably \"line reconstruction\" and \"preprocessing,\" but these are rare.\nThe main phases of the front end include the following:\nMiddle end.\nThe middle end, also known as \"optimizer,\" performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code. The middle end contains those optimizations that are independent of the CPU architecture being targeted.\nThe main phases of the middle end include the following:\nCompiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.\nThe scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program. There is a trade-off between the granularity of the optimizations and the cost of compilation. For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears. In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations that are only possible by considering the behavior of multiple functions simultaneously.\nInterprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.\nDue to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.\nBack end.\nThe back end is responsible for the CPU architecture specific optimizations and for code generation\".\"\nThe main phases of the back end include the following:\nCompiler correctness.\nCompiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.\nCompiled versus interpreted languages.\nHigher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that \"requires\" it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language \u2013 for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.\nInterpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language).\nFurther, compilers can contain interpreters for optimization reasons. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.\nSome language specifications spell out that implementations \"must\" include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.\nTypes.\nOne classification of compilers is by the platform on which their generated code executes. This is known as the \"target platform.\"\nA \"native\" or \"hosted\" compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.\nThe output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers.\nThe lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the codice_1 directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers.\nWhile a common compiler type outputs machine code, there are many other types:"}
{"id": "5741", "revid": "269251", "url": "https://en.wikipedia.org/wiki?curid=5741", "title": "Monetary policy of central banks", "text": ""}
{"id": "5742", "revid": "8882122", "url": "https://en.wikipedia.org/wiki?curid=5742", "title": "Castrato", "text": "A castrato (Italian, plural: \"castrati\") is a type of classical male singing voice equivalent to that of a soprano, mezzo-soprano, or contralto. The voice is produced by castration of the singer before puberty, or it occurs in one who, due to an endocrinological condition, never reaches sexual maturity.\nCastration before puberty (or in its early stages) prevents a boy's larynx from being transformed by the normal physiological events of puberty. As a result, the vocal range of prepubescence (shared by both sexes) is largely retained, and the voice develops into adulthood in a unique way. Prepubescent castration for this purpose diminished greatly in the late 18th century and was made illegal in the Papal States, the last to prohibit it, in 1870.\nAs the castrato's body grew, his lack of testosterone meant that his epiphyses (bone-joints) did not harden in the normal manner. Thus the limbs of the castrati often grew unusually long, as did their ribs. This, combined with intensive training, gave them unrivalled lung-power and breath capacity. Operating through small, child-sized vocal cords, their voices were also extraordinarily flexible, and quite different from the equivalent adult female voice. Their vocal range was higher than that of the uncastrated adult male. Listening to the only surviving recordings of a castrato (see below), one can hear that the lower part of the voice sounds like a \"super-high\" tenor, with a more falsetto-like upper register above that.\nCastrati were rarely referred to as such: in the 18th century, the euphemism \"musico\" (pl \"musici\") was much more generally used, although it usually carried derogatory implications; another synonym was \"evirato,\" literally meaning \"emasculated\". Eunuch is a more general term since, historically, many eunuchs were castrated after puberty and thus the castration had no impact on their voices.\nHistory.\nCastration as a means of subjugation, enslavement or other punishment has a very long history, dating back to ancient Sumer. In a Western context, eunuch singers are known to have existed from the early Byzantine Empire. In Constantinople around 400 AD, the empress Aelia Eudoxia had a eunuch choir-master, Brison, who may have established the use of castrati in Byzantine choirs, though whether Brison himself was a singer and whether he had colleagues who were eunuch singers is not certain. By the 9th century, eunuch singers were well-known (not least in the choir of Hagia Sophia) and remained so until the sack of Constantinople by the Western forces of the Fourth Crusade in 1204. Their fate from then until their reappearance in Italy more than three hundred years later is not clear. It seems likely that the Spanish tradition of soprano falsettists may have hidden castrati. Much of Spain was under Muslim rulers during the Middle Ages, and castration had a history going back to the ancient Near East. Stereotypically, eunuchs served as harem guards, but they were also valued as high-level political appointees since they could not start a dynasty which would threaten the ruler.\nEuropean classical tradition.\nCastrati first appeared in Italy in the mid-16th century, though at first the terms describing them were not always clear. The phrase \"soprano maschio\" (male soprano), which could also mean falsettist, occurs in the \"Due Dialoghi della Musica\" (Two dialogues upon music) of Luigi Dentice, an Oratorian priest, published in Rome in 1553. On 9 November 1555 Cardinal Ippolito II d'Este (famed as the builder of the Villa d'Este at Tivoli), wrote to Guglielmo Gonzaga, Duke of Mantua (1538\u20131587), that he has heard that the Duke was interested in his \"cantoretti\" (little singers) and offered to send him two, so that he could choose one for his own service. This is a rare term but probably does equate to \"castrato\". The Cardinal's nephew, Alfonso II d'Este, Duke of Ferrara, was another early enthusiast, inquiring about castrati in 1556. There were certainly castrati in the Sistine Chapel choir in 1558, although not described as such: on 27 April of that year, Hernando Bustamante, a Spaniard from Palencia, was admitted (the first castrati so termed who joined the Sistine choir were Pietro Paolo Folignato and Girolamo Rossini, admitted in 1599). Surprisingly, considering the later French distaste for castrati they certainly existed in France at this time also, being known of in Paris, Orl\u00e9ans, Picardy and Normandy, though they were not abundant: the King of France himself had difficulty in obtaining them. By 1574, there were castrati in the Ducal court chapel at Munich, where the Kapellmeister (music director) was the famous Orlando di Lasso. In 1589, by the bull \"Cum pro nostro pastorali munere\", Pope Sixtus V re-organised the choir of St Peter's, Rome specifically to include castrati.\nThus the castrati came to supplant both boys (whose voices broke after only a few years) and falsettists (whose voices were weaker and less reliable) from the top line in such choirs. Women were banned by the Pauline dictum \"mulieres in ecclesiis taceant\" (\"let women keep silent in the churches\"; see I Corinthians, ch. 14, v. 34).\nThe Italian castrati were often rumored to have unusually long lives, but a 1993 study found that their lifespans were average.\nOpera.\nAlthough the castrato (or musico) predates opera, there is some evidence that castrati had parts in the earliest operas. In the first performance of Monteverdi's \"Orfeo\" (1607), for example, they played subsidiary roles, including Speranza and (possibly) that of Euridice. Although female roles were performed by castrati in some of the papal states, this was increasingly rare; by 1680, they had supplanted \"normal\" male voices in lead roles, and retained their position as \"primo uomo\" for about a hundred years; an Italian opera not featuring at least one renowned castrato in a lead part would be doomed to fail. Because of the popularity of Italian opera throughout 18th-century Europe (except France), singers such as Ferri, Farinelli, Senesino and Pacchierotti became the first operatic superstars, earning enormous fees and hysterical public adulation. The strictly hierarchical organisation of \"opera seria\" favoured their high voices as symbols of heroic virtue, though they were frequently mocked for their strange appearance and bad acting. In his 1755 \"Reflections upon theatrical expression in tragedy\", Roger Pickering wrote:\nFarinelli drew every Body to the Haymarket. What a Pipe! What Modulation! What Extasy to the Ear! But, Heavens! What Clumsiness! What Stupidity! What Offence to the Eye! Reader, if of the City, thou mayest probably have seen in the Fields of Islington or Mile-End or, If thou art in the environs of St James', thou must have observed in the Park with what Ease and Agility a cow, heavy with calf, has rose up at the command of the Milk-woman's foot: thus from the mossy bank sprang the DIVINE FARINELLI.\nThe means by which future singers were prepared could lead to premature death. To prevent the child from experiencing the intense pain of castration, many were inadvertently administered lethal doses of opium or some other narcotic, or were killed by overlong compression of the carotid artery in the neck (intended to render them unconscious during the castration procedure).\nDuring the 18th century itself, the music historian Charles Burney was sent from pillar to post in search of places where \"the operation\" was carried out:\nI enquired throughout Italy at what place boys were chiefly qualified for singing by castration, but could get no certain intelligence. I was told at Milan that it was at Venice; at Venice that it was at Bologna; but at Bologna the fact was denied, and I was referred to Florence; from Florence to Rome, and from Rome I was sent to Naples\u00a0... it is said that there are shops in Naples with this inscription: 'QUI SI CASTRANO RAGAZZI' (\"Here boys are castrated\"); but I was utterly unable to see or hear of any such shops during my residence in that city.\nThe training of the boys was rigorous. The regimen of one singing school in Rome (c. 1700) consisted of one hour of singing difficult and awkward pieces, one hour practising trills, one hour practising ornamented passaggi, one hour of singing exercises in their teacher's presence and in front of a mirror so as to avoid unnecessary movement of the body or facial grimaces, and one hour of literary study; all this, moreover, before lunch. After, half an hour would be devoted to musical theory, another to writing counterpoint, an hour copying down the same from dictation, and another hour of literary study. During the remainder of the day, the young castrati had to find time to practice their harpsichord playing, and to compose vocal music, either sacred or secular depending on their inclination. This demanding schedule meant that, if sufficiently talented, they were able to make a debut in their mid-teens with a perfect technique and a voice of a flexibility and power no woman or ordinary male singer could match.\nIn the 1720s and 1730s, at the height of the craze for these voices, it has been estimated that upwards of 4,000 boys were castrated annually in the service of art. Many came from poor homes and were castrated by their parents in the hope that their child might be successful and lift them from poverty (this was the case with Senesino). There are, though, records of some young boys asking to be operated on to preserve their voices (e.g. Caffarelli, who was from a wealthy family: his grandmother gave him the income from two vineyards to pay for his studies). Caffarelli was also typical of many castrati in being famous for tantrums on and off-stage, and for amorous adventures with noble ladies. Some, as described by Casanova, preferred gentlemen (noble or otherwise). Only a small percentage of boys castrated to preserve their voices had successful careers on the operatic stage; the better \"also-rans\" sang in cathedral or church choirs, but because of their marked appearance and the ban on their marrying, there was little room for them in society outside a musical context.\nThe castrati came in for a great amount of scurrilous and unkind abuse, and as their fame increased, so did the hatred of them. They were often castigated as malign creatures who lured men into homosexuality. There were homosexual castrati, as Casanova's accounts of 18th-century Italy bear witness. He mentions meeting an abb\u00e9 whom he took for a girl in disguise, only later discovering that \"she\" was a famous castrato. In Rome in 1762 he attended a performance at which the prima donna was a castrato, \"the favourite pathic\" of Cardinal Borghese, who dined every evening with his protector. From his behaviour on stage \"it was obvious that he hoped to inspire the love of those who liked him as a man, and probably would not have done so as a woman\".\nDecline.\nBy the late 18th century, changes in operatic taste and social attitudes spelled the end for castrati. They lingered on past the end of the \"ancien r\u00e9gime\" (which their style of opera parallels), and two of their number, Pacchierotti and Crescentini, performed before Napoleon. The last great operatic castrato was Giovanni Battista Velluti (1781\u20131861), who performed the last operatic castrato role ever written: Armando in \"Il crociato in Egitto\" by Meyerbeer (Venice, 1824). Soon after this they were replaced definitively as the first men of the operatic stage by a new breed of heroic tenor, as first incarnated by the Frenchman Gilbert-Louis Duprez, the earliest so-called \"king of the high Cs\". His successors have included such singers as Enrico Tamberlik, Jean de Reszke, Francesco Tamagno, Enrico Caruso, Giovanni Martinelli, Beniamino Gigli, Jussi Bj\u00f6rling, Franco Corelli and Luciano Pavarotti, among others.\nAfter the unification of Italy in 1861, \"eviration\" was officially made illegal (the new Italian state had adopted the previous penal code of the Kingdom of Sardinia which expressly forbade the practice). In 1878, Pope Leo XIII prohibited the hiring of new castrati by the church: only in the Sistine Chapel and in other papal basilicas in Rome did a few castrati linger. A group photo of the Sistine Choir taken in 1898 shows that by then only six remained (plus the \"Direttore Perpetuo\", the fine soprano castrato Domenico Mustaf\u00e0), and in 1902 a ruling was extracted from Pope Leo that no further castrati should be admitted. The official end to the castrati came on St. Cecilia's Day, 22 November 1903, when the new pope, Pius X, issued his \"motu proprio\", \"Tra le Sollecitudini\" ('Amongst the Cares'), which contained this instruction: \"Whenever\u00a0... it is desirable to employ the high voices of sopranos and contraltos, these parts must be taken by boys, according to the most ancient usage of the Church.\"\nThe last Sistine castrato to survive was Alessandro Moreschi, the only castrato to have made solo recordings. While an interesting historical record, these discs of his give us only a glimpse of the castrato voice \u2013 although he had been renowned as \"The Angel of Rome\" at the beginning of his career, some would say he was past his prime when the recordings were made in 1902 and 1904 and he never attempted to sing opera. Domenico Salvatori, a castrato who was contemporary with Moreschi, made some ensemble recordings with him but has no surviving solo recordings. The recording technology of the day was not of modern high quality. Salvatori died in 1909; Moreschi retired officially in March 1913, and died in 1922.\nThe Catholic Church's involvement in the castrato phenomenon has long been controversial, and there have recently been calls for it to issue an official apology for its role. As early as 1748, Pope Benedict XIV tried to ban castrati from churches, but such was their popularity at the time that he realised that doing so might result in a drastic decline in church attendance.\nThe rumours of another castrato sequestered in the Vatican for the personal delectation of the Pontiff until as recently as 1959 have been proven false. The singer in question was a pupil of Moreschi's, Domenico Mancini, such a successful imitator of his teacher's voice that even Lorenzo Perosi, Direttore Perpetuo of the Sistine Choir from 1898 to 1956 and a strenuous opponent of the practice of castrato singers, thought he was a castrato. Mancini was in fact a moderately skilful falsettist and professional double bass player.\nModern castrati and similar voices.\nSo-called \"natural\" or \"endocrinological castrati\" are born with hormonal anomalies, such as Klinefelter's syndrome and Kallmann's syndrome, or have undergone unusual physical or medical events during their early lives that reproduce the vocal effects of castration without being castrated. Basically, a male can retain his child voice if it never changes during puberty. The retained voice can be the treble voice shared by both sexes in childhood and is the same as boy soprano voice. But as evidence shows, many castratos, such as Senesino and Caffarelli, were actually altos (mezzo-soprano) \u2013 not sopranos.\nJimmy Scott, Robert Crowe and Radu Marian are examples of this type of high male voice. Michael Maniaci is somewhat different, in that he has no hormonal or other anomalies, but for some unknown reason, his voice did not \"break\" in the usual manner, leaving him still able to sing in the soprano register. Other uncastrated male adults sing soprano, generally using some form of falsetto but in a much higher range than most countertenors. Examples are Aris Christofellis, J\u00f6rg Waschinski, and Ghio Nannini.\nHowever, it is believed the castrati possessed more of a tenorial chest register (the aria \"Navigante che non spera\" in Leonardo Vinci's opera \"Il Medo\", written for Farinelli, requires notes down to C3, 131\u00a0Hz). Similar low-voiced singing can be heard from the jazz vocalist Jimmy Scott, whose range matches approximately that used by female blues singers. High-pitched singer Jordan Smith has demonstrated having more of a tenorial chest register.\nActor Chris Colfer has soprano voice. Colfer has stated in interviews that when his voice began to change at puberty he sang in a high voice \"constantly\" in an effort to retain his range. Actor and singer Alex Newell has soprano range. Voice actor Walter Tetley may or may not have been a \"castrato\"; Bill Scott, a co-worker of Tetley's during their later work in television, once half-jokingly quipped that Tetley's mother \"had him fixed\" to protect the child star's voice-acting career. Tetley never personally divulged the exact reason for his condition, which left him with the voice of a preteen boy for his entire adult life. Famed agriculture professor George Washington Carver was also reputed to have been castrated and had a high, childlike voice and stunted growth even in adulthood.\nTurkish popular singer Cem Adrian has the ability to sing from bass to soprano, his vocal folds having been reported to be three times the average length."}
{"id": "5743", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5743", "title": "Counting-out game", "text": "A counting-out game or counting-out rhyme is a simple method of 'randomly' selecting a person from a group, often used by children for the purpose of playing another game. It usually requires no materials, and is achieved with spoken words or hand gestures. The historian Henry Carrington Bolton suggested in his 1888 book \"Counting Out Rhymes of Children\" that the custom of counting out originated in the \"superstitious practice of divination by lot.\"\nMany such methods involve one person pointing at each participant in a circle of players while reciting a rhyme. A new person is pointed at as each word is said. The player who is selected at the conclusion of the rhyme is \"it\" or \"out\". In an alternate version, the circle of players may each put two feet in and at the conclusion of the rhyme, that player removes one foot and the rhyme starts over with the next person. In this case, the first player that has both feet removed is \"it\" or \"out\". In theory a counting rhyme is determined entirely by the starting selection (and would result in a modulo operation), but in practice they are often accepted as random selections because the number of words has not been calculated beforehand, so the result is unknown until someone is selected.\nA variant of counting-out game, known as the Josephus problem, represents a famous theoretical problem in mathematics and computer science.\nExamples.\nSeveral simple games can be played to select one person from a group, either as a straightforward winner, or as someone who is eliminated. Rock, Paper, Scissors, Odd or Even and Blue Shoe require no materials and are played using hand gestures, although with the former it is possible for a player to win or lose through skill rather than luck. Coin flipping and drawing straws are fair methods of randomly determining a player. Fizz Buzz is a spoken word game where if a player slips up and speaks a word out of sequence, they are eliminated.\nCultural references.\nA scene in the Marx Brothers movie \"Duck Soup\" plays on the fact that counting-out games are not really random. Faced with selecting someone to go on a dangerous mission, the character Chicolini (Chico Marx) chants:\nonly to stop as he realizes he is about to select himself. He then says, \"I did it wrong. Wait, wait, I start here\", and repeats the chant\u2014with the same result. After that, he says, \"That's no good too. I got it!\" and reduces the chant to\nAnd with this version he finally manages to \"randomly\" select someone else."}
{"id": "5746", "revid": "6917124", "url": "https://en.wikipedia.org/wiki?curid=5746", "title": "Cryptography/Hashfunction", "text": ""}
{"id": "5747", "revid": "50457", "url": "https://en.wikipedia.org/wiki?curid=5747", "title": "Cryptography/Key", "text": ""}
{"id": "5749", "revid": "3627743", "url": "https://en.wikipedia.org/wiki?curid=5749", "title": "Key size", "text": "In cryptography, key size, key length, or key space is the number of bits in a key used by a cryptographic algorithm (such as a cipher).\nKey length defines the upper-bound on an algorithm's security (i.e. a logarithmic measure of the fastest known attack against an algorithm), since the security of all algorithms can be violated by brute-force attacks. Ideally, the lower-bound on an algorithm's security is by design equal to the key length (that is, the security is determined entirely by the keylength, or in other words, the algorithm's design doesn't detract from the degree of security inherent in the key length). Indeed, most symmetric-key algorithms are designed to have security equal to their key length. However, after design, a new attack might be discovered. For instance, Triple DES was designed to have a 168 bit key, but an attack of complexity 2112 is now known (i.e. Triple DES now only has 112 bits of security, and of the 168 bits in the key the attack has rendered 56 'ineffective' towards security). Nevertheless, as long as the security (understood as 'the amount of effort it would take to gain access') is sufficient for a particular application, then it doesn't matter if key length and security coincide. This is important for asymmetric-key algorithms, because no such algorithm is known to satisfy this property; elliptic curve cryptography comes the closest with an effective security of roughly half its key length.\nSignificance.\nKeys are used to control the operation of a cipher so that only the correct key can convert encrypted text (ciphertext) to plaintext. Many ciphers are actually based on publicly known algorithms or are open source and so it is only the difficulty of obtaining the key that determines security of the system, provided that there is no analytic attack (i.e. a \"structural weakness\" in the algorithms or protocols used), and assuming that the key is not otherwise available (such as via theft, extortion, or compromise of computer systems). The widely accepted notion that the security of the system should depend on the key alone has been explicitly formulated by Auguste Kerckhoffs (in the 1880s) and Claude Shannon (in the 1940s); the statements are known as Kerckhoffs' principle and Shannon's Maxim respectively.\nA key should, therefore, be large enough that a brute-force attack (possible against any encryption algorithm) is infeasible \u2013 i.e. would take too long to execute. Shannon's work on information theory showed that to achieve so-called \"perfect secrecy\", the key length must be at least as large as the message and only used once (this algorithm is called the one-time pad). In light of this, and the practical difficulty of managing such long keys, modern cryptographic practice has discarded the notion of perfect secrecy as a requirement for encryption, and instead focuses on \"computational security\", under which the computational requirements of breaking an encrypted text must be infeasible for an attacker.\nKey size and encryption system.\nEncryption systems are often grouped into families. Common families include symmetric systems (e.g. AES) and asymmetric systems (e.g. RSA); they may alternatively be grouped according to the central algorithm used (e.g. elliptic curve cryptography).\nAs each of these is of a different level of cryptographic complexity, it is usual to have different key sizes for the same level of security, depending upon the algorithm used. For example, the security available with a 1024-bit key using asymmetric RSA is considered approximately equal in security to an 80-bit key in a symmetric algorithm.\nThe actual degree of security achieved over time varies, as more computational power and more powerful mathematical analytic methods become available. For this reason, cryptologists tend to look at indicators that an algorithm or key length shows signs of potential vulnerability, to move to longer key sizes or more difficult algorithms. For example, , a 1039-bit integer was factored with the special number field sieve using 400 computers over 11 months. The factored number was of a special form; the special number field sieve cannot be used on RSA keys. The computation is roughly equivalent to breaking a 700 bit RSA key. However, this might be an advance warning that 1024 bit RSA used in secure online commerce should be deprecated, since they may become breakable in the near future. Cryptography professor Arjen Lenstra observed that \"Last time, it took nine years for us to generalize from a special to a nonspecial, hard-to-factor number\" and when asked whether 1024-bit RSA keys are dead, said: \"The answer to that question is an unqualified yes.\"\nThe 2015 Logjam attack revealed additional dangers in using Diffie-Helman key exchange when only one or a few common 1024-bit or smaller prime moduli are in use. This common practice allows large amounts of communications to be compromised at the expense of attacking a small number of primes.\nBrute-force attack.\nEven if a symmetric cipher is currently unbreakable by exploiting structural weaknesses in its algorithm, it is possible to run through the entire space of keys in what is known as a \"brute-force attack\". Since longer symmetric keys require exponentially more work to brute force search, a sufficiently long symmetric key makes this line of attack impractical.\nWith a key of length \"n\" bits, there are 2n possible keys. This number grows very rapidly as \"n\" increases. The large number of operations (2128) required to try all possible 128-bit keys is widely considered out of reach for conventional digital computing techniques for the foreseeable future. However, experts anticipate alternative computing technologies that may have processing power superior to current computer technology. If a suitably sized quantum computer capable of running Grover's algorithm reliably becomes available, it would reduce a 128-bit key down to 64-bit security, roughly a DES equivalent. This is one of the reasons why AES supports a 256-bit key length. See the discussion on the relationship between key lengths and quantum computing attacks at the bottom of this page for more information.\nSymmetric algorithm key lengths.\nUS Government export policy has long restricted the \"strength\" of cryptography that can be sent out of the country. For many years the limit was 40 bits. Today, a key length of 40 bits offers little protection against even a casual attacker with a single PC. In response, by the year 2000, most of the major US restrictions on the use of strong encryption were relaxed. However, not all regulations have been removed, and encryption registration with the U.S. Bureau of Industry and Security is still required to export \"mass market encryption commodities, software and components with encryption exceeding 64 bits\" ().\nIBM's Lucifer cipher was selected in 1974 as the base for what would become the Data Encryption Standard. Lucifer's key length was reduced from 128 bits to 56 bits, which the NSA and NIST argued was sufficient. The NSA has major computing resources and a large budget; some cryptographers including Whitfield Diffie and Martin Hellman complained that this made the cipher so weak that NSA computers would be able to break a DES key in a day through brute force parallel computing. The NSA disputed this, claiming that brute-forcing DES would take them something like 91 years. However, by the late 90s, it became clear that DES could be cracked in a few days' time-frame with custom-built hardware such as could be purchased by a large corporation or government. The book \"Cracking DES\" (O'Reilly and Associates) tells of the successful attempt in 1998 to break 56-bit DES by a brute-force attack mounted by a cyber civil rights group with limited resources; see EFF DES cracker. Even before that demonstration, 56 bits was considered insufficient length for symmetric algorithm keys; DES has been replaced in many applications by Triple DES, which has 112 bits of security when used 168-bit keys (triple key). In 2002, Distributed.net and its volunteers broke a 64-bit RC5 key after several years effort, using about seventy thousand (mostly home) computers.\nThe Advanced Encryption Standard published in 2001 uses key sizes of 128, 192 or 256 bits. Many observers consider 128 bits sufficient for the foreseeable future for symmetric algorithms of AES's quality until quantum computers become available. However, as of 2015, the U.S. National Security Agency has issued guidance that it plans to switch to quantum computing resistant algorithms and now requires 256-bit AES keys for data classified up to Top Secret.\nIn 2003, the U.S. National Institute for Standards and Technology, NIST proposed phasing out 80-bit keys by 2015. At 2005, 80-bit keys were allowed only until 2010.\nSince 2015, NIST guidance says that \"the use of keys that provide less than 112 bits of security strength for key agreement is now disallowed.\" NIST approved symmetric encryption algorithms include three-key Triple DES, and AES. Approvals for two-key Triple DES and Skipjack were withdrawn in 2015; the NSA's Skipjack algorithm used in its Fortezza program employs 80-bit keys.\nAsymmetric algorithm key lengths.\nThe effectiveness of public key cryptosystems depends on the intractability (computational and theoretical) of certain mathematical problems such as integer factorization. These problems are time-consuming to solve, but usually faster than trying all possible keys by brute force. Thus, asymmetric keys must be longer for equivalent resistance to attack than symmetric algorithm keys. The most common methods are assumed to be weak against sufficiently powerful quantum computers in the future.\nSince 2015, NIST recommends a minimum of 2048-bit keys for RSA, an update to the widely-accepted recommendation of a 1024-bit minimum since at least 2002.\n1024-bit RSA keys are equivalent in strength to 80-bit symmetric keys, 2048-bit RSA keys to 112-bit symmetric keys, 3072-bit RSA keys to 128-bit symmetric keys, and 15360-bit RSA keys to 256-bit symmetric keys. In 2003, RSA Security claimed that 1024-bit keys were likely to become crackable some time between 2006 and 2010, while 2048-bit keys are sufficient until 2030. the largest RSA key publicly known to be cracked is RSA-250 with 829 bits.\nThe Finite Field Diffie-Hellman algorithm has roughly the same key strength as RSA for the same key sizes. The work factor for breaking Diffie-Hellman is based on the discrete logarithm problem, which is related to the integer factorization problem on which RSA's strength is based. Thus, a 2048-bit Diffie-Hellman key has about the same strength as a 2048-bit RSA key.\nElliptic-curve cryptography (ECC) is an alternative set of asymmetric algorithms that is equivalently secure with shorter keys, requiring only approximately twice the bits as the equivalent symmetric algorithm. A 256-bit ECDH key has approximately the same safety factor as a 128-bit AES key. A message encrypted with an elliptic key algorithm using a 109-bit long key was broken in 2004.\nThe NSA previously recommended 256-bit ECC for protecting classified information up to the SECRET level, and 384-bit for TOP SECRET; In 2015 it announced plans to transition to quantum-resistant algorithms by 2024, and until then recommends 384-bit for all classified information.\nEffect of quantum computing attacks on key strength.\nThe two best known quantum computing attacks are based on Shor's algorithm and Grover's algorithm. Of the two, Shor's offers the greater risk to current security systems.\nDerivatives of Shor's algorithm are widely conjectured to be effective against all mainstream public-key algorithms including RSA, Diffie-Hellman and elliptic curve cryptography. According to Professor Gilles Brassard, an expert in quantum computing: \"The time needed to factor an RSA integer is the same order as the time needed to use that same integer as modulus for a single RSA encryption. In other words, it takes no more time to break RSA on a quantum computer (up to a multiplicative constant) than to use it legitimately on a classical computer.\" The general consensus is that these public key algorithms are insecure at any key size if sufficiently large quantum computers capable of running Shor's algorithm become available. The implication of this attack is that all data encrypted using current standards based security systems such as the ubiquitous SSL used to protect e-commerce and Internet banking and SSH used to protect access to sensitive computing systems is at risk. Encrypted data protected using public-key algorithms can be archived and may be broken at a later time.\nMainstream symmetric ciphers (such as AES or Twofish) and collision resistant hash functions (such as SHA) are widely conjectured to offer greater security against known quantum computing attacks. They are widely thought most vulnerable to Grover's algorithm. Bennett, Bernstein, Brassard, and Vazirani proved in 1996 that a brute-force key search on a quantum computer cannot be faster than roughly 2\"n\"/2 invocations of the underlying cryptographic algorithm, compared with roughly 2\"n\" in the classical case. Thus in the presence of large quantum computers an \"n\"-bit key can provide at least \"n\"/2 bits of security. Quantum brute force is easily defeated by doubling the key length, which has little extra computational cost in ordinary use. This implies that at least a 256-bit symmetric key is required to achieve 128-bit security rating against a quantum computer. As mentioned above, the NSA announced in 2015 that it plans to transition to quantum-resistant algorithms.\nAccording to NSA:\n, the NSA's Commercial National Security Algorithm Suite includes:"}
