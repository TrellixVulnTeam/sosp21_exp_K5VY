{"id": "7214", "revid": "1011809782", "url": "https://en.wikipedia.org/wiki?curid=7214", "title": "Callisto (mythology)", "text": "In Greek mythology, Callisto or Kallisto (; ) was a nymph, or the daughter of King Lycaon; the myth varies in such details. She was one of the followers of Artemis (Diana for the Romans) who attracted Zeus. According to some writers, Zeus transformed himself into the figure of Artemis to lure Callisto and seduce her. She became pregnant and when this was eventually discovered, she was expelled from Artemis's group, after which a furious Hera, the wife of Zeus, transformed her into a bear. Later, just as she was about to be killed by her son when he was hunting, she was set among the stars as Ursa Major (\"the Great Bear\"). She was the bear-mother of the Arcadians, through her son Arcas by Zeus.\nThe fourth Galilean moon of Jupiter and a main belt asteroid are named after Callisto.\nMyth.\nAs a follower of Artemis, Callisto, who Hesiod said was the daughter of Lycaon, king of Arcadia, took a vow to remain a virgin, as did all the nymphs of Artemis.\nAccording to Hesiod, she was seduced by Zeus, and of the consequences that followed:\n[Callisto] chose to occupy herself with wild-beasts in the mountains together with Artemis, and, when she was seduced by Zeus, continued some time undetected by the goddess, but afterwards, when she was already with child, was seen by her bathing and so discovered. Upon this, the goddess was enraged and changed her into a beast. Thus she became a bear and gave birth to a son called Arkas. \nAccording to the mythographer Apollodorus, Zeus disguised himself as Artemis or Apollo, in order to lure Callisto into his embrace.\nAccording to Ovid, it was Jupiter who took the form of Diana so that he might evade his wife Juno's detection, forcing himself upon Callisto while she was separated from Diana and the other nymphs. Callisto's subsequent pregnancy was discovered several months later while she was bathing with Diana and her fellow nymphs. Diana became enraged when she saw that Callisto was pregnant and expelled her from the group. Callisto later gave birth to Arcas. Juno then took the opportunity to avenge her wounded pride and transformed the nymph into a bear. Sixteen years later Callisto, still a bear, encountered her son Arcas hunting in the forest. Just as Arcas was about to kill his own mother with his javelin, Jupiter averted the tragedy by placing mother and son amongst the stars as Ursa Major and Minor, respectively. Juno, enraged that her attempt at revenge had been frustrated, appealed to Tethys that the two might never meet her waters, thus providing a poetic explanation for their circumpolar positions in ancient times.\nEither Artemis \"slew Kallisto with a shot of her silver bow,\" perhaps urged by the wrath of Juno (Hera) or later Arcas, the eponym of Arcadia, nearly killed his bear-mother, when she had wandered into the forbidden precinct of Zeus. In every case, Zeus placed them both in the sky as the constellations Ursa Major, called \"Arktos\" (\u03b1\u03c1\u03ba\u03c4\u03bf\u03c2), the \"Bear\", by Greeks, and Ursa Minor.\nOrigin of the myth.\nThe name \"Kalliste\" (), \"most beautiful\", may be recognized as an epithet of the goddess herself, though none of the inscriptions at Athens that record priests of \"Artemis Kalliste\" (), date before the third century BCE. Artemis Kalliste was worshiped in Athens in a shrine which lay outside the Dipylon gate, by the side of the road to the Academy. W. S. Ferguson suggested that Artemis Soteira and Artemis Kalliste were joined in a common cult administered by a single priest. The bearlike character of Artemis herself was a feature of the Brauronia.\nThe myth in \"Catasterismi\" may be derived from the fact that a set of constellations appear close together in the sky, in and near the Zodiac sign of Libra, namely Ursa Minor, Ursa Major, Bo\u00f6tes, and Virgo. The constellation Bo\u00f6tes, was explicitly identified in the Hesiodic \"Astronomia\" () as Arcas, the \"Bear-warden\" (\"Arktophylax\"; ):\nHe is Arkas the son of Kallisto and Zeus, and he lived in the country about Lykaion. After Zeus had seduced Kallisto, Lykaon, pretending not to know of the matter, entertained Zeus, as Hesiod says, and set before him on the table the babe [Arkas] which he had cut up.\nThe stars of Ursa Major were all circumpolar in Athens of 400 BCE, and all but the stars in the Great Bear's left foot were circumpolar in Ovid's Rome, in the first century CE. Now, however, due to the precession of the equinoxes, the feet of the Great Bear constellation do sink below the horizon from Rome and especially from Athens; however, Ursa Minor (Arcas) does remain completely above the horizon, even from latitudes as far south as Honolulu and Hong Kong.\nAccording to Julien d'Huy, who used phylogenetic and statistical tools, the story could be a recent transformation of a Palaeolithic myth.\nIn art.\nCallisto's story was sometimes depicted in classical art, where the moment of transformation into a bear was the most popular. From the Renaissance on a series of major history paintings as well as many smaller cabinet paintings and book illustrations, usually called \"Diana and Callisto\", depicted the traumatic moment of discovery of the pregnancy, as the goddess and her nymphs bathed in a pool, following Ovid's account. The subject's attraction was undoubtedly mainly the opportunity it offered for a group of several females to be shown largely nude.Titian's \"Diana and Callisto\" (1556-1559), was the greatest (though not the first) of these, quickly disseminated by a print by Cornelius Cort. Here, as in most subsequent depictions, Diana points angrily, as Callisto is held by two nymphs, who may be pulling off what little clothing remains on her. Other versions include one by Rubens, and \"Diana Bathing with her Nymphs with Actaeon and Callisto\" by Rembrandt, which unusually combines the moment with the arrival of Actaeon. The basic composition is rather unusually consistent. Carlo Ridolfi said there was a version by Giorgione, who died in 1510, though his many attributions to Giorgione of paintings that are now lost are treated with suspicion by scholars. Other, less dramatic, treatments before Titian established his composition are by Palma Vecchio and Dosso Dossi. Annibale Carracci's \"The Loves of the Gods\" includes an image of Juno urging Diana to shoot Callisto in ursine form.\nAlthough Ovid places the discovery in the ninth month of Callisto's pregnancy, in paintings she is generally shown with a rather modest bump for late pregnancy. With the \"Visitation\" in religious art, this was the leading recurring subject in history painting that required showing pregnancy in art, which Early Modern painters still approached with some caution. In any case, the narrative required that the rest of the group had not previously noticed the pregnancy.\nCallisto being seduced by Zeus/Jupiter in disguise was also a popular subject, usually called \"Jupiter and Callisto\"; it was the clearest common subject with lesbian lovers from classical mythology. The two lovers are usually shown happily embracing in a bower. The violent rape described by Ovid as following Callisto's realization of what is going on is rarely shown. In versions before about 1700 Callisto may show some doubt about what is going on, as in the versions by Rubens. It was especially popular in the 18th century, when depictions were increasingly erotic; Fran\u00e7ois Boucher painted several versions.\nAeschylus' tragedy \"Callisto\" is lost."}
{"id": "7216", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7216", "title": "Centromer", "text": ""}
{"id": "7218", "revid": "38193755", "url": "https://en.wikipedia.org/wiki?curid=7218", "title": "Cookie", "text": "A cookie is a baked or cooked food that is typically small, flat and sweet. It usually contains flour, sugar and some type of oil or fat. It may include other ingredients such as raisins, oats, chocolate chips, nuts, etc.\nIn most English-speaking countries except for the United States, crunchy cookies are called biscuits. Many Canadians also use this term. Chewier biscuits are sometimes called \"cookies\" even in the United Kingdom. Some cookies may also be named by their shape, such as date squares or bars.\nBiscuit or cookie variants include sandwich biscuits, such as custard creams, Jammie Dodgers, Bourbons and Oreos, with marshmallow or jam filling and sometimes dipped in chocolate or another sweet coating. Cookies are often served with beverages such as milk, coffee or tea and sometimes \"dunked\", an approach which releases more flavour from confections by dissolving the sugars, while also softening their texture. Factory-made cookies are sold in grocery stores, convenience stores and vending machines. Fresh-baked cookies are sold at bakeries and coffeehouses, with the latter ranging from small business-sized establishments to multinational corporations such as Starbucks.\nTerminology.\nIn many English-speaking countries outside North America, including the United Kingdom, the most common word for a crisp cookie is biscuit. The term cookie is normally used to describe chewier ones. However, in many regions both terms are used. The container used to store cookies may be called a cookie jar.\nIn Scotland the term cookie is sometimes used to describe a plain bun.\nCookies that are baked as a solid layer on a sheet pan and then cut, rather than being baked as individual pieces, are called in British English bar cookies or traybakes.\nEtymology.\nThe word dates from at least 1701 in Scottish usage where the word meant \"plain bun\", rather than thin baked good, and so it is not certain whether it is the same word. From 1808, the word \"cookie\" is attested \"...in the sense of \"small, flat, sweet cake\" in American English. The American use is derived from Dutch koekje \"little cake,\" which is a diminutive of \"koek\" (\"cake\"), which came from the Middle Dutch word \"koke\". Another claim is that the American name derives from the Dutch word \"koekje\" or more precisely its informal, dialect variant \"koekie\" which means \"little cake,\" and arrived in American English with the Dutch settlement of New Netherland, in the early 1600s.\nAccording to the Scottish National Dictionary, its Scottish name derives from the diminutive form (+ suffix \"-ie\") of the word \"cook\", giving the Middle Scots \"cookie\", \"cooky\" or \"cu(c)kie\". There was much trade and cultural contact across the North Sea between the Low Countries and Scotland during the Middle Ages, which can also be seen in the history of curling and, perhaps, golf.\nDescription.\nCookies are most commonly baked until crisp or just long enough that they remain soft, but some kinds of cookies are not baked at all. Cookies are made in a wide variety of styles, using an array of ingredients including sugars, spices, chocolate, butter, peanut butter, nuts, or dried fruits. The softness of the cookie may depend on how long it is baked.\nA general theory of cookies may be formulated this way. Despite its descent from cakes and other sweetened breads, the cookie in almost all its forms has abandoned water as a medium for cohesion. Water in cakes serves to make the base (in the case of cakes called \"batter\") as thin as possible, which allows the bubbles \u2013 responsible for a cake's fluffiness \u2013 to better form. In the cookie, the agent of cohesion has become some form of oil. Oils, whether they be in the form of butter, vegetable oils, or lard, are much more viscous than water and evaporate freely at a much higher temperature than water. Thus a cake made with butter or eggs instead of water is far denser after removal from the oven.\nOils in baked cakes do not behave as baking soda tends to in the finished result. Rather than evaporating and thickening the mixture, they remain, saturating the bubbles of escaped gases from what little water there might have been in the eggs, if added, and the carbon dioxide released by heating the baking powder. This saturation produces the most texturally attractive feature of the cookie, and indeed all fried foods: crispness saturated with a moisture (namely oil) that does not sink into it.\nHistory.\nCookie-like hard wafers have existed for as long as baking is documented, in part because they deal with travel very well, but they were usually not sweet enough to be considered cookies by modern standards.\nCookies appear to have their origins in 7th century AD Persia, shortly after the use of sugar became relatively common in the region. They spread to Europe through the Muslim conquest of Spain. By the 14th century, they were common in all levels of society throughout Europe, from royal cuisine to street vendors.\nWith global travel becoming widespread at that time, cookies made a natural travel companion, a modernized equivalent of the travel cakes used throughout history. One of the most popular early cookies, which traveled especially well and became known on every continent by similar names, was the jumble, a relatively hard cookie made largely from nuts, sweetener, and water.\nCookies came to America through the Dutch in New Amsterdam in the late 1620s. The Dutch word \"koekje\" was Anglicized to \"cookie\" or cooky. The earliest reference to cookies in America is in 1703, when \"The Dutch in New York provided...'in 1703...at a funeral 800 cookies...'\"\nThe most common modern cookie, given its style by the creaming of butter and sugar, was not common until the 18th century.\nClassification.\nCookies are broadly classified according to how they are formed or made, including at least these categories:\nOther types of cookies are classified for other reasons, such as their ingredients, size, or intended time of serving:\nReception.\nLeah Ettman from Nutrition Action has criticized the high calorie count and fat content of supersized cookies, which are extra large cookies; she cites the Panera Kitchen Sink Cookie, a supersized chocolate chip cookie, which measures 5 1/2 inches in diameter and has 800 calories. For busy people who eat breakfast cookies in the morning, Kate Bratskeir from the \"Huffington Post\" recommends lower-sugar cookies filled with \"heart-healthy nuts and fiber-rich oats\". A book on nutrition by Paul Insel et al notes that \"low-fat\" or \"diet cookies\" may have the same number of calories as regular cookies, due to added sugar.\nPopular culture.\nThere are a number of slang usages of the term \"cookie\". The slang use of \"cookie\" to mean a person, \"especially an attractive woman\" is attested to in print since 1920. The catchphrase \"that's the way the cookie crumbles\", which means \"that's just the way things happen\" is attested to in print in 1955. Other slang terms include \"smart cookie\u201d and \u201ctough cookie.\u201d According to \"The Cambridge International Dictionary of Idioms\", a smart cookie is \u201csomeone who is clever and good at dealing with difficult situations.\u201d The word \"cookie\" has been vulgar slang for \"vagina\" in the US since 1970. The word \"cookies\" is used to refer to the contents of the stomach, often in reference to vomiting (e.g., \"pop your cookies\" a 1960s expression, or \"toss your cookies\", a 1970s expression). The expression \"cookie cutter\", in addition to referring literally to a culinary device to rolled cookie dough into shapes, is also used metaphorically to refer to items or things \"having the same configuration or look as many others\" (e.g., a \"cookie cutter tract house\") or to label something as \"stereotyped or formulaic\" (e.g., an action movie filled with \"generic cookie cutter characters\").\n\"Cookie duster\" is a whimsical expression for a mustache.\nCookie Monster is a Muppet on the long-running children's television show \"Sesame Street.\" He is best known for his voracious appetite for cookies and his famous eating phrases, such as \"Me want cookie!\", \"Me eat cookie!\" (or simply \"COOKIE!\"), and \"Om nom nom nom\" (said through a mouth full of food)."}
{"id": "7220", "revid": "38527097", "url": "https://en.wikipedia.org/wiki?curid=7220", "title": "Common Gateway Interface", "text": "In computing, Common Gateway Interface (CGI) is an interface specification that enables web servers to execute an external program, typically to process user requests. \nSuch programs are often written in a scripting language and are commonly referred to as \"CGI scripts\", but they may include compiled programs. \nA typical use case occurs when a Web user submits a Web form on a web page that uses CGI. The form's data is sent to the Web server within an HTTP request with a URL denoting a CGI script. The Web server then launches the CGI script in a new computer process, passing the form data to it. The output of the CGI script, usually in the form of HTML, is returned by the script to the Web server, and the server relays it back to the browser as its response to the browser's request.\nDeveloped in the early 1990s, CGI was the earliest common method available that allowed a Web page to be interactive. Although still in use, CGI is relatively inefficient compared to newer technologies and has largely been replaced by them.\nHistory.\nIn 1993 the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list. The other Web server developers adopted it, and it has been a standard for Web servers ever since. A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined. This work resulted in RFC 3875, which specified CGI Version 1.1. Specifically mentioned in the RFC are the following contributors:\nHistorically CGI scripts were often written using the C language. RFC 3875 \"The Common Gateway Interface (CGI)\" partially defines CGI using C, in saying that environment variables \"are accessed by the C library routine getenv() or variable environ\".\nThe name CGI comes from the early days of the Web, where \"Web masters\" wanted to connect legacy information systems such as databases to their Web servers. The CGI program was executed by the server that provided a common \"gateway\" between the Web server and the legacy information system.\nPurpose of the CGI specification.\nEach Web server runs HTTP server software, which responds to requests from web browsers. Generally, the HTTP server has a directory (folder), which is designated as a document collection \u2014 files that can be sent to Web browsers connected to this server. For example, if the Web server has the domain name codice_1, and its document collection is stored at codice_2 in the local file system, then the Web server will respond to a request for codice_3 by sending to the browser the (pre-written) file codice_4.\nFor pages constructed on the fly, the server software may defer requests to separate programs and relay the results to the requesting client (usually, a Web browser that displays the page to the end user). In the early days of the Web, such programs were usually small and written in a scripting language; hence, they were known as \"scripts\".\nSuch programs usually require some additional information to be specified with the request. For instance, if Wikipedia were implemented as a script, one thing the script would need to know is whether the user is logged in and, if logged in, under which name. The content at the top of a Wikipedia page depends on this information.\nHTTP provides ways for browsers to pass such information to the Web server, e.g. as part of the URL. The server software must then pass this information through to the script somehow.\nConversely, upon returning, the script must provide all the information required by HTTP for a response to the request: the HTTP status of the request, the document content (if available), the document type (e.g. HTML, PDF, or plain text), et cetera.\nInitially, different server software would use different ways to exchange this information with scripts. As a result, it wasn't possible to write scripts that would work unmodified for different server software, even though the information being exchanged was the same. Therefore, it was decided to specify a way for exchanging this information: CGI (the \"Common Gateway Interface\", as it defines a common way for server software to interface with scripts).\nWebpage generating programs invoked by server software that operate according to the CGI specification are known as \"CGI scripts\".\nThis specification was quickly adopted and is still supported by all well-known server software, such as Apache, IIS, and (with an extension) node.js-based servers.\nAn early use of CGI scripts was to process forms. In the beginning of HTML, HTML forms typically had an \"action\" attribute and a button designated as the \"submit\" button. When the submit button is pushed the URI specified in the \"action\" attribute would be sent to the server with the data from the form sent as a query string. If the \"action\" specifies a CGI script then the CGI script would be executed and it then produces an HTML page.\nUsing CGI scripts.\nA Web server allows its owner to configure which URLs shall be handled by which CGI scripts.\nThis is usually done by marking a new directory within the document collection as containing CGI scripts \u2014 its name is often codice_5. For example, codice_6 could be designated as a CGI directory on the Web server. When a Web browser requests a URL that points to a file within the CGI directory (e.g., codice_7), then, instead of simply sending that file (codice_8) to the Web browser, the HTTP server runs the specified script and passes the output of the script to the Web browser. That is, anything that the script sends to standard output is passed to the Web client instead of being shown on-screen in a terminal window.\nAs remarked above, the CGI specification defines how additional information passed with the request is passed to the script.\nFor instance, if a slash and additional directory name(s) are appended to the URL immediately after the name of the script (in this example, codice_9), then that path is stored in the codice_10 environment variable before the script is called. If parameters are sent to the script via an HTTP GET request (a question mark appended to the URL, followed by param=value pairs; in the example, codice_11), then those parameters are stored in the codice_12 environment variable before the script is called. If parameters are sent to the script via an HTTP POST request, they are passed to the script's standard input. The script can then read these environment variables or data from standard input and adapt to the Web browser's request.\nExample.\nThe following Perl program shows all the environment variables passed by the Web server:\n=head1 DESCRIPTION\nprintenv \u2014 a CGI program that just prints its environment\n=cut\nprint \"Content-Type: text/plain\\n\\n\";\nfor my $var ( sort keys %ENV ) {\n printf \"%s=\\\"%s\\\"\\n\", $var, $ENV{$var};\nIf a Web browser issues a request for the environment variables at codice_13, a 64-bit Windows 7 Web server running cygwin returns the following information:\n COMSPEC=\"C:\\Windows\\system32\\cmd.exe\"\n DOCUMENT_ROOT=\"C:/Program Files (x86)/Apache Software Foundation/Apache2.4/htdocs\"\n GATEWAY_INTERFACE=\"CGI/1.1\"\n HOME=\"/home/SYSTEM\"\n HTTP_ACCEPT=\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n HTTP_ACCEPT_CHARSET=\"ISO-8859-1,utf-8;q=0.7,*;q=0.7\"\n HTTP_ACCEPT_ENCODING=\"gzip, deflate, br\"\n HTTP_ACCEPT_LANGUAGE=\"en-us,en;q=0.5\"\n HTTP_CONNECTION=\"keep-alive\"\n HTTP_HOST=\"example.com\"\n HTTP_USER_AGENT=\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:67.0) Gecko/20100101 Firefox/67.0\"\n PATH=\"/home/SYSTEM/bin:/bin:/cygdrive/c/progra~2/php:/cygdrive/c/windows/system32:...\"\n PATHEXT=\".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\"\n PATH_INFO=\"/foo/bar\"\n PATH_TRANSLATED=\"C:\\Program Files (x86)\\Apache Software Foundation\\Apache2.4\\htdocs\\foo\\bar\"\n QUERY_STRING=\"var1=value1&amp;var2=with%20percent%20encoding\"\n REMOTE_ADDR=\"127.0.0.1\"\n REMOTE_PORT=\"63555\"\n REQUEST_METHOD=\"GET\"\n REQUEST_URI=\"/cgi-bin/printenv.pl/foo/bar?var1=value1&amp;var2=with%20percent%20encoding\"\n SCRIPT_FILENAME=\"C:/Program Files (x86)/Apache Software Foundation/Apache2.4/cgi-bin/printenv.pl\"\n SCRIPT_NAME=\"/cgi-bin/printenv.pl\"\n SERVER_ADDR=\"127.0.0.1\"\n SERVER_ADMIN=\"(server admin's email address)\"\n SERVER_NAME=\"127.0.0.1\"\n SERVER_PORT=\"80\"\n SERVER_PROTOCOL=\"HTTP/1.1\"\n SERVER_SIGNATURE=\"\"\n SERVER_SOFTWARE=\"Apache/2.4.39 (Win32) PHP/7.3.7\"\n SYSTEMROOT=\"C:\\Windows\"\n TERM=\"cygwin\"\n WINDIR=\"C:\\Windows\"\nSome, but not all, of these variables are defined by the CGI standard.\nSome, such as codice_10, codice_12, and the ones starting with codice_16, pass information along from the HTTP request.\nFrom the environment, it can be seen that the Web browser is Firefox running on a Windows 7 PC, the Web server is Apache running on a system that emulates Unix, and the CGI script is named codice_17.\nThe program could then generate any content, write that to standard output, and the Web server will transmit it to the browser.\nThe following are environment variables passed to CGI programs:\nThe program returns the result to the Web server in the form of standard output, beginning with a header and a blank line.\nThe header is encoded in the same way as an HTTP header and must include the MIME type of the document returned. The headers, supplemented by the Web server, are generally forwarded with the response back to the user.\nHere is a simple CGI program written in Python 3 along with the HTML that handles a simple addition problem.\ncodice_42:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n &lt;body&gt;\n &lt;form action=\"add.cgi\" method=\"POST\"&gt;\n &lt;fieldset&gt;\n &lt;legend&gt;Enter two numbers to add&lt;/legend&gt;\n &lt;label&gt;First Number: &lt;input type=\"number\" name=\"num1\"&gt;&lt;/label&gt;&lt;br&gt;\n &lt;label&gt;Second Number: &lt;input type=\"number\" name=\"num2\"&gt;&lt;/label&gt;&lt;br&gt;\n &lt;/fieldset&gt;\n &lt;button&gt;Add&lt;/button&gt;\n &lt;/form&gt;\n &lt;/body&gt;\n&lt;/html&gt;\ncodice_43:\nimport cgi, cgitb\ncgitb.enable()\ninput_data = cgi.FieldStorage()\nprint('Content-Type: text/html') # HTML is following\nprint(\") # Leave a blank line\nprint('&lt;h1&gt;Addition Results&lt;/h1&gt;')\ntry:\n num1 = int(input_data[\"num1\"].value)\n num2 = int(input_data[\"num2\"].value)\nexcept:\n print('&lt;output&gt;Sorry, the script cannot turn your inputs into numbers (integers).&lt;/output&gt;')\n raise SystemExit(1)\nprint('&lt;output&gt;{0} + {1} = {2}&lt;/output&gt;'.format(num1, num2, num1 + num2))\nThis Python 3 CGI program gets the inputs from the HTML and adds the two numbers together.\nDeployment.\nA Web server that supports CGI can be configured to interpret a URL that it serves as a reference to a CGI script. A common convention is to have a codice_44 directory at the base of the directory tree and treat all executable files within this directory (and no other, for security) as CGI scripts. Another popular convention is to use filename extensions; for instance, if CGI scripts are consistently given the extension codice_45, the Web server can be configured to interpret all such files as CGI scripts. While convenient, and required by many prepackaged scripts, it opens the server to attack if a remote user can upload executable code with the proper extension.\nIn the case of HTTP PUT or POSTs, the user-submitted data are provided to the program via the standard input. The Web server creates a subset of the environment variables passed to it and adds details pertinent to the HTTP environment.\nUses.\nCGI is often used to process input information from the user and produce the appropriate output. An example of a CGI program is one implementing a wiki. If the user agent requests the name of an entry, the Web server executes the CGI program. The CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result. The Web server receives the output from the CGI program and transmits it to the user agent. Then if the user agent clicks the \"Edit page\" button, the CGI program populates an HTML codice_46 or other editing control with the page's contents. Finally if the user agent clicks the \"Publish page\" button, the CGI program transforms the updated HTML into the source of that entry's page and saves it.\nSecurity.\nCGI programs run, by default, in the security context of the Web server. When first introduced a number of example scripts were provided with the reference distributions of the NCSA, Apache and CERN Web servers to show how shell scripts or C programs could be coded to make use of the new CGI. One such example script was a CGI program called PHF that implemented a simple phone book.\nIn common with a number of other scripts at the time, this script made use of a function: codice_47. The function was supposed to sanitize its argument, which came from user input and then pass the input to the Unix shell, to be run in the security context of the Web server. The script did not correctly sanitize all input and allowed new lines to be passed to the shell, which effectively allowed multiple commands to be run. The results of these commands were then displayed on the Web server. If the security context of the Web server allowed it, malicious commands could be executed by attackers.\nThis was the first widespread example of a new type of Web based attack, where unsanitized data from Web users could lead to execution of code on a Web server. Because the example code was installed by default, attacks were widespread and led to a number of security advisories in early 1996.\nAlternatives.\nFor each incoming HTTP request, a Web server creates a new CGI process for handling it and destroys the CGI process after the HTTP request has been handled. Creating and destroying a process can consume much more CPU and memory than the actual work of generating the output of the process, especially when the CGI program still needs to be interpreted by a virtual machine. For a high number of HTTP requests, the resulting workload can quickly overwhelm the Web server.\nThe overhead involved in CGI process creation and destruction can be reduced by the following techniques:\nThe optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget. Web frameworks offer an alternative to using CGI scripts to interact with user agents."}
{"id": "7222", "revid": "16185737", "url": "https://en.wikipedia.org/wiki?curid=7222", "title": "Choctaw", "text": "The Choctaw (in the Choctaw language, Chahta) are a Native American people originally occupying what is now the Southeastern United States (modern-day Alabama, Florida, Mississippi and Louisiana). Their Choctaw language belongs to the Muskogean language family group. In the present day, they are organized as the federally recognized Choctaw Nation, but also smaller bands located in Mississippi, Alabama and Louisiana.\nThe Choctaw people are descendants of the Hopewell and Mississippian cultures, who lived throughout the east of the Mississippi River valley and its tributaries. About 1,700 years ago, the Hopewell people built Nanih Waiya, a great earthwork mound located in what is central present-day Mississippi. It is still considered sacred by the Choctaw. The early Spanish explorers of the mid-16th century in the Southeast encountered Mississippian-culture villages and chiefs. The anthropologist John R. Swanton suggested that the Choctaw derived their name from an early leader. Henry Halbert, a historian, suggests that their name is derived from the Choctaw phrase \"Hacha hatak\" (river people).\nThe Choctaw coalesced as a people in the 17th century and developed three distinct political and geographical divisions: eastern, western and southern. These different groups sometimes created distinct, independent alliances with nearby European powers. These included the French, based on the Gulf Coast and in Louisiana; the English of the Southeast and the Spanish of Florida and Louisiana during the colonial era. \nDuring the American Revolution, most Choctaw supported the Thirteen Colonies' bid for independence from the British Crown. Together with the Chickasaw, the Choctaw fought as allies of the new United States under General Anthony Wayne against the Indians of the old Northwest Territory. The Choctaw also fought as allies of the U.S. during the War of 1812 and the Red Stick War, and most notably at the Battle of New Orleans. The Choctaw never went to war against the United States but they were forcibly relocated in 1831-1833, as part of the Indian Removal, in order for the US to take over their land for development by European Americans.\nIn the 19th century, the Choctaw were classified by European Americans as one of the \"Five Civilized Tribes\" because they adopted numerous practices of their United States neighbors. The Choctaw and the United States (US) agreed to nine treaties. By the last three, the US gained vast land cessions; they removed most Choctaw to Indian Territory, sending them on a forced migration far from their homelands. The Choctaw were the first Native American tribe forced to relocate under the Indian Removal Act. The Choctaw were exiled because the U.S. government wanted to use their resources, focusing primarily on settlements and agricultural development by European Americans. Some US leaders believed that by reducing conflict between the peoples, they were saving the Choctaw from extinction. The Choctaw negotiated the largest area and most desirable lands in Indian Territory. Their early government had three districts, each with its own chief, who together with the town chiefs sat on their National Council. They appointed a Choctaw Delegate to represent them to the US government in Washington, DC.\nBy the 1831 Treaty of Dancing Rabbit Creek, those Choctaw who chose to stay in the newly formed state of Mississippi were to be considered state and U.S. citizens; they were one of the first major non-European ethnic groups to be granted citizenship. Article 14 in the 1830 treaty with the Choctaw stated Choctaws may wish to become citizens of the United States under the 14th Article of the Treaty of Dancing Rabbit Creek on all of the combined lands which were consolidated under Article I from all previous treaties between the United States and the Choctaw.\nDuring the American Civil War, against the advising of Peter Pitchlynn, the Choctaw in both Oklahoma and Mississippi mostly sided with the Confederate States of America. The Confederacy had suggested to their leaders that it would support a state under Indian control if it won the war.\nAfter the Civil War, the Mississippi, Alabama and Louisiana Choctaw fell into obscurity for some time. The Choctaw in Oklahoma no longer considered the Mississippi Choctaw part of the Choctaw Nation. However, Jack Amos legally challenged the Choctaw Nation's stance at the turn of the 20th century.\nIn 1978, the Supreme Court of the United States held that all remnants of the Choctaw Nation are entitled to all rights of the federally recognized Nation. The American Indian Policy Review Commission Final Report Volume I, Chapter 11, Page 468 on May 19, 1977 federally acknowledged/recognized the existence of the Choctaw Communities of Mobile and Washington Counties which are along the Tombigbee and Mobile Rivers where Choctaw Treaties were negotiated in various Choctaw Treaties.\nThe Choctaw in Oklahoma struggled to build a nation. They transferred the Choctaw Academy there and opened an academy for girls in the 1840s. In the aftermath of the Dawes Act in the late 19th century, the US dissolved tribal governments in order to extinguish Indian land claims and admit the Indian and Oklahoma territories as a state in 1907. From that period, the US appointed chiefs of the Choctaw and other tribes in the former Indian Territory.\nDuring World War I, Choctaw soldiers served in the U.S. military as the first Native American codetalkers, using the Choctaw language. After the Indian Reorganization Act of 1934, the Choctaw reconstituted their government. The Choctaw Nation had kept their culture alive despite years of pressure for assimilation.\nThe Choctaw are the third-largest federally recognized tribe. Since the mid-twentieth century, the Choctaw have created new institutions, such as a tribal college, housing authority and justice system. Today the Choctaw Nation of Oklahoma, the Mississippi Band of Choctaw Indians and the Jena Band of Choctaw Indians are the federally recognized Choctaw tribes. Mississippi also recognizes another band and smaller Choctaw groups are located in Louisiana, Alabama and Texas. The Alabama Choctaw who are federally recognized under 24 C.F.R 1000 and 25 U.S.C. 4101 called the Native American Housing Self-Determination Act of 1986 (formerly the \"Indian\" Housing Act of 1937) under which the United States Federal Government jointly owns the MOWA Choctaw Indian Reservation as land held in trust as a reservation and for the MOWA Band of Choctaw Indians per multiple deeds in public records in Mobile County, Alabama Department of Revenue Records. The Department of Interior has listed the MOWA Band of Choctaw Indians as a trustee of Natural Resources in the Southeast Region of the United States. The National Park Service under the Secretary of Interior has posted public notice of the MOWA Choctaw Indian Reservation in Alabama. The Office of the Secretary of Interior issued the MOWA Band of Choctaw Indians its Federal Bureau of Investigations ORI number formally acknowledging the Government to Government relationship in 1999.\nThe MOWA Band of Choctaw Indians in Alabama and the Alabama Inter-Tribal Council, which is composed solely of non-federally recognized tribes under Chief Framon Weaver, obtained a US Supreme Court ruling that sovereign immunity applies not only to entities such as the Alabama Inter-Tribal Council as an arm of the tribe, but also that sovereign immunity is inherent and possessed of Indians because they are Indians. This decision of the U.S. Federal Court of Appeals was upheld by the United States Supreme Court in 2002.\nPrehistory.\nPaleo-Indian period.\nMany thousands of years ago groups classified by anthropologists as Paleo-Indians lived in what today is referred to as the American South. These groups were hunter-gatherers who hunted a wide range of animals, including a variety of megafauna, which became extinct following the end of the Pleistocene age. The 19th-century historian Horatio B. Cushman noted that Choctaw oral history accounts suggested their ancestors had known of mammoths in the Tombigbee River area; this suggests that the Choctaw ancestors had been in the Mississippi area for at least 4,000\u20138,000 years. Cushman wrote: \"the ancient Choctaw through their tradition (said) 'they saw the mighty beasts of the forests, whose tread shook the earth.\" Scholars believe that Paleo-Indians were specialized, highly mobile foragers who hunted late Pleistocene fauna such as bison, mastodons, caribou, and mammoths. Direct evidence in the Southeast is meager, but archaeological discoveries in related areas support this hypothesis.\nWoodland culture.\nLater cultures became more complex. Moundbuilding cultures included the Woodland period people who first built Nanih Waiya. Scholars believe the mound was contemporary with such earthworks as Igomar Mound in Mississippi and Pinson Mounds in Tennessee. Based on dating of surface artifacts, the Nanih Waiya mound was likely constructed and first occupied by indigenous peoples about 0\u2013300 CE, in the Middle Woodland period.\nThe original site was bounded on three sides by an earthwork circular enclosure, about ten feet high and encompassing a square mile. Occupation of Nanih Waiya and several smaller nearby mounds likely continued through 700 CE, the Late Woodland Period. The smaller mounds may also have been built by later cultures. As they have been lost to cultivation since the late 19th century and the area has not been excavated, theories have been speculation.\nMississippian culture.\nThe Mississippian culture was a Native American culture that flourished in what is now the Midwestern, Eastern, and Southeastern United States from 800 to 1500 CE. The Mississippian culture developed in the lower Mississippi river valley and its tributaries, including the Ohio River. In present-day Mississippi, Moundville, Plaquemine,\nWhen the Spanish made their first forays inland in the 16th century from the shores of the Gulf of Mexico, they encountered some chiefdoms of the Mississippians, but others were already in decline, or had disappeared. The Mississippian culture are the peoples encountered by other early Spanish explorers, beginning on April 2, 1513, with Juan Ponce de Le\u00f3n's Florida landing and the 1526 Lucas V\u00e1zquez de Ayll\u00f3n expedition in South Carolina and Georgia region. A Spanish expedition in the later 16th century, in what is now western North Carolina, encountered people of the Mississippian culture at Joara and settlements further west. The Spanish built a fort at Joara and left a garrison there, as well as five other forts. The following year all the Spanish garrisons were killed and the forts destroyed by the Native Americans, who ended Spanish colonization attempts in the interior.\nContact era.\nAfter the castaway Cabeza de Vaca of the ill-fated Narv\u00e1ez expedition returned to Spain, he described to the Court that the New World was the \"richest country in the world.\" It commissioned the Spaniard Hernando de Soto to lead the first expedition into the interior of the North American continent. De Soto, convinced of the \"riches\", wanted Cabeza de Vaca to accompany him on the expedition. Cabeza de Vaca declined because of a payment dispute. From 1540 to 1543, Hernando de Soto traveled through present-day Florida and Georgia, and then into the Alabama and Mississippi areas that would later be inhabited by the Choctaw.\nDe Soto had the best-equipped militia at the time. As the brutalities of the de Soto expedition through the Southeast became known, ancestors to the Choctaw rose in defense. The Battle of Mabila, an ambush arranged by Chief Tuskaloosa, was a turning point for the de Soto venture. The battle \"broke the back\" of the campaign, and they never fully recovered.\nHistory.\n17th century emergence.\nThe archaeological record for the period between 1567 and 1699 is not complete or well-studied. It appears that some Mississippian settlements were abandoned well before the 17th century. Similarities in pottery coloring and burials suggest the following scenario for the emergence of the distinctive Choctaw society.\nAccording to Patricia Galloway, the Choctaw region of Mississippi, generally located between the Yazoo basin to the north and the Natchez bluffs to the south, was slowly occupied by Burial Urn people from the Bottle Creek Indian Mounds area in the Mobile, Alabama delta, along with remnants of people from the Moundville chiefdom (near present-day Tuscaloosa, Alabama), which had collapsed some years before. Facing severe depopulation, they fled westward, where they combined with the Plaquemines and a group of \"prairie people\" living near the area. When this occurred is not clear. In the space of several generations, they created a new society which became known as Choctaw (albeit with a strong Mississippian background).\nOther scholars note the Choctaw oral history recounting their long migration from west of the Mississippi River.\nThe contemporary historian Patricia Galloway argues from fragmentary archaeological and cartographic evidence that the Choctaw did not exist as a unified people before the 17th century. Only then did various southeastern peoples, remnants of Moundville, Plaquemine, and other Mississippian cultures, coalesce to form a self-consciously Choctaw people. The historical homeland of the Choctaw, or of the peoples from whom the Choctaw nation arose, included the area of \"Nanih Waiya\", an earthwork mound in present-day Winston County, Mississippi, which they considered sacred ground. Their homeland was bounded by the Tombigbee River to the east, the Pearl River on the north and west, and \"the Leaf-Pascagoula system\" to the South. This area was mostly uninhabited during the Mississippian -culture period.\nWhile Nanih Waiya mound continued to be a ceremonial center and object of veneration, scholars believe Native Americans traveled to it during the Mississippian culture period. From the 17th century on, the Choctaw occupied this area and revered this site as the center of their origin stories. These included stories of migration to this site from west of the great river (believed to refer to the Mississippi River.)\nIn \"Histoire de La Louisiane\" (Paris, 1758), French explorer Antoine-Simon Le Page du Pratz recounted that \"...when I asked them from whence the Chat-kas came, to express the suddenness of their appearance they replied that they had come out from under the earth.\" American scholars took this as intended to explain the Choctaws' immediate appearance, and not a literal creation account. It was perhaps the first European writing that included part of the Choctaw origin story.\nEarly 19th century and contemporary Choctaw storytellers describe that the Choctaw people emerged from either Nanih Waiya mound or cave. A companion story describes their migration journey from the west, beyond the Mississippi River, when they were directed by their leader's use of a sacred red pole.\nFrench colonization (1682).\nIn 1682 La Salle was the first French explorer to venture into the southeast along the Mississippi River. His expedition did not meet with the Choctaw; it established a post along the Arkansas River. The post signaled to the English that the French were serious at colonization in the South. The Choctaw allied with French colonists as a defense against the English, who had been taking Choctaws as captives for the Indian slave trade.\nThe first direct recorded contact between the Choctaw and the French was with Pierre Le Moyne d'Iberville in 1699; indirect contact had likely occurred between the Choctaw and British settlers through other tribes, including the Creek and Chickasaw. The Choctaw, along with other tribes, had formed a relationship with New France, French Louisiana. Illegal fur trading may have led to further unofficial contact.\nAs the historian Greg O'Brien has noted, the Choctaw developed three distinct political and geographic regions, which during the colonial period sometimes had differing alliances with trading partners among the French, Spanish and English. They also expressed differences during and after the American Revolutionary War. Their divisions were roughly eastern, western (near present-day Vicksburg, Mississippi) and southern (Six Towns). Each division was headed by a principal chief, and subordinate chiefs led each of the towns within the area. All the chiefs would meet on a National Council, but the society was highly decentralized for some time.\nThe French were the main trading partners of the Choctaw before the Seven Years' War, and the British had established some trading. Trade deputes between the eastern and western division led to the Choctaw Civil War being fought between 1747 and 1750, with the pro-French eastern division emerging victorious. After Great Britain defeated France in the Seven Years' War, it ceded its territory east of the Mississippi River. From 1763 to 1781, Britain was the Choctaw main trading partner. With Spanish forces based in New Orleans in 1766, when they took over French territory west of the Mississippi, the Choctaw sometimes traded with them to the west. Spain declared war against Great Britain during the American Revolution in 1779.\nUnited States relations.\nAmerican Revolutionary War.\nDuring the American Revolution, the Choctaw divided over whether to support Britain or Spain. Some Choctaw warriors from the western and eastern divisions supported the British in the defense of Mobile and Pensacola. Chief Franchimastab\u00e9 led a Choctaw war party with British forces against American rebels in Natchez. The Americans had left by the time Franchimastab\u00e9 arrived, but the Choctaw occupied Natchez for weeks and convinced residents to remain loyal to Britain.\nOther Choctaw companies joined Washington's army during the war, and served the entire duration. Bob Ferguson, a Southeastern Indian historian, noted, \"[In] 1775 the American Revolution began a period of new alignments for the Choctaws and other southern Indians. Choctaw scouts served under Washington, Morgan, Wayne and Sullivan.\"\nOver a thousand Choctaw fought for Britain, largely against Spain's campaigns along the Gulf Coast. At the same time, a significant number of Choctaw aided Spain.\nPost-American Revolutionary War.\nFerguson wrote that with the end of the Revolution, \"'Franchimastabe', Choctaw head chief, went to Savannah, Georgia to secure American trade.\" In the next few years, some Choctaw scouts served in Ohio with U.S. General Anthony Wayne in the Northwest Indian War.\nGeorge Washington (first U.S. President) and Henry Knox (first U.S. Secretary of War) proposed the cultural transformation of Native Americans. Washington believed that Native American society was inferior to that of the European Americans, while recognizing the Choctaws and the other Civilized Tribes as equals (something very much uncommon for American leaders at the time). He formulated a policy to encourage the \"civilizing\" process, and Thomas Jefferson continued it. The historian Robert Remini wrote, \"[T]hey presumed that once the Indians adopted the practice of private property, built homes, farmed, educated their children, and embraced Christianity, these Native Americans would win acceptance from white Americans.\"\nWashington's six-point plan included impartial justice toward Indians; regulated buying of Indian lands; promotion of commerce; promotion of experiments to civilize or improve Indian society; presidential authority to give presents; and punishing those who violated Indian rights. The government appointed agents, such as Benjamin Hawkins, to live among the Indians and to teach them through example and instruction, how to live like whites. While living among the Choctaw for nearly 30 years, Hawkins married Lavinia Downs, a Choctaw woman. As the people had a matrilineal system of property and hereditary leadership, their children were born into the mother's clan and gained their status from her people. In the late eighteenth and early nineteenth century, a number of Scots-Irish traders lived among the Choctaw and married high-status women. Choctaw chiefs saw these as strategic alliances to build stronger relationships with the Americans in a changing environment that influenced ideas of capital and property. The children of such marriages were Choctaw, first and foremost. Some of the sons were educated in Anglo-American schools and became important interpreters and negotiators for Choctaw-US relations.\nHopewell council and treaty (1786).\nStarting in October 1785, \"Taboca\", a Choctaw prophet/chief, led over 125 Choctaws to the Keowee, near Seneca Old Town, now known as Hopewell, South Carolina. After two months of travel, they met with U.S. representatives Benjamin Hawkins, Andrew Pickens, and Joseph Martin. In high Choctaw ceremonial symbolism, they named, adopted, smoked, and performed dances, revealing the complex and serious nature of Choctaw diplomacy. One such dance was the eagle tail dance. The Choctaw explained that the bald eagle, who has direct contact with the upper world of the sun, is a symbol of peace. Choctaw women painted in white would adopt and name commissioners as kin. Smoking sealed the agreements between peoples, and the shared pipes sanctified peace between the two nations.\nAfter the rituals, the Choctaw asked John Woods to live with them to improve communication with the U.S. In exchange they allowed Taboca to visit the United States Congress. On January 3, 1786, the Treaty of Hopewell was signed. Article 11 stated, \"[T]he hatchet shall be forever buried, and the peace given by the United States of America, and friendship re-established between the said states on the one part, and all the Choctaw nation on the other part, shall be universal; and the contracting parties shall use their utmost endeavors to maintain the peace given as aforesaid, and friendship re-established.\"\nThe treaty required Choctaws to return escaped slaves to colonists, to turn over any Choctaw convicted of crimes by the U.S., establish borderlines between the U.S. and Choctaw Nation, and the return any property captured from colonists during the Revolutionary War.\nAfter the Revolutionary War, the Choctaw were reluctant to ally themselves with countries hostile to the United States. John Swanton wrote, \"the Choctaw were never at war with the Americans. A few were induced by \"Tecumseh\" (a Shawnee leader who sought support from various Native American tribes) to ally themselves with the hostile Creeks [in the early 19th century], but the Nation as a whole was kept out of anti-American alliances by the influence of \"Apushmataha\", greatest of all Choctaw chiefs.\"\nWar of 1812.\nEarly in 1811, the Shawnee leader \"Tecumseh\" gathered Indian tribes in an alliance to try to expel U.S. settlers from the Northwest area south of the Great Lakes. Tecumseh met the Choctaws to persuade them to join the alliance. \"Pushmataha\", considered by historians to be the greatest Choctaw leader, countered Tecumseh's influence. As chief for the Six Towns (southern) district, Pushmataha strongly resisted such a plan, arguing that the Choctaw and their neighbors the Chickasaw had always lived in peace with European Americans, had learned valuable skills and technologies, and had received honest treatment and fair trade. The joint Choctaw-Chickasaw council voted against alliance with Tecumseh. On Tecumseh's departure, Pushmataha accused him of tyranny over his own Shawnee and other tribes. Pushmataha warned Tecumseh that he would fight against those who fought the United States.\nOn the eve of the War of 1812, Governor William C. C. Claiborne of Louisiana sent interpreter Simon Favre to give a talk to the Choctaws, urging them to stay out of this \"white man's war.\" Ultimately, however, the Choctaw did become involved, and with the outbreak of the war, Pushmataha led the Choctaws in alliance with the U.S., arguing in favor of opposing the Creek Red Sticks' alliance with Britain after the massacre at Fort Mims. Pushmataha arrived at St. Stephens, Alabama in mid-1813 with an offer of alliance and recruitment. He was escorted to Mobile to speak with General Flournoy, then commanding the district. Flournoy initially declined Pushmataha's offer and offended the chief. However, Flournoy's staff quickly convinced him to reverse his decision. A courier with a message accepting the offer of alliance caught up with Pushmataha at St. Stephens.\nReturning to Choctaw territory, Pushmataha raised a company of 125 Choctaw warriors with a rousing speech and was commissioned (as either a lieutenant colonel or a brigadier general) in the United States Army at St. Stephens. After observing that the officers and their wives would promenade along the Alabama River, Pushmataha summoned his own wife to St. Stephens to accompany him.\nHe joined the U.S. Army under General Ferdinand Claiborne in mid-November, and some 125 Choctaw warriors took part in an attack on Creek forces at Kantachi (near present day Econochaca, Alabama) on 23 December 1813. With this victory, Choctaw began to volunteer in greater numbers from the other two districts of the tribe. By February 1814, a larger band of Choctaws under Pushmataha had joined General Andrew Jackson's force for the sweeping of the Creek territories near Pensacola, Florida. Many Choctaw departed from Jackson's main force after the final defeat of the Creek at the Battle of Horseshoe Bend. By the Battle of New Orleans, only a few Choctaw remained with the army; they were the only Native American tribe represented in the battle.\nDoak's Stand (1820).\nIn October 1820, Andrew Jackson and Thomas Hinds were sent as commissioners representing the United States, to conduct a treaty that would require the Choctaw to surrender to the United States a portion of their country located in present day Mississippi. They met with chiefs, mingos (leaders), and headsmen such as Colonel Silas Dinsmore and Chief Pushmataha at Doak's Stand on the Natchez Trace.\nThe convention began on October 10 with a talk by \"Sharp Knife\", the nickname of Jackson, to more than 500 Choctaws. Pushmataha accused Jackson of deceiving them about the quality of land west of the Mississippi. Pushmataha responded to Jackson's retort with \"I know the country well ... The grass is everywhere very short ... There are but few beavers, and the honey and fruit are rare things.\" Jackson resorted to threats, which pressured the Choctaws to sign the Doak's Stand treaty. Pushmataha would continue to argue with Jackson about the conditions of the treaty. Pushmataha assertively stated \"that no alteration shall be made in the boundaries of the portion of our territory that will remain, until the Choctaw people are sufficiently progressed in the arts of civilization to become citizens of the States, owning land and homes of their own, on an equal footing with the white people.\" Jackson responded with \"That ... is a magnificent rangement and we consent to it, [American Citizenship], readily.\" Historian Anna Lewis stated that \"Apuckshunubbee\", a Choctaw district chief, was blackmailed by Jackson to sign the treaty. On October 18, the Treaty of Doak's Stand was signed.\nArticle 4 of the Treaty of Doak's Stand prepared Choctaws to become U.S. citizens when he or she became \"civilized.\" This article would later influence Article 14 in the Treaty of Dancing Rabbit Creek.\nNegotiations with the US government (1820s).\nApuckshunubbee, Pushmataha, and Mosholatubbee, the principal chiefs of the three divisions of Choctaw, led a delegation to Washington City (the 19th century name for Washington, D.C.) to discuss the problems of European Americans' squatting on Choctaw lands. They sought either expulsion of the settlers or financial compensation for the loss of their lands. The group also included Talking Warrior, Red Fort, Nittahkachee, who was later Principal Chief; Col. Robert Cole and David Folsom, both Choctaw of mixed-race ancestry; Captain Daniel McCurtain, and Major John Pitchlynn, the U.S. interpreter, who had been raised by the Choctaw after having been orphaned when young and married a Choctaw woman. Apuckshunubbee died in Maysville, Kentucky of an accident during the trip before the party reached Washington.\nPushmataha met with President James Monroe and gave a speech to Secretary of War John C. Calhoun, reminding him of the longstanding alliances between the United States and the Choctaws. He said, \"[I] can say and tell the truth that no Choctaw ever drew his bow against the United States ... My nation has given of their country until it is very small. We are in trouble.\" On January 20, 1825, Choctaw chiefs signed the Treaty of Washington City, by which the Choctaw ceded more territory to the United States.\nPushmataha died in Washington of a respiratory disease described as croup, before the delegation returned to the Choctaw Nation. He was given full U.S. military burial honors at the Congressional Cemetery in Washington, D.C.\nThe deaths of these two strong division leaders was a major loss to the Choctaw Nation, but younger leaders were arising who were educated in European-American schools and led adaptation of the culture. Threatened with European-American encroachment, the Choctaw continued to adapt and take on some technology, housing styles, and accepted missionaries to the Choctaw Nation, in the hopes of being accepted by the Mississippi and national government. In 1825 the National Council approved the founding of the Choctaw Academy for education of its young men, urged by Peter Pitchlynn, a young leader and future chief. The school was established in Blue Spring, Scott County, Kentucky; it was operated there until 1842, when the staff and students were transferred to the Choctaw Nation, Indian Territory. There they founded the Spencer Academy in 1844.\nWith the election of Andrew Jackson as president in 1828, many of the Choctaw realized that removal was inevitable. They continued to adopt useful European practices but faced Jackson's and settlers' unrelenting pressure.\n1830 election and treaty.\nIn March 1830 the division chiefs resigned, and the National Council elected Greenwood LeFlore, chief of the western division, as Principal Chief of the nation to negotiate with the US government on their behalf, the first time such a position had been authorized. Believing removal was inevitable and hoping to preserve rights for Choctaw in Indian Territory and Mississippi, LeFlore drafted a treaty and sent it to Washington, DC. There was considerable turmoil in the Choctaw Nation among people who thought he would and could resist removal, but the chiefs had agreed they could not undertake armed resistance.\nTreaty of Dancing Rabbit Creek (1830).\nAt Andrew Jackson's request, the United States Congress opened what became a fierce debate on an Indian Removal Bill. In the end, the bill passed, but the vote was very close. The Senate passed the measure 28 to 19, while in the House it narrowly passed, 102 to 97. Jackson signed the legislation into law June 30, 1830, and turned his focus onto the Choctaw in Mississippi Territory.\nOn August 25, 1830, the Choctaw were supposed to meet with Andrew Jackson in Franklin, Tennessee, but Greenwood Leflore, a district Choctaw chief, informed Secretary of War John H. Eaton that his warriors were fiercely opposed to attending. President Jackson was angered. Journalist Len Green writes \"although angered by the Choctaw refusal to meet him in Tennessee, Jackson felt from LeFlore's words that he might have a foot in the door and dispatched Secretary of War Eaton and John Coffee to meet with the Choctaws in their nation.\" Jackson appointed Eaton and General John Coffee as commissioners to represent him to meet the Choctaws at the Dancing Rabbit Creek near present-day Noxubee, Mississippi Territory. although the actual site of the Treaty was never specifically mentioned.\nThe commissioners met with the chiefs and headmen on September 15, 1830, at Dancing Rabbit Creek. In a carnival-like atmosphere, they tried to explain the policy of removal to an audience of 6,000 men, women, and children. The Choctaws faced migration or submitting to U.S. law as citizens. The treaty required them to cede their remaining traditional homeland to the United States; however, a provision in the treaty made removal more acceptable.\nOn September 27, 1830, the Treaty of Dancing Rabbit Creek was signed. It represented one of the largest transfers of land that was signed between the U.S. Government and Native Americans without being instigated by warfare. By the treaty, the Choctaw signed away their remaining traditional homelands, opening them up for European-American settlement. Article 14 allowed for some Choctaw to stay in Mississippi, and nearly 1,300 Choctaws chose to do so. They were one of the first major non-European ethnic group to become U.S. citizens. Article 22 sought to put a Choctaw representative in the U.S. House of Representatives. The Choctaw at this crucial time split into two distinct groups: the Choctaw Nation of Oklahoma and the Mississippi Band of Choctaw Indians. The nation retained its autonomy, but the tribe in Mississippi submitted to state and federal laws.\nRemoval era.\nAfter ceding nearly , the Choctaw emigrated in three stages: the first in the fall of 1831, the second in 1832 and the last in 1833. Nearly 15,000 Choctaws made the move to what would be called Indian Territory and then later Oklahoma. About 2,500 died along the Trail of Tears. The Treaty of Dancing Rabbit Creek was ratified by the U.S. Senate on February 25, 1831, and the President was anxious to make it a model of removal. Principal Chief George W. Harkins wrote a farewell letter to the American people before the removals began. It was widely published\nAlexis de Tocqueville, noted French political thinker and historian, witnessed the Choctaw removals while in Memphis, Tennessee in 1831:\nApproximately 4,000\u20136,000 Choctaw remained in Mississippi in 1831 after the initial removal efforts. The U.S. agent William Ward, who was responsible for Choctaw registration in Mississippi under article XIV, strongly opposed their treaty rights. Although estimates suggested 5000 Choctaw remained in Mississippi, only 143 family heads (for a total of 276 adult persons) received lands under the provisions of Article 14. For the next ten years, the Choctaws in Mississippi were objects of increasing legal conflict, racism, harassment, and intimidation. The Choctaws described their situation in 1849: \"we have had our habitations torn down and burned, our fences destroyed, cattle turned into our fields and we ourselves have been scourged, manacled, fettered and otherwise personally abused, until by such treatment some of our best men have died.\" Joseph B. Cobb, who moved to Mississippi from Georgia, described the Choctaw as having \"no nobility or virtue at all, and in some respect he found blacks, especially native Africans, more interesting and admirable, the red man's superior in every way. The Choctaw and Chickasaw, the tribes he knew best, were beneath contempt, that is, even worse than black slaves.\" Removal continued throughout the 19th and 20th centuries. In 1846 1,000 Choctaw removed, and in 1903, another 300 Mississippi Choctaw were persuaded to move to the Nation in Oklahoma. By 1930 only 1,665 remained in Mississippi.\nPre-Civil War (1840).\nIn the 1840s, the Choctaw chief Greenwood LeFlore stayed in Mississippi after the signing of Treaty of Dancing Rabbit Creek and became an American citizen, a successful businessman, and a state politician. He was elected as a Mississippi representative and senator, was a fixture of Mississippi high society, and a personal friend of Jefferson Davis. He represented his county in the state house for two terms and served as a state senator for one term. Some of the elite used Latin language, an indulgence used by some politicians. LeFlore, in defense of his heritage, spoke in the Choctaw language and asked the Senate floor which was better understood, Latin or Choctaw.\nMidway through the Great Irish Famine (1845\u20131849), the Choctaw agency at Fort Smith, Arkansas organised a collection which amounted to $170 and sent it to help starving Irish men, women, and children. The Arkansas Intelligencer reported that \"all subscribed, agents, missionaries, traders and Indians, a considerable portion of which fund was made up by the latter.\"\nIt had been just 16 years since the Choctaw people had experienced the Trail of Tears, and they had faced starvation ... It was an amazing gesture. By today's standards, it might be a million dollars\" according to Judy Allen, editor of the Choctaw Nation of Oklahoma's newspaper, \"Bishinik\", based at the Oklahoma Choctaw tribal headquarters in Durant, Oklahoma.\nTo mark the 150th anniversary, eight Irish people retraced the Trail of Tears. In the late 20th century, Irish President Mary Robinson extolled the donation in a public commemoration. On 18 June 2017 the Kindred Spirits memorial by the sculptor Alex Pentek, a circle of six-metre-tall steel feathers making a bowl and representing both the Choctaw tradition and a symbolic bowl of food, was unveiled in Midleton, Co Cork. A Choctaw delegation, which included Chief Gary Batton, Chief of the Chocktaw Nation, and Assistant Chief Jack Austin Jr., attended the memorial's dedication ceremony that involved presentations of both Choctaw and Irish culture. On 12 March 2018 the Irish Taoiseach Leo Varadkar announced a new scholarship program to allow Choctaw students to travel to and study in Ireland. In the spring of 2020, during the COVID-19 pandemic, an Irish charity drive managed to raise over 1.8 million dollars to supply important amenities for the struggling Navajo and Hopi Nations as a sort of repayment for the Choctaws' donation.\nFor the Choctaw who remained in or returned to Mississippi after 1855, the situation deteriorated. Many lost their lands and money to unscrupulous whites. The state of Mississippi refused the Choctaw any participation in government. Their limited understanding of the English language caused them to live in isolated groups. In addition, they were prohibited from attending any of the few institutions of higher learning, as the European Americans considered them free people of color and excluded from the segregated white institutions. The state had no public schools prior to those established during the Reconstruction era.\n1853 World's Fair.\nIn May 1853, Choctaws sailed out of Mobile, Alabama for Boston and New York. They were to participate in America's \"first\" world's fair: Exhibition of the Industry of All Nations.\nAmerican Civil War (1861).\nAt the beginning of the American Civil War, Albert Pike was appointed as Confederate envoy to Native Americans. In this capacity he negotiated several treaties, including the Treaty with Choctaws and Chickasaws in July 1861. The treaty covered sixty-four terms, covering many subjects, such as Choctaw and Chickasaw nation sovereignty, Confederate States of America citizenship possibilities, and an entitled delegate in the House of Representatives of the Confederate States of America. In 1891, Horatio B. Cushman, a noted author and historian, wrote that the \"United States abandoned the Choctaws and Chickasaws\" when Confederate troops had entered into their nation.\nTrans-Mississippi Theater.\nSome Choctaw identified with the Southern cause and a few owned slaves. In addition, they remembered and resented the Indian removals from thirty years earlier, and the poor services they received from the federal government. There were several reasons the Choctaw Nation agreed to sign the Choctaw &amp; Chickasaw/Confederate treaty. Soon Confederate battalions were formed in Indian Territory and later in Mississippi in support of the southern cause.\nWestern Theater.\nThe Confederacy encouraged the recruitment of American Indians east of the Mississippi River in 1862. John W. Pierce and Samuel G. Spann organized the Choctaw Indians in Mississippi between 1862 and 1863.\nPierce's 1st Choctaw Battalion was established in February 1863. They tracked Confederate deserters in Jones County and surrounding areas. After a Confederate troop train wreck, referred to as the Chunky Creek Train Wreck of 1863, near Hickory, Mississippi, the battalion led rescue and recovery efforts. Led by Jack Amos and Elder Jackson, the Indians rushed to the scene, stripped, and plunged into the flooded creek. Many of the passengers were rescued due to their heroic acts. Noted historian Clara Sue Kidwell wrote, \"in an act of heroism in Mississippi, Choctaws rescued twenty-three survivors and retrieved ninety bodies when a Confederate troop train plunged off a bridge and fell into the Chunky River.\" The battalion was at the Battle of Ponchatoula in March 1863. After the battle, a majority of the Indians deserted. The remaining members returned to Ponchatoula where some were captured. The prisoners were taken to New Orleans and later New York City, where two died. Pierce's 1st Choctaw Battalion was disbanded on May 9, 1863.\nAfter S. G. Spann was authorized to raise Indian troops in April 1863, he soon established a recruiting camp in Mobile, Alabama and Newton County, Mississippi. Spann placed recruiting advertisements in the \"Mobile Advertiser and Register\". The advertisements appeared in the newspaper for most of the summer of 1863. Spann's organization was known as Spann's Independent Scouts. It was soon re-organized as the 18th Battalion, Alabama Cavalry. The unit helped with Gideon J. Pillow's conscription efforts in the fall of 1863. Spann was the commander of U.C.V. Camp Dabney H. Maury which was based in Newton, Mississippi. Spann lived in Meridian, Mississippi at the time he wrote about the deeds of the Choctaw during the Civil War.\nUnder Reconstruction (1865).\nMississippi Choctaw.\nFrom about 1865 to 1914, Mississippi Choctaws were largely ignored by governmental, health, and educational services and fell into obscurity. In the aftermath of the Civil War, their issues were pushed aside in the struggle between defeated Confederates, freedmen and Union sympathizers. Records about the Mississippi Choctaw during this period are few. They had no legal recourse, and were often bullied and intimidated by local whites, who tried to re-establish white supremacy. They chose to live in isolation and practiced their culture as they had for generations.\nFollowing the Reconstruction era and conservative Democrats' regaining political power in the late 1870s, white state legislators passed laws establishing Jim Crow laws and legal segregation by race. In addition, they effectively disfranchised freedmen and Native Americans by the new Mississippi constitution of 1890, which changed rules regarding voter registration and elections to discriminate against both groups. The white legislators effectively divided society into two groups: white and \"colored,\" into which they classified Mississippi Choctaw and other Native Americans. They subjected the Choctaw to racial segregation and exclusion from public facilities along with freedmen and their descendants. The Choctaw were non-white, landless, and had minimal legal protection.\nBecause the state remained dependent on agriculture, despite the declining price of cotton, most landless men earned a living by becoming sharecroppers. The women created and sold traditional hand-woven baskets. Choctaw sharecropping declined following World War II as major planters had adopted mechanization, which reduced the need for labor.\nChoctaw Nation.\nThe Confederacy's loss was also the Choctaw Nation's loss. Prior to removal, the Choctaws had interacted with Africans in their native homeland of Mississippi, and the wealthiest had bought slaves. The Choctaw who developed larger plantations adopted chattel slavery, as practiced by European Americans, to gain sufficient labor. During the antebellum period, enslaved African Americans had more formal legal protection under United States law than did the Choctaw. Moshulatubbee, the chief of the western region, held slaves, as did many of the Europeans who married into the Choctaw nation. The Choctaw took slaves with them to Indian Territory during removal, and descendants purchased others there. They kept slavery until 1866. After the Civil War, they were required by treaty with the United States to emancipate the slaves within their Nation and, for those who chose to stay, offer them full citizenship and rights. Former slaves of the Choctaw Nation were called the Choctaw Freedmen. After considerable debate, the Choctaw Nation granted Choctaw Freedmen citizenship in 1885. In post-war treaties, the US government also acquired land in the western part of the territory and access rights for railroads to be built across Indian Territory. Choctaw chief, Allen Wright, suggested \"Oklahoma\" (red man, a \"portmanteau\" of the Choctaw words \"okla\" \"man\" and \"humma\" \"red\") as the name of a territory created from Indian Territory in 1890.\nThe improved transportation afforded by the railroads increased the pressure on the Choctaw Nation. It drew large-scale mining and timber operations, which added to tribal receipts. But, the railroads and industries also attracted European-American settlers, including new immigrants to the United States.\nWith the goal of assimilating the Native Americans, the Curtis Act of 1898, sponsored by a Native American who believed that was the way for his people to do better, ended tribal governments. In addition, it proposed the end of communal, tribal lands. Continuing the struggle over land and assimilation, the US proposed the end to the tribal lands held in common, and allotment of lands to tribal members in severalty (individually). The US declared land in excess of the registered households needs to be \"surplus\" to the tribe, and took it for sale to new European-American settlers. In addition, individual ownership meant that Native Americans could sell their individual plots. This would also enable new settlers to buy land from those Native Americans who wished to sell. The US government set up the Dawes Commission to manage the land allotment policy; it registered members of the tribe and made allocations of lands.\nBeginning in 1894, the Dawes Commission was established to register Choctaw and other families of the Indian Territory, so that the former tribal lands could be properly distributed among them. The final list included 18,981 citizens of the Choctaw Nation, 1,639 Mississippi Choctaw, and 5,994 former slaves (and descendants of former slaves), most held by Choctaws in the Indian/Oklahoma Territory. (At the same time, the Dawes Commission registered members of the other Five Civilized Tribes for the same purpose. The Dawes Rolls have become important records for proving tribal membership.) Following completion of the land allotments, the US proposed to end tribal governments of the Five Civilized Tribes and admit the two territories jointly as a state.\nTerritory transition to Oklahoma statehood (1889).\nThe establishment of Oklahoma Territory following the Civil War was a required land cession by the Five Civilized Tribes, who had supported the Confederacy. The government used its railroad access to the Oklahoma Territory to stimulate development there. The Indian Appropriations Bill of 1889 included an amendment by Illinois Representative William McKendree Springer, that authorized President Benjamin Harrison to open the two million acres (8,000\u00a0km\u00b2) of Oklahoma Territory for settlement, resulting in the Land Run of 1889. The Choctaw Nation was overwhelmed with new settlers and could not regulate their activities. In the late 19th century, Choctaws suffered almost daily from violent crimes, murders, thefts and assaults from whites and from other Choctaws. Intense factionalism divided the traditionalistic \"Nationalists\" and pro-assimilation \"Progressives,\" who fought for control.\nIn 1905, delegates of the Five Civilized Tribes met at the Sequoyah Convention to write a constitution for an Indian-controlled state. They wanted to have Indian Territory admitted as the State of Sequoyah. Although they took a thoroughly developed proposal to Washington, DC, seeking approval, eastern states' representatives opposed it, not wanting to have two western states created in the area, as the Republicans feared that both would be Democrat-dominated, as the territories had a southern tradition of settlement. President Theodore Roosevelt, a Republican, ruled that the Oklahoma and Indian territories had to be jointly admitted as one state, Oklahoma. To achieve this, tribal governments had to end and all residents accept state government. Many of the leading Native American representatives from the Sequoyah Convention participated in the new state convention. Its constitution was based on many elements of the one developed for the State of Sequoyah.\nIn 1906 the U.S. dissolved the governments of the Five Civilized Tribes. This action was part of continuing negotiations by Native Americans and European Americans over the best proposals for the future. The Choctaw Nation continued to protect resources not stipulated in treaty or law. On November 16, 1907, Oklahoma was admitted to the union as the 46th state.\nMississippi Choctaw Delegation to Washington (1914).\nBy 1907, the Mississippi Choctaw were in danger of becoming extinct. The Dawes Commission had sent a large number of the Mississippi Choctaws to Indian Territory, and only 1,253 members remained. Meetings were held in April and May 1913 to try to find a solution to this problem. Wesley Johnson was elected chief of the newly formed Mississippi, Alabama, and Louisiana Choctaw Council at the May 1913 meeting. After some deliberation, the council selected delegates to send to Washington, D.C. to bring attention to their plight. Historian Robert Bruce Ferguson wrote in his 2015 article that:\nIn late January 1914, Chief Wesley Johnson and his delegates (Culbertson Davis and Emil John) traveled to Washington, D. C. ... While they were in Washington, Johnson, Davis, and John met with numerous senators &amp; representatives and persuaded the federals to bring the Choctaw case before Congress. On February 5th, their mission culminated with the meeting of President Woodrow Wilson. Culbertson Davis presented a beaded Choctaw belt as a token of goodwill to the President.\nNearly two years after the trip to Washington, the Indian Appropriations Act of May 18, 1916 was passed. A stipulation allowed $1,000 for a investigation on the Mississippi Choctaws' condition. John R. T. Reeves was to \"investigate the condition of the Indians living in Mississippi and report to Congress ... as to their needs for additional land and school facilities ...\" Reeves submitted his report on November 6, 1916.\nHearing at Union, Mississippi.\nIn March 1917, federal representatives held hearings, attended by around 100 Choctaws, to examine the needs of the Mississippi Choctaws. Some of the congressmen who presided over the hearings were: Charles D. Carter of Oklahoma, William W. Hastings of Oklahoma, Carl T. Hayden of Arizona, John N. Tillman of Arkansas, and William W. Venable of Mississippi. These hearings resulted in improvements such as improved access to health care, housing, and schools.\nAfter Cato H. Sells investigated the Choctaws' condition, the U. S. Bureau of Indian Affairs established the Choctaw Agency on October 8 of 1918. The Choctaw Agency was based in Philadelphia, Mississippi, the center of Indian activity. Dr. Frank J. McKinley was its first superintendent, and he was also the physician.\nBefore 1916, six Indian schools operated in three counties: two in Leake, three in Neshoba, and one in Newton. The names of those schools were: Tubby Rock Indian School, Calcutta Indian School, Revenue Indian school, Red Water Indian School, and Gum Springs Indian School. The Newton Indian school's name is not known. The agency established new schools in the following Indian communities: Bogue Chitto, Bogue Homo, Conehatta, Pearl River, Red Water, Standing Pine, and Tucker. Under segregation, few schools were open to Choctaw children, whom the white southerners classified as non-whites.\nThe Mississippi Choctaws' improvements may have continued if it wasn't dramatically interrupted by world events. World War I slowed down progress for the Indians as Washington's bureaucracy focused on the war. Some Mississippi Choctaws also served during the war. The Spanish Influenza also slowed progress as many Choctaws were killed by the world-wide epidemic.\nWorld War I (1918).\nIn the closing days of World War I, a group of Oklahoma Choctaws serving in the U.S. Army used their native language as the basis for secret communication among Americans, as Germans could not understand it. They are now called the Choctaw Code Talkers. The Choctaws were the Native American innovators who served as code talkers. Captain Lawrence, a company commander, overheard Solomon Louis and Mitchell Bobb conversing in the Choctaw language. He learned there were eight Choctaw men in the battalion.\nFourteen Choctaw Indian men in the Army's 36th Division trained to use their language for military communications. Their communications, which could not be understood by Germans, helped the American Expeditionary Force win several key battles in the Meuse-Argonne Campaign in France, during the last big German offensive of the war. Within 24 hours after the US Army starting using the Choctaw speakers, they turned the tide of battle by controlling their communications. In less than 72 hours, the Germans were retreating and the Allies were on full attack. The 14 Choctaw Code Talkers were Albert Billy, Mitchell Bobb, Victor Brown, Ben Caterby, James Edwards, Tobias Frazer, Ben Hampton, Solomon Louis, Pete Maytubby, Jeff Nelson, Joseph Oklahombi, Robert Taylor, Calvin Wilson, and Captain Walter Veach.\nMore than 70 years passed before the contributions of the Choctaw Code talkers were fully recognized. On November 3, 1989, in recognition of the important role the Choctaw Code Talkers played during World War I, the French government presented the \"Chevalier de L'Ordre National du M\u00e9rite\" (the Knight of the National Order of Merit) to the Choctaws Code Talkers.\nThe US Army again used Choctaw speakers for coded language during World War II.\nReorganization (1934).\nDuring the Great Depression and the Roosevelt Administration, officials began numerous initiatives to alleviate some of the social and economic conditions in the South. The 1933 \"Special Narrative Report\" described the dismal state of welfare of Mississippi Choctaws, whose population by 1930 had slightly increased to 1,665 people. John Collier, the US Commissioner for Indian Affairs (now BIA), had worked for a decade on Indian affairs and been developing ideas to change federal policy. He used the report as instrumental support to re-organize the Mississippi Choctaw as the Mississippi Band of Choctaw Indians. This enabled them to establish their own tribal government, and gain a beneficial relationship with the federal government.\nIn 1934, President Franklin Roosevelt signed into law the Indian Reorganization Act. This law proved critical for survival of the Mississippi Choctaw. Baxter York, Emmett York, and Joe Chitto worked on gaining recognition for the Choctaw. They realized that the only way to gain recognition was to adopt a constitution. A rival organization, the Mississippi Choctaw Indian Federation, opposed tribal recognition because of fears of dominance by the Bureau of Indian Affairs (BIA). They disbanded after leaders of the opposition were moved to another jurisdiction. The first Mississippi Band of Choctaw Indians tribal council members were Baxter and Emmett York with Joe Chitto as the first chairperson.\nWith the tribe's adoption of government, in 1944 the Secretary of the Interior declared that would be held in trust for the Choctaw of Mississippi. Lands in Neshoba and surrounding counties were set aside as a federal Indian reservation. Eight communities were included in the reservation land: Bogue Chitto, Bogue Homa, Conehatta, Crystal Ridge, Pearl River, Red Water, Tucker, and Standing Pine.\nUnder the Indian Reorganization Act, the Mississippi Choctaws re-organized on April 20, 1945 as the Mississippi Band of Choctaw Indians. This gave them some independence from the Democrat-dominated state government, which continued with enforcement of racial segregation and discrimination.\nWorld War II (1941).\nWorld War II was a significant turning point for Choctaws and Native Americans in general. Although the Treaty of Dancing Rabbit Creek stated Mississippi Choctaws had U.S. citizenship, they had become associated with \"colored people\" as non-white in a state that had imposed racial segregation under Jim Crow laws. State services for Native Americans were non-existent. The state was poor and still dependent on agriculture. In its system of segregation, services for minorities were consistently underfunded. The state constitution and voter registration rules dating from the turn of the 20th century kept most Native Americans from voting, making them ineligible to serve on juries or to be candidates for local or state offices. They were without political representation.\nA Mississippi Choctaw veteran stated, \"Indians were not supposed to go in the military back then ... the military was mainly for whites. My category was white instead of Indian. I don't know why they did that. Even though Indians weren't citizens of this country, couldn't register to vote, didn't have a draft card or anything, they took us anyway.\"\nVan Barfoot, a Choctaw from Mississippi, who was a sergeant and later a second lieutenant in the U.S. Army, 157th Infantry, 45th Infantry Division, received the Medal of Honor. Barfoot was commissioned a second lieutenant after he destroyed two German machine gun nests, took 17 prisoners, and disabled an enemy tank.\nPost-Reorganization.\nThe first Mississippi Band of Choctaw Indians regular tribal council meeting was held on July 10, 1945. The members were Joe Chitto (Chairman), J.C. Allen (Vice Chairman), Nicholas Bell (Secretary Treasurer), Tom Bell, Preatice Jackson, Dempsey Morris, Woodrow W. Jackson, Lonnie Anderson, Joseph Farve, Phillip Farve, Will Wilson, Hensley Gibson, Will Jimmie, Baxter York, Ennis Martin, and Jimpson McMillan.\nAfter World War II, pressure in Congress mounted to reduce Washington's authority on Native American lands and liquidate the government's responsibilities to them. In 1953 the House of Representatives passed Resolution 108, proposing an end to federal services for 13 tribes deemed ready to handle their own affairs. The same year, Public Law 280 transferred jurisdiction over tribal lands to state and local governments in five states. Within a decade Congress terminated federal services to more than sixty groups despite intense opposition by Indians. Congress settled on a policy to terminate tribes as quickly as possible. Out of concern for the isolation of many Native Americans in rural areas, the federal government created relocation programs to cities to try to expand their employment opportunities. Indian policy experts hoped to expedite assimilation of Native Americans to the larger American society, which was becoming urban. In 1959, the Choctaw Termination Act was passed. Unless repealed by the federal government, the Choctaw Nation of Oklahoma would effectively be terminated as a sovereign nation as of August 25, 1970.\nPresident John F. Kennedy halted further termination in 1961 and decided against implementing additional terminations. He did enact some of the last terminations in process, such as with the Ponca. Both presidents Lyndon Johnson and Richard Nixon repudiated termination of the federal government's relationship with Native American tribes.\nMississippi Choctaw Self-Determination era.\nThe Choctaw people continued to struggle economically due to bigotry, cultural isolation, and lack of jobs. The Choctaw, who for 150 years had been neither white nor black, were \"left where they had always been\"\u2014in poverty. Will D. Campbell, a Baptist minister and Civil Rights activist, witnessed the destitution of the Choctaw. He would later write, \"the thing I remember the most ... was the depressing sight of the Choctaws, their shanties along the country roads, grown men lounging on the dirt streets of their villages in demeaning idleness, sometimes drinking from a common bottle, sharing a roll-your-own cigarette, their half-clad children a picture of hurting that would never end.\" With reorganization and establishment of tribal government, however, over the next decades they took control of \"schools, health care facilities, legal and judicial systems, and social service programs.\"\nThe Choctaws witnessed the social forces that brought Freedom Summer and its after effects to their ancient homeland. The civil rights movement produced significant social change for the Choctaw in Mississippi, as their civil rights were enhanced. Prior to the Civil Rights Act of 1964, most jobs were given to whites, then blacks. Donna Ladd wrote that a Choctaw, now in her 40s, remembers \"as a little girl, she thought that a 'white only' sign in a local store meant she could only order white, or vanilla, ice cream. It was a small story, but one that shows how a third race can easily get left out of the attempts for understanding.\"\nOn June 21, 1964 James Chaney, Andrew Goodman, and Michael Schwerner (renowned civil rights workers) disappeared; their remains were later found in a newly constructed dam. A crucial turning point in the FBI investigation came when the charred remains of the murdered civil rights workers' station wagon was found on a Mississippi Choctaw reservation. Two Choctaw women, who were in the back seat of a deputy's patrol car, said they witnessed the meeting\nof two conspirators who expressed their desire to \"beat-up\" the boys. The end of legalized racial segregation permitted the Choctaws to participate in public institutions and facilities that had been reserved exclusively for white patrons.\nPhillip Martin, who had served in the U. S. Army in Europe during World War II, returned to visit his former Neshoba County, Mississippi home. After seeing the poverty of his people, he decided to stay to help. Martin served as chairperson in various Choctaw committees up until 1977.\nMartin was elected as Chief of the Mississippi Band of Choctaw Indians. He served a total of 30 years, being re-elected until 2007. Martin died in Jackson, Mississippi, on February 4, 2010. He was eulogized as a visionary leader, who had lifted his people out of poverty with businesses and casinos built on tribal land.\n1960s to present.\nIn the social changes around the civil rights era, between 1965 and 1982 many Choctaw Native Americans renewed their commitments to the value of their ancient heritage. Working to celebrate their own strengths and exercise appropriate rights; they dramatically reversed the trend toward abandonment of Indian culture and tradition. During the 1960s, Community Action programs connected with Native Americans were based on citizen participation. In the 1970s, the Choctaw repudiated the extremes of Indian activism. The Oklahoma Choctaw sought a local grassroots solution to reclaim their cultural identity and sovereignty as a nation. The Mississippi Choctaw would lay the foundations of business ventures.\nFederal policy under President Richard M. Nixon encouraged giving tribes more authority for self-determination, within a policy of federal recognition. Realizing the damage that had been done by termination of tribal status, he ended the federal emphasis of the 1950s on termination of certain tribes' federally recognized status and relationships with the federal government:\nSoon after this, Congress passed the landmark Indian Self-Determination and Education Assistance Act of 1975; this completed a 15-year period of federal policy reform with regard to American Indian tribes. The legislation authorized processes by which tribes could negotiate contracts with the BIA to manage directly more of their education and social service programs. In addition, it provided direct grants to help tribes develop plans for assuming such responsibility. It also provided for Indian parents' participation on local school boards.\nBeginning in 1979 the Mississippi Choctaw tribal council worked on a variety of economic development initiatives, first geared toward attracting industry to the reservation. They had many people available to work, natural resources, and no state or federal taxes. Industries have included automotive parts, greeting cards, direct mail and printing, and plastic-molding. The Mississippi Band of Choctaw Indians is one of the state's largest employers, running 19 businesses and employing 7,800 people.\nStarting with New Hampshire in 1963, numerous state governments began to operate lotteries and other gambling in order to raise money for government services, often promoting the programs by promising to earmark revenues to fund education, for instance. In 1987 the Supreme Court of the United States ruled that federally recognized tribes could operate gaming facilities on reservations, as this was sovereign territory, and be free from state regulation. As tribes began to develop gaming, starting with bingo, in 1988 the U.S. Congress enacted the Indian Gaming Regulatory Act (IGRA). It set the broad terms for Native American tribes to operate casinos, requiring that they do so only in states that had already authorized private gaming. Since then development of casino gaming has been one of the chief sources for many tribes of new revenues.\nThe Choctaw Nation of Oklahoma developed gaming operations and a related resort: the Choctaw Casino Resort and Choctaw Casino Bingo are their popular gaming destinations in Durant. Located near the Oklahoma-Texas border, these sites attract residents of Southern Oklahoma and North Texas. The largest regional population base from which they draw is the Dallas-Fort Worth Metroplex.\nThe Mississippi Band of Choctaw Indians (MBCI) unsuccessfully sought state agreement to develop gaming under the Ray Mabus administration. But in 1992 Mississippi Governor Kirk Fordice gave permission for the MBCI to develop Class III gaming. They have developed one of the largest casino resorts in the nation; it is located in Philadelphia, Mississippi near the Pearl River. The Silver Star Casino opened its doors in 1994. The Golden Moon Casino opened in 2002. The casinos are collectively known as the Pearl River Resort.\nAfter nearly two hundred years, the Choctaw have regained control of the ancient sacred site of Nanih Waiya. Mississippi protected the site for years as a state park. In 2006, the state legislature passed a bill to return \"Nanih Waiya\" to the Choctaw.\nJack Abramoff and Indian casino lobbying.\nIn the second half of the 1990s, lobbyist Jack Abramoff was employed by Preston Gates Ellis &amp; Rouvelas Meeds LLP, the lobbying arm in Washington, DC of the Preston Gates &amp; Ellis LLP law firm based in Seattle, Washington. In 1995, Abramoff began representing Native American tribes who wanted to develop gambling casinos, starting with the Mississippi Band of Choctaw Indians.\nThe Choctaw originally had lobbied the federal government directly, but beginning in 1994, they found that many of the congressional members who had responded to their issues had either retired or were defeated in the \"Republican Revolution\" of the 1994 elections. Nell Rogers, the tribe's specialist on legislative affairs, had a friend who was familiar with the work of Abramoff and his father as Republican activists. The tribe contacted Preston Gates, and soon after hired the firm and Abramoff.\nAbramoff succeeded in gaining defeat of a Congressional bill to use the unrelated business income tax (UBIT) to tax Native American casinos; it was sponsored by Reps. Bill Archer (R-TX) and Ernest Istook (R-OK). Since the matter involved taxation, Abramoff enlisted help from Grover Norquist, a Republican acquaintance from college, and his Americans for Tax Reform (ATR). The bill was eventually defeated in 1996 in the Senate, due in part to grassroots work by ATR. The Choctaw paid $60,000 in fees to Abramoff.\nAccording to \"Washington Business Forward\", a lobbying trade magazine, Senator Tom DeLay was also a major figure in achieving defeat of the bill. The fight strengthened Abramoff's alliance with him.\nPurporting to represent Native Americans before Congress and state governments in the developing field of gaming, Jack Abramoff and Michael Scanlon used fraudulent means to gain profits of $15 million in total payments from the Mississippi Band of Choctaw Indians. After Congressional oversight hearings were held in 2004 on the lobbyists' activities, federal criminal charges were brought against Abramoff and Scanlon. In an e-mail sent January 29, 2002, Abramoff had written to Scanlon, \"I have to meet with the monkeys from the Choctaw tribal council.\"\nOn January 3, 2006, Abramoff pleaded guilty to three felony counts \u2014 conspiracy, fraud, and tax evasion. The charges were based principally on his lobbying activities in Washington on behalf of Native American tribes. In addition, Abramoff and other defendants must make restitution of at least $25 million that was defrauded from clients, most notably the Native American tribes.\n2011 Federal Bureau of Investigation raid.\nIn July 2011, agents from the FBI \"seized\" Pearl River Resort informational assets. The Los Angeles Times reported that the Indians are \"faced with infighting over a disputed election for tribal chief and an FBI investigation targeting the tribe's casinos.\"\nOther bands.\nOther Choctaw bands located in the United States include:\nThe MOWA Choctaw reside on a 600-acre reservation in southwestern Alabama with a total enrolled population of 3,600 (total population over 10,000). The tribe has the last Indian school in Alabama named Calcedeaver in Mount Vernon, Mobile County, Alabama. The Senate Select Committee on Indian Affairs voted 11\u20132 in support of federal recognition for the MOWA Choctaw. To date the tribe has had 12 Congressional Bills, 3 appeals through the office of federal acknowledgement, and a federal lawsuit directed at its efforts for federal recognition.\nFederal recognition is not a pre-requisite to accord a tribe sovereign immunity, according to \"John. S. Bottomly v Passamaquoddy Tribe et al.\" 595 F.2d 1061 (1st Cir. 1979). In Bottomly, the United States Supreme Court held that the availability of sovereign immunity is not conditioned on formal federal recognition of a particular tribe. Therefore a tribe, its chief, nor its tribal officials do not need to prove that it has been federally recognized in order to assert immunity from suit for acts done in their official tribal capacity.\n-Bayou Lacombe Choctaw\n-Clifton Choctaw\n-Jena Band of Choctaw Indians\n-Choctaw-Apache of Ebarb\nIn the 2010 Census.\nIn the 2010 US Census, there were people who identified as Choctaw living in every state of the Union. The states with the largest Choctaw populations were:\nCulture.\nThe Choctaw people are believed to have coalesced in the 17th century, perhaps from peoples from Alabama and the Plaquemine culture. Their culture continued to evolve in the Southeast. The Choctaw practiced Head flattening as a ritual adornment for its people, but the practice eventually fell out of favor. Some of their communities had extensive trade and interaction with Europeans, including people from Spain, France, and England greatly shaped it as well. After the United States was formed and its settlers began to move into the Southeast, the Choctaw were among the Five Civilized Tribes, who adopted some of their ways. They transitioned to yeoman farming methods, and accepted European Americans and African Americans into their society. In mid-summer the Mississippi Band of Choctaw Indians celebrate their traditional culture during the Choctaw Indian Fair with ball games, dancing, cooking and entertainment.\nClans.\nWithin the Choctaws were two distinct moieties: \"Imoklashas\" (elders) and \"Inhulalatas\" (youth). Each moiety had several clans or \"Iskas\"; it is estimated there were about 12 Iskas altogether. The people had a matrilineal kinship system, with children born into the clan or iska of the mother and taking their social status from it. In this system, their maternal uncles had important roles. Identity was established first by moiety and iska; so a Choctaw identified first as Imoklasha or Inhulata, and second as Choctaw. Children belonged to the Iska of their mother. The following were some major districts:\nBy the early 1930s, the anthropologist John Swanton wrote of the Choctaw: \"[T]here are only the faintest traces of groups with truly totemic designations, the animal and plant names which occur seeming not to have had a totemic connotation.\"\nSwanton wrote, \"Adam Hodgson ... told ... that there were tribes or families among the Indians, somewhat similar to the Scottish clans; such as, the Panther family, the Bird family, Raccoon Family, the Wolf family.\" The following are possible totemic clan designations:\nGames.\nChoctaw stickball, the oldest field sport in North America, was also known as the \"little brother of war\" because of its roughness and substitution for war. When disputes arose between Choctaw communities, stickball provided a civil way to settle issues. The stickball games would involve as few as twenty or as many as 300 players. The goal posts could be from a few hundred feet apart to a few miles. Goal posts were sometimes located within each opposing team's village. A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject. The Mississippi Band of Choctaw Indians continue to practice the sport.\nChunkey was a game using a stone-shaped disk that was about 1\u20132\u00a0inches in length.\nPlayers would throw the disk down a corridor so that it could roll past the players at great speed. As the disk rolled down the corridor, players would throw wooden shafts at it. The object of the game was to strike the disk or prevent your opponents from hitting it.\nOther games included using corn, cane, and moccasins. The corn game used five to seven kernels of corn. One side was blackened and the other side white. Players won points based on each color. One point was awarded for the black side and 5\u20137 points for the white side. There were usually only two players.\nLanguage.\nThe Choctaw language is a member of the Muskogean family and was well known among the frontiersmen, such as Andrew Jackson and William Henry Harrison, of the early 19th century. The language is closely related to Chickasaw, and some linguists consider the two dialects a single language. The Choctaw language is the essence of tribal culture, tradition, and identity. Many Choctaw adults learned to speak the language before speaking English. The language is a part of daily life on the Mississippi Choctaw reservation. The following table is an example of Choctaw text and its translation:\nReligion.\nThe Choctaw believed in a good spirit and an evil spirit. They may have been sun, or \"Hushtahli\", worshippers. The historian John Swanton wrote,\n[T]he Choctaws anciently regarded the sun as a deity ... the sun was ascribed the power of life and death. He was represented as looking down upon the earth, and as long as he kept his flaming eye fixed on any one, the person was safe ... fire, as the most striking representation of the sun, was considered as possessing intelligence, and as acting in concert with the sun ... [having] constant intercourse with the sun ...\nThe word \"nanpisa\" (the one who sees) expressed the reverence the Choctaw had for the sun.\nChoctaw prophets were known to have addressed the sun. John Swanton wrote, \"an old Choctaw informed Wright that before the arrival of the missionaries, they had no conception of prayer. He added, \"I have indeed heard it asserted by some, that anciently their hopaii, or prophets, on some occasions were accustomed to address the sun ...\"\nTraditional clothing.\nThe colorful dresses worn by today's Choctaw are made by hand. They are based on designs of their ancestors, who adapted 19th-century European-American styles to their needs. Today many Choctaw wear such traditional clothing mainly for special events. Choctaw elders, especially the women, dress in their traditional garb every day. Choctaw dresses are trimmed by full diamond, half diamond or circle, and crosses that represent stickball sticks.\nCommunal economy.\nEarly Choctaw communities worked communally and shared their harvest. They had trouble understanding why English settlers allowed their poor to suffer from hunger. In Ireland, the generosity of the Choctaw nation during their Great Famine in the mid-nineteenth century is remembered to this day and recently marked by a sculpture, 'Kindred Spirits', in a park at Midleton, Cork.\nTreaties.\nLand was the most valuable asset, which the Native Americans held in collective stewardship. The United States systematically obtained Choctaw land for conventional European-American settlement through treaties, legislation, and threats of warfare. Although the Choctaw made treaties with Great Britain, France, Spain, and the Confederate States of America; the nation signed only nine treaties with the United States. Some treaties which the US made with other nations, such as the Treaty of San Lorenzo, indirectly affected the Choctaw.\nReservations.\nReservations can be found in Louisiana (Jena Band of Choctaw Indians), Mississippi (Mississippi Band of Choctaw Indians), and Oklahoma (Choctaw Nation of Oklahoma). The Oklahoma reservation is defined by treaty. Other population centers can be found throughout the United States."}
{"id": "7223", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7223", "title": "Choctaws", "text": ""}
{"id": "7224", "revid": "87355", "url": "https://en.wikipedia.org/wiki?curid=7224", "title": "Calypso", "text": "Calypso usually refers to:\nCalypso may also refer to:"}
{"id": "7225", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7225", "title": "Chemical affinity", "text": "In chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.\nHistory.\nEarly theories.\nThe idea of \"affinity\" is extremely old. Many attempts have been made at identifying its origins. The majority of such attempts, however, except in a general manner, end in futility since \"affinities\" lie at the basis of all magic, thereby pre-dating science. Physical chemistry, however, was one of the first branches of science to study and formulate a \"theory of affinity\". The name \"affinitas\" was first used in the sense of chemical relation by German philosopher Albertus Magnus near the year 1250. Later, those as Robert Boyle, John Mayow, Johann Glauber, Isaac Newton, and Georg Stahl put forward ideas on elective affinity in attempts to explain how heat is evolved during combustion reactions.\nThe term \"affinity\" has been used figuratively since c. 1600 in discussions of structural relationships in chemistry, philology, etc., and reference to \"natural attraction\" is from 1616. \"Chemical affinity\", historically, has referred to the \"force\" that causes chemical reactions. as well as, more generally, and earlier, the \u2033tendency to combine\u2033 of any pair of substances. The broad definition, used generally throughout history, is that chemical affinity is that whereby substances enter into or resist decomposition.\nThe modern term chemical affinity is a somewhat modified variation of its eighteenth-century precursor \"elective affinity\" or elective attractions, a term that was used by the 18th century chemistry lecturer William Cullen. Whether Cullen coined the phrase is not clear, but his usage seems to predate most others, although it rapidly became widespread across Europe, and was used in particular by the Swedish chemist Torbern Olof Bergman throughout his book (1775). Affinity theories were used in one way or another by most chemists from around the middle of the 18th century into the 19th century to explain and organise the different combinations into which substances could enter and from which they could be retrieved. Antoine Lavoisier, in his famed 1789 \"Trait\u00e9 \u00c9l\u00e9mentaire de Chimie (Elements of Chemistry)\", refers to Bergman's work and discusses the concept of elective affinities or attractions.\nAccording to chemistry historian Henry Leicester, the influential 1923 textbook \"Thermodynamics and the Free Energy of Chemical Reactions\" by Gilbert N. Lewis and Merle Randall led to the replacement of the term \"affinity\" by the term \"free energy\" in much of the English-speaking world.\nAccording to Prigogine, the term was introduced and developed by Th\u00e9ophile de Donder.\nGoethe used the concept in his novel Elective Affinities (1809).\nVisual representations.\nThe affinity concept was very closely linked to the visual representation of substances on a table. The first-ever \"affinity table\", which was based on displacement reactions, was published in 1718 by the French chemist \u00c9tienne Fran\u00e7ois Geoffroy. Geoffroy's name is best known in connection with these tables of \"affinities\" (\"tables des rapports\"), which were first presented to the French Academy of Sciences in 1718 and 1720, as shown below:\nDuring the 18th century many versions of the table were proposed with leading chemists like Torbern Bergman in Sweden and Joseph Black in Scotland adapting it to accommodate new chemical discoveries. All the tables were essentially lists, prepared by collating observations on the actions of substances one upon another, showing the varying degrees of affinity exhibited by analogous bodies for different reagents.\nCrucially, the table was the central graphic tool used to teach chemistry to students and its visual arrangement was often combined with other kinds diagrams. Joseph Black, for example, used the table in combination with chiastic and circlet diagrams to visualise the core principles of chemical affinity. Affinity tables were used throughout Europe until the early 19th century when they were displaced by affinity concepts introduced by Claude Berthollet.\nModern conceptions.\nIn chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.\nIn modern terms, we relate affinity to the phenomenon whereby certain atoms or molecules have the tendency to aggregate or bond. For example, in the 1919 book \"Chemistry of Human Life\" physician George W. Carey states that, \"Health depends on a proper amount of iron phosphate Fe3(PO4)2 in the blood, for the molecules of this salt have chemical affinity for oxygen and carry it to all parts of the organism.\" In this antiquated context, chemical affinity is sometimes found synonymous with the term \"magnetic attraction\". Many writings, up until about 1925, also refer to a \"law of chemical affinity\".\nIlya Prigogine summarized the concept of affinity, saying, \"All chemical reactions drive the system to a state of equilibrium in which the \"affinities\" of the reactions vanish.\"\nThermodynamics.\nThe present IUPAC definition is that affinity \"A\" is the negative partial derivative of Gibbs free energy \"G\" with respect to extent of reaction \"\u03be\" at constant pressure and temperature. That is,\nIt follows that affinity is positive for spontaneous reactions.\nIn 1923, the Belgian mathematician and physicist Th\u00e9ophile de Donder derived a relation between affinity and the Gibbs free energy of a chemical reaction. Through a series of derivations, de Donder showed that if we consider a mixture of chemical species with the possibility of chemical reaction, it can be proven that the following relation holds:\nWith the writings of Th\u00e9ophile de Donder as precedent, Ilya Prigogine and Defay in \"Chemical Thermodynamics\" (1954) defined chemical affinity as the rate of change of the uncompensated heat of reaction \"Q\"' as the reaction progress variable or reaction extent \"\u03be\" grows infinitesimally:"}
{"id": "7227", "revid": "1840598", "url": "https://en.wikipedia.org/wiki?curid=7227", "title": "Comet Hale\u2013Bopp", "text": "Comet Hale\u2013Bopp (formally designated C/1995\u00a0O1) is a comet that was perhaps the most widely observed of the 20th century and one of the brightest seen for many decades.\nAlan Hale and Thomas Bopp discovered Comet Hale\u2013Bopp separately on July 23, 1995, before it became visible to the naked eye. It is difficult to predict the maximum brightness of new comets with any degree of certainty, but Hale\u2013Bopp met or exceeded most predictions when it passed perihelion on April 1, 1997, reaching about magnitude \u20131.8. It was visible to the naked eye for a record 18\u00a0months, twice as long as the Great Comet of 1811, the previous record holder. Accordingly, Hale\u2013Bopp was dubbed the great comet of 1997.\nDiscovery.\nThe comet was discovered independently on July 23, 1995, by two observers, Alan Hale and Thomas Bopp, both in the United States.\nHale had spent many hundreds of hours searching for comets without success, and was tracking known comets from his driveway in New Mexico when he chanced upon Hale\u2013Bopp just after midnight. The comet had an apparent magnitude of 10.5 and lay near the globular cluster M70 in the constellation of Sagittarius. Hale first established that there was no other deep-sky object near M70, and then consulted a directory of known comets, finding that none were known to be in this area of the sky. Once he had established that the object was moving relative to the background stars, he emailed the Central Bureau for Astronomical Telegrams, the clearing house for astronomical discoveries.\nBopp did not own a telescope. He was out with friends near Stanfield, Arizona, observing star clusters and galaxies when he chanced across the comet while at the eyepiece of his friend's telescope. He realized he might have spotted something new when, like Hale, he checked his star maps to determine if any other deep-sky objects were known to be near M70, and found that there were none. He alerted the Central Bureau for Astronomical Telegrams through a Western Union telegram. Brian G. Marsden, who had run the bureau since 1968, laughed, \"Nobody sends telegrams anymore. I mean, by the time that telegram got here, Alan Hale had already e-mailed us three times with updated coordinates.\"\nThe following morning, it was confirmed that this was a new comet, and it was given the designation C/1995\u00a0O1. The discovery was announced in International Astronomical Union circular 6187.\nEarly observation.\nHale\u2013Bopp's orbital position was calculated as 7.2\u00a0astronomical units (AU) from the Sun, placing it between Jupiter and Saturn and by far the greatest distance from Earth at which a comet had been discovered by amateurs. Most comets at this distance are extremely faint, and show no discernible activity, but Hale\u2013Bopp already had an observable coma. A precovery image taken at the Anglo-Australian Telescope in 1993 was found to show the then-unnoticed comet some 13\u00a0AU from the Sun, a distance at which most comets are essentially unobservable. (Halley's Comet was more than 100 times fainter at the same distance from the Sun.) Analysis indicated later that its comet nucleus was 60\u00b120\u00a0kilometres in diameter, approximately six times the size of Halley.\nIts great distance and surprising activity indicated that comet Hale\u2013Bopp might become very bright when it reached perihelion in 1997. However, comet scientists were wary \u2013 comets can be extremely unpredictable, and many have large outbursts at great distance only to diminish in brightness later. Comet Kohoutek in 1973 had been touted as a 'comet of the century' and turned out to be unspectacular.\nPerihelion.\nHale\u2013Bopp became visible to the naked eye in May 1996, and although its rate of brightening slowed considerably during the latter half of that year, scientists were still cautiously optimistic that it would become very bright. It was too closely aligned with the Sun to be observable during December 1996, but when it reappeared in January 1997 it was already bright enough to be seen by anyone who looked for it, even from large cities with light-polluted skies.\nThe Internet was a growing phenomenon at the time, and numerous websites that tracked the comet's progress and provided daily images from around the world became extremely popular. The Internet played a large role in encouraging the unprecedented public interest in comet Hale\u2013Bopp.\nAs the comet approached the Sun, it continued to brighten, shining at 2nd\u00a0magnitude in February, and showing a growing pair of tails, the blue gas tail pointing straight away from the Sun and the yellowish dust tail curving away along its orbit. On March 9, a solar eclipse in China, Mongolia and eastern Siberia allowed observers there to see the comet in the daytime. Hale\u2013Bopp had its closest approach to Earth on March 22, 1997, at a distance of 1.315\u00a0AU.\nAs it passed perihelion on April 1, 1997, the comet developed into a spectacular sight. It shone brighter than any star in the sky except Sirius, and its dust tail stretched 40\u201345 degrees across the sky. The comet was visible well before the sky got fully dark each night, and while many great comets are very close to the Sun as they pass perihelion, comet Hale\u2013Bopp was visible all night to northern hemisphere observers.\nAfter perihelion.\nAfter its perihelion passage, the comet moved into the southern celestial hemisphere. The comet was much less impressive to southern hemisphere observers than it had been in the northern hemisphere, but southerners were able to see the comet gradually fade from view during the second half of 1997. The last naked-eye observations were reported in December 1997, which meant that the comet had remained visible without aid for 569\u00a0days, or about 18 and a half months. The previous record had been set by the Great Comet of 1811, which was visible to the naked eye for about 9 months.\nThe comet continued to fade as it receded, but is still being tracked by astronomers. In October 2007, 10 years after the perihelion and at distance of 25.7 AU from Sun, the comet was still active as indicated by the detection of the CO-driven coma. Herschel Space Observatory images taken in 2010 suggest comet Hale\u2013Bopp is covered in a fresh frost layer. Hale\u2013Bopp was again detected in December 2010 when it was 30.7 AU away from the Sun, and on August 7, 2012, at a 33.2 AU distance from the Sun. Astronomers expect that the comet will remain observable with large telescopes until perhaps 2020, by which time it will be nearing 30th\u00a0magnitude. By this time it will become very difficult to distinguish the comet from the large numbers of distant galaxies of similar brightness.\nOrbital changes.\nThe comet likely made its previous perihelion 4,200 years ago, in July 2215\u00a0BC. The estimated closest approach to Earth was 1.4\u00a0AU, and it may have been observed in ancient Egypt during the 6th dynasty reign of the Pharaoh Pepi II (Reign: 2247 \u2013 c. 2216 BC). Pepi's pyramid at Saqqara contains a text referring to an \"nhh-star\" as a companion of the pharaoh in the heavens, where \"\" is the hieroglyph for long hair.\nHale\u2013Bopp may have had a near collision with Jupiter in early June 2215\u00a0BC, which probably caused a dramatic change in its orbit, and 2215\u00a0BC may have been its first passage through the inner Solar System. The comet's current orbit is almost perpendicular to the plane of the ecliptic, so further close approaches to planets will be rare. However, in April 1996 the comet passed within 0.77\u00a0AU of Jupiter, close enough for its orbit to be measurably affected by the planet's gravity. The comet's orbit was shortened considerably to a period of roughly 2,533\u00a0years, and it will next return to the inner Solar System around the year 4385. Its greatest distance from the Sun (aphelion) will be about 370\u00a0AU, reduced from about 525\u00a0AU.\nThe estimated probability of Hale-Bopp's striking Earth in future passages through the inner Solar System is remote, about 2.5\u00d710\u22129 per orbit. However, given that the comet nucleus is around 60\u00a0km in diameter, the consequences of such an impact would be apocalyptic. Weissman conservatively estimates the diameter at 35\u00a0km; an estimated density of 0.6 g/cm3 then gives a cometary mass of 1.3\u00d71019 g. At a probable impact velocity of 52.5\u00a0km/s, impact energy can be calculated as 1.9\u00d71032 ergs, or 4.4\u00d7109 megatons, about 44 times the estimated energy of the K-T impact event.\nOver many orbits, the cumulative effect of gravitational perturbations on comets with high orbital inclinations and small perihelion distances is generally to reduce the perihelion distance to very small values. Hale\u2013Bopp has about a 15%\u00a0chance of eventually becoming a sungrazing comet through this process.\nScientific results.\nComet Hale\u2013Bopp was observed intensively by astronomers during its perihelion passage, and several important advances in cometary science resulted from these observations. The dust production rate of the comet was very high (up to 2.0 kg/s), which may have made the inner coma optically thick. Based on the properties of the dust grains\u2014high temperature, high albedo and strong 10\u00a0\u03bcm silicate emission feature\u2014the astronomers concluded the dust grains are smaller than observed in any other comet.\nHale\u2013Bopp showed the highest ever linear polarization detected for any comet. Such polarization is the result of solar radiation getting scattered by the dust particles in the coma of the comet and depends on the nature of the grains. It further confirms that the dust grains in the coma of comet Hale\u2013Bopp were smaller than inferred in any other comet.\nSodium tail.\nOne of the most remarkable discoveries was that the comet had a third type of tail. In addition to the well-known gas and dust tails, Hale\u2013Bopp also exhibited a faint sodium tail, only visible with powerful instruments with dedicated filters. Sodium emission had been previously observed in other comets, but had not been shown to come from a tail. Hale\u2013Bopp's sodium tail consisted of neutral atoms (not ions), and extended to some 50\u00a0million kilometres in length.\nThe source of the sodium appeared to be the inner coma, although not necessarily the nucleus. There are several possible mechanisms for generating a source of sodium atoms, including collisions between dust grains surrounding the nucleus, and \"sputtering\" of sodium from dust grains by ultraviolet light. It is not yet established which mechanism is primarily responsible for creating Hale\u2013Bopp's sodium tail, and the narrow and diffuse components of the tail may have different origins.\nWhile the comet's dust tail roughly followed the path of the comet's orbit and the gas tail pointed almost directly away from the Sun, the sodium tail appeared to lie between the two. This implies that the sodium atoms are driven away from the comet's head by radiation pressure.\nDeuterium abundance.\nThe abundance of deuterium in comet Hale\u2013Bopp in the form of heavy water was found to be about twice that of Earth's oceans. If Hale\u2013Bopp's deuterium abundance is typical of all comets, this implies that although cometary impacts are thought to be the source of a significant amount of the water on Earth, they cannot be the only source.\nDeuterium was also detected in many other hydrogen compounds in the comet. The ratio of deuterium to normal hydrogen was found to vary from compound to compound, which astronomers believe suggests that cometary ices were formed in interstellar clouds, rather than in the solar nebula. Theoretical modelling of ice formation in interstellar clouds suggests that comet Hale\u2013Bopp formed at temperatures of around 25\u201345\u00a0kelvins.\nOrganics.\nSpectroscopic observations of Hale\u2013Bopp revealed the presence of many organic chemicals, several of which had never been detected in comets before. These complex molecules may exist within the cometary nucleus, or might be synthesised by reactions in the comet.\nDetection of argon.\nHale\u2013Bopp was the first comet where the noble gas argon was detected. Noble gases are chemically inert and vary from low to high volatility. Since different noble elements have different sublimation temperatures, and don't interact with other elements, they can be used for probing the temperature histories of the cometary ices. Krypton has a sublimation temperature of 16\u201320\u00a0K and was found to be depleted more than 25 times relative to the solar abundance, while argon with its higher sublimation temperature was enriched relative to the solar abundance. Together these observations indicate that the interior of Hale\u2013Bopp has always been colder than 35\u201340\u00a0K, but has at some point been warmer than 20\u00a0K. Unless the solar nebula was much colder and richer in argon than generally believed, this suggests that the comet formed beyond Neptune in the Kuiper belt region and then migrated outward to the Oort cloud.\nRotation.\nComet Hale\u2013Bopp's activity and outgassing were not spread uniformly over its nucleus, but instead came from several specific jets. Observations of the material streaming away from these jets allowed astronomers to measure the rotation period of the comet, which was found to be about 11 hours 46 minutes.\nBinary nucleus question.\nIn 1997 a paper was published that hypothesised the existence of a binary nucleus to fully explain the observed pattern of comet Hale\u2013Bopp's dust emission observed in October 1995. The paper was based on theoretical analysis, and did not claim an observational detection of the proposed satellite nucleus, but estimated that it would have a diameter of about 30\u00a0km, with the main nucleus being about 70\u00a0km across, and would orbit in about three days at a distance of about 180\u00a0km. This analysis was confirmed by observations in 1996 using Wide-Field Planetary Camera 2 of the Hubble Space Telescope which had taken images of the comet that revealed the satellite.\nAlthough observations using adaptive optics in late 1997 and early 1998 showed a double peak in the brightness of the nucleus, controversy still exists over whether such observations can only be explained by a binary nucleus. The discovery of the satellite was not confirmed by other observations. Also, while comets have been observed to break up before, no case had been found of a stable binary nucleus until the subsequent discovery of .\nUFO claims.\nIn November 1996, amateur astronomer Chuck Shramek (1950\u20132000) of Houston, Texas took a CCD image of the comet which showed a fuzzy, slightly elongated object nearby. His computer sky-viewing program did not identify the star, so Shramek called the Art Bell radio program \"Coast to Coast AM\" to announce that he had discovered a \"Saturn-like object\" following Hale\u2013Bopp. UFO enthusiasts such as remote viewing proponent, and Emory University political science professor Courtney Brown soon concluded that there was an alien spacecraft following the comet.\nSeveral astronomers claimed that the object was simply the 8.5-magnitude star SAO141894, including Alan Hale. They noted that the star did not appear on Shramek's computer program because the user preferences were set incorrectly. Art Bell claimed to have obtained an image of the object from an anonymous astrophysicist who was about to confirm its discovery. However, astronomers Olivier Hainaut and David Tholen of the University of Hawaii stated that the alleged photo was an altered copy of one of their own comet images.\nThirty-nine members of the Heaven's Gate cult committed mass suicide in March 1997 with the intention of teleporting to a spaceship which they believed was flying behind the comet. \nNancy Lieder, who claims to receive messages from aliens through an implant in her brain, stated that Hale\u2013Bopp was a fiction designed to distract the population from the coming arrival of \"Nibiru\" or \"Planet X\", a giant planet whose close passage would disrupt the Earth's rotation, causing global cataclysm. Her original date for the apocalypse was May 2003, which passed without incident, but various conspiracy websites continued to predict the coming of Nibiru, most of whom tied it to the 2012 phenomenon. Lieder and others' claims of the planet Nibiru have been repeatedly debunked by scientists.\nLegacy.\nIts lengthy period of visibility and extensive coverage in the media meant that Hale\u2013Bopp was probably the most-observed comet in history, making a far greater impact on the general public than the return of Halley's Comet in 1986, and certainly seen by a greater number of people than witnessed any of Halley's previous appearances. For instance, 69% of Americans had seen Hale\u2013Bopp by April 9, 1997.\nHale\u2013Bopp was a record-breaking comet\u2014the farthest comet from the Sun discovered by amateurs, with the largest well-measured cometary nucleus known after 95P/Chiron, and it was visible to the naked eye for twice as long as the previous record-holder. It was also brighter than magnitude\u00a00 for eight\u00a0weeks, longer than any other recorded comet.\nCarolyn Shoemaker and her husband Gene, both famous for co-discovering comet Shoemaker\u2013Levy 9, were involved in a car crash after photographing the comet. Gene died in the crash and his ashes were sent to the Moon aboard NASA's \"Lunar Prospector\" mission along with an image of Hale\u2013Bopp, \"the last comet that the Shoemakers observed together\"."}
{"id": "7229", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7229", "title": "C-star algebra", "text": ""}
{"id": "7230", "revid": "12416903", "url": "https://en.wikipedia.org/wiki?curid=7230", "title": "Conspiracy", "text": "A conspiracy, also known as a plot, is a secret plan or agreement between persons (called conspirers or conspirators) for an unlawful or harmful purpose, such as murder or treason, especially with political motivation, while keeping their agreement secret from the public or from other people affected by it. In a political sense, conspiracy refers to a group of people united in the goal of usurping, altering or overthrowing an established political power. Depending on the circumstances, a conspiracy may also be a crime, or a civil wrong. The term generally implies wrongdoing or illegality on the part of the conspirators, as people would not need to conspire to engage in activities that were lawful and ethical, or to which no one would object.\nThere are some coordinated activities that people engage in with secrecy that are not generally thought of as conspiracies. For example, intelligence agencies such as the American CIA and the British MI6 necessarily make plans in secret to spy on suspected enemies of their respective countries, but this kind of activity is generally not considered to be a conspiracy so long as their goal is to fulfill their official functions, and not something like improperly enriching themselves. Similarly, the coaches of competing sports teams routinely meet behind closed doors to plan game strategies and specific plays designed to defeat their opponents, but this activity is not considered a conspiracy because this is considered a legitimate part of the sport. Furthermore, a conspiracy must be engaged in knowingly. The continuation of social traditions that work to the advantage of certain groups and to the disadvantage of certain other groups, though possibly unethical, is not a conspiracy if participants in the practice are not carrying it forward for the purpose of perpetuating this advantage.\nOn the other hand, if the intent of carrying out a conspiracy exists, then there is a conspiracy even if the details are never agreed to aloud by the participants. CIA covert operations, for instance, are by their very nature hard to prove definitively. But research into the agency's work, as well as revelations by former CIA employees, has suggested several cases where the agency tried to influence events. Between 1947 and 1989, the United States tried to change other nations' governments 72 times. During the Cold War, 26 of the United States' covert operations successfully brought a U.S.-backed government to power; the remaining 40 failed.\nA \"conspiracy theory\" is a belief that a conspiracy has actually been decisive in producing a political event of which the theorists strongly disapprove. Political scientist Michael Barkun has described conspiracy theories as relying on the view that the universe is governed by design, and embody three principles: nothing happens by accident, nothing is as it seems, and everything is connected. Another common feature is that conspiracy theories evolve to incorporate whatever evidence exists against them, so that they become, as Barkun writes, a closed system that is unfalsifiable, and therefore \"a matter of faith rather than proof\"."}
{"id": "7231", "revid": "2428506", "url": "https://en.wikipedia.org/wiki?curid=7231", "title": "Cytoprotectant", "text": ""}
{"id": "7232", "revid": "33238341", "url": "https://en.wikipedia.org/wiki?curid=7232", "title": "Cholistan Desert", "text": "The Cholistan Desert (; Punjabi: ), also locally known as Rohi (), is a large desert in the southern part of Punjab, Pakistan that forms part of the Greater Thar Desert, which extends to Sindh province and the Indian state of Rajasthan. It is one of two large deserts in Punjab, the other being Thal Desert. The name is derived from the Turkic word \"chol\", meaning \"sands,\" and \"istan\", a Persian suffix meaning \"land of.\"\nIn ancient times, Cholistan was a fertile region with a large river fed by meltwater from the Himalayas, and so has a high density of ancient settlements from the Indus Valley Civilization period dating back as early as 4000 BCE. The region later became a center for caravan trade, leading to the construction of numerous forts in the medieval period to protect trade routes - of which the Derawar Fort is the best preserved example.\nGeography.\nCholistan covers an area of in the Bahawalpur, Bahawalnagar, and Rahim Yar Khan districts of southern Punjab. The nearest major city is Bahawalpur city, from the edge of the desert. The desert stretches about 480 kilometres in length, with a width varying between 32 and 192 kilometres. It is located between 27\u00b042\u038400\u0384\u0384 to 29\u00b0 45\u038400\u0384\u0384 north, and 69\u00b057' 30'\u2032 to 72\u00b0 52' 30'\u2032 east. 81% of the desert is sandy, while 19% is characterized by alluvial flats and small sandy dunes. The entire region is subject to desertification due to poor vegetation cover resulting in wind erosion.\nClimate.\nCholistan's climate is characterized as an arid and semi-arid Tropical desert, with very low annual humidity. The mean temperature in Cholistan is , with the hottest month being July with a mean temperature of . Summer temperatures can surpass , and sometimes rise over during periods of drought. Winter temperatures occasionally dip to . Average rainfall in Cholistan is up to 180mm, with July and August being the wettest months, although droughts are common. Water is collected seasonally in a system of natural pools called \"Toba,\" or manmade pools call \"Kund\". Subsoil water is found at a depth of 30\u201340 meters, but is typically brackish, and unsuitable for most plant growth.\nGeology.\nCholistan was formed during the Pleistocene period. Geologically, Cholistan is divided into the Greater Cholistan and Lesser Cholistan, which are roughly divided by the dry bed of the ancient Hakra River. Greater Cholistan is a mostly sandy area in the south and west part of the desert up to the border with India, and covers an area of . Sand dunes in this area reach over 100 meters in height. Soil in the region is also highly saline. Lesser Cholistan is an arid and slightly less sandy region approximately in area which extends north and east from the old Hakra river bed, historically up to the banks of the Sutlej River.\nSoil quality is generally poor with little organic matter in the Greater Cholistan, and compacted alluvial clays in the Lesser Cholistan. A canal system built during the British era led to irrigation of the northern part of Lesser Cholistan.\nHistory.\nThough now an arid region, Cholistan once had a large river flowing through it that was formed by the waters of the Sutlej and Yamuna Rivers. The dry bed of the Hakra River runs through the area, along which many settlements of the Indus Valley Civilization/Harappan Culture have been discovered, including the large urban site of Ganweriwal. The river system supported settlements in the region between 4000 BCE and 600 BCE when the river changed course. The river carried significant amounts of water, and flowed until at least where Derawar Fort is now located.\nOver 400 Harappan sites had been listed in Cholistan in the 1970s, with a further 37 added in the 1990s. The high density of settlements in Cholistan suggest it may have been one of the most productive regions of the Indus Valley Civilization. In the post-Harappan period, Cholistan was part of the Cemetery H culture which grew as a surviving regional variant of the Harappan Culture, which was then followed by the Painted Grey Ware culture.\nThe region became a center for caravan trade, leading to the construction of a dense network of forts in the medieval period - of which the Derawar Fort is the best preserved example. Other large forts in Cholistan include Meergarh, Jaangarh, Marotgarh, Maujgarh, Dingarh, Khangarh, Khairgarh, Bijnotgarh and Islamgarh - with the suffix \"garh\" denoting \"fort.\" These forts are part of the Tentative List of UNESCO World Heritage Sites, and run roughly parallel to the Indus and Sutlej Rivers 40 miles to the south. Smaller forts in the area include Bara, Bhagla, Duheinwala, Falji, Kandera, Liara, Murid, Machki, Nawankot, and Phulra forts.\nEconomy.\nLivestock.\nThe backbone of Cholistan economy is animal rearing. Few other livelihood opportunities aside from livestock farming are available in the region. Agricultural farming away from the irrigated regions in Lower Cholistan are unavailable due to the lack of steady water-supply.\nCamels in particular are prized in Cholistan for their meat and milk, use as transportation, and for entertainment such as racing and camel dancing. Two types of camels are found in Cholistan: \"Marrecha,\" or \"Mahra,\" is used for transportation or racing/dancing. \"Berella\" is used for milk production, and can produce 10\u201315 liters of milk per day per animal.\nIt has the major importance for satisfying the area's major needs for cottage industry as well as milk meat and fat. Because of the nomadic way of life the main wealth of the people are their cattle that are bred for sale, milked or shorn for their wool. Moreover, isolated as they were, they had to depend upon themselves for all their needs like food, clothing, and all the items of daily use. So all their crafts initially stemmed from necessity but later on they started exporting their goods to the other places as well. The estimated number of livestock in the desert areas is 1.6 million.\nCotton and wool products.\nCholistan produces very superior type of carpet wool as compared to that produced in other parts of Pakistan. From this wool they knit beautiful carpets, rugs and other woolen items. This includes blankets, which is also a local necessity for the desert as it is not always dust and heat, but winter nights here are very cold too, usually below freezing points. Khes and pattu are also manufactured with wool or cotton. Khes is a form of blanket with a field of black white and pattu has a white ground base. Cholistan is now selling the wool for it brings maximum profit.\nTextiles.\nIt may be mentioned that cotton textiles have always been a hallmark of craft of Indus valley civilization. Various kinds of khaddar-cloth are made for local consumption, and fine khaddar bedclothes and coarse lungies are woven here. A beautiful cloth called Sufi is also woven of silk and cotton, or with cotton wrap and silk wool. Gargas are made with numerous patterns and color, having complicated embroidery, mirror, and patchwork. Ajrak is another specialty of Cholistan. It is a special and delicate printing technique on both sides of the cloth in indigo blue and red patterns covering the base cloth. Cotton turbans and shawls are also made here. Chunri is another form of dopattas, having innumerable colors and patterns like dots, squares, and circles on it.\nPeople.\nAs per the 1998 Census of Pakistan, a total of 128,019, with a 2015 estimate of 229,071, with 70% living in Lesser Cholistan. The average household size is 6.65.\nLocal crafts.\nAs mentioned above, the Indus Valley has always been occupied by the wandering nomadic tribes, who are fond of isolated areas, as such areas allow them to lead life free of foreign intrusion, enabling them to establish their own individual and unique cultures. Cholistan till the era of Mughal rule had also been isolated from outside influence. During the rule of Mughal Emperor Akbar, it became a proper productive unit. The entire area was ruled by a host of kings who securely guarded their frontiers. The rulers were the great patrons of art, and the various crafts underwent a simultaneous and parallel development, influencing each other. Masons, stone carvers, artisans, artists, and designers started rebuilding the old cities and new sites, and with that flourished new courts, paintings, weaving, and pottery. The fields of architecture, sculpture, terra cotta, and pottery developed greatly in this phase.\nCamel products.\nCamels are highly valued by the desert dwellers. Camels are not only useful for transportation and loading purposes, but its skin and wool are also quite worthwhile. Camel wool is spun and woven into beautiful woolen blankets known as falsies and into stylish and durable rugs. The camel's leather is also utilized in making caps, goblets, and expensive lampshades.\nLeather work.\nLeather work is another important local cottage industry due to the large number of livestock here. Other than the products mentioned above, Khusa (shoes) is a specialty of this area. Cholistani khusas are very famous for the quality of workmanship, variety, and richness of designs especially when stitched and embroidered with golden or brightly colored threads.\nJewellery.\nThe people of Cholistan are fond of jewelry, especially gold jewelry. The chief ornaments made and worn by them are \"Nath\" (nose gay), \"Katmala\" (necklace) \"Kangan\" (bracelet), and \"Pazeb\" (anklets). Gold and silver bangles are also a product of Cholistan. The locals similarly work in enamel, producing enamel buttons, earrings, bangles, and rings.\nEcology.\nFlora.\nSubsoil water in Cholistan is typically brackish, and unsuitable for most plant growth. Native trees, shrubs, and grasses are drought tolerant. There are 131 plant species in Cholistan from 89 genera and 24 families. \nMost common of them are below;\nA man-made forest called \"Dingarh\" was developed by The Pakistan Council of Research in Water Resources (PCRWR) on more than 100 ha. Dunes were fixed and stabilized by mechanical and vegetative means, and the area is now covered with trees with orchards of \"zizyphus,\" date palms, and grassland grown with collected rainwater and saline groundwater.\nFauna.\nThe wildlife of Cholistan desert mostly consists of migratory birds, especially Houbara bustard who migrates to this part during winters. This species of birds is most famous in the hunting season, even though they are endangered in Pakistan (vulnerable globally), according to IUCN Red List. Their population has decreased from 4,746 in 2001 to just a few dozens in recent times. \nIn December 2016, a Qatari prince, had his hunting license rejected due to the species being endangered. Another prince, Dr. Fahad was fined with Rs. 80,000 ($760) and all of the birds he caught were set free for hunting without permit and license.\nThe other endangered species in this desert is Chinkara, their population has also decreased from 3,000 in 2007 to just a few above 1,000 in 2010 due to non-permit hunting of the species by influential political families.\nTerra cotta.\nThe Indus civilization was the earliest centre of ceramics, and thus the pottery of Cholistan has a long history. Local soil is very fine and suitable for making pottery. The fineness of the earth can be observed on the Kacha houses which are actually plastered with mud but look like they have been white washed. The chief Cholistani ceramic articles are their surahies, piyalas, and glasses, remarkable for their lightness and fine finishing.\nIn the early times only the art of pottery and terracotta developed, but from the seventh century onwards, a large number of temples and images were also built on account of the intensified religious passions and the accumulation of wealth in cities. The building activity reached to such an extent that some cities actually became city temples. In fact the area particularly came to be known for its forts, villas, palaces, havelis,"}
{"id": "7233", "revid": "14604687", "url": "https://en.wikipedia.org/wiki?curid=7233", "title": "Causant\u00edn mac Cin\u00e1eda", "text": "' or ' (in Modern Gaelic: '; died 877) was a king of the Picts. He is often known as Constantine I in reference to his place in modern lists of kings of Scots, but contemporary sources described only as a Pictish king. A son of (\"Kenneth MacAlpin\"), he succeeded his uncle as Pictish king following the latter's death on 13 April 862. It is likely that Causant\u00edn's reign witnessed increased activity by Vikings, based in Ireland, Northumbria and northern Britain. He died fighting one such invasion.\nSources.\nVery few records of ninth century events in northern Britain survive. The main local source from the period is the \"Chronicle of the Kings of Alba\", a list of kings from Cin\u00e1ed mac Ailp\u00edn (died 858) to Cin\u00e1ed mac Ma\u00edl Coluim (died 995). The list survives in the Poppleton Manuscript, a thirteenth-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added from the tenth century onwards. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Cin\u00e1ed mac Ailp\u00edn may date from the end of the tenth century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain. The Pictish king-lists originally ended with this Causant\u00edn, who was reckoned the seventieth and last king of the Picts.\nFor narrative history the principal sources are the \"Anglo-Saxon Chronicle\" and the Irish annals. While Scandinavian sagas describe events in 9th century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. If the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts\u2014the modern regions of north-west England and all of northern and western Scotland\u2014are non-existent, and archaeology and toponymy are of primary importance.\nLanguages and names.\nWriting a century before Causant\u00edn was born, Bede recorded five languages in Britain. Latin, the common language of the church; Old English, the language of the Angles and Saxons; Irish, spoken on the western coasts of Britain and in Ireland; Brythonic, ancestor of the Welsh language, spoken in large parts of western Britain; and Pictish, spoken in northern Britain. By the ninth century a sixth language, Old Norse, had arrived with the Vikings.\nAmla\u00edb and \u00cdmar.\nViking activity in northern Britain appears to have reached a peak during Causant\u00edn's reign. Viking armies were led by a small group of men who may have been kinsmen. Among those noted by the Irish annals, the \"Chronicle of the Kings of Alba\" and the \"Anglo-Saxon Chronicle\" are \u00cdvarr\u2014\u00cdmar in Irish sources\u2014who was active from East Anglia to Ireland, Halfd\u00e1n\u2014Albdann in Irish, Healfdene in Old English\u2014 and Amla\u00edb or \u00d3l\u00e1fr. As well as these leaders, various others related to them appear in the surviving record.\nViking activity in Britain increased in 865 when the Great Heathen Army, probably a part of the forces which had been active in Francia, landed in East Anglia. The following year, having obtained tribute from the East Anglian King Edmund, the Great Army moved north, seizing York, chief city of the Northumbrians. The Great Army defeated an attack on York by the two rivals for the Northumbrian throne, Osberht and \u00c6lla, who had put aside their differences in the face of a common enemy. Both would-be kings were killed in the failed assault, probably on 21 March 867. Following this, the leaders of the Great Army are said to have installed one Ecgberht as king of the Northumbrians. Their next target was Mercia where King Burgred, aided by his brother-in-law King \u00c6thelred of Wessex, drove them off.\nWhile the kingdoms of East Anglia, Mercia and Northumbria were under attack, other Viking armies were active in the far north. Amla\u00edb and Auisle (\u00c1sl or Au\u00f0g\u00edsl), said to be his brother, brought an army to Fortriu and obtained tribute and hostages in 866. Historians disagree as to whether the army returned to Ireland in 866, 867 or even in 869. Late sources of uncertain reliability state that Auisle was killed by Amla\u00edb in 867 in a dispute over Amla\u00edb's wife, the daughter of Cin\u00e1ed. It is unclear whether, if accurate, this woman should be identified as a daughter of Cin\u00e1ed mac Ailp\u00edn, and thus Causant\u00edn's sister, or as a daughter of Cin\u00e1ed mac Conaing, king of Brega. While Amla\u00edb and Auisle were in north Britain, the \"Annals of Ulster\" record that \u00c1ed Findliath, High King of Ireland, took advantage of their absence to destroy the longphorts along the northern coasts of Ireland. \u00c1ed Findliath was married to Causant\u00edn's sister M\u00e1el Muire. She later married \u00c1ed's successor Flann Sinna. Her death is recorded in 913.\nIn 870, Amla\u00edb and \u00cdvarr attacked Dumbarton Rock, where the River Leven meets the River Clyde, the chief place of the kingdom of Alt Clut, south-western neighbour of Pictland. The siege lasted four months before the fortress fell to the Vikings who returned to Ireland with many prisoners, \"Angles, Britons and Picts\", in 871. Archaeological evidence suggests that Dumbarton Rock was largely abandoned and that Govan replaced it as the chief place of the kingdom of Strathclyde, as Alt Clut was later known. King Artgal of Alt Clut did not long survive these events, being killed \"at the instigation\" of Causant\u00edn son of Cin\u00e1ed two years later. Artgal's son and successor Run was married to a sister of Causant\u00edn.\nAmla\u00edb disappears from Irish annals after his return to Ireland in 871. According to the \"Chronicle of the Kings of Alba\" he was killed by Causant\u00edn either in 871 or 872 when he returned to Pictland to collect further tribute. His ally \u00cdvarr died in 873.\nLast days of the Pictish kingdom.\nIn 875, the \"Chronicle\" and the \"Annals of Ulster\" again report a Viking army in Pictland. A battle, fought near Dollar, was a heavy defeat for the Picts; the \"Annals of Ulster\" say that \"a great slaughter of the Picts resulted\". In 877, shortly after building a new church for the Culdees at St Andrews, Causant\u00edn was captured and executed (or perhaps killed in battle) after defending against Viking raiders. Although there is agreement on the time and general manner of his death, it is not clear where this happened. Some believe he was beheaded on a Fife beach, following a battle at Fife Ness, near Crail. William Forbes Skene reads the \"Chronicle\" as placing Causant\u00edn's death at Inverdovat (by Newport-on-Tay), which appears to match the Prophecy of Berch\u00e1n. The account in the \"Chronicle of Melrose\" names the place as the \"Black Cave,\" and John of Fordun calls it the \"Black Den\". Causant\u00edn was buried on Iona.\nAftermath.\nCausant\u00edn's son Domnall and his descendants represented the main line of the kings of Alba and later Scotland."}
{"id": "7234", "revid": "237572", "url": "https://en.wikipedia.org/wiki?curid=7234", "title": "Constantine II (emperor)", "text": "Constantine II (; February 316 \u2013 340) was Roman emperor from 337 to 340. Son of Constantine the Great and co-emperor alongside his brothers, his attempt to exert his perceived rights of primogeniture led to his death in a failed invasion of Italy in 340.\nCareer.\nThe eldest son of Constantine the Great and Fausta, Constantine II was born in Arles in February 316 and raised as a Christian.\n\"Caesar\".\nOn 1 March 317, he was made Caesar. In 323, at the age of seven, he took part in his father's campaign against the Sarmatians. At age ten, he became commander of Gaul, following the death of his half-brother Crispus. An inscription dating to 330 records the title of \"Alamannicus\", so it is probable that his generals won a victory over the Alamanni. His military career continued when Constantine I made him field commander during the 332 campaign against the Goths.\n\"Augustus\".\nFollowing the death of his father in 337, Constantine II initially became emperor jointly with his brothers Constantius II and Constans, with the Empire divided between them and their cousins, the \"caesars\" Dalmatius and Hannibalianus. This arrangement barely survived Constantine I\u2019s death, as his sons arranged the slaughter of most of the rest of the family by the army. As a result, the three brothers gathered together in Pannonia and there, on 9 September 337, divided the Roman world among themselves. Constantine, proclaimed \"Augustus\" by the troops received Gaul, Britannia and Hispania.\nHe was soon involved in the struggle between factions rupturing the unity of the Christian Church. The Western portion of the Empire, under the influence of the Popes in Rome, favoured Catholicism over Arianism, and through their intercession they convinced Constantine to free Athanasius, allowing him to return to Alexandria. This action aggravated Constantius II, who was a committed supporter of Arianism.\nConstantine was initially the guardian of his younger brother Constans, whose portion of the empire was Italia, Africa and Illyricum. Constantine soon complained that he had not received the amount of territory that was his due as the eldest son. Annoyed that Constans had received Thrace and Macedonia after the death of Dalmatius, Constantine demanded that Constans hand over the African provinces, to which he agreed in order to maintain a fragile peace. Soon, however, they began quarreling over which parts of the African provinces belonged to Carthage, and thus Constantine, and which belonged to Italy, and therefore Constans.\nFurther complications arose when Constans came of age and Constantine, who had grown accustomed to dominating his younger brother, would not relinquish the guardianship. In 340 Constantine marched into Italy at the head of his troops. Constans, at that time in Dacia, detached and sent a select and disciplined body of his Illyrian troops, stating that he would follow them in person with the remainder of his forces. Constantine was engaged in military operations and was killed in an ambush outside Aquileia. Constans then took control of his deceased brother's realm."}
{"id": "7235", "revid": "82835", "url": "https://en.wikipedia.org/wiki?curid=7235", "title": "Constantine II of Scotland", "text": "Constantine, son of \u00c1ed (Medieval Gaelic: \"Causant\u00edn mac \u00c1eda\"; Modern Gaelic: \"C\u00f2iseam mac Aoidh\", known in most modern regnal lists as Constantine II; born no later than 879; died 952) was an early King of Scotland, known then by the Gaelic name \"Alba\". The Kingdom of Alba, a name which first appears in Constantine's lifetime, was situated in modern-day Scotland.\nThe core of the kingdom was formed by the lands around the River Tay. Its southern limit was the River Forth, northwards it extended towards the Moray Firth and perhaps to Caithness, while its western limits are uncertain. Constantine's grandfather Kenneth I of Scotland (Cin\u00e1ed mac Ailp\u00edn, died 858) was the first of the family recorded as a king, but as king of the Picts. This change of title, from king of the Picts to king of Alba, is part of a broader transformation of Pictland and the origins of the Kingdom of Alba are traced to Constantine's lifetime.\nHis reign, like those of his predecessors, was dominated by the actions of Viking rulers in the British Isles, particularly the U\u00ed \u00cdmair (\"the grandsons of \u00cdmar\", or Ivar the Boneless). During Constantine's reign the rulers of the southern kingdoms of Wessex and Mercia, later the Kingdom of England, extended their authority northwards into the disputed kingdoms of Northumbria. At first, the southern rulers allied with him against the Vikings, but in 934 \u00c6thelstan, unprovoked, invaded Scotland both by sea and land with a huge retinue that included four Welsh Kings. He ravaged southern Alba but there is no record of any battles. He had withdrawn by September. Three years later in 937, probably in retaliation for the invasion of Alba, King Constantine allied with Olaf Guthfrithson, King of Dublin, and Owain, King of Strathclyde, but they were defeated at the battle of Brunanburh. In 943 Constantine abdicated the throne and retired to the C\u00e9li D\u00e9 (Culdee) monastery of St Andrews where he died in 952. He was succeeded by his predecessor's son Malcolm\u00a0I (M\u00e1el Coluim mac Domnaill).\nConstantine's reign of 43 years, exceeded in Scotland only by that of King William the Lion before the Union of the Crowns in 1603, is believed to have played a defining part in the gaelicisation of Pictland, in which his patronage of the Irish C\u00e9li D\u00e9 monastic reformers was a significant factor. During his reign the words \"Scots\" and \"Scotland\" () are first used to mean part of what is now Scotland. The earliest evidence for the ecclesiastical and administrative institutions which would last until the Davidian Revolution also appears at this time.\nSources.\nCompared to neighbouring Ireland and Anglo-Saxon England, few records of 9th- and 10th-century events in Scotland survive. The main local source from the period is the \"Chronicle of the Kings of Alba\", a list of kings from Kenneth MacAlpin (died 858) to Kenneth\u00a0II (Cin\u00e1ed mac Ma\u00edl Coluim, died 995). The list survives in the Poppleton Manuscript, a 13th-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added in the 10th and 12th centuries. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Kenneth MacAlpin may date from the end of the 10th century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain.\nFor narrative history the principal sources are the \"Anglo-Saxon Chronicle\" and the Irish annals. The evidence from charters created in the Kingdom of England provides occasional insight into events in Scotland. While Scandinavian sagas describe events in 10th-century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. Mainland European sources rarely concern themselves with affairs in any part of the British Isles, and even less commonly with events in Scotland, but the life of Saint Cathr\u00f3e of Metz, a work of hagiography written in Germany at the end of the 10th century, provides plausible details of the saint's early life in north Britain.\nWhile the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts\u2014the modern regions of north-west England and all of northern and western Scotland\u2014are non-existent, and archaeology and toponymy are of primary importance.\nPictland from Constant\u00edn mac Fergusa to Constantine I.\nThe dominant kingdom in eastern Scotland before the Viking Age was the northern Pictish kingdom of Fortriu on the shores of the Moray Firth. By the 9th century, the Gaels of D\u00e1l Riata (Dalriada) were subject to the kings of Fortriu of the family of Constant\u00edn mac Fergusa (Constantine son of Fergus). Constant\u00edn's family dominated Fortriu after 789 and perhaps, if Constant\u00edn was a kinsman of \u00d3engus I of the Picts (\u00d3engus son of Fergus), from around 730. The dominance of Fortriu came to an end in 839 with a defeat by Viking armies reported by the \"Annals of Ulster\" in which King Uen of Fortriu and his brother Bran, Constant\u00edn's nephews, together with the king of D\u00e1l Riata, \u00c1ed mac Boanta, \"and others almost innumerable\" were killed. These deaths led to a period of instability lasting a decade as several families attempted to establish their dominance in Pictland. By around 848 Kenneth MacAlpin had emerged as the winner.\nLater national myth made Kenneth MacAlpin the creator of the kingdom of Scotland, the founding of which was dated from 843, the year in which he was said to have destroyed the Picts and inaugurated a new era. The historical record for 9th-century Scotland is meagre, but the Irish annals and the 10th-century \"Chronicle of the Kings of Alba\" agree that Kenneth was a Pictish king, and call him \"king of the Picts\" at his death. The same style is used of Kenneth's brother Donald\u00a0I (Domnall mac Ailp\u00edn) and sons Constantine\u00a0I (Constant\u00edn mac Cin\u00e1eda) and \u00c1ed (\u00c1ed mac Cin\u00e1eda).\nThe kingdom ruled by Kenneth's descendants\u2014older works used the name House of Alpin to describe them but descent from Kenneth was the defining factor, Irish sources referring to \"Clann Cin\u00e1eda meic Ailp\u00edn\" (\"the Clan of Kenneth MacAlpin\")\u2014lay to the south of the previously dominant kingdom of Fortriu, centred in the lands around the River Tay. The extent of Kenneth's nameless kingdom is uncertain, but it certainly extended from the Firth of Forth in the south to the Mounth in the north. Whether it extended beyond the mountainous spine of north Britain\u2014Druim Alban\u2014is unclear. The core of the kingdom was similar to the old counties of Mearns, Forfar, Perth, Fife, and Kinross. Among the chief ecclesiastical centres named in the records are Dunkeld, probably seat of the bishop of the kingdom, and \"Cell R\u00edgmonaid\" (modern St Andrews).\nKenneth's son Constantine died in 876, probably killed fighting against a Viking army which had come north from Northumbria in 874. According to the king lists, he was counted the 70th and last king of the Picts in later times.\nBritain and Ireland at the end of the 9th century.\nIn 899 Alfred the Great, king of Wessex, died leaving his son Edward the Elder as ruler of England south of the River Thames and his daughter \u00c6thelfl\u00e6d and son-in-law \u00c6thelred ruling the western, English part of Mercia. The situation in the Danish kingdoms of eastern England is less clear. King Eohric was probably ruling in East Anglia, but no dates can reliably be assigned to the successors of Guthfrith of York in Northumbria. It is known that Guthfrith was succeeded by Sigurd and Cnut, although whether these men ruled jointly or one after the other is uncertain. Northumbria may have been divided by this time between the Viking kings in York and the local rulers, perhaps represented by Eadulf, based at Bamburgh who controlled the lands from the River Tyne or River Tees to the Forth in the north.\nIn Ireland, Flann Sinna, married to Constantine's aunt M\u00e1el Muire, was dominant. The years around 900 represented a period of weakness among the Vikings and Norse-Gaels of Dublin. They are reported to have been divided between two rival leaders. In 894 one group left Dublin, perhaps settling on the Irish Sea coast of Britain between the River Mersey and the Firth of Clyde. The remaining Dubliners were expelled in 902 by Flann Sinna's son-in-law Cerball mac Muirec\u00e1in, and soon afterwards appeared in western and northern Britain.\nTo the south-west of Constantine's lands lay the kingdom of Strathclyde. This extended north into the Lennox, east to the River Forth, and south into the Southern Uplands. In 900 it was probably ruled by King Dyfnwal.\nThe situation of the Gaelic kingdoms of D\u00e1l Riata in western Scotland is uncertain. No kings are known by name after \u00c1ed mac Boanta. The Frankish \"Annales Bertiniani\" may record the conquest of the Inner Hebrides, the seaward part of D\u00e1l Riata, by Northmen in 849. In addition to these, the arrival of new groups of Vikings from northern and western Europe was still commonplace. Whether there were Viking or Norse-Gael kingdoms in the Western Isles or the Northern Isles at this time is debated.\nEarly life.\n\u00c1ed, Constantine's father, succeeded Constantine's uncle and namesake Constantine\u00a0I in 876 but was killed in 878. \u00c1ed's short reign is glossed as being of no importance by most king lists. Although the date of his birth is nowhere recorded, Constantine\u00a0II cannot have been born any later than the year after his father's death, \"i.e.\", 879. His name may suggest that he was born a few years earlier, during the reign of his uncle Constantine\u00a0I.\nAfter \u00c1ed's death, there is a two-decade gap until the death of Donald\u00a0II (Domnall mac Constant\u00edn) in 900 during which nothing is reported in the Irish annals. The entry for the reign between \u00c1ed and Donald\u00a0II is corrupt in the \"Chronicle of the Kings of Alba\", and in this case the \"Chronicle\" is at variance with every other king list. According to the \"Chronicle\", \u00c1ed was followed by Eochaid, a grandson of Kenneth MacAlpin, who is somehow connected with Giric, but all other lists say that Giric ruled after \u00c1ed and make great claims for him. Giric is not known to have been a kinsman of Kenneth's, although it has been suggested that he was related to him by marriage. The major changes in Pictland which began at about this time have been associated by Alex Woolf and Archie Duncan with Giric's reign.\nWoolf suggests that Constantine and his cousin Donald may have passed Giric's reign in exile in Ireland where their aunt M\u00e1el Muire was wife of two successive High Kings of Ireland, \u00c1ed Findliath and Flann Sinna. Giric died in 889. If he had been in exile, Constantine may have returned to Pictland where his cousin Donald\u00a0II became king. Donald's reputation is suggested by the epithet \"dasachtach\", a word used of violent madmen and mad bulls, attached to him in the 11th-century writings of Flann Mainistrech, echoed by his description in the \"Prophecy of Berchan\" as \"the rough one who will think relics and psalms of little worth\". Wars with the Viking kings in Britain and Ireland continued during Donald's reign and he was probably killed fighting yet more Vikings at Dunnottar in the Mearns in 900. Constantine succeeded him as king.\nVikings and bishops.\nThe earliest event recorded in the \"Chronicle of the Kings of Alba\" in Constantine's reign is an attack by Vikings and the plundering of Dunkeld \"and all Albania\" in his third year. This is the first use of the word Albania, the Latin form of the Old Irish \"Alba\", in the \"Chronicle\" which until then describes the lands ruled by the descendants of Cin\u00e1ed as Pictavia.\nThese Norsemen could have been some of those who were driven out of Dublin in 902, or were the same group who had defeated Domnall in 900. The \"Chronicle\" states that the Northmen were killed in \"Srath Erenn\", which is confirmed by the \"Annals of Ulster\" which records the death of \u00cdmar grandson of \u00cdmar and many others at the hands of the men of Fortriu in 904. This \u00cdmar was the first of the U\u00ed \u00cdmair, the grandsons of \u00cdmar, to be reported; three more grandsons of \u00cdmar appear later in Constant\u00edn's reign. The \"Fragmentary Annals of Ireland\" contain an account of the battle, and this attributes the defeat of the Norsemen to the intercession of Saint Columba following fasting and prayer. An entry in the \"Chronicon Scotorum\" under the year 904 may possibly contain a corrupted reference to this battle.\nThe next event reported by the \"Chronicle of the Kings of Alba\" is dated to 906. This records that:King Constantine and Bishop Cellach met at the \"Hill of Belief\" near the royal city of Scone and pledged themselves that the laws and disciplines of the faith, and the laws of churches and gospels, should be kept \"pariter cum Scottis\". The meaning of this entry, and its significance, have been the subject of debate.\nThe phrase \"pariter cum Scottis\" in the Latin text of the \"Chronicle\" has been translated in several ways. William Forbes Skene and Alan Orr Anderson proposed that it should be read as \"in conformity with the customs of the Gaels\", relating it to the claims in the king lists that Giric liberated the church from secular oppression and adopted Irish customs. It has been read as \"together with the Gaels\", suggesting either public participation or the presence of Gaels from the western coasts as well as the people of the east coast. Finally, it is suggested that it was the ceremony which followed \"the custom of the Gaels\" and not the agreements.\nThe idea that this gathering agreed to uphold Irish laws governing the church has suggested that it was an important step in the gaelicisation of the lands east of Druim Alban. Others have proposed that the ceremony in some way endorsed Constantine's kingship, prefiguring later royal inaugurations at Scone. Alternatively, if Bishop Cellach was appointed by Giric, it may be that the gathering was intended to heal a rift between king and church.\nReturn of the U\u00ed \u00cdmair.\nFollowing the events at Scone, there is little of substance reported for a decade. A story in the \"Fragmentary Annals of Ireland\", perhaps referring to events some time after 911, claims that Queen \u00c6thelfl\u00e6d, who ruled in Mercia, allied with the Irish and northern rulers against the Norsemen on the Irish sea coasts of Northumbria. The \"Annals of Ulster\" record the defeat of an Irish fleet from the kingdom of Ulaid by Vikings \"on the coast of England\" at about this time.\nIn this period the \"Chronicle of the Kings of Alba\" reports the death of Cormac mac Cuilenn\u00e1in, king of Munster, in the eighth year of Constantine's reign. This is followed by an undated entry which was formerly read as \"In his time Domnall [i.e. Dyfnwal], king of the [Strathclyde] Britons died, and Domnall son of \u00c1ed was elected\". This was thought to record the election of a brother of Constantine named Domnall to the kingship of the Britons of Strathclyde and was seen as early evidence of the domination of Strathclyde by the kings of Alba. The entry in question is now read as \"...Dyfnwal... and Domnall son \u00c1ed king of Ailech died\", this Domnall being a son of \u00c1ed Findliath who died on 915. Finally, the deaths of Flann Sinna and Niall Gl\u00fandub are recorded.\nThere are more reports of Viking fleets in the Irish Sea from 914 onwards. By 916 fleets under Sihtric C\u00e1ech and Ragnall, said to be grandsons of \u00cdmar (that is, they belonged to the same U\u00ed \u00cdmair kindred as the \u00cdmar who was killed in 904), were very active in Ireland. Sihtric inflicted a heavy defeat on the armies of Leinster and retook Dublin in 917. The following year Ragnall appears to have returned across the Irish sea intent on establishing himself as king at York. The only precisely dated event in the summer of 918 is the death of Queen \u00c6thelfl\u00e6d on 918 at Tamworth, Staffordshire. \u00c6thelfl\u00e6d had been negotiating with the Northumbrians to obtain their submission, but her death put an end to this and her successor, her brother Edward the Elder, was occupied with securing control of Mercia.\nThe northern part of Northumbria, and perhaps the whole kingdom, had probably been ruled by Ealdred son of Eadulf since 913. Faced with Ragnall's invasion, Ealdred came north seeking assistance from Constantine. The two advanced south to face Ragnall, and this led to a battle somewhere on the banks of the River Tyne, probably at Corbridge where Dere Street crosses the river. The Battle of Corbridge appears to have been indecisive; the \"Chronicle of the Kings of Alba\" is alone in giving Constantine the victory.\nThe report of the battle in the \"Annals of Ulster\" says that none of the kings or mormaers among the men of Alba were killed. This is the first surviving use of the word mormaer; other than the knowledge that Constantine's kingdom had its own bishop or bishops and royal villas, this is the only hint to the institutions of the kingdom.\nAfter Corbridge, Ragnall enjoyed only a short respite. In the south, Alfred's son Edward had rapidly secured control of Mercia and had a burh constructed at Bakewell in the Peak District from which his armies could easily strike north. An army from Dublin led by Ragnall's kinsman Sihtric struck at north-western Mercia in 919, but in 920 or 921 Edward met with Ragnall and other kings. The \"Anglo-Saxon Chronicle\" states that these kings \"chose Edward as father and lord\". Among the other kings present were Constantine, Ealdred son of Eadwulf, and the king of Strathclyde, Owain ap Dyfnwal. Here, again, a new term appears in the record, the \"Anglo-Saxon Chronicle\" for the first time using the word \"scottas\", from which Scots derives, to describe the inhabitants of Constantine's kingdom in its report of these events.\nEdward died in 924. His realms appear to have been divided with the West Saxons recognising \u00c6lfweard while the Mercians chose \u00c6thelstan who had been raised at \u00c6thelfl\u00e6d's court. \u00c6lfweard died within weeks of his father and \u00c6thelstan was inaugurated as king of all of Edward's lands in 925.\n\u00c6thelstan.\nBy 926 Sihtric had evidently acknowledged \u00c6thelstan as overlord, adopting Christianity and marrying a sister of \u00c6thelstan at Tamworth. Within the year he appears to have forsaken his new faith and repudiated his wife, but before \u00c6thelstan could respond, Sihtric died suddenly in 927. His kinsman, perhaps brother, Gofraid, who had remained as his deputy in Dublin, came from Ireland to take power in York, but failed. \u00c6thelstan moved quickly, seizing much of Northumbria. In less than a decade, the kingdom of the English had become by far the greatest power in Britain and Ireland, perhaps stretching as far north as the Firth of Forth.\nJohn of Worcester's chronicle suggests that \u00c6thelstan faced opposition from Constantine, from Owain, and from the Welsh kings. William of Malmesbury writes that Gofraid, together with Sihtric's young son Olaf Cuaran fled north and received refuge from Constantine, which led to war with \u00c6thelstan. A meeting at Eamont Bridge on 927 was sealed by an agreement that Constantine, Owain, Hywel Dda, and Ealdred would \"renounce all idolatry\": that is, they would not ally with the Viking kings. William states that \u00c6thelstan stood godfather to a son of Constantine, probably Indulf (Ildulb mac Constant\u00edn), during the conference.\n\u00c6thelstan followed up his advances in the north by securing the recognition of the Welsh kings. For the next seven years, the record of events in the north is blank. \u00c6thelstan's court was attended by the Welsh kings, but not by Constantine or Owain. This absence of record means that \u00c6thelstan's reasons for marching north against Constantine in 934 are unclear.\n\u00c6thelstan's invasion is reported in brief by the \"Anglo-Saxon Chronicle\", and later chroniclers such as John of Worcester, William of Malmesbury, Henry of Huntingdon, and Symeon of Durham add detail to that bald account. \u00c6thelstan's army began gathering at Winchester by 934, and travelled north to Nottingham by . He was accompanied by many leaders, including the Welsh kings Hywel Dda, Idwal Foel, and Morgan ab Owain. From Mercia the army continued to Chester-le-Street, before resuming the march accompanied by a fleet of ships. Owain was defeated and Symeon states that the army went as far north as Dunnottar and Fortriu, while the fleet is said to have raided Caithness, by which a much larger area, including Sutherland, is probably intended. It is unlikely that Constantine's personal authority extended so far north, so the attacks were probably directed at his allies, comprising simple looting expeditions.\nThe \"Annals of Clonmacnoise\" state that \"the Scottish men compelled [\u00c6thelstan] to return without any great victory\", while Henry of Huntingdon claims that the English faced no opposition. A negotiated settlement might have ended matters: according to John of Worcester, a son of Constantine was given as a hostage to \u00c6thelstan and Constantine himself accompanied the English king on his return south. He witnessed a charter with \u00c6thelstan at Buckingham on 934 in which he is described as \"subregulus\", \"i.e.\", a king acknowledging \u00c6thelstan's overlordship, the only place there is any record of such a description. However, there is no record of Constantine having ever submitted to \u00c6thelstan's overlordship or that he considered himself such. The following year, Constantine was again in England at \u00c6thelstan's court, this time at Cirencester where he appears as a witness, as the first of several kings, followed by Owain and Hywel Dda, who subscribed to the diploma. At Christmas of 935, Owain was once more at \u00c6thelstan's court along with the Welsh kings, but Constantine was not. His return to England less than two years later would be in very different circumstances.\nBrunanburh and after.\nFollowing his departure from \u00c6thelstan's court after 935, there is no further report of Constantine until 937. In that year, together with Owain and Olaf Guthfrithson of Dublin, Constantine invaded England. The resulting battle of Brunanburh\u2014\"D\u00fan Brunde\"\u2014is reported in the \"Annals of Ulster\" as follows:a great battle, lamentable and terrible was cruelly fought... in which fell uncounted thousands of the Northmen.\u00a0 ...And on the other side, a multitude of Saxons fell; but \u00c6thelstan, the king of the Saxons, obtained a great victory. The battle was remembered in England a generation later as \"the Great Battle\". When reporting the battle, the \"Anglo-Saxon Chronicle\" abandons its usual terse style in favour of a heroic poem vaunting the great victory. In this the \"hoary\" Constantine, by now around 60 years of age, is said to have lost a son in the battle, a claim which the \"Chronicle of the Kings of Alba\" confirms. The \"Annals of Clonmacnoise\" give his name as Cellach. For all its fame, the site of the battle is uncertain and several sites have been advanced, with Bromborough on the Wirral the most favoured location.\nBrunanburh, for all that it had been a famous and bloody battle, settled nothing. On 939 \u00c6thelstan, the \"pillar of the dignity of the western world\" in the words of the \"Annals of Ulster\", died at Malmesbury. He was succeeded by his brother Edmund, then aged 18. \u00c6thelstan's realm, seemingly made safe by the victory of Brunanburh, collapsed in little more than a year from his death when Amla\u00edb returned from Ireland and seized Northumbria and the Mercian Danelaw. Edmund spent the remainder of Constant\u00edn's reign rebuilding his kingdom.\nFor Constantine's last years as king there is only the meagre record of the \"Chronicle of the Kings of Alba\". The death of \u00c6thelstan is reported, as are two others. The first of these, in 938, is that of Dubacan, mormaer of Angus or son of the mormaer. Unlike the report of 918, on this occasion the title mormaer is attached to a geographical area, but it is unknown whether the Angus of 938 was in any way similar to the later mormaerdom or earldom. The second death, entered with that of \u00c6thelstan, is that of Eochaid mac Ailp\u00edn, who might, from his name, have been a kinsman of Constant\u00edn.\nAbdication and posterity.\nBy the early 940s Constantine was an old man in his late sixties or seventies. The kingdom of Alba was too new to be said to have a customary rule of succession, but Pictish and Irish precedents favoured an adult successor descended from Kenneth MacAlpin. Constantine's surviving son Indulf, probably baptised in 927, would have been too young to be a serious candidate for the kingship in the early 940s, and the obvious heir was Constantine's nephew, Malcolm I. As Malcolm was born no later than 901, by the 940s he was no longer a young man, and may have been impatient. Willingly or not\u2014the 11th-century \"Prophecy of Berch\u00e1n\", a verse history in the form of a supposed prophecy, states that it was not a voluntary decision\u2014Constantine abdicated in 943 and entered a monastery, leaving the kingdom to Malcolm.\nAlthough his retirement might have been involuntary, the \"Life\" of Cathr\u00f3e of Metz and the \"Prophecy of Berch\u00e1n\" portray Constantine as a devout king. The monastery to which Constantine retired, and where he is said to have been abbot, was probably that of St Andrews. This had been refounded in his reign and given to the reforming C\u00e9li D\u00e9 (Culdee) movement. The C\u00e9li D\u00e9 were subsequently to be entrusted with many monasteries throughout the kingdom of Alba until replaced in the 12th century by new orders imported from France.\nSeven years later the \"Chronicle of the Kings of Alba\" says:[Malcolm\u00a0I] plundered the English as far as the river Tees, and he seized a multitude of people and many herds of cattle: and the Scots called this the raid of Albidosorum, that is, Nainndisi. But others say that Constantine made this raid, asking of the king, Malcolm, that the kingship should be given to him for a week's time, so that he could visit the English. In fact, it was Malcolm who made the raid, but Constantine incited him, as I have said. Woolf suggests that the association of Constantine with the raid is a late addition, one derived from a now-lost saga or poem.\nConstantine's death in 952 is recorded by the Irish annals, who enter it among ecclesiastics. His son Indulf would become king on Malcolm's death. The last of Constantine's certain descendants to be king in Alba was a great-grandson, Constantine\u00a0III (Constant\u00edn mac Cuil\u00e9in). Another son had died at Brunanburh, and, according to John of Worcester, Amla\u00edb mac Gofraid was married to a daughter of Constantine. It is possible that Constantine had other children, but like the name of his wife, or wives, this has not been recorded.\nThe form of kingdom which appeared in Constantine's reign continued in much the same way until the Davidian Revolution in the 12th century. As with his ecclesiastical reforms, his political legacy was the creation of a new form of Scottish kingship that lasted for two centuries after his death."}
{"id": "7236", "revid": "16680171", "url": "https://en.wikipedia.org/wiki?curid=7236", "title": "Constantine the Great", "text": "Constantine I (; ; 27 February 22 May 337), also known as Constantine the Great, was a Roman emperor from 306 to 337. Born in Naissus, Dacia Mediterranea (now Ni\u0161, Serbia), he was the son of Flavius Constantius, an Illyrian army officer who became one of the four emperors of the Tetrarchy. His mother, Helena, was Greek and of low birth. Constantine served with distinction under emperors Diocletian and Galerius campaigning in the eastern provinces against barbarians and the Persians, before being recalled west in 305 to fight under his father in Britain. After his father's death in 306, Constantine was acclaimed as emperor by the army at Eboracum (York). He emerged victorious in the civil wars against emperors Maxentius and Licinius to become sole ruler of the Roman Empire by 324.\nAs emperor, Constantine enacted administrative, financial, social and military reforms to strengthen the empire. He restructured the government, separating civil and military authorities. To combat inflation he introduced the solidus, a new gold coin that became the standard for Byzantine and European currencies for more than a thousand years. The Roman army was reorganised to consist of mobile units (comitatenses), and garrison troops (limitanei) capable of countering internal threats and barbarian invasions. Constantine pursued successful campaigns against the tribes on the Roman frontiers\u2014the Franks, the Alamanni, the Goths and the Sarmatians\u2014even resettling territories abandoned by his predecessors during the Crisis of the Third Century.\nConstantine was the first Roman emperor to convert to Christianity. Although he lived much of his life as a pagan, and later as a catechumen, he began to favor Christianity beginning in 312, finally becoming a Christian and being baptised by either Eusebius of Nicomedia, an Arian bishop, or Pope Sylvester I, which is maintained by the Catholic Church and the Coptic Orthodox Church. He played an influential role in the proclamation of the Edict of Milan in 313, which declared tolerance for Christianity in the Roman Empire. He convoked the First Council of Nicaea in 325, which produced the statement of Christian belief known as the Nicene Creed. The Church of the Holy Sepulchre was built on his orders at the purported site of Jesus' tomb in Jerusalem and became the holiest place in Christendom. The papal claim to temporal power in the High Middle Ages was based on the fabricated Donation of Constantine. He has historically been referred to as the \"First Christian Emperor\" and he did favour the Christian Church. While some modern scholars debate his beliefs and even his comprehension of Christianity, he is venerated as a saint in Eastern Christianity.\nThe age of Constantine marked a distinct epoch in the history of the Roman Empire. He built a new imperial residence at Byzantium and renamed the city Constantinople (now Istanbul) after himself (the laudatory epithet of \"New Rome\" emerged in his time, and was never an official title). It subsequently became the capital of the Empire for more than a thousand years, the later Eastern Roman Empire being referred to as the \"Byzantine Empire\" by modern historians. His more immediate political legacy was that he replaced Diocletian's Tetrarchy with the \"de facto\" principle of dynastic succession, by leaving the empire to his sons and other members of the Constantinian dynasty. His reputation flourished during the lifetime of his children and for centuries after his reign. The medieval church held him up as a paragon of virtue, while secular rulers invoked him as a prototype, a point of reference and the symbol of imperial legitimacy and identity. Beginning with the Renaissance, there were more critical appraisals of his reign, due to the rediscovery of anti-Constantinian sources. Trends in modern and recent scholarship have attempted to balance the extremes of previous scholarship.\nSources.\nConstantine was a ruler of major importance, and he has always been a controversial figure. The fluctuations in his reputation reflect the nature of the ancient sources for his reign. These are abundant and detailed, but they have been strongly influenced by the official propaganda of the period and are often one-sided; no contemporaneous histories or biographies dealing with his life and rule have survived. The nearest replacement is Eusebius's \"Vita Constantini\"\u2014a mixture of eulogy and hagiography written between AD 335 and circa AD 339\u2014that extols Constantine's moral and religious virtues. The \"Vita\" creates a contentiously positive image of Constantine, and modern historians have frequently challenged its reliability. The fullest secular life of Constantine is the anonymous \"Origo Constantini\", a work of uncertain date, which focuses on military and political events to the neglect of cultural and religious matters.\nLactantius' \"De Mortibus Persecutorum\", a political Christian pamphlet on the reigns of Diocletian and the Tetrarchy, provides valuable but tendentious detail on Constantine's predecessors and early life. The ecclesiastical histories of Socrates, Sozomen, and Theodoret describe the ecclesiastic disputes of Constantine's later reign. Written during the reign of Theodosius II (AD 408\u2013450), a century after Constantine's reign, these ecclesiastical historians obscure the events and theologies of the Constantinian period through misdirection, misrepresentation, and deliberate obscurity. The contemporary writings of the orthodox Christian Athanasius, and the ecclesiastical history of the Arian Philostorgius also survive, though their biases are no less firm.\nThe epitomes of Aurelius Victor (\"De Caesaribus\"), Eutropius (\"Breviarium\"), Festus (\"Breviarium\"), and the anonymous author of the \"Epitome de Caesaribus\" offer compressed secular political and military histories of the period. Although not Christian, the epitomes paint a favourable image of Constantine but omit reference to Constantine's religious policies. The \"Panegyrici Latini\", a collection of panegyrics from the late third and early fourth centuries, provide valuable information on the politics and ideology of the tetrarchic period and the early life of Constantine. Contemporary architecture, such as the Arch of Constantine in Rome and palaces in Gamzigrad and C\u00f3rdoba, epigraphic remains, and the coinage of the era complement the literary sources.\nEarly life.\nFlavius Valerius Constantinus, as he was originally named, was born in the city of Naissus (today Ni\u0161, Serbia), part of the Dardania province of Moesia on 27 February, probably AD 272. His father was Flavius Constantius, an Illyrian, and a native of Dardania province of Moesia (later Dacia Ripensis). Constantine probably spent little time with his father who was an officer in the Roman army, part of the Emperor Aurelian's imperial bodyguard. Being described as a tolerant and politically skilled man, Constantius advanced through the ranks, earning the governorship of Dalmatia from Emperor Diocletian, another of Aurelian's companions from Illyricum, in 284 or 285. Constantine's mother was Helena, a Greek woman of low social standing from Helenopolis of Bithynia. It is uncertain whether she was legally married to Constantius or merely his concubine. His main language was Latin, and during his public speeches he needed Greek translators.\nIn July AD 285, Diocletian declared Maximian, another colleague from Illyricum, his co-emperor. Each emperor would have his own court, his own military and administrative faculties, and each would rule with a separate praetorian prefect as chief lieutenant. Maximian ruled in the West, from his capitals at Mediolanum (Milan, Italy) or Augusta Treverorum (Trier, Germany), while Diocletian ruled in the East, from Nicomedia (\u0130zmit, Turkey). The division was merely pragmatic: the empire was called \"indivisible\" in official panegyric, and both emperors could move freely throughout the empire. In 288, Maximian appointed Constantius to serve as his praetorian prefect in Gaul. Constantius left Helena to marry Maximian's stepdaughter Theodora in 288 or 289.\nDiocletian divided the Empire again in AD 293, appointing two caesars (junior emperors) to rule over further subdivisions of East and West. Each would be subordinate to their respective augustus (senior emperor) but would act with supreme authority in his assigned lands. This system would later be called the Tetrarchy. Diocletian's first appointee for the office of Caesar was Constantius; his second was Galerius, a native of Felix Romuliana. According to Lactantius, Galerius was a brutal, animalistic man. Although he shared the paganism of Rome's aristocracy, he seemed to them an alien figure, a semi-barbarian. On 1 March, Constantius was promoted to the office of caesar, and dispatched to Gaul to fight the rebels Carausius and Allectus. In spite of meritocratic overtones, the Tetrarchy retained vestiges of hereditary privilege, and Constantine became the prime candidate for future appointment as caesar as soon as his father took the position. Constantine went to the court of Diocletian, where he lived as his father's heir presumptive.\nIn the East.\nConstantine received a formal education at Diocletian's court, where he learned Latin literature, Greek, and philosophy. The cultural environment in Nicomedia was open, fluid, and socially mobile; in it, Constantine could mix with intellectuals both pagan and Christian. He may have attended the lectures of Lactantius, a Christian scholar of Latin in the city. Because Diocletian did not completely trust Constantius\u2014none of the Tetrarchs fully trusted their colleagues\u2014Constantine was held as something of a hostage, a tool to ensure Constantius' best behavior. Constantine was nonetheless a prominent member of the court: he fought for Diocletian and Galerius in Asia and served in a variety of tribunates; he campaigned against barbarians on the Danube in AD 296 and fought the Persians under Diocletian in Syria (AD 297), as well as under Galerius in Mesopotamia (AD 298\u2013299). By late AD 305, he had become a tribune of the first order, a \"tribunus ordinis primi\".\nConstantine had returned to Nicomedia from the eastern front by the spring of AD 303, in time to witness the beginnings of Diocletian's \"Great Persecution\", the most severe persecution of Christians in Roman history. In late 302, Diocletian and Galerius sent a messenger to the oracle of Apollo at Didyma with an inquiry about Christians. Constantine could recall his presence at the palace when the messenger returned, when Diocletian accepted his court's demands for universal persecution. On 23 February AD 303, Diocletian ordered the destruction of Nicomedia's new church, condemned its scriptures to the flames, and had its treasures seized. In the months that followed, churches and scriptures were destroyed, Christians were deprived of official ranks, and priests were imprisoned.\nIt is unlikely that Constantine played any role in the persecution. In his later writings, he would attempt to present himself as an opponent of Diocletian's \"sanguinary edicts\" against the \"Worshippers of God\", but nothing indicates that he opposed it effectively at the time. Although no contemporary Christian challenged Constantine for his inaction during the persecutions, it remained a political liability throughout his life.\nOn 1 May AD 305, Diocletian, as a result of a debilitating sickness taken in the winter of AD 304\u2013305, announced his resignation. In a parallel ceremony in Milan, Maximian did the same. Lactantius states that Galerius manipulated the weakened Diocletian into resigning, and forced him to accept Galerius' allies in the imperial succession. According to Lactantius, the crowd listening to Diocletian's resignation speech believed, until the last moment, that Diocletian would choose Constantine and Maxentius (Maximian's son) as his successors. It was not to be: Constantius and Galerius were promoted to \"augusti\", while Severus and Maximinus Daia, Galerius' nephew, were appointed their caesars respectively. Constantine and Maxentius were ignored.\nSome of the ancient sources detail plots that Galerius made on Constantine's life in the months following Diocletian's abdication. They assert that Galerius assigned Constantine to lead an advance unit in a cavalry charge through a swamp on the middle Danube, made him enter into single combat with a lion, and attempted to kill him in hunts and wars. Constantine always emerged victorious: the lion emerged from the contest in a poorer condition than Constantine; Constantine returned to Nicomedia from the Danube with a Sarmatian captive to drop at Galerius' feet. It is uncertain how much these tales can be trusted.\nIn the West.\nConstantine recognized the implicit danger in remaining at Galerius' court, where he was held as a virtual hostage. His career depended on being rescued by his father in the west. Constantius was quick to intervene. In the late spring or early summer of AD 305, Constantius requested leave for his son to help him campaign in Britain. After a long evening of drinking, Galerius granted the request. Constantine's later propaganda describes how he fled the court in the night, before Galerius could change his mind. He rode from post-house to post-house at high speed, hamstringing every horse in his wake. By the time Galerius awoke the following morning, Constantine had fled too far to be caught. Constantine joined his father in Gaul, at Bononia (Boulogne) before the summer of AD 305.\nFrom Bononia, they crossed the Channel to Britain and made their way to Eboracum (York), capital of the province of Britannia Secunda and home to a large military base. Constantine was able to spend a year in northern Britain at his father's side, campaigning against the Picts beyond Hadrian's Wall in the summer and autumn. Constantius' campaign, like that of Septimius Severus before it, probably advanced far into the north without achieving great success. Constantius had become severely sick over the course of his reign, and died on 25 July 306 in Eboracum. Before dying, he declared his support for raising Constantine to the rank of full augustus. The Alamannic king Chrocus, a barbarian taken into service under Constantius, then proclaimed Constantine as augustus. The troops loyal to Constantius' memory followed him in acclamation. Gaul and Britain quickly accepted his rule; Hispania, which had been in his father's domain for less than a year, rejected it.\nConstantine sent Galerius an official notice of Constantius' death and his own acclamation. Along with the notice, he included a portrait of himself in the robes of an augustus. The portrait was wreathed in bay. He requested recognition as heir to his father's throne, and passed off responsibility for his unlawful ascension on his army, claiming they had \"forced it upon him\". Galerius was put into a fury by the message; he almost set the portrait and messenger on fire. His advisers calmed him, and argued that outright denial of Constantine's claims would mean certain war. Galerius was compelled to compromise: he granted Constantine the title \"caesar\" rather than \"augustus\" (the latter office went to Severus instead). Wishing to make it clear that he alone gave Constantine legitimacy, Galerius personally sent Constantine the emperor's traditional purple robes. Constantine accepted the decision, knowing that it would remove doubts as to his legitimacy.\nEarly rule.\nConstantine's share of the Empire consisted of Britain, Gaul, and Spain, and he commanded one of the largest Roman armies which was stationed along the important Rhine frontier. He remained in Britain after his promotion to emperor, driving back the tribes of the Picts and securing his control in the northwestern dioceses. He completed the reconstruction of military bases begun under his father's rule, and he ordered the repair of the region's roadways. He then left for Augusta Treverorum (Trier) in Gaul, the Tetrarchic capital of the northwestern Roman Empire. The Franks learned of Constantine's acclamation and invaded Gaul across the lower Rhine over the winter of 306\u2013307\u00a0AD. He drove them back beyond the Rhine and captured Kings Ascaric and Merogais; the kings and their soldiers were fed to the beasts of Trier's amphitheatre in the \"adventus\" (arrival) celebrations which followed.\nConstantine began a major expansion of Trier. He strengthened the circuit wall around the city with military towers and fortified gates, and he began building a palace complex in the northeastern part of the city. To the south of his palace, he ordered the construction of a large formal audience hall and a massive imperial bathhouse. He sponsored many building projects throughout Gaul during his tenure as emperor of the West, especially in Augustodunum (Autun) and Arelate (Arles). According to Lactantius, Constantine followed a tolerant policy towards Christianity, although he was not yet a Christian himself. He probably judged it a more sensible policy than open persecution and a way to distinguish himself from the \"great persecutor\" Galerius. He decreed a formal end to persecution and returned to Christians all that they had lost during them.\nConstantine was largely untried and had a hint of illegitimacy about him; he relied on his father's reputation in his early propaganda, which gave as much coverage to his father's deeds as to his. His military skill and building projects, however, soon gave the panegyrist the opportunity to comment favourably on the similarities between father and son, and Eusebius remarked that Constantine was a \"renewal, as it were, in his own person, of his father's life and reign\". Constantinian coinage, sculpture, and oratory also show a new tendency for disdain towards the \"barbarians\" beyond the frontiers. He minted a coin issue after his victory over the Alemanni which depicts weeping and begging Alemannic tribesmen, \"the Alemanni conquered\" beneath the phrase \"Romans' rejoicing\". There was little sympathy for these enemies; as his panegyrist declared, \"It is a stupid clemency that spares the conquered foe.\"\nMaxentius' rebellion.\nFollowing Galerius' recognition of Constantine as caesar, Constantine's portrait was brought to Rome, as was customary. Maxentius mocked the portrait's subject as the son of a harlot and lamented his own powerlessness. Maxentius, envious of Constantine's authority, seized the title of emperor on 28 October 306\u00a0AD. Galerius refused to recognize him but failed to unseat him. Galerius sent Severus against Maxentius, but during the campaign, Severus' armies, previously under command of Maxentius' father Maximian, defected, and Severus was seized and imprisoned. Maximian, brought out of retirement by his son's rebellion, left for Gaul to confer with Constantine in late 307\u00a0AD. He offered to marry his daughter Fausta to Constantine and elevate him to augustan rank. In return, Constantine would reaffirm the old family alliance between Maximian and Constantius and offer support to Maxentius' cause in Italy. Constantine accepted and married Fausta in Trier in late summer 307\u00a0AD. Constantine now gave Maxentius his meagre support, offering Maxentius political recognition.\nConstantine remained aloof from the Italian conflict, however. Over the spring and summer of 307 AD, he had left Gaul for Britain to avoid any involvement in the Italian turmoil; now, instead of giving Maxentius military aid, he sent his troops against Germanic tribes along the Rhine. In 308 AD, he raided the territory of the Bructeri, and made a bridge across the Rhine at Colonia Agrippinensium (Cologne). In 310 AD, he marched to the northern Rhine and fought the Franks. When not campaigning, he toured his lands advertising his benevolence and supporting the economy and the arts. His refusal to participate in the war increased his popularity among his people and strengthened his power base in the West. Maximian returned to Rome in the winter of 307\u2013308 AD, but soon fell out with his son. In early 308\u00a0AD, after a failed attempt to usurp Maxentius' title, Maximian returned to Constantine's court.\nOn 11 November 308 AD, Galerius called a general council at the military city of Carnuntum (Petronell-Carnuntum, Austria) to resolve the instability in the western provinces. In attendance were Diocletian, briefly returned from retirement, Galerius, and Maximian. Maximian was forced to abdicate again and Constantine was again demoted to caesar. Licinius, one of Galerius' old military companions, was appointed augustus in the western regions. The new system did not last long: Constantine refused to accept the demotion, and continued to style himself as augustus on his coinage, even as other members of the Tetrarchy referred to him as a caesar on theirs. Maximinus Daia was frustrated that he had been passed over for promotion while the newcomer Licinius had been raised to the office of augustus and demanded that Galerius promote him. Galerius offered to call both Maximinus and Constantine \"sons of the augusti\", but neither accepted the new title. By the spring of 310\u00a0AD, Galerius was referring to both men as augusti.\nMaximian's rebellion.\nIn 310 AD, a dispossessed Maximian rebelled against Constantine while Constantine was away campaigning against the Franks. Maximian had been sent south to Arles with a contingent of Constantine's army, in preparation for any attacks by Maxentius in southern Gaul. He announced that Constantine was dead, and took up the imperial purple. In spite of a large donative pledge to any who would support him as emperor, most of Constantine's army remained loyal to their emperor, and Maximian was soon compelled to leave. Constantine soon heard of the rebellion, abandoned his campaign against the Franks, and marched his army up the Rhine. At Cabillunum (Chalon-sur-Sa\u00f4ne), he moved his troops onto waiting boats to row down the slow waters of the Sa\u00f4ne to the quicker waters of the Rhone. He disembarked at Lugdunum (Lyon). Maximian fled to Massilia (Marseille), a town better able to withstand a long siege than Arles. It made little difference, however, as loyal citizens opened the rear gates to Constantine. Maximian was captured and reproved for his crimes. Constantine granted some clemency, but strongly encouraged his suicide. In July 310 AD, Maximian hanged himself.\nIn spite of the earlier rupture in their relations, Maxentius was eager to present himself as his father's devoted son after his death. He began minting coins with his father's deified image, proclaiming his desire to avenge Maximian's death. Constantine initially presented the suicide as an unfortunate family tragedy. By 311\u00a0AD, however, he was spreading another version. According to this, after Constantine had pardoned him, Maximian planned to murder Constantine in his sleep. Fausta learned of the plot and warned Constantine, who put a eunuch in his own place in bed. Maximian was apprehended when he killed the eunuch and was offered suicide, which he accepted. Along with using propaganda, Constantine instituted a \"damnatio memoriae\" on Maximian, destroying all inscriptions referring to him and eliminating any public work bearing his image.\nThe death of Maximian required a shift in Constantine's public image. He could no longer rely on his connection to the elder Emperor Maximian, and needed a new source of legitimacy. In a speech delivered in Gaul on 25 July 310 AD, the anonymous orator reveals a previously unknown dynastic connection to Claudius II, a 3rd-century emperor famed for defeating the Goths and restoring order to the empire. Breaking away from tetrarchic models, the speech emphasizes Constantine's ancestral prerogative to rule, rather than principles of imperial equality. The new ideology expressed in the speech made Galerius and Maximian irrelevant to Constantine's right to rule. Indeed, the orator emphasizes ancestry to the exclusion of all other factors: \"No chance agreement of men, nor some unexpected consequence of favor, made you emperor,\" the orator declares to Constantine.\nThe oration also moves away from the religious ideology of the Tetrarchy, with its focus on twin dynasties of Jupiter and Hercules. Instead, the orator proclaims that Constantine experienced a divine vision of Apollo and Victory granting him laurel wreaths of health and a long reign. In the likeness of Apollo, Constantine recognized himself as the saving figure to whom would be granted \"rule of the whole world\", as the poet Virgil had once foretold. The oration's religious shift is paralleled by a similar shift in Constantine's coinage. In his early reign, the coinage of Constantine advertised Mars as his patron. From 310\u00a0AD on, Mars was replaced by Sol Invictus, a god conventionally identified with Apollo. There is little reason to believe that either the dynastic connection or the divine vision are anything other than fiction, but their proclamation strengthened Constantine's claims to legitimacy and increased his popularity among the citizens of Gaul.\nCivil wars.\nWar against Maxentius.\nBy the middle of 310 AD, Galerius had become too ill to involve himself in imperial politics. His final act survives: a letter to provincials posted in Nicomedia on 30 April 311 AD, proclaiming an end to the persecutions, and the resumption of religious toleration. He died soon after the edict's proclamation, destroying what little remained of the tetrarchy. Maximinus mobilized against Licinius, and seized Asia Minor. A hasty peace was signed on a boat in the middle of the Bosphorus. While Constantine toured Britain and Gaul, Maxentius prepared for war. He fortified northern Italy, and strengthened his support in the Christian community by allowing it to elect a new Bishop of Rome, Eusebius.\nMaxentius' rule was nevertheless insecure. His early support dissolved in the wake of heightened tax rates and depressed trade; riots broke out in Rome and Carthage; and Domitius Alexander was able to briefly usurp his authority in Africa. By 312\u00a0AD, he was a man barely tolerated, not one actively supported, even among Christian Italians. In the summer of 311\u00a0AD, Maxentius mobilized against Constantine while Licinius was occupied with affairs in the East. He declared war on Constantine, vowing to avenge his father's \"murder\". To prevent Maxentius from forming an alliance against him with Licinius, Constantine forged his own alliance with Licinius over the winter of 311\u2013312\u00a0AD, and offered him his sister Constantia in marriage. Maximinus considered Constantine's arrangement with Licinius an affront to his authority. In response, he sent ambassadors to Rome, offering political recognition to Maxentius in exchange for a military support. Maxentius accepted. According to Eusebius, inter-regional travel became impossible, and there was military buildup everywhere. There was \"not a place where people were not expecting the onset of hostilities every day\".\nConstantine's advisers and generals cautioned against preemptive attack on Maxentius; even his soothsayers recommended against it, stating that the sacrifices had produced unfavourable omens. Constantine, with a spirit that left a deep impression on his followers, inspiring some to believe that he had some form of supernatural guidance, ignored all these cautions. Early in the spring of 312\u00a0AD, Constantine crossed the Cottian Alps with a quarter of his army, a force numbering about 40,000. The first town his army encountered was Segusium (Susa, Italy), a heavily fortified town that shut its gates to him. Constantine ordered his men to set fire to its gates and scale its walls. He took the town quickly. Constantine ordered his troops not to loot the town, and advanced with them into northern Italy.\nAt the approach to the west of the important city of Augusta Taurinorum (Turin, Italy), Constantine met a large force of heavily armed Maxentian cavalry. In the ensuing battle Constantine's army encircled Maxentius' cavalry, flanked them with his own cavalry, and dismounted them with blows from his soldiers' iron-tipped clubs. Constantine's armies emerged victorious. Turin refused to give refuge to Maxentius' retreating forces, opening its gates to Constantine instead. Other cities of the north Italian plain sent Constantine embassies of congratulation for his victory. He moved on to Milan, where he was met with open gates and jubilant rejoicing. Constantine rested his army in Milan until mid-summer 312\u00a0AD, when he moved on to Brixia (Brescia).\nBrescia's army was easily dispersed, and Constantine quickly advanced to Verona, where a large Maxentian force was camped. Ruricius Pompeianus, general of the Veronese forces and Maxentius' praetorian prefect, was in a strong defensive position, since the town was surrounded on three sides by the Adige. Constantine sent a small force north of the town in an attempt to cross the river unnoticed. Ruricius sent a large detachment to counter Constantine's expeditionary force, but was defeated. Constantine's forces successfully surrounded the town and laid siege. Ruricius gave Constantine the slip and returned with a larger force to oppose Constantine. Constantine refused to let up on the siege, and sent only a small force to oppose him. In the desperately fought encounter that followed, Ruricius was killed and his army destroyed. Verona surrendered soon afterwards, followed by Aquileia, Mutina (Modena), and Ravenna. The road to Rome was now wide open to Constantine.\nMaxentius prepared for the same type of war he had waged against Severus and Galerius: he sat in Rome and prepared for a siege. He still controlled Rome's praetorian guards, was well-stocked with African grain, and was surrounded on all sides by the seemingly impregnable Aurelian Walls. He ordered all bridges across the Tiber cut, reportedly on the counsel of the gods, and left the rest of central Italy undefended; Constantine secured that region's support without challenge. Constantine progressed slowly along the \"Via Flaminia\", allowing the weakness of Maxentius to draw his regime further into turmoil. Maxentius' support continued to weaken: at chariot races on 27 October, the crowd openly taunted Maxentius, shouting that Constantine was invincible. Maxentius, no longer certain that he would emerge from a siege victorious, built a temporary boat bridge across the Tiber in preparation for a field battle against Constantine. On 28 October 312 AD, the sixth anniversary of his reign, he approached the keepers of the Sibylline Books for guidance. The keepers prophesied that, on that very day, \"the enemy of the Romans\" would die. Maxentius advanced north to meet Constantine in battle.\nConstantine adopts the Greek letters Chi Rho for Christ's initials.\nMaxentius' forces were still twice the size of Constantine's, and he organized them in long lines facing the battle plain with their backs to the river. Constantine's army arrived on the field bearing unfamiliar symbols on their standards and their shields. According to Lactantius \"Constantine was directed in a dream to cause the heavenly sign to be delineated on the shields of his soldiers, and so to proceed to battle. He did as he had been commanded, and he marked on their shields the letter \u03a7, with a perpendicular line drawn through it and turned round thus at the top, being the cipher of Christ. Having this sign (\u2627), his troops stood to arms.\" Eusebius describes a vision that Constantine had while marching at midday in which \"he saw with his own eyes the trophy of a cross of light in the heavens, above the sun, and bearing the inscription, \"In Hoc Signo Vinces\"\" (\"In this sign thou shalt conquer\"). In Eusebius's account, Constantine had a dream the following night in which Christ appeared with the same heavenly sign and told him to make an army standard in the form of the \"labarum\". Eusebius is vague about when and where these events took place, but it enters his narrative before the war begins against Maxentius. He describes the sign as Chi (\u03a7) traversed by Rho (\u03a1) to form \u2627, representing the first two letters of the Greek word (Christos). A medallion was issued at Ticinum in 315 AD which shows Constantine wearing a helmet emblazoned with the \"Chi Rho\", and coins issued at Siscia in 317/318\u00a0AD repeat the image. The figure was otherwise rare and is uncommon in imperial iconography and propaganda before the 320s. It wasn't completely unknown, however, being an abbreviation of the Greek word chr\u0113ston (good), having previously appeared on the coins of Ptolemy III, Euergetes I (247-222 BCE).\nConstantine deployed his own forces along the whole length of Maxentius' line. He ordered his cavalry to charge, and they broke Maxentius' cavalry. He then sent his infantry against Maxentius' infantry, pushing many into the Tiber where they were slaughtered and drowned. The battle was brief, and Maxentius' troops were broken before the first charge. His horse guards and praetorians initially held their position, but they broke under the force of a Constantinian cavalry charge; they also broke ranks and fled to the river. Maxentius rode with them and attempted to cross the bridge of boats (Ponte Milvio), but he was pushed into the Tiber and drowned by the mass of his fleeing soldiers.\nIn Rome.\nConstantine entered Rome on 29 October 312 AD, and staged a grand \"adventus\" in the city which was met with jubilation. Maxentius' body was fished out of the Tiber and decapitated, and his head was paraded through the streets for all to see. After the ceremonies, the disembodied head was sent to Carthage, and Carthage offered no further resistance. Unlike his predecessors, Constantine neglected to make the trip to the Capitoline Hill and perform customary sacrifices at the Temple of Jupiter. However, he did visit the Senatorial Curia Julia, and he promised to restore its ancestral privileges and give it a secure role in his reformed government; there would be no revenge against Maxentius' supporters. In response, the Senate decreed him \"title of the first name\", which meant that his name would be listed first in all official documents, and they acclaimed him as \"the greatest Augustus\". He issued decrees returning property that was lost under Maxentius, recalling political exiles, and releasing Maxentius' imprisoned opponents.\nAn extensive propaganda campaign followed, during which Maxentius' image was purged from all public places. He was written up as a \"tyrant\" and set against an idealized image of Constantine the \"liberator\". Eusebius is the best representative of this strand of Constantinian propaganda. Maxentius' rescripts were declared invalid, and the honours that he had granted to leaders of the Senate were also invalidated. Constantine also attempted to remove Maxentius' influence on Rome's urban landscape. All structures built by him were rededicated to Constantine, including the Temple of Romulus and the Basilica of Maxentius. At the focal point of the basilica, a stone statue was erected of Constantine holding the Christian \"labarum\" in its hand. Its inscription bore the message which the statue illustrated: By this sign, Constantine had freed Rome from the yoke of the tyrant.\nConstantine also sought to upstage Maxentius' achievements. For example, the Circus Maximus was redeveloped so that its seating capacity was 25 times larger than that of Maxentius' racing complex on the Via Appia. Maxentius' strongest military supporters were neutralized when he disbanded the Praetorian Guard and Imperial Horse Guard. The tombstones of the Imperial Horse Guard were ground up and used in a basilica on the Via Labicana, and their former base was redeveloped into the Lateran Basilica on 9 November 312\u00a0AD\u2014barely two weeks after Constantine captured the city. The Legio II Parthica was removed from Albano Laziale, and the remainder of Maxentius' armies were sent to do frontier duty on the Rhine.\nWars against Licinius.\nIn the following years, Constantine gradually consolidated his military superiority over his rivals in the crumbling Tetrarchy. In 313, he met Licinius in Milan to secure their alliance by the marriage of Licinius and Constantine's half-sister Constantia. During this meeting, the emperors agreed on the so-called Edict of Milan,\nofficially granting full tolerance to Christianity and all religions in the Empire.\nThe document had special benefits for Christians, legalizing their religion and granting them restoration for all property seized during Diocletian's persecution. It repudiates past methods of religious coercion and used only general terms to refer to the divine sphere\u2014\"Divinity\" and \"Supreme Divinity\", \"summa divinitas\".\nThe conference was cut short, however, when news reached Licinius that his rival Maximinus had crossed the Bosporus and invaded European territory. Licinius departed and eventually defeated Maximinus, gaining control over the entire eastern half of the Roman Empire. Relations between the two remaining emperors deteriorated, as Constantine suffered an assassination attempt at the hands of a character that Licinius wanted elevated to the rank of Caesar; Licinius, for his part, had Constantine's statues in Emona destroyed. In either 314 or 316 AD, the two Augusti fought against one another at the Battle of Cibalae, with Constantine being victorious. They clashed again at the Battle of Mardia in 317, and agreed to a settlement in which Constantine's sons Crispus and Constantine II, and Licinius' son Licinianus were made \"caesars\". After this arrangement, Constantine ruled the dioceses of Pannonia and Macedonia and took residence at Sirmium, whence he could wage war on the Goths and Sarmatians in 322, and on the Goths in 323, defeating and killing their leader Rausimod.\nIn the year 320, Licinius allegedly reneged on the religious freedom promised by the Edict of Milan in 313 and began to oppress Christians anew,\ngenerally without bloodshed, but resorting to confiscations and sacking of Christian office-holders. Although this characterization of Licinius as anti-Christian is somewhat doubtful, the fact is that he seems to have been far less open in his support of Christianity than Constantine. Therefore, Licinius was prone to see the Church as a force more loyal to Constantine than to the Imperial system in general, as the explanation offered by the Church historian Sozomen.\nThis dubious arrangement eventually became a challenge to Constantine in the West, climaxing in the great civil war of 324. Licinius, aided by Gothic mercenaries, represented the past and the ancient pagan faiths. Constantine and his Franks marched under the standard of the \"labarum\", and both sides saw the battle in religious terms. Outnumbered, but fired by their zeal, Constantine's army emerged victorious in the Battle of Adrianople. Licinius fled across the Bosphorus and appointed Martinian, his \"magister officiorum\", as nominal Augustus in the West, but Constantine next won the Battle of the Hellespont, and finally the Battle of Chrysopolis on 18 September 324. Licinius and Martinian surrendered to Constantine at Nicomedia on the promise their lives would be spared: they were sent to live as private citizens in Thessalonica and Cappadocia respectively, but in 325 Constantine accused Licinius of plotting against him and had them both arrested and hanged; Licinius' son (the son of Constantine's half-sister) was killed in 326. Thus Constantine became the sole emperor of the Roman Empire.\nLater rule.\nFoundation of Constantinople.\nDiocletian had chosen Nicomedia in the East as his capital during the Tetrarchy - not far from Byzantium, well situated to defend Thrace, Asia, and Egypt, all of which had required his military attention. Constantine had recognized the shift of the center of gravity of the Empire from the remote and depopulated West to the richer cities of the East, and the military strategic importance of protecting the Danube from barbarian excursions and Asia from a hostile Persia in choosing his new capital as well as being able to monitor shipping traffic between the Black Sea and the Mediterranean. \nLicinius' defeat came to represent the defeat of a rival centre of pagan and Greek-speaking political activity in the East, as opposed to the Christian and Latin-speaking Rome, and it was proposed that a new Eastern capital should represent the integration of the East into the Roman Empire as a whole, as a center of learning, prosperity, and cultural preservation for the whole of the Eastern Roman Empire. Among the various locations proposed for this alternative capital, Constantine appears to have toyed earlier with Serdica (present-day Sofia), as he was reported saying that \"Serdica is my Rome\". Sirmium and Thessalonica were also considered. Eventually, however, Constantine decided to work on the Greek city of Byzantium, which offered the advantage of having already been extensively rebuilt on Roman patterns of urbanism, during the preceding century, by Septimius Severus and Caracalla, who had already acknowledged its strategic importance. The city was thus founded in 324, dedicated on 11 May 330 and renamed \"Constantinopolis\" (\"Constantine's City\" or Constantinople in English). Special commemorative coins were issued in 330 to honor the event. The new city was protected by the relics of the True Cross, the Rod of Moses and other holy relics, though a cameo now at the Hermitage Museum also represented Constantine crowned by the tyche of the new city. The figures of old gods were either replaced or assimilated into a framework of Christian symbolism. Constantine built the new Church of the Holy Apostles on the site of a temple to Aphrodite. Generations later there was the story that a divine vision led Constantine to this spot, and an angel no one else could see led him on a circuit of the new walls. The capital would often be compared to the 'old' Rome as \"Nova Roma Constantinopolitana\", the \"New Rome of Constantinople\".\nReligious policy.\nConstantine was the first emperor to stop the persecution of Christians and to legalize Christianity, along with all other religions/cults in the Roman Empire. In February 313, he met with Licinius in Milan and developed the Edict of Milan, which stated that Christians should be allowed to follow their faith without oppression. This removed penalties for professing Christianity, under which many had been martyred previously, and it returned confiscated Church property. The edict protected all religions from persecution, not only Christianity, allowing anyone to worship any deity that they chose. A similar edict had been issued in 311 by Galerius, senior emperor of the Tetrarchy, which granted Christians the right to practise their religion but did not restore any property to them. The Edict of Milan included several clauses which stated that all confiscated churches would be returned, as well as other provisions for previously persecuted Christians. Scholars debate whether Constantine adopted his mother Helena's Christianity in his youth, or whether he adopted it gradually over the course of his life.\nConstantine possibly retained the title of \"pontifex maximus\" which emperors bore as heads of the ancient Roman religion until Gratian renounced the title. According to Christian writers, Constantine was over 40 when he finally declared himself a Christian, making it clear that he owed his successes to the protection of the Christian High God alone. Despite these declarations of being a Christian, he waited to be baptized on his deathbed, believing that the baptism would release him of any sins he committed in the course of carrying out his policies while emperor. He supported the Church financially, built basilicas, granted privileges to clergy (such as exemption from certain taxes), promoted Christians to high office, and returned property confiscated during the long period of persecution. His most famous building projects include the Church of the Holy Sepulchre and Old Saint Peter's Basilica. In constructing the Old Saint Peter's Basilica, Constantine went to great lengths to erect the basilica on top of St. Peter's resting place, so much so that it even affected the design of the basilica, including the challenge of erecting it on the hill where St. Peter rested, making its complete construction time over 30 years from the date Constantine ordered it to be built.\nConstantine might not have patronized Christianity alone. He built a triumphal arch in 315 to celebrate his victory in the Battle of the Milvian Bridge (312) which was decorated with images of the goddess Victoria, and sacrifices were made to pagan gods at its dedication, including Apollo, Diana, and Hercules. Absent from the Arch are any depictions of Christian symbolism. However, the Arch was commissioned by the Senate, so the absence of Christian symbols may reflect the role of the Curia at the time as a pagan redoubt.\nIn 321, he legislated that the \"venerable Sunday\" should be a day of rest for all citizens. In 323, he issued a decree banning Christians from participating in state sacrifices. After the pagan gods had disappeared from his coinage, Christian symbols appeared as Constantine's attributes, the chi rho between his hands or on his labarum, as well on the coin itself.\nThe reign of Constantine established a precedent for the emperor to have great influence and authority in the early Christian councils, most notably the dispute over Arianism. Constantine disliked the risks to societal stability that religious disputes and controversies brought with them, preferring to establish an orthodoxy. His influence over the Church councils was to enforce doctrine, root out heresy, and uphold ecclesiastical unity; the Church's role was to determine proper worship, doctrines, and dogma.\nNorth African bishops struggled with Christian bishops who had been ordained by Donatus in opposition to Caecilian from 313 to 316. The African bishops could not come to terms, and the Donatists asked Constantine to act as a judge in the dispute. Three regional Church councils and another trial before Constantine all ruled against Donatus and the Donatism movement in North Africa. In 317, Constantine issued an edict to confiscate Donatist church property and to send Donatist clergy into exile. More significantly, in 325 he summoned the First Council of Nicaea, most known for its dealing with Arianism and for instituting the Nicene Creed. He enforced the council's prohibition against celebrating the Lord's Supper on the day before the Jewish Passover, which marked a definite break of Christianity from the Judaic tradition. From then on, the solar Julian Calendar was given precedence over the lunisolar Hebrew calendar among the Christian churches of the Roman Empire.\nConstantine made some new laws regarding the Jews; some of them were unfavorable towards Jews, although they were not harsher than those of his predecessors. It was made illegal for Jews to seek converts or to attack other Jews who had converted to Christianity. They were forbidden to own Christian slaves or to circumcise their slaves. On the other hand, Jewish clergy were given the same exemptions as Christian clergy.\nAdministrative reforms.\nBeginning in the mid-3rd century, the emperors began to favor members of the equestrian order over senators, who had a monopoly on the most important offices of the state. Senators were stripped of the command of legions and most provincial governorships, as it was felt that they lacked the specialized military upbringing needed in an age of acute defense needs; such posts were given to equestrians by Diocletian and his colleagues, following a practice enforced piecemeal by their predecessors. The emperors, however, still needed the talents and the help of the very rich, who were relied on to maintain social order and cohesion by means of a web of powerful influence and contacts at all levels. Exclusion of the old senatorial aristocracy threatened this arrangement.\nIn 326, Constantine reversed this pro-equestrian trend, raising many administrative positions to senatorial rank and thus opening these offices to the old aristocracy; at the same time, he elevated the rank of existing equestrian office-holders to senator, degrading the equestrian order in the process (at least as a bureaucratic rank). The title of \"perfectissimus\" was granted only to mid- or low-level officials by the end of the 4th century.\nBy the new Constantinian arrangement, one could become a senator by being elected praetor or by fulfilling a function of senatorial rank. From then on, holding actual power and social status were melded together into a joint imperial hierarchy. Constantine gained the support of the old nobility with this, as the Senate was allowed itself to elect praetors and quaestors, in place of the usual practice of the emperors directly creating new magistrates (\"adlectio\"). An inscription in honor of city prefect (336\u2013337) Ceionius Rufus Albinus states that Constantine had restored the Senate \"the \"auctoritas\" it had lost at Caesar's time\".\nThe Senate as a body remained devoid of any significant power; nevertheless, the senators had been marginalized as potential holders of imperial functions during the 3rd century but could now dispute such positions alongside more upstart bureaucrats. Some modern historians see in those administrative reforms an attempt by Constantine at reintegrating the senatorial order into the imperial administrative elite to counter the possibility of alienating pagan senators from a Christianized imperial rule; however, such an interpretation remains conjectural, given the fact that we do not have the precise numbers about pre-Constantine conversions to Christianity in the old senatorial milieu. Some historians suggest that early conversions among the old aristocracy were more numerous than previously supposed.\nConstantine's reforms had to do only with the civilian administration. The military chiefs had risen from the ranks since the Crisis of the Third Century but remained outside the senate, in which they were included only by Constantine's children.\nMonetary reforms.\nThe third century saw runaway inflation associated with the production of fiat money to pay for public expenses, and Diocletian tried unsuccessfully to re-establish trustworthy minting of silver and billon coins. The failure resided in the fact that the silver currency was overvalued in terms of its actual metal content, and therefore could only circulate at much discounted rates. Constantine stopped minting the Diocletianic \"pure\" silver \"argenteus\" soon after 305, while the billon currency continued to be used until the 360s. From the early 300s on, Constantine forsook any attempts at restoring the silver currency, preferring instead to concentrate on minting large quantities of the gold solidus, 72 of which made a pound of gold. New and highly debased silver pieces continued to be issued during his later reign and after his death, in a continuous process of retariffing, until this bullion minting ceased in 367, and the silver piece was continued by various denominations of bronze coins, the most important being the \"centenionalis\". These bronze pieces continued to be devalued, assuring the possibility of keeping fiduciary minting alongside a gold standard. The author of \"De Rebus Bellicis\" held that the rift widened between classes because of this monetary policy; the rich benefited from the stability in purchasing power of the gold piece, while the poor had to cope with ever-degrading bronze pieces. Later emperors such as Julian the Apostate insisted on trustworthy mintings of the bronze currency.\nConstantine's monetary policies were closely associated with his religious policies; increased minting was associated with the confiscation of all gold, silver, and bronze statues from pagan temples between 331 and 336 which were declared to be imperial property. Two imperial commissioners for each province had the task of getting the statues and melting them for immediate minting, with the exception of a number of bronze statues that were used as public monuments in Constantinople.\nExecutions of Crispus and Fausta.\nConstantine had his eldest son Crispus seized and put to death by \"cold poison\" at Pola (Pula, Croatia) sometime between 15 May and 17 June 326. In July, he had his wife Empress Fausta (stepmother of Crispus) killed in an overheated bath. Their names were wiped from the face of many inscriptions, references to their lives were eradicated from the literary record, and the memory of both was condemned. Eusebius, for example, edited out any praise of Crispus from later copies of \"Historia Ecclesiastica\", and his \"Vita Constantini\" contains no mention of Fausta or Crispus at all. Few ancient sources are willing to discuss possible motives for the events, and the few that do are of later provenance and are generally unreliable. At the time of the executions, it was commonly believed that Empress Fausta was either in an illicit relationship with Crispus or was spreading rumors to that effect. A popular myth arose, modified to allude to the Hippolytus\u2013Phaedra legend, with the suggestion that Constantine killed Crispus and Fausta for their immoralities; the largely fictional \"Passion of Artemius\" explicitly makes this connection. The myth rests on slim evidence as an interpretation of the executions; only late and unreliable sources allude to the relationship between Crispus and Fausta, and there is no evidence for the modern suggestion that Constantine's \"godly\" edicts of 326 and the irregularities of Crispus are somehow connected.\nAlthough Constantine created his apparent heirs \"Caesars\", following a pattern established by Diocletian, he gave his creations a hereditary character, alien to the tetrarchic system: Constantine's Caesars were to be kept in the hope of ascending to Empire, and entirely subordinated to their Augustus, as long as he was alive. Therefore, an alternative explanation for the execution of Crispus was, perhaps, Constantine's desire to keep a firm grip on his prospective heirs, this\u2014and Fausta's desire for having her sons inheriting instead of their half-brother\u2014being reason enough for killing Crispus; the subsequent execution of Fausta, however, was probably meant as a reminder to her children that Constantine would not hesitate in \"killing his own relatives when he felt this was necessary\".\nLater campaigns.\nConstantine considered Constantinople his capital and permanent residence. He lived there for a good portion of his later life. In 328 construction was completed on Constantine's Bridge at Sucidava, (today Celei in Romania) in hopes of reconquering Dacia, a province that had been abandoned under Aurelian. In the late winter of 332, Constantine campaigned with the Sarmatians against the Goths. The weather and lack of food cost the Goths dearly: reportedly, nearly one hundred thousand died before they submitted to Rome. In 334, after Sarmatian commoners had overthrown their leaders, Constantine led a campaign against the tribe. He won a victory in the war and extended his control over the region, as remains of camps and fortifications in the region indicate. Constantine resettled some Sarmatian exiles as farmers in Illyrian and Roman districts, and conscripted the rest into the army. The new frontier in Dacia was along the Brazda lui Novac line supported by new castra. Constantine took the title \"Dacicus maximus\" in 336.\nIn the last years of his life, Constantine made plans for a campaign against Persia. In a letter written to the king of Persia, Shapur, Constantine had asserted his patronage over Persia's Christian subjects and urged Shapur to treat them well. The letter is undatable. In response to border raids, Constantine sent Constantius to guard the eastern frontier in 335. In 336, Prince Narseh invaded Armenia (a Christian kingdom since 301) and installed a Persian client on the throne. Constantine then resolved to campaign against Persia himself. He treated the war as a Christian crusade, calling for bishops to accompany the army and commissioning a tent in the shape of a church to follow him everywhere. Constantine planned to be baptized in the Jordan River before crossing into Persia. Persian diplomats came to Constantinople over the winter of 336\u2013337, seeking peace, but Constantine turned them away. The campaign was called off, however, when Constantine became sick in the spring of 337.\nSickness and death.\nConstantine knew death would soon come. Within the Church of the Holy Apostles, Constantine had secretly prepared a final resting-place for himself. It came sooner than he had expected. Soon after the Feast of Easter 337, Constantine fell seriously ill. He left Constantinople for the hot baths near his mother's city of Helenopolis (Altinova), on the southern shores of the Gulf of Nicomedia (present-day Gulf of \u0130zmit). There, in a church his mother built in honor of Lucian the Apostle, he prayed, and there he realized that he was dying. Seeking purification, he became a catechumen, and attempted a return to Constantinople, making it only as far as a suburb of Nicomedia. He summoned the bishops, and told them of his hope to be baptized in the River Jordan, where Christ was written to have been baptized. He requested the baptism right away, promising to live a more Christian life should he live through his illness. The bishops, Eusebius records, \"performed the sacred ceremonies according to custom\". He chose the Arianizing bishop Eusebius of Nicomedia, bishop of the city where he lay dying, as his baptizer. In postponing his baptism, he followed one custom at the time which postponed baptism until after infancy. It has been thought that Constantine put off baptism as long as he did so as to be absolved from as much of his sin as possible. Constantine died soon after at a suburban villa called Achyron, on the last day of the fifty-day festival of Pentecost directly following Pascha (or Easter), on 22 May 337.\nAlthough Constantine's death follows the conclusion of the Persian campaign in Eusebius's account, most other sources report his death as occurring in its middle. Emperor Julian the Apostate (a nephew of Constantine), writing in the mid-350s, observes that the Sassanians escaped punishment for their ill-deeds, because Constantine died \"in the middle of his preparations for war\". Similar accounts are given in the \"Origo Constantini\", an anonymous document composed while Constantine was still living, and which has Constantine dying in Nicomedia; the \"Historiae abbreviatae\" of Sextus Aurelius Victor, written in 361, which has Constantine dying at an estate near Nicomedia called Achyrona while marching against the Persians; and the \"Breviarium\" of Eutropius, a handbook compiled in 369 for the Emperor Valens, which has Constantine dying in a nameless state villa in Nicomedia. From these and other accounts, some have concluded that Eusebius's \"Vita\" was edited to defend Constantine's reputation against what Eusebius saw as a less congenial version of the campaign.\nFollowing his death, his body was transferred to Constantinople and buried in the Church of the Holy Apostles, in a porphyry sarcophagus that was described in the 10th century by Constantine VII Porphyrogenitus in the \"De Ceremoniis\". His body survived the plundering of the city during the Fourth Crusade in 1204, but was destroyed at some point afterwards. Constantine was succeeded by his three sons born of Fausta, Constantine II, Constantius II and Constans. A number of relatives were killed by followers of Constantius, notably Constantine's nephews Dalmatius (who held the rank of Caesar) and Hannibalianus, presumably to eliminate possible contenders to an already complicated succession. He also had two daughters, Constantina and Helena, wife of Emperor Julian.\nLegacy.\nConstantine gained his honorific of \"the Great\" from Christian historians long after he had died, but he could have claimed the title on his military achievements and victories alone. He reunited the Empire under one emperor, and he won major victories over the Franks and Alamanni in 306\u2013308, the Franks again in 313\u2013314, the Goths in 332, and the Sarmatians in 334. By 336, he had reoccupied most of the long-lost province of Dacia which Aurelian had been forced to abandon in 271. At the time of his death, he was planning a great expedition to end raids on the eastern provinces from the Persian Empire. He served for almost 31 years (combining his years as co-ruler and sole ruler), the second longest-serving emperor behind Augustus.\nIn the cultural sphere, Constantine revived the clean-shaven face fashion of the Roman emperors from Augustus to Trajan, which was originally introduced among the Romans by Scipio Africanus. This new Roman imperial fashion lasted until the reign of Phocas.\nThe Holy Roman Empire reckoned Constantine among the venerable figures of its tradition. In the later Byzantine state, it became a great honor for an emperor to be hailed as a \"new Constantine\"; ten emperors carried the name, including the last emperor of the Eastern Roman Empire. Charlemagne used monumental Constantinian forms in his court to suggest that he was Constantine's successor and equal. Constantine acquired a mythic role as a warrior against heathens. The motif of the Romanesque equestrian, the mounted figure in the posture of a triumphant Roman emperor, became a visual metaphor in statuary in praise of local benefactors. The name \"Constantine\" itself enjoyed renewed popularity in western France in the eleventh and twelfth centuries. \nThe Ni\u0161 Constantine the Great Airport is named in honor of him. A large Cross was planned to be built on a hill overlooking Ni\u0161, but the project was cancelled. In 2012, a memorial was erected in Ni\u0161 in his honor. The \"Commemoration of the Edict of Milan\" was held in Ni\u0161 in 2013.\nCanonization.\nThe Orthodox Church considers Constantine a saint (\u0386\u03b3\u03b9\u03bf\u03c2 \u039a\u03c9\u03bd\u03c3\u03c4\u03b1\u03bd\u03c4\u03af\u03bd\u03bf\u03c2, Saint Constantine), having a feast day on 21 May, and calls him \"isapostolos\" (\u03b9\u03c3\u03b1\u03c0\u03cc\u03c3\u03c4\u03bf\u03bb\u03bf\u03c2 \u039a\u03c9\u03bd\u03c3\u03c4\u03b1\u03bd\u03c4\u03af\u03bd\u03bf\u03c2)\u2014an equal of the Apostles.\nHistoriography.\nConstantine was presented as a paragon of virtue during his lifetime. Pagans showered him with praise, such as Praxagoras of Athens, and Libanius. His nephew and son-in-law Julian the Apostate, however, wrote the satire \"Symposium, or the Saturnalia\" in 361, after the last of his sons died; it denigrated Constantine, calling him inferior to the great pagan emperors, and given over to luxury and greed. Following Julian, Eunapius began\u2014and Zosimus continued\u2014a historiographic tradition that blamed Constantine for weakening the Empire through his indulgence to the Christians.\nConstantine was presented as an ideal ruler during the Middle Ages, the standard against which any king or emperor could be measured. The Renaissance rediscovery of anti-Constantinian sources prompted a re-evaluation of his career. German humanist Johannes Leunclavius discovered Zosimus' writings and published a Latin translation in 1576. In its preface, he argued that Zosimus' picture of Constantine offered a more balanced view than that of Eusebius and the Church historians. Cardinal Caesar Baronius criticized Zosimus, favoring Eusebius' account of the Constantinian era. Baronius' \"Life of Constantine\" (1588) presents Constantine as the model of a Christian prince. Edward Gibbon aimed to unite the two extremes of Constantinian scholarship in his work \"The History of the Decline and Fall of the Roman Empire\" (1776\u201389) by contrasting the portraits presented by Eusebius and Zosimus. He presents a noble war hero who transforms into an Oriental despot in his old age, \"degenerating into a cruel and dissolute monarch\".\nModern interpretations of Constantine's rule begin with Jacob Burckhardt's \"The Age of Constantine the Great\" (1853, rev. 1880). Burckhardt's Constantine is a scheming secularist, a politician who manipulates all parties in a quest to secure his own power. Henri Gr\u00e9goire followed Burckhardt's evaluation of Constantine in the 1930s, suggesting that Constantine developed an interest in Christianity only after witnessing its political usefulness. Gr\u00e9goire was skeptical of the authenticity of Eusebius' \"Vita\", and postulated a pseudo-Eusebius to assume responsibility for the vision and conversion narratives of that work. Otto Seeck's \"Geschichte des Untergangs der antiken Welt\" (1920\u201323) and Andr\u00e9 Piganiol's \"L'empereur Constantin\" (1932) go against this historiographic tradition. Seeck presents Constantine as a sincere war hero whose ambiguities were the product of his own na\u00efve inconsistency. Piganiol's Constantine is a philosophical monotheist, a child of his era's religious syncretism. Related histories by Arnold Hugh Martin Jones (\"Constantine and the Conversion of Europe\", 1949) and Ramsay MacMullen (\"Constantine\", 1969) give portraits of a less visionary and more impulsive Constantine.\nThese later accounts were more willing to present Constantine as a genuine convert to Christianity. Norman H. Baynes began a historiographic tradition with \"Constantine the Great and the Christian Church\" (1929) which presents Constantine as a committed Christian, reinforced by Andreas Alf\u00f6ldi's \"The Conversion of Constantine and Pagan Rome\" (1948), and Timothy Barnes's \"Constantine and Eusebius\" (1981) is the culmination of this trend. Barnes' Constantine experienced a radical conversion which drove him on a personal crusade to convert his empire. Charles Matson Odahl's \"Constantine and the Christian Empire\" (2004) takes much the same tack. In spite of Barnes' work, arguments continue over the strength and depth of Constantine's religious conversion. Certain themes in this school reached new extremes in T.G. Elliott's \"The Christianity of Constantine the Great\" (1996), which presented Constantine as a committed Christian from early childhood. Paul Veyne's 2007 work \"Quand notre monde est devenu chr\u00e9tien\" holds a similar view which does not speculate on the origin of Constantine's Christian motivation, but presents him as a religious revolutionary who fervently believed that he was meant \"to play a providential role in the millenary economy of the salvation of humanity\".\nDonation of Constantine.\nLatin Rite Catholics considered it inappropriate that Constantine was baptized only on his death bed by an unorthodox bishop, as it undermined the authority of the Papacy, and a legend emerged by the early fourth century that Pope Sylvester I (314\u2013335) had cured the pagan emperor from leprosy. According to this legend, Constantine was soon baptized and began the construction of a church in the Lateran Palace. The Donation of Constantine appeared in the eighth century, most likely during the pontificate of Pope Stephen II (752\u2013757), in which the freshly converted Constantine gives \"the city of Rome and all the provinces, districts, and cities of Italy and the Western regions\" to Sylvester and his successors. In the High Middle Ages, this document was used and accepted as the basis for the Pope's temporal power, though it was denounced as a forgery by Emperor Otto III and lamented as the root of papal worldliness by Dante Alighieri. Philologist and Catholic priest Lorenzo Valla proved that the document was indeed a forgery.\nGeoffrey of Monmouth's \"Historia\".\nDuring the medieval period, Britons regarded Constantine as a king of their own people, particularly associating him with Caernarfon in Gwynedd. While some of this is owed to his fame and his proclamation as Emperor in Britain, there was also confusion of his family with Magnus Maximus's supposed wife Elen and her son, another Constantine . In the 12th century Henry of Huntingdon included a passage in his \"Historia Anglorum\" that the Emperor Constantine's mother was a Briton, making her the daughter of King Cole of Colchester. Geoffrey of Monmouth expanded this story in his highly fictionalized \"Historia Regum Britanniae\", an account of the supposed Kings of Britain from their Trojan origins to the Anglo-Saxon invasion. According to Geoffrey, Cole was King of the Britons when Constantius, here a senator, came to Britain. Afraid of the Romans, Cole submitted to Roman law so long as he retained his kingship. However, he died only a month later, and Constantius took the throne himself, marrying Cole's daughter Helena. They had their son Constantine, who succeeded his father as King of Britain before becoming Roman Emperor.\nHistorically, this series of events is extremely improbable. Constantius had already left Helena by the time he left for Britain. Additionally, no earlier source mentions that Helena was born in Britain, let alone that she was a princess. Henry's source for the story is unknown, though it may have been a lost hagiography of Helena."}
{"id": "7237", "revid": "6603820", "url": "https://en.wikipedia.org/wiki?curid=7237", "title": "Common Language Infrastructure", "text": "The Common Language Infrastructure (CLI) is an open specification (technical standard) developed by Microsoft and standardized by ISO and Ecma that describes executable code and a runtime environment that allows multiple high-level languages to be used on different computer platforms without being rewritten for specific architectures. This implies it is platform agnostic. The .NET Framework, .NET and Mono are implementations of the CLI.\nOverview.\nAmong other things, the CLI specification describes the following four aspects:\nAll compatible languages compile to Common Intermediate Language (CIL), which is an intermediate language that is abstracted from the platform hardware. When the code is executed, the platform-specific VES will compile the CIL to the machine language according to the specific hardware and operating system.\nStandardization and licensing.\nIn August 2000, Microsoft, Hewlett-Packard, Intel, and others worked to standardize CLI. By December 2001, it was ratified by the Ecma, with ISO standardization following in April 2003.\nMicrosoft and its partners hold patents for CLI. Ecma and ISO require that all patents essential to implementation be made available under \"reasonable and non-discriminatory (RAND) terms.\" It is common for RAND licensing to require some royalty payment, which could be a cause for concern with Mono. As of January 2013, neither Microsoft nor its partners have identified any patents essential to CLI implementations subject to RAND terms.\nAs of July 2009, Microsoft added C# and CLI to the list of specifications that the Microsoft Community Promise applies to, so anyone can safely implement specified editions of the standards without fearing a patent lawsuit from Microsoft. To implement the CLI standard requires conformance to one of the supported and defined profiles of the standard, the minimum of which is the kernel profile. The kernel profile is actually a very small set of types to support in comparison to the well known core library of default .NET installations. However, the conformance clause of the CLI allows for extending the supported profile by adding new methods and types to classes, as well as deriving from new namespaces. But it does not allow for adding new members to interfaces. This means that the features of the CLI can be used and extended, as long as the conforming profile implementation does not change the behavior of a program intended to run on that profile, while allowing for unspecified behavior from programs written specifically for that implementation.\nIn 2012, Ecma and ISO published the new edition of the CLI standard, which is not covered by the Community Promise."}
{"id": "7239", "revid": "1012653171", "url": "https://en.wikipedia.org/wiki?curid=7239", "title": "Cricket World Cup", "text": "The Cricket World Cup (officially known as ICC Men's Cricket World Cup) is the international championship of One Day International (ODI) cricket. The event is organised by the sport's governing body, the International Cricket Council (ICC), every four years, with preliminary qualification rounds leading up to a finals tournament. The tournament is one of the world's most viewed sporting events and is considered the \"flagship event of the international cricket calendar\" by the ICC.\nThe first World Cup was organised in England in June 1975, with the first ODI cricket match having been played only four years earlier. However, a separate Women's Cricket World Cup had been held two years before the first men's tournament, and a tournament involving multiple international teams had been held as early as 1912, when a triangular tournament of Test matches was played between Australia, England and South Africa. The first three World Cups were held in England. From the 1987 tournament onwards, hosting has been shared between countries under an unofficial rotation system, with fourteen ICC members having hosted at least one match in the tournament.\nThe current format involves a qualification phase, which takes place over the preceding three years, to determine which teams qualify for the tournament phase. In the tournament phase, 10 teams, including the automatically qualifying host nation, compete for the title at venues within the host nation over about a month. A total of twenty teams have competed in the eleven editions of the tournament, with ten teams competing in the recent 2019 tournament. Australia has won the tournament five times, India and West Indies twice each, while Pakistan, Sri Lanka and England have won it once each. The best performance by a non-full-member team came when Kenya made the semi-finals of the 2003 tournament.\nEngland are the current champions after winning the 2019 edition. The next tournament will be held in India in 2023.\nHistory.\nThe first international cricket match was played between Canada and the United States, on 24 and 25 September 1844. However, the first credited Test match was played in 1877 between Australia and England, and the two teams competed regularly for The Ashes in subsequent years. South Africa was admitted to Test status in 1889. Representative cricket teams were selected to tour each other, resulting in bilateral competition. Cricket was also included as an Olympic sport at the 1900 Paris Games, where Great Britain defeated France to win the gold medal. This was the only appearance of cricket at the Summer Olympics.\nThe first multilateral competition at international level was the 1912 Triangular Tournament, a Test cricket tournament played in England between all three Test-playing nations at the time: England, Australia and South Africa. The event was not a success: the summer was exceptionally wet, making play difficult on damp uncovered pitches, and crowd attendances were poor, attributed to a \"surfeit of cricket\". Since then, international Test cricket has generally been organised as bilateral series: a multilateral Test tournament was not organised again until the triangular Asian Test Championship in 1999.\nThe number of nations playing Test cricket increased gradually over time, with the addition of West Indies in 1928, New Zealand in 1930, India in 1932, and Pakistan in 1952. However, international cricket continued to be played as bilateral Test matches over three, four or five days.\nIn the early 1960s, English county cricket teams began playing a shortened version of cricket which only lasted for one day. Starting in 1962 with a four-team knockout competition known as the Midlands Knock-Out Cup, and continuing with the inaugural Gillette Cup in 1963, one-day cricket grew in popularity in England. A national Sunday League was formed in 1969. The first One-Day International match was played on the fifth day of a rain-aborted Test match between England and Australia at Melbourne in 1971, to fill the time available and as compensation for the frustrated crowd. It was a forty over game with eight balls per over.\nIn the late 1970s, Kerry Packer established the rival World Series Cricket (WSC) competition. It introduced many of the now commonplace features of One Day International cricket, including coloured uniforms, matches played at night under floodlights with a white ball and dark sight screens, and, for television broadcasts, multiple camera angles, effects microphones to capture sounds from the players on the pitch, and on-screen graphics. The first of the matches with coloured uniforms was the WSC Australians in wattle gold versus WSC West Indians in coral pink, played at VFL Park in Melbourne on 17 January 1979. The success and popularity of the domestic one-day competitions in England and other parts of the world, as well as the early One-Day Internationals, prompted the ICC to consider organising a Cricket World Cup.\nPrudential World Cups (1975\u20131983).\nThe inaugural Cricket World Cup was hosted in 1975 by England, the only nation able to put forward the resources to stage an event of such magnitude at the time. The 1975 tournament started on 7 June. The first three events were held in England and officially known as the Prudential Cup after the sponsors Prudential plc. The matches consisted of 60 six-ball overs per team, played during the daytime in traditional form, with the players wearing cricket whites and using red cricket balls.\nEight teams participated in the first tournament: Australia, England, India, New Zealand, Pakistan, and the West Indies (the six Test nations at the time), together with Sri Lanka and a composite team from East Africa. One notable omission was South Africa, who were banned from international cricket due to apartheid. The tournament was won by the West Indies, who defeated Australia by 17 runs in the final at Lord's. Roy Fredricks of West Indies was the first batsmen who got hit-wicket in ODI during the 1975 World Cup final.\nThe 1979 World Cup saw the introduction of the ICC Trophy competition to select non-Test playing teams for the World Cup, with Sri Lanka and Canada qualifying. The West Indies won a second consecutive World Cup tournament, defeating the hosts England by 92 runs in the final. At a meeting which followed the World Cup, the International Cricket Conference agreed to make the competition a quadrennial event.\nThe 1983 event was hosted by England for a third consecutive time. By this stage, Sri Lanka had become a Test-playing nation, and Zimbabwe qualified through the ICC Trophy. A fielding circle was introduced, away from the stumps. Four fieldsmen needed to be inside it at all times. The teams faced each other twice, before moving into the knock-outs. India was crowned champions after upsetting the West Indies by 43 runs in the final.\nDifferent champions (1987\u20131996).\nIndia and Pakistan jointly hosted the 1987 tournament, the first time that the competition was held outside England. The games were reduced from 60 to 50 overs per innings, the current standard, because of the shorter daylight hours in the Indian subcontinent compared with England's summer. Australia won the championship by defeating England by 7 runs in the final, the closest margin in the World Cup final until the 2019 edition between England and New Zealand.\nThe 1992 World Cup, held in Australia and New Zealand, introduced many changes to the game, such as coloured clothing, white balls, day/night matches, and a change to the fielding restriction rules. The South African cricket team participated in the event for the first time, following the fall of the apartheid regime and the end of the international sports boycott. Pakistan overcame a dismal start in the tournament to eventually defeat England by 22 runs in the final and emerge as winners.\nThe 1996 championship was held in the Indian subcontinent for a second time, with the inclusion of Sri Lanka as host for some of its group stage matches. In the semi-final, Sri Lanka, heading towards a crushing victory over India at Eden Gardens after the hosts lost eight wickets while scoring 120 runs in pursuit of 252, were awarded victory by default after crowd unrest broke out in protest against the Indian performance. Sri Lanka went on to win their maiden championship by defeating Australia by seven wickets in the final at Lahore.\nAustralian treble (1999\u20132007).\nIn 1999 the event was hosted by England, with some matches also being held in Scotland, Ireland, Wales and the Netherlands. Twelve teams contested the World Cup. Australia qualified for the semi-finals after reaching their target in their Super 6 match against South Africa off the final over of the match. They then proceeded to the final with a tied match in the semi-final also against South Africa where a mix-up between South African batsmen Lance Klusener and Allan Donald saw Donald drop his bat and stranded mid-pitch to be run out. In the final, Australia dismissed Pakistan for 132 and then reached the target in less than 20 overs and with eight wickets in hand.\nSouth Africa, Zimbabwe and Kenya hosted the 2003 World Cup. The number of teams participating in the event increased from twelve to fourteen. Kenya's victories over Sri Lanka and Zimbabwe, among others\u00a0\u2013 and a forfeit by the New Zealand team, which refused to play in Kenya because of security concerns\u00a0\u2013 enabled Kenya to reach the semi-finals, the best result by an associate. In the final, Australia made 359 runs for the loss of two wickets, the largest ever total in a final, defeating India by 125 runs.\nIn 2007 the tournament was hosted by the West Indies and expanded to sixteen teams. Following Pakistan's upset loss to World Cup debutants Ireland in the group stage, Pakistani coach Bob Woolmer was found dead in his hotel room. Jamaican police had initially launched a murder investigation into Woolmer's death but later confirmed that he died of heart failure. Australia defeated Sri Lanka in the final by 53 runs (D/L) in farcical light conditions, and extended their undefeated run in the World Cup to 29 matches and winning three straight championships.\nHosts triumph (2011\u20132019).\nIndia, Sri Lanka and Bangladesh together hosted the 2011 World Cup. Pakistan were stripped of their hosting rights following the terrorist attack on the Sri Lankan cricket team in 2009, with the games originally scheduled for Pakistan redistributed to the other host countries. The number of teams participating in the World Cup was reduced to fourteen. Australia lost their final group stage match against Pakistan on 19 March 2011, ending an unbeaten streak of 35 World Cup matches, which had begun on 23 May 1999. India won their second World Cup title by beating Sri Lanka by 6 wickets in the final in Mumbai, and became the first country to win the final on home soil.\nAustralia and New Zealand jointly hosted the 2015 World Cup. The number of participants remained at fourteen. Ireland was the most successful Associate nation with a total of three wins in the tournament. New Zealand beat South Africa in a thrilling first semi-final to qualify for their maiden World Cup final. Australia defeated New Zealand by seven wickets in the final at Melbourne to lift the World Cup for the fifth time.\nThe 2019 World Cup was hosted by England and Wales. The number of participants was reduced to 10. New Zealand defeated India in the first semi-final, which was pushed over to the reserve day due to rain. England defeated the defending champions, Australia, in the second semi-final. Neither finalist had previously won the World Cup. In the final, the scores were tied at 241 after 50 overs and the match went to a super over, after which the scores were again tied at 15. The World Cup was won by England, whose boundary count was greater than New Zealand's.\nFormat.\nQualification.\nFrom the first World Cup in 1975 up to the 2019 World Cup, the majority of teams taking part qualified automatically. Until the 2015 World Cup this was mostly through having Full Membership of the ICC, and for the 2019 World Cup this was mostly through ranking position in the ICC ODI Championship.\nSince the second World Cup in 1979 up to the 2019 World Cup, the teams that qualified automatically were joined by a small number of others who qualified for the World Cup through the qualification process. The first qualifying tournament being the ICC Trophy; later the process expanding with pre-qualifying tournaments. For the 2011 World Cup, the ICC World Cricket League replaced the past pre-qualifying processes; and the name \"ICC Trophy\" was changed to \"ICC World Cup Qualifier\". The World Cricket League was the qualification system provided to allow the Associate and Affiliate members of the ICC more opportunities to qualify. The number of teams qualifying varied throughout the years.\nFrom the 2023 World Cup onwards, only the host nation(s) will qualify automatically. All countries will participate in a series of leagues to determine qualification, with automatic promotion and relegation between divisions from one World Cup cycle to the next.\nTournament.\nThe format of the Cricket World Cup has changed greatly over the course of its history. Each of the first four tournaments was played by eight teams, divided into two groups of four. The competition consisted of two stages, a group stage and a knock-out stage. The four teams in each group played each other in the round-robin group stage, with the top two teams in each group progressing to the semi-finals. The winners of the semi-finals played against each other in the final. With South Africa returning in the fifth tournament in 1992 as a result of the end of the apartheid boycott, nine teams played each other once in the group phase, and the top four teams progressed to the semi-finals. The tournament was further expanded in 1996, with two groups of six teams. The top four teams from each group progressed to quarter-finals and semi-finals.\nA distinct format was used for the 1999 and 2003 World Cups. The teams were split into two pools, with the top three teams in each pool advancing to the \"Super 6\". The \"Super 6\" teams played the three other teams that advanced from the other group. As they advanced, the teams carried their points forward from previous matches against other teams advancing alongside them, giving them an incentive to perform well in the group stages. The top four teams from the \"Super 6\" stage progressed to the semi-finals, with the winners playing in the final.\nThe format used in the 2007 World Cup involved 16 teams allocated into four groups of four. Within each group, the teams played each other in a round-robin format. Teams earned points for wins and half-points for ties. The top two teams from each group moved forward to the \"Super 8\" round. The \"Super 8\" teams played the other six teams that progressed from the different groups. Teams earned points in the same way as the group stage, but carried their points forward from previous matches against the other teams who qualified from the same group to the \"Super 8\" stage. The top four teams from the \"Super 8\" round advanced to the semi-finals, and the winners of the semi-finals played in the final.\nThe format used in the 2011 and 2015 World Cups featured two groups of seven teams, each playing in a round-robin format. The top four teams from each group proceeded to the knock out stage consisting of quarter-finals, semi-finals and ultimately the final.\nIn the 2019 World Cup, the number of teams participating dropped to 10. Every team were scheduled to play against each other once in a round robin format, before entering the semifinals, a similar format to the 1992 World Cup.\nTrophy.\nThe ICC Cricket World Cup Trophy is presented to the winners of the World Cup. The current trophy was created for the 1999 championships, and was the first permanent prize in the tournament's history. Prior to this, different trophies were made for each World Cup. The trophy was designed and produced in London by a team of craftsmen from Garrard &amp; Co over a period of two months.\nThe current trophy is made from silver and gilt, and features a golden globe held up by three silver columns. The columns, shaped as stumps and bails, represent the three fundamental aspects of cricket: batting, bowling and fielding, while the globe characterises a cricket ball. The seam is tilted to symbolize the axial tilt of the Earth. It stands 60\u00a0centimetres high and weighs approximately 11\u00a0kilograms. The names of the previous winners are engraved on the base of the trophy, with space for a total of twenty inscriptions. The ICC keeps the original trophy. A replica differing only in the inscriptions is permanently awarded to the winning team.\nMedia coverage.\nThe tournament is one of the world's most-viewed sporting events. The 2011 Cricket World Cup final was televised in over 200 countries to over 2.2\u00a0billion television viewers. Television rights, mainly for the 2011 and 2015 World Cup, were sold for over US$1.1\u00a0billion, and sponsorship rights were sold for a further US$500\u00a0million. The 2003 Cricket World Cup matches were attended by 626,845 people, while the 2007 Cricket World Cup sold more than 672,000 tickets. The 2015 World Cup Sold over 1.1 million tickets which was a Record .\nSuccessive World Cup tournaments have generated increasing media attention as One-Day International cricket has become more established. The 2003 World Cup in South Africa was the first to sport a mascot, Dazzler the zebra. An orange mongoose known as \"Mello\" was the mascot for the 2007 Cricket World Cup. Stumpy, a blue elephant was the mascot for the 2011 World Cup.\nOn 13 February, the opening of the 2015 tournament was celebrated with a Google Doodle.\nDue to England making the 2019 final, the match was domestically picked up for terrestrial broadcast by Channel 4 (with a move to More4 later in the match) in a rights share with local telecaster Sky Sports.\nSelection of hosts.\nThe International Cricket Council's executive committee votes for the hosts of the tournament after examining the bids made by the nations keen to hold a Cricket World Cup.\nEngland hosted the first three competitions. The ICC decided that England should host the first tournament because it was ready to devote the resources required to organising the inaugural event. India volunteered to host the third Cricket World Cup, but most ICC members preferred England as the longer period of daylight in England in June meant that a match could be completed in one day. The 1987 Cricket World Cup was held in India and Pakistan, the first hosted outside England.\nMany of the tournaments have been jointly hosted by nations from the same geographical region, such as South Asia in 1987, 1996 and 2011, Australasia (in Australia and New Zealand) in 1992 and 2015, Southern Africa in 2003 and West Indies in 2007.\nTournament Summary.\nTwenty nations have qualified for the Cricket World Cup at least once. Seven teams have competed in every tournament, six of which have won the title. The West Indies won the first two tournaments, Australia has won five, India has won two, while Pakistan, Sri Lanka and England have each won once. The West Indies (1975 and 1979) and Australia (1987, 1999, 2003, 2007 and 2015) are the only teams to have won consecutive titles. Australia has played in seven of the twelve finals (1975, 1987, 1996, 1999, 2003, 2007 and 2015). New Zealand has yet to win the World Cup, but has been runners-up two times (2015 and 2019). The best result by a non-Test playing nation is the semi-final appearance by Kenya in the 2003 tournament; while the best result by a non-Test playing team on their debut is the Super 8 (second round) by Ireland in 2007.\nSri Lanka, as a co-host of the 1996 World Cup, was the first host to win the tournament, though the final was held in Pakistan. India won in 2011 as host and was the first team to win a final played in their own country. Australia and England repeated the feat in 2015 and 2019 respectively. Other than this, England made it to the final as a host in 1979. Other countries which have achieved or equalled their best World Cup results while co-hosting the tournament are New Zealand as finalists in 2015, Zimbabwe who reached the Super Six in 2003, and Kenya as semi-finalists in 2003. In 1987, co-hosts India and Pakistan both reached the semi-finals, but were eliminated by England and Australia respectively. Australia in 1992, England in 1999, South Africa in 2003, and Bangladesh in 2011 have been host teams that were eliminated in the first round.\nTeams' performances.\nAn overview of the teams' performances in every World Cup:\nBefore the 1992 World Cup, South Africa was banned due to apartheid.\nThe number of wins followed by Run-rate is the criteria for determining the rankings till the 1987 World Cup.\nThe number of points followed by, head to head performance and then net run-rate is the criteria for determining the rankings for the World Cups from 1992 onwards.\nLegend\nDebutant teams.\n\u2020\nOverview.\nThe table below provides an overview of the performances of teams over past World Cups, as of the end of the 2019 tournament. Teams are sorted by best performance, then by appearances, total number of wins, total number of games, and alphabetical order respectively.\n\"Note:\"\nTeams in World Cup.\n\u2020\nAwards.\nMan of the tournament.\nSince 1992, one player has been declared as \"Man of the Tournament\" at the end of the World Cup finals:\nMan of the Match in the Final.\nThere were no Man of the Tournament awards before 1992 but Man of the Match awards have always been given for individual matches. Winning the Man of the Match in the final is logically noteworthy, as this indicates the player deemed to have played the biggest part in the World Cup final. To date the award has always gone to a member of the winning side. The Man of the Match award in the final of the competition has been awarded to:"}
{"id": "7241", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7241", "title": "Commonwealth Heads of Government Meeting", "text": "The Commonwealth Heads of Government Meeting (CHOGM; or) is a biennial summit meeting of the \"de facto\" leaders from all Commonwealth nations. Despite the name, the head of state may be present in the meeting instead of the head of government, especially among semi-presidential states. Every two years the meeting is held in a different member state and is chaired by that nation's respective Prime Minister or President who becomes the Commonwealth Chair-in-Office until the next meeting. Queen Elizabeth II, who is the Head of the Commonwealth, attended every CHOGM beginning with Ottawa in 1973 until Perth in 2011, although her formal participation only began in 1997. She was represented by the Prince of Wales at the 2013 meeting as the 87-year-old monarch was curtailing long-distance travel. The Queen attended the 2015 summit in Malta and the 2018 summit (delayed one year) held in London.\nThe first CHOGM was held in 1971 in Singapore, and there have been 25\nheld in total: the most recent was held in London, England. They are held once every two years, although this pattern has twice been interrupted. They are held around the Commonwealth, rotating by invitation amongst its members.\nIn the past, CHOGMs have attempted to orchestrate common policies on certain contentious issues and current events, with a special focus on issues affecting member nations. CHOGMs have discussed the continuation of apartheid rule in South Africa and how to end it, military coups in Pakistan and Fiji, and allegations of electoral fraud in Zimbabwe. Sometimes the member states agree on a common idea or solution, and release a joint statement declaring their opinion. More recently, beginning at the 1997 CHOGM, the meeting has had an official 'theme', set by the host nation, on which the primary discussions have been focused.\nHistory.\nThe meetings originated with the leaders of the self-governing colonies of the British Empire. The First Colonial Conference in 1887 was followed by periodic meetings, known as Imperial Conferences from 1907, of government leaders of the Empire. The development of the independence of the dominions, and the creation of a number of new dominions, as well as the invitation of Southern Rhodesia (which also attended as a \"sui generis\" colony), changed the nature of the meetings. As the dominion leaders asserted themselves more and more at the meetings, it became clear that the time for 'imperial' conferences was over.\nFrom the ashes of the Second World War, seventeen Commonwealth Prime Ministers' Conferences were held between 1944 and 1969. Of these, sixteen were held in London, reflecting then-prevailing views of the Commonwealth as the continuation of the Empire and the centralisation of power in the British Commonwealth Office (the one meeting outside London, in Lagos, was an extraordinary meeting held in January 1966 to co-ordinate policies towards Rhodesia). Two supplementary meetings were also held during this period: a Commonwealth Statesmen's meeting to discuss peace terms in April 1945, and a Commonwealth Economic Conference in 1952.\nThe 1960s saw an overhaul of the Commonwealth. The swift expansion of the Commonwealth after decolonisation saw the newly independent countries demand the creation of the Commonwealth Secretariat, and the United Kingdom, in response, successfully founding the Commonwealth Foundation. This decentralisation of power demanded a reformulation of the meetings. Instead of the meetings always being held in London, they would rotate across the membership, subject to countries' ability to host the meetings: beginning with Singapore in 1971. They were also renamed the 'Commonwealth Heads of Government Meetings' to reflect the growing diversity of the constitutional structures in the Commonwealth.\nStructure.\nThe core of the CHOGM are the executive sessions, which are the formal gatherings of the heads of government to do business. However, the majority of the important decisions are held not in the main meetings themselves, but at the informal 'retreats': introduced at the second CHOGM, in Ottawa, by Prime Minister of Canada Pierre Trudeau, but reminiscent of the excursions to Chequers or Dorneywood in the days of the Prime Ministers' Conferences. The rules are very strict: allowing the head of the delegation, his or her spouse, and one other person. The additional member can be of any capacity (personal, political, security, etc.), but he or she has only occasional and intermittent access to the head. It is usually at the retreat where, isolated from their advisers, the heads resolve the most intransigent issues: leading to the Gleneagles Agreement in 1977, the Lusaka Declaration in 1979, the Langkawi Declaration in 1989, the Millbrook Programme in 1995, the Aso Rock Declaration in 2003, and the Colombo Declaration on Sustainable, Inclusive and Equitable Development in 2013.\nThe 'fringe' of civil society organisations, including the Commonwealth Family and local groups, adds a cultural dimension to the event, and brings the CHOGM a higher media profile and greater acceptance by the local population. First officially recognised at Limassol in 1993, these events, spanning a longer period than the meeting itself, have, to an extent, preserved the length of the CHOGM: but only in the cultural sphere. Other meetings, such as those of the Commonwealth Ministerial Action Group, Commonwealth Business Council, and respective foreign ministers, have also dealt with business away from the heads of government themselves.\nAs the scope of the CHOGM has expanded beyond the meetings of the heads of governments themselves, the CHOGMs have become progressively shorter, and their business compacted into less time. The 1971 CHOGM lasted for nine days, and the 1977 and 1991 CHOGMs for seven days each. However, Harare's epochal CHOGM was the last to last a week; the 1993 CHOGM lasted for five days, and the contentious 1995 CHOGM for only three-and-a-half. The 2005 and subsequent conferences were held over two to two-and-a-half-days. However, recent CHOGMs have also featured several days of pre-summit Commonwealth Forums on business, women, youth, as well as the Commonwealth People's Forum and meetings of foreign ministers.\nIssues.\nDuring the 1980s, CHOGMs were dominated by calls for the Commonwealth to impose sanctions on South Africa to pressure the country to end apartheid. The division between Britain, during the government of Margaret Thatcher which resisted the call for sanctions and African Commonwealth countries, and the rest of the Commonwealth was intense at times and led to speculation that the organisation might collapse. According to one of Margaret Thatcher's former aides, Mrs. Thatcher, very privately, used to say that CHOGM stood for \"Compulsory Handouts to Greedy Mendicants.\" According to her daughter, Denis Thatcher also referred to CHOGM as standing for 'C**ns Holidaying on Government Money'.\nIn 2011, British Prime Minister David Cameron informed the British House of Commons that his proposals to reform the rules governing royal succession, a change which would require the approval of all sixteen Commonwealth realms, was approved at the 28\u201330 October CHOGM in Perth, subsequently referred to as the Perth Agreement.\nAgenda.\nUnder the Millbrook Commonwealth Action Programme, each CHOGM is responsible for renewing the remit of the Commonwealth Ministerial Action Group, whose responsibility it is to uphold the Harare Declaration on the core political principles of the Commonwealth.\nIncidents.\nA bomb exploded at the Sydney Hilton Hotel, the venue for the February 1978 Commonwealth Heads of Government Regional Meeting. Twelve foreign heads of government were staying in the hotel at the time. Most delegates were evacuated by Royal Australian Air Force helicopters and the meeting was moved to Bowral, protected by 800 soldiers of the Australian Army.\nAs the convocation of heads of governments and permanent Commonwealth staff and experts, CHOGMs are the highest institution of action in the Commonwealth, and rare occasions on which Commonwealth leaders all come together. CHOGMs have been the venues of many of the Commonwealth's most dramatic events. Robert Mugabe announced Zimbabwe's immediate withdrawal from the Commonwealth at the 2003 CHOGM, and Nigeria's execution of Ken Saro-Wiwa and eight others on the first day of the 1995 CHOGM led to that country's suspension.\nIt has also been the trigger of a number of events that have shaken participating countries domestically. The departure of Uganda's President Milton Obote to the 1971 CHOGM allowed Idi Amin to overthrow Obote's government. Similarly, President James Mancham's attendance of the 1977 CHOGM gave Prime Minister France-Albert Ren\u00e9 the opportunity to seize power in the Seychelles.\nList of meetings.\nThe 25th CHOGM was originally scheduled for Vanuatu in 2017 but the country rescinded its offer to host after Cyclone Pam devastated the country's infrastructure in March 2015. The meeting was rescheduled for the United Kingdom in the spring of 2018 which also resulted in the 26th CHOGM, originally scheduled for 2019, to be rescheduled for 2020. However, due to the coronavirus pandemic, the 26th CHOGM which was scheduled for 22\u201327 June 2020 has been postponed until at least 2021. It is still expected to be held in Kigali, Rwanda."}
{"id": "7242", "revid": "15122211", "url": "https://en.wikipedia.org/wiki?curid=7242", "title": "Chinese classics", "text": "Chinese classic texts or canonical texts () or simply dianji (\u5178\u7c4d) refers to the Chinese texts which originated before the imperial unification by the Qin dynasty in 221 BC, particularly the \"Four Books and Five Classics\" of the Neo-Confucian tradition, themselves a customary abridgment of the \"Thirteen Classics\". All of these pre-Qin texts were written in classical Chinese. All three canons are collectively known as the classics (\u00a0, \u00a0, \"j\u012bng\", \u00a0\"warp\").\nChinese classic texts may more broadly refer to texts written either in vernacular Chinese or in the classical Chinese that was current until the fall of the last imperial dynasty, the Qing, in 1912. These can include \"shi\" (, historical works), \"zi\" (, philosophical works belonging to schools of thought other than the Confucian but also including works on agriculture, medicine, mathematics, astronomy, divination, art criticism, and other miscellaneous writings) and \"ji\" (, literary works) as well as \"jing (Chinese medicine)\".\nIn the Ming and Qing dynasties, the Four Books and Five Classics were the subject of mandatory study by those Confucian scholars who wished to take the imperial exams to become government officials. Any political discussion was full of references to this background, and one could not be one of the literati (or, in some periods, even a military officer) without having memorized them. Generally, children first memorized the Chinese characters of the \"Three Character Classic\" and the \"Hundred Family Surnames\" and then went on to memorize the other classics. The literate elite therefore shared a common culture and set of values.\nScholarship on these texts naturally divides itself into two periods, before and after the burning of the books during the fall of the Qin dynasty, when many of the original pre-Qin texts were lost.\nBefore 221 BC.\nIt is often difficult or impossible to precisely date pre-Qin works beyond their being \"pre-Qin\", a period of 1000 years. Information in ancient China was often passed down orally for generations before being written down, so the order of composition of the texts need not be the same as that of their attributed \"authors\".\nThe below list is therefore organized in the order found in the Siku Quanshu, the imperial library of the Qing dynasty. The Siku classifies all works into 4 top-level branches: the Confucian Classics and their secondary literature; history; philosophy; and poetry. There are sub-categories within each branch, but due to the small number of pre-Qin works in the Classics, History and Poetry branches, the sub-categories are only reproduced for the Philosophy branch.\nThe Thirteen Classics.\nThe most important pre-Qin works that later became the official curriculum of the imperial examination system from the Song dynasty onward are the Thirteen Classics. This is a slightly different organization of the works of the Classics branch. In total, these works total to more than 600,000 characters that must be memorized in order to pass the examination. Moreover, these works are accompanied by extensive commentary and annotation, containing approximately 300 million characters by some estimates."}
{"id": "7243", "revid": "5605596", "url": "https://en.wikipedia.org/wiki?curid=7243", "title": "Call centre", "text": "A call centre (Commonwealth spelling) or call center (American spelling; see spelling differences) is a centralised office used for receiving or transmitting a large volume of enquiries by telephone. An inbound call centre is operated by a company to administer incoming product or service support or information enquiries from consumers. Outbound call centres are operated for telemarketing, for solicitation of charitable or political donations, debt collection, market research, emergency notifications, and urgent/critical needs blood banks. A contact centre, further extension to call centres administers centralised handling of individual communications, including letters, faxes, live support software, social media, instant message, and e-mail.\nA call center has an open workspace for call center agents, with workstations that include a computer and display for each agent and connected to an inbound/outbound call management system, and one or more supervisor stations. It can be independently operated or networked with additional centers, often linked to a corporate computer network, including mainframes, microcomputer/servers and LANs. Increasingly, the voice and data pathways into the center are linked through a set of new technologies called computer telephony integration.\nThe contact center is a central point from which all customer contacts are managed. Through contact centers, valuable information about company are routed to appropriate people, contacts to be tracked and data to be gathered. It is generally a part of the company's customer relationship management infrastructure. The majority of large companies use contact centers as a means of managing their customer interactions. These centers can be operated by either an in-house department responsible or outsourcing customer interaction to a third-party agency (known as Outsourcing Call Centres).\nHistory.\nAnswering services, as known in the 1960s through the 1980s, earlier and slightly later, involved a business that specifically provided the service. Primarily by the use of an off-premises extension (OPX) for each subscribing business, connected at a switchboard at the answering service business, the answering service would answer the otherwise unattended phones of the subscribing businesses with a live operator. The live operator could take messages or relay information, doing so with greater human interactivity than a mechanical answering machine. Although undoubtedly more costly (the human service, the cost of setting up and paying the phone company for the OPX on a monthly basis), it had the advantage of being more ready to respond to the unique needs of after-hours callers. The answering service operators also had the option of calling the client and alerting them to, particularly important calls.\nThe origins of call centers date back to the 1960s with the UK-based Birmingham Press and Mail, which installed Private Automated Business Exchanges (PABX) to have rows of agents handling customer contacts. By 1973, call centers received mainstream attention after Rockwell International patented its Galaxy Automatic Call Distributor (GACD) for a telephone booking system as well as the popularization of telephone headsets as seen on televised NASA Mission Control Center events.\nDuring the late 1970s, call center technology expanded to include telephone sales, airline reservations, and banking systems. The term \"call center\" was first published and recognised by the \"Oxford English Dictionary\" in 1983. The 1980s experienced the development of toll-free telephone numbers to increase the efficiency of agents and overall call volume. Call centers increased with the deregulation of long-distance calling and growth in information-dependent industries.\nAs call centres expanded, unionisation occurred in North America to gain members including the Communications Workers of America and the United Steelworkers. In Australia, the National Union of Workers represents unionised workers; their activities form part of the Australian labour movement. In Europe, Uni Global Union of Switzerland is involved in assisting unionisation in this realm and in Germany Vereinte Dienstleistungsgewerkschaft represents call centre workers.\nDuring the 1990s, call centres expanded internationally and developed into two additional subsets of communication, contact centres, and outsourced bureau centres. A contact centre is defined as a coordinated system of people, processes, technologies, and strategies that provides access to information, resources, and expertise, through appropriate channels of communication, enabling interactions that create value for the customer and organization. In contrast to in-house management, outsourced bureau contact centres are a model of contact centre that provide services on a \"pay per use\" model. The overheads of the contact centre are shared by many clients, thereby supporting a very cost effective model, especially for low volumes of calls. The modern contact centre includes automated call blending of inbound and outbound calls as well as predictive dialing capabilities dramatically increasing agents productivity. Latest implementations with more complex systems, require highly skilled operational and management staff that can use multichannel online and offline tools to improve customer interactions.\nTechnology.\nCall centre technologies include: speech recognition software which allowed Interactive Voice Response (IVR) systems to handle first levels of customer support, text mining, natural language processing to allow better customer handling, agent training via interactive scripting and automatic mining using best practices from past interactions, support automation and many other technologies to improve agent productivity and customer satisfaction. Automatic lead selection or lead steering is also intended to improve efficiencies, both for inbound and outbound campaigns. This allows inbound calls to be directly routed to the appropriate agent for the task, whilst minimising wait times and long lists of irrelevant options for people calling in.\nFor outbound calls, lead selection allows management to designate what type of leads go to which agent based on factors including skill, socioeconomic factors, past performance, and percentage likelihood of closing a sale per lead.\nThe universal queue standardises the processing of communications across multiple technologies such as fax, phone, and email. The virtual queue provides callers with an alternative to waiting on hold when no agents are available to handle inbound call demand.\nPremises-based technology.\nHistorically, call centres have been built on Private branch exchange (PBX) equipment that is owned, hosted, and maintained by the call centre operator. The PBX can provide functions such as automatic call distribution, interactive voice response, and skills-based routing.\nVirtual call centre.\nIn a virtual call centre model, the call centre operator (business) pays a monthly or annual fee to a vendor that hosts the call centre telephony and data equipment in their own facility, cloud-based. In this model, the operator does not own, operate or host the equipment on which the call centre runs. Agents connect to the vendor's equipment through traditional PSTN telephone lines, or over voice over IP. Calls to and from prospects or contacts originate from or terminate at the vendor's data centre, rather than at the call centre operator's premises. The vendor's telephony equipment (at times data servers) then connects the calls to the call centre operator's agents.\nVirtual call centre technology allows people to work from home or any other location instead of in a traditional, centralised, call centre location, which increasingly allows people 'on the go' or with physical or other disabilities to work from desired locations - i.e. not leaving their house. The only required equipment is Internet access and a workstation. The companies are preferring Virtual Call Centre services due to cost advantage. Companies can start their call centre business immediately without installing the basic infrastructure like Dialer, ACD and IVRS.\nVirtual call centres became increasingly used after the COVID-19 pandemic restricted businesses from operating with large groups of people working in close proximity.\nCloud computing.\nThrough the use of application programming interfaces (APIs), hosted and on-demand call centres that are built on cloud-based software as a service (SaaS) platforms can integrate their functionality with cloud-based applications for customer relationship management (CRM), lead management and more.\nDevelopers use APIs to enhance cloud-based call centre platform functionality\u2014including Computer telephony integration (CTI) APIs which provide basic telephony controls and sophisticated call handling from a separate application, and configuration APIs which enable graphical user interface (GUI) controls of administrative functions.\nOutsourcing.\nOutsourced call centres are often located in developing countries, where wages are significantly lower. These include the call centre industries in the Philippines, Bangladesh, and India.\nCompanies that regularly utilise outsourced contact centre services include British Sky Broadcasting and Orange in the telecommunications industry, Adidas in the sports and leisure sector, Audi in car manufacturing and charities such as the RSPCA.\nIndustries.\nHealthcare.\nThe healthcare industry has used outbound call centre programmes for years to help manage billing, collections, and patient communication. The inbound call centre is a new and increasingly popular service for many types of healthcare facilities, including large hospitals. Inbound call centres can be outsourced or managed in-house.\nThese healthcare call centres are designed to help streamline communications, enhance patient retention and satisfaction, reduce expenses and improve operational efficiencies.\nHospitality.\nMany large hospitality companies such as the Hilton Hotels Corporation and Marriott International make use of call centres to manage reservations. These are known in the industry as \"central reservations offices\". Staff members at these call centres take calls from clients wishing to make reservations or other inquiries via a public number, usually a 1-800 number. These centres may operate as many as 24 hours per day, seven days a week, depending on the call volume the chain receives.\nEvaluation.\nMathematical theory.\nQueueing theory is a branch of mathematics in which models of service systems have been developed. A call centre can be seen as a queueing network and results from queueing theory such as the probability an arriving customer needs to wait before starting service useful for provisioning capacity. (Erlang's C formula is such a result for an M/M/c queue and approximations exist for an M/G/k queue.) Statistical analysis of call centre data has suggested arrivals are governed by an inhomogeneous Poisson process and jobs have a log-normal service time distribution. Simulation algorithms are increasingly being used to model call arrival, queueing and service levels.\nCall centre operations have been supported by mathematical models beyond queueing, with operations research, which considers a wide range of optimisation problems seeking to reduce waiting times while keeping server utilisation and therefore efficiency high.\nCriticism.\nCall centres have received criticism for low pay rates and restrictive working practices for employees, which have been deemed as a dehumanising environment. Other research illustrates how call centre workers develop ways to counter or resist this environment by integrating local cultural sensibilities or embracing a vision of a new life. Most call centres provide electronic reports that outline performance metrics, quarterly highlights and other information about the calls made and received. This has the benefit of helping the company to plan the workload and time of its employees. However, it has also been argued that such close monitoring breaches the human right to privacy.\nComplaints are often logged by callers who find the staff do not have enough skill or authority to resolve problems, as well as appearing apathetic. These concerns are due to a business process that exhibits levels of variability because the experience a customer gets and results a company achieves on a given call are dependent upon the quality of the agent. Call centres are beginning to address this by using agent-assisted automation to standardise the process all agents use. However, more popular alternatives are using personality and skill based approaches. The various challenges encountered by call operators are discussed by several authors.\nMedia portrayals.\nIndian call centres have been the focus of several documentary films, the 2004 film \"Thomas L. Friedman Reporting: The Other Side of Outsourcing\", the 2005 films \"John and Jane\", \"Nalini by Day, Nancy by Night\", and \"1-800-India: Importing a White-Collar Economy\", and the 2006 film \"Bombay Calling\", among others. An Indian call centre is also the subject of the 2006 film \"Outsourced\" and a key location in the 2008 film, \"Slumdog Millionaire\". The 2014 BBC fly on the wall documentary series \"The Call Centre\" gave an often distorted although humorous view of life in a Welsh call centre."}
{"id": "7244", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7244", "title": "Corrodo Gini", "text": ""}
{"id": "7245", "revid": "10951369", "url": "https://en.wikipedia.org/wiki?curid=7245", "title": "Caliph", "text": ""}
{"id": "7246", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=7246", "title": "Charles Messier", "text": "Charles Messier (; 26\u00a0June 1730 \u2013 12\u00a0April 1817) was a French astronomer. He published an astronomical catalogue consisting of 110\u00a0nebulae and faint star clusters, which came to be known as the \"Messier objects\". The purpose of the catalogue was to help astronomical observers distinguish between permanent and transient visually diffuse objects in the sky.\nBiography.\nMessier was born in Badonviller in the Lorraine region of France, the tenth of twelve children of Fran\u00e7oise B. Grandblaise and Nicolas Messier, a Court usher. Six of his brothers and sisters died while young, and his father died in 1741. Charles' interest in astronomy was stimulated by the appearance of the great six-tailed comet in 1744 and by an annular solar eclipse visible from his hometown on 25\u00a0July 1748.\nIn 1751, Messier entered the employ of Joseph Nicolas Delisle, the astronomer of the French Navy, who instructed him to keep careful records of his observations. Messier's first documented observation was that of the Mercury transit of 6\u00a0May 1753, followed by his observations journals at Cluny Hotel and at the French Navy observatories.\nIn 1764, Messier was made a fellow of the Royal Society; in 1769, he was elected a foreign member of the Royal Swedish Academy of Sciences; and on 30\u00a0June 1770, he was elected to the French Academy of Sciences.\nMessier discovered 13 comets:\nHe also co-discovered Comet C/1801 N1, a discovery shared with several other observers including Pons, M\u00e9chain, and Bouvard. (Comet Pons-Messier-M\u00e9chain-Bouvard)\nNear the end of his life, Messier self-published a booklet connecting the great comet of 1769 to the birth of Napoleon, who was in power at the time of publishing. According to Meyer:\nMessier is buried in P\u00e8re Lachaise Cemetery, Paris, in Section\u00a011. The grave is faintly inscribed, and is near the grave of Fr\u00e9d\u00e9ric Chopin, slightly to the west and directly north, and behind the small mausoleum of the jeweller Abraham-Louis Breguet.\nMessier catalogue.\nMessier's occupation as a comet hunter led him to continually come across fixed diffuse objects in the night sky which could be mistaken for comets. He compiled a list of them, in collaboration with his friend and assistant Pierre M\u00e9chain (who may have found at least 20 of the objects), to avoid wasting time sorting them out from the comets they were looking for. The entries are now known to be 39\u00a0galaxies, 4\u00a0planetary nebulae, 7\u00a0other types of nebulae, and 55\u00a0star clusters.\nMessier did his observing with a 100\u00a0mm (four-inch) refracting telescope from H\u00f4tel de Cluny (now the Mus\u00e9e national du Moyen \u00c2ge), in downtown Paris, France. The list he compiled only contains objects found in the area of the sky Messier could observe, from the north celestial pole to a declination of about \u221235.7\u00b0\u00a0. They are not organized scientifically by object type, or by location. The first version of Messier's catalogue contained 45\u00a0objects and was published in 1774 in the journal of the French Academy of Sciences in Paris. In addition to his own discoveries, this version included objects previously observed by other astronomers, with only 17 of the 45\u00a0objects being discovered by Messier himself. By 1780 the catalog had increased to 80\u00a0objects.\nThe final version of the catalogue was published in 1781, in the 1784 issue of \"Connaissance des Temps\". The final list of Messier objects had grown to 103. On several occasions between 1921 and 1966, astronomers and historians discovered evidence of another seven objects that were observed either by Messier or by M\u00e9chain, shortly after the final version was published. These seven objects, M\u00a0104 through M\u00a0110, are accepted by astronomers as \"official\" Messier objects.\nThe objects' Messier designations, from M\u00a01 to M\u00a0110, are still used by professional and amateur astronomers today and their relative brightness makes them popular objects in the amateur astronomical community.\nLegacy.\nThe lunar crater Messier and the asteroid 7359 Messier were named in his honor."}
{"id": "7247", "revid": "1004911420", "url": "https://en.wikipedia.org/wiki?curid=7247", "title": "Cemetery H culture", "text": "The Cemetery H culture was a Bronze Age culture in the Punjab region in the northern part of the Indian subcontinent, from about 1900 BC until about 1300 BC. It is regarded as a regional form of the late phase of the Harappan (Indus Valley) civilisation (alongside the Jhukar culture of Sindh and Rangpur culture of Gujarat), but also as the manifestation of a first wave of Indo-Aryan migrations, predating the migrations of the proto-Rig Vedic people.\nOrigins.\nThe Cemetery H culture was located in and around the Punjab region in present-day India and Pakistan. It was named after a cemetery found in \"area H\" at Harappa. Remains of the culture have been dated from about 1900 BC until about 1300 BC.\nAccording to Rafique Mughal, the Cemetery H culture developed out of the northern part of the Indus Valley Civilization around 1700 BC, being part of the Punjab Phase, one of three cultural phases that developed in the Localization Era or \"Late Harappan phase\" of the Indus Valley Tradition. According to Kenoyer, the Cemetery H culture \"may only reflect a change in the focus of settlement organization from that which was the pattern of the earlier Harappan phase and not cultural discontinuity, urban decay, invading aliens, or site abandonment, all of which have been suggested in the past.\" According to Kennedy and Mallory &amp; Adams, the Cemetery H culture also \"shows clear biological affinities\" with the earlier population of Harappa.\nSome traits of the Cemetery H culture have been associated with the Swat culture, which has been regarded as evidence of the Indo-Aryan movement toward the Indian subcontinent. According to Parpola, the Cemetery H culture represents a first wave of Indo-Aryan migration from as early as 1900 BC, which was followed by a migration to the Punjab c. 1700-1400 BC. According to Kochhar, the Swat IV co-founded the Harappan Cemetery H phase in Punjab (2000-1800 BC), while the Rigvedic Indo-Aryans of Swat V later absorbed the Cemetery H people and gave rise to the Painted Grey Ware culture (to 1400 BC).\nTogether with the Gandhara grave culture and the Ochre Coloured Pottery culture, the Cemetery H culture is considered by some scholars as a factor in the formation of the Vedic civilization.\nFeatures.\nThe distinguishing features of this culture include:\nSome of the designs painted on the Cemetery H funerary urns have been interpreted through the lens of Vedic mythology: for instance, peacocks with hollow bodies and a small human form inside, which has been interpreted as the souls of the dead, and a hound that can be seen as the hound of Yama, the god of death. This may indicate the introduction of new religious beliefs during this period, but the archaeological evidence does not support the hypothesis that the Cemetery H people were the destroyers of the Harappan cities.\nArchaeology.\nCremation in India is first attested in the Cemetery H culture, a practice previously described in the Vedas. The Rigveda contains a reference to the emerging practice, in RV 10.15.14, where the forefathers \"both cremated (\"agnidagdh\u00e1-\") and uncremated (\"\u00e1nagnidagdha-\")\" are invoked."}
{"id": "7248", "revid": "14448901", "url": "https://en.wikipedia.org/wiki?curid=7248", "title": "Corrado Gini", "text": "Corrado Gini (Motta di Livenza, 23 May 1884 \u2013 Rome, 13 March 1965) was an Italian statistician, demographer and sociologist who developed the Gini coefficient, a measure of the income inequality in a society. Gini was a proponent of organicism and applied it to nations.\nCareer.\nGini was born on May 23, 1884, in Motta di Livenza, near Treviso, into an old landed family. He entered the Faculty of Law at the University of Bologna, where in addition to law he studied mathematics, economics, and biology.\nGini's scientific work ran in two directions: towards the social sciences and towards statistics. His interests ranged well beyond the formal aspects of statistics\u2014to the laws that govern biological and social phenomena.\nHis first published work was \"Il sesso dal punto di vista statistico\" (1908). This work is a thorough review of the natal sex ratio, looking at past theories and at how new hypothesis fit the statistical data. In particular, it presents evidence that the tendency to produce one or the other sex of child is, to some extent, heritable.\nIn 1910, he acceded to the Chair of Statistics in the University of Cagliari and then at Padua in 1913.\nHe founded the statistical journal \"Metron\" in 1920, directing it until his death; it only accepted articles with practical applications.\nHe became a professor at the Sapienza University of Rome in 1925. At the University, he founded a lecture course on sociology, maintaining it until his retirement. He also set up the School of Statistics in 1928, and, in 1936, the Faculty of Statistical, Demographic and Actuarial Sciences.\nUnder fascism.\nIn 1926, he was appointed President of the Central Institute of Statistics in Rome. This he organised as a single centre for Italian statistical services. He was a close intimate of Mussolini throughout the 20s. He resigned from his position within the institute in 1932.\nIn 1927 he published a treatise entitled \"The Scientific Basis of Fascism\".\nIn 1929, Gini founded the Italian Committee for the Study of Population Problems (\"Comitato italiano per lo studio dei problemi della popolazione) \" which, two years later, organised the first Population Congress in Rome.\nA eugenicist apart from being a demographer, Gini led an expedition to survey Polish populations, among them the Karaites. Gini was throughout the 20s a supporter of fascism, and expressed his hope that Nazi Germany and Fascist Italy would emerge as victors in WW2. However, he never supported any measure of exclusion of the Jews.\nMilestones during the rest of his career include:\nItalian Unionist Movement.\nOn October 12, 1944, Gini joined with the Calabrian activist Santi Paladino, and fellow-statistician Ugo Damiani to found the Italian Unionist Movement, for which the emblem was the Stars and Stripes, the Italian flag and a world map. According to the three men, the Government of the United States should annex all free and democratic nations worldwide, thereby transforming itself into a world government, and allowing Washington, D.C. to maintain Earth in a perpetual condition of peace. The party existed up to 1948 but had little success and its aims were not supported by the United States.\nOrganicism and nations.\nGini was a proponent of organicism and saw nations as organic in nature. Gini shared the view held by Oswald Spengler that populations go through a cycle of birth, growth, and decay. Gini claimed that nations at a primitive level have a high birth rate, but, as they evolve, the upper classes birth rate drops while the lower class birth rate, while higher, will inevitably deplete as their stronger members emigrate, die in war, or enter into the upper classes. If a nation continues on this path without resistance, Gini claimed the nation would enter a final decadent stage where the nation would degenerate as noted by decreasing birth rate, decreasing cultural output, and the lack of imperial conquest. At this point, the decadent nation with its aging population can be overrun by a more youthful and vigorous nation. Gini's organicist theories of nations and natality are believed to have influenced policies of Italian Fascism.\nHonours.\nThe following honorary degrees were conferred upon him:"}
{"id": "7249", "revid": "41320891", "url": "https://en.wikipedia.org/wiki?curid=7249", "title": "Crankshaft", "text": "A crankshaft is a shaft driven by a crank mechanism, consisting of a series of cranks and crankpins to which the connecting rods of an engine are attached. It is a mechanical part able to perform a conversion between reciprocating motion and rotational motion. In a reciprocating engine, it translates reciprocating motion of the piston into rotational motion, whereas in a reciprocating compressor, it converts the rotational motion into reciprocating motion. In order to do the conversion between two motions, the crankshaft has \"crank throws\" or \"crankpins\", additional bearing surfaces whose axis is offset from that of the crank, to which the \"big ends\" of the connecting rods from each cylinder attach.\nIt is typically connected to a flywheel to reduce the pulsation characteristic of the four-stroke cycle, and sometimes a torsional or vibrational damper at the opposite end, to reduce the torsional vibrations often caused along the length of the crankshaft by the cylinders farthest from the output end acting on the torsional elasticity of the metal.\nHistory.\nCrank mechanism.\nHan China.\nThe earliest hand-operated cranks appeared in China during the Han Dynasty (202 BC-220 AD). They were used for silk-reeling, hemp-spinning, for the agricultural winnowing fan, in the water-powered flour-sifter, for hydraulic-powered metallurgic bellows, and in the well windlass. The rotary winnowing fan greatly increased the efficiency of separating grain from husks and stalks. However, the potential of the crank of converting circular motion into reciprocal motion never seems to have been fully realized in China, and the crank was typically absent from such machines until the turn of the 20th century.\nRoman Empire.\nA crank in the form of an eccentrically-mounted handle of the rotary handmill appeared in 5th century BC Celtiberian Spain and ultimately spread across the Roman Empire. A Roman iron crank dating to the 2nd century AD was excavated in Augusta Raurica, Switzerland. The crank-operated Roman mill is dated to the late 2nd century.\nEvidence for the crank combined with a connecting rod appears in the Hierapolis mill, dating to the 3rd century; they are also found in stone sawmills in Roman Syria and Ephesus dating to the 6th century. The pediment of the Hierapolis mill shows a waterwheel fed by a mill race powering via a gear train two frame saws which cut blocks by the way of some kind of connecting rods and cranks. The crank and connecting rod mechanisms of the other two archaeologically-attested sawmills worked without a gear train. Water-powered marble saws in Germany were mentioned by the late 4th century poet Ausonius; about the same time, these mill types seem also to be indicated by Gregory of Nyssa from Anatolia.\nMedieval Europe.\nA rotary grindstone operated by a crank handle is shown in the Carolingian manuscript \"Utrecht Psalter\"; the pen drawing of around 830 goes back to a late antique original. Cranks used to turn wheels are also depicted or described in various works dating from the tenth to thirteenth centuries.\nThe first depictions of the compound crank in the carpenter's brace appear between 1420 and 1430 in northern European artwork. The rapid adoption of the compound crank can be traced in the works of an unknown German engineer writing on the state of military technology during the Hussite Wars: first, the connecting-rod, applied to cranks, reappeared; second, double-compound cranks also began to be equipped with connecting-rods; and third, the flywheel was employed for these cranks to get them over the 'dead-spot'. The concept was much improved by the Italian engineer and writer Roberto Valturio in 1463, who devised a boat with five sets, where the parallel cranks are all joined to a single power source by one connecting-rod, an idea also taken up by his compatriot Italian painter Francesco di Giorgio.\nThe crank had become common in Europe by the early 15th century, as seen in the works of the military engineer Konrad Kyeser (1366\u2013after 1405). Devices depicted in Kyeser's \"Bellifortis\" include cranked windlasses for spanning siege crossbows, cranked chain of buckets for water-lifting and cranks fitted to a wheel of bells. Kyeser also equipped the Archimedes' screws for water-raising with a crank handle, an innovation which subsequently replaced the ancient practice of working the pipe by treading.\nPisanello painted a piston-pump driven by a water-wheel and operated by two simple cranks and two connecting-rods.\nThe 15th also century saw the introduction of cranked rack-and-pinion devices, called cranequins, which were fitted to the crossbow's stock as a means of exerting even more force while spanning the missile weapon. In the textile industry, cranked reels for winding skeins of yarn were introduced.\nCrankshaft.\nMedieval Near East.\nThe non-manual crank appears in several of the hydraulic devices described by the Ban\u016b M\u016bs\u0101 brothers in their 9th-century \"Book of Ingenious Devices\". These automatically operated cranks appear in several devices, two of which contain an action which approximates to that of a crankshaft, anticipating Al-Jazari's invention by several centuries and its first appearance in Europe by over five centuries. However, the automatic crank described by the Banu Musa would not have allowed a full rotation, but only a small modification was required to convert it to a crankshaft.\nArab engineer Al-Jazari (1136\u20131206), in the Artuqid Sultanate, described a crank and connecting rod system in a rotating machine in two of his water-raising machines. The author Sally Ganchy identified a crankshaft in his twin-cylinder pump mechanism, including both the crank and shaft mechanisms. According to historian Donald Routledge Hill, Al-Jazari invented the crankshaft.\nRenaissance Europe.\nThe Italian physician Guido da Vigevano (c. 1280\u22121349), planning for a new crusade, made illustrations for a paddle boat and war carriages that were propelled by manually turned compound cranks and gear wheels, identified as an early crankshaft prototype by Lynn Townsend White. The \"Luttrell Psalter\", dating to around 1340, describes a grindstone which was rotated by two cranks, one at each end of its axle; the geared hand-mill, operated either with one or two cranks, appeared later in the 15th century.\nAround 1480, the early medieval rotary grindstone was improved with a treadle and crank mechanism. Cranks mounted on push-carts first appear in a German engraving of 1589. Crankshafts were also described by Leonardo da Vinci (1452\u20131519) and a Dutch farmer and windmill owner by the name Cornelis Corneliszoon van Uitgeest in 1592. His wind-powered sawmill used a crankshaft to convert a windmill's circular motion into a back-and-forward motion powering the saw. Corneliszoon was granted a patent for his crankshaft in 1597.\nModern Europe.\nFrom the 16th century onwards, evidence of cranks and connecting rods integrated into machine design becomes abundant in the technological treatises of the period: Agostino Ramelli's \"The Diverse and Artifactitious Machines\" of 1588 depicts eighteen examples, a number that rises in the \"Theatrum Machinarum Novum\" by Georg Andreas B\u00f6ckler to 45 different machines. Cranks were formerly common on some machines in the early 20th century; for example almost all phonographs before the 1930s were powered by clockwork motors wound with cranks. Reciprocating piston engines use cranks to convert the linear piston motion into rotational motion. Internal combustion engines of early 20th century automobiles were usually started with hand cranks, before electric starters came into general use. The 1918 Reo owner's manual describes how to hand crank the automobile:\nInternal combustion engines.\nLarge engines are usually multicylinder to reduce pulsations from individual firing strokes, with more than one piston attached to a complex crankshaft. Many small engines, such as those found in mopeds or garden machinery, are single cylinder and use only a single piston, simplifying crankshaft design.\nA crankshaft is subjected to enormous stresses, potentially equivalent of several tonnes of force. The crankshaft is connected to the fly-wheel (used to smooth out shock and convert energy to torque), the engine block, using bearings on the main journals, and to the pistons via their respective con-rods. An engine loses up to 75% of its generated energy in the form of friction, noise and vibration in the crankcase and piston area. The remaining losses occur in the valvetrain (timing chains, belts, pulleys, camshafts, lobes, valves, seals etc.) heat and blow by.\nBearings.\nThe crankshaft has a linear axis about which it rotates, typically with several bearing journals riding on replaceable bearings (the main bearings) held in the engine block. As the crankshaft undergoes a great deal of sideways load from each cylinder in a multicylinder engine, it must be supported by several such bearings, not just one at each end. This was a factor in the rise of V8 engines, with their shorter crankshafts, in preference to straight-8 engines. The long crankshafts of the latter suffered from an unacceptable amount of flex when engine designers began using higher compression ratios and higher rotational speeds. High performance engines often have more main bearings than their lower performance cousins for this reason.\nPiston stroke.\nThe distance the axis of the crank throws from the axis of the crankshaft determines the piston stroke measurement, and thus engine displacement. A common way to increase the low-speed torque of an engine is to increase the stroke, sometimes known as \"shaft-stroking.\" This also increases the reciprocating vibration, however, limiting the high speed capability of the engine. In compensation, it improves the low speed operation of the engine, as the longer intake stroke through smaller valve(s) results in greater turbulence and mixing of the intake charge. Most modern high speed production engines are classified as \"over square\" or short-stroke, wherein the stroke is less than the diameter of the cylinder bore. As such, finding the proper balance between shaft-stroking speed and length leads to better results.\nEngine configuration.\nThe configuration, meaning the number of pistons and their placement in relation to each other leads to straight, V or flat engines. The same basic engine block can sometimes be used with different crankshafts, however, to alter the firing order. For instance, the 90\u00b0 V6 engine configuration, in older days sometimes derived by using six cylinders of a V8 engine with a 3 throw crankshaft, produces an engine with an inherent pulsation in the power flow due to the \"gap\" between the firing pulses alternates between short and long pauses because the 90 degree engine block does not correspond to the 120 degree spacing of the crankshaft. The same engine, however, can be made to provide evenly spaced power pulses by using a crankshaft with an individual crank throw for each cylinder, spaced so that the pistons are actually phased 120\u00b0 apart, as in the GM 3800 engine. While most production V8 engines use four crank throws spaced 90\u00b0 apart, high-performance V8 engines often use a \"flat\" crankshaft with throws spaced 180\u00b0 apart, essentially resulting in two straight four engines running on a common crankcase. The difference can be heard as the flat-plane crankshafts result in the engine having a smoother, higher-pitched sound than cross-plane (for example, IRL IndyCar Series compared to NASCAR Sprint Cup Series, or a Ferrari 355 compared to a Chevrolet Corvette). This type of crankshaft was also used on early types of V8 engines. See the main article on crossplane crankshafts.\nEngine balance.\nFor some engines it is necessary to provide counterweights for the reciprocating mass of each piston and connecting rod to improve engine balance. These are typically cast as part of the crankshaft but, occasionally, are bolt-on pieces. While counter weights add a considerable amount of weight to the crankshaft, it provides a smoother running engine and allows higher RPM levels to be reached.\nFlying arms.\nIn some engine configurations, the crankshaft contains direct links between adjacent crank pins, without the usual intermediate main bearing. These links are called \"flying arms\". This arrangement is sometimes used in V6 and V8 engines, as it enables the engine to be designed with different V angles than what would otherwise be required to create an even firing interval, while still using fewer main bearings than would normally be required with a single piston per crankthrow. This arrangement reduces weight and engine length at the expense of less crankshaft rigidity.\nRotary aircraft engines.\nSome early aircraft engines were a rotary engine design, where the crankshaft was fixed to the airframe and instead the cylinders rotated with the propeller.\nRadial engines.\nThe radial engine is a reciprocating type internal combustion engine configuration in which the cylinders point outward from a central crankshaft like the spokes of a wheel. It resembles a stylized star when viewed from the front, and is called a \"star engine\" (German Sternmotor, French Moteur en \u00e9toile) in some languages. The radial configuration was very commonly used in aircraft engines before turbine engines became predominant.\nConstruction.\nCrankshafts can be monolithic (made in a single piece) or assembled from several pieces. Monolithic crankshafts are most common, but some smaller and larger engines use assembled crankshafts.\nForging and casting, and machining.\nCrankshafts can be forged from a steel bar usually through roll forging or cast in ductile steel. Today more and more manufacturers tend to favor the use of forged crankshafts due to their lighter weight, more compact dimensions and better inherent damping. With forged crankshafts, vanadium microalloyed steels are mostly used as these steels can be air cooled after reaching high strengths without additional heat treatment, with exception to the surface hardening of the bearing surfaces. The low alloy content also makes the material cheaper than high alloy steels. Carbon steels are also used, but these require additional heat treatment to reach the desired properties. Cast iron crankshafts are today mostly found in cheaper production engines (such as those found in the Ford Focus diesel engines) where the loads are lower. Some engines also use cast iron crankshafts for low output versions while the more expensive high output version use forged steel.\nCrankshafts can also be machined out of a billet, often a bar of high quality vacuum remelted steel. Though the fiber flow (local inhomogeneities of the material's chemical composition generated during casting) does not follow the shape of the crankshaft (which is undesirable), this is usually not a problem since higher quality steels, which normally are difficult to forge, can be used. These crankshafts tend to be very expensive due to the large amount of material that must be removed with lathes and milling machines, the high material cost, and the additional heat treatment required. However, since no expensive tooling is needed, this production method allows small production runs without high costs.\nIn an effort to reduce costs, used crankshafts may also be machined. A good core may often be easily reconditioned by a crankshaft grinding process. Severely damaged crankshafts may also be repaired with a welding operation, prior to grinding, that utilizes a submerged arc welding machine. To accommodate the smaller journal diameters a ground crankshaft has, and possibly an oversized thrust dimension, undersize engine bearings are used to allow for precise clearances during operation.\nMachining or remanufacturing crankshafts are precision machined to exact tolerances with no odd size crankshaft bearings or journals. Thrust surfaces are micro-polished to provide precise surface finishes for smooth engine operation and reduced thrust bearing wear. Every journal is inspected and measured with critical accuracy. After machining, oil holes are chamfered to improve lubrication and every journal polished to a smooth finish for long bearing life. Remanufactured crankshafts are thoroughly cleaned with special emphasis to flushing and brushing out oil passages to remove any contaminants. Remanufacturing a crankshaft typically involves the following steps:\nStress on crankshafts.\nThe shaft is subjected to various forces but generally needs to be analysed in two positions. Firstly, failure may occur at the position of maximum bending; this may be at the centre of the crank or at either end. In such a condition the failure is due to bending and the pressure in the cylinder is maximal. Second, the crank may fail due to twisting, so the conrod needs to be checked for shear at the position of maximal twisting. The pressure at this position is the maximal pressure, but only a fraction of maximal pressure.\"\"\nCounter-rotating crankshafts.\nIn a conventional piston-crank arrangement in an engine or compressor, a piston is connected to a crankshaft by a connecting rod. As the piston moves through its stroke, the connecting rod varies its angle to the direction of motion of the piston and as the connecting rod is free to rotate at its connection to both the piston and crankshaft, no torque is transmitted by the connecting rod and forces transmitted by the connecting rod are transmitted along the longitudinal axis of the connecting rod. The force exerted by the piston on the connecting rod results in a reaction force exerted by the connecting rod back on the piston. When the connecting rod makes an angle to the direction of motion of the piston, the reaction force exerted by the connecting rod on the piston has a lateral component. This lateral force pushes the piston sideways against the cylinder wall. As the piston moves within the cylinder, this lateral force causes additional friction between the piston and cylinder wall. Friction accounts for approximately 20% of all losses in an internal combustion engine, of which approximately 50% is due to piston cylinder friction \nIn a paired counter-rotating crankshaft arrangement, each piston is connected to two crankshafts so lateral forces due to the angle of the connecting rods cancel each other out. This reduces piston-cylinder friction and therefore fuel consumption. The symmetrical arrangement reduces the requirement for counterweights, reducing overall mass and making it easier for the engine to accelerate and decelerate. It also eliminates engine rocking and torque effects. Several counter-rotating crankshaft arrangements have been patented, for example US2010/0263621. An early example of a counter-rotating crankshaft arrangement is the Lanchester flat-twin engine."}
{"id": "7250", "revid": "1013170869", "url": "https://en.wikipedia.org/wiki?curid=7250", "title": "CNS", "text": "CNS may refer to:"}
{"id": "7251", "revid": "41400767", "url": "https://en.wikipedia.org/wiki?curid=7251", "title": "Central nervous system", "text": "The central nervous system (CNS) is the part of the nervous system consisting primarily of the brain and spinal cord. The CNS is so named because the brain integrates the received information and coordinates and influences the activity of all parts of the bodies of bilaterally symmetric animals\u2014i.e., all multicellular animals except sponges and jellyfish. It consists of a large nerve running from the anterior to the posterior, with the anterior end is enlarged into the brain. Not all animals with a central nervous system have a brain, although the large majority do.\nThe rest of this article exclusively discusses the vertebrate central nervous system, which is radically distinct from all other animals.\nOverview.\nIn vertebrates the brain and spinal cord are both enclosed in the meninges. The meninges provide a a barrier to chemicals dissolved in the blood, protecting the brain from most neurotoxins commonly found in food. Within the meninges the brain and spinal cord are bathed in cerebral spinal fluid which replaces the body fluid found outside the cells of all bilateral animals. \nIn vertebrates the CNS is contained within the dorsal body cavity, with the brain is housed in the cranial cavity within the skull, and the spinal cord is housed in the spinal canal within the vertebrae. Within the CNS, the interneuronal space is filled with a large amount of supporting non-nervous cells called neuroglia or glia from the Greek for \"glue\".\nIn vertebrates the CNS also includes the retina and the optic nerve (cranial nerve II), as well as the olfactory nerves and olfactory epithelium. As parts of the CNS, they connect directly to brain neurons without intermediate ganglia. The olfactory epithelium is the only central nervous tissue outside the meninges in direct contact with the environment, which opens up a pathway for therapeutic agents which cannot otherwise cross the meninges barrier. \nStructure.\nThe CNS consists of the two major structures: the brain and spinal cord. The brain is encased in the skull, and protected by the cranium. The spinal cord is continuous with the brain and lies caudally to the brain. It is protected by the vertebrae. The spinal cord reaches from the base of the skull, continues through or starting below the foramen magnum, and terminates roughly level with the first or second lumbar vertebra, occupying the upper sections of the vertebral canal.\nWhite and gray matter.\nMicroscopically, there are differences between the neurons and tissue of the CNS and the peripheral nervous system (PNS). The CNS is composed of white and gray matter. This can also be seen macroscopically on brain tissue. The white matter consists of axons and oligodendrocytes, while the gray matter consists of neurons and unmyelinated fibers. Both tissues include a number of glial cells (although the white matter contains more), which are often referred to as supporting cells of the CNS. Different forms of glial cells have different functions, some acting almost as scaffolding for neuroblasts to climb during neurogenesis such as bergmann glia, while others such as microglia are a specialized form of macrophage, involved in the immune system of the brain as well as the clearance of various metabolites from the brain tissue. Astrocytes may be involved with both clearance of metabolites as well as transport of fuel and various beneficial substances to neurons from the capillaries of the brain. Upon CNS injury astrocytes will proliferate, causing gliosis, a form of neuronal scar tissue, lacking in functional neurons.\nThe brain (cerebrum as well as midbrain and hindbrain) consists of a cortex, composed of neuron-bodies constituting gray matter, while internally there is more white matter that form tracts and commissures. Apart from cortical gray matter there is also subcortical gray matter making up a large number of different nuclei.\nSpinal cord.\nFrom and to the spinal cord are projections of the peripheral nervous system in the form of spinal nerves (sometimes segmental nerves). The nerves connect the spinal cord to skin, joints, muscles etc. and allow for the transmission of efferent motor as well as afferent sensory signals and stimuli. This allows for voluntary and involuntary motions of muscles, as well as the perception of senses.\nAll in all 31 spinal nerves project from the brain stem, some forming plexa as they branch out, such as the brachial plexa, sacral plexa etc. Each spinal nerve will carry both sensory and motor signals, but the nerves synapse at different regions of the spinal cord, either from the periphery to sensory relay neurons that relay the information to the CNS or from the CNS to motor neurons, which relay the information out.\nThe spinal cord relays information up to the brain through spinal tracts through the final common pathway to the thalamus and ultimately to the cortex.\nCranial nerves.\nApart from the spinal cord, there are also peripheral nerves of the PNS that synapse through intermediaries or ganglia directly on the CNS. These 12 nerves exist in the head and neck region and are called cranial nerves. Cranial nerves bring information to the CNS to and from the face, as well as to certain muscles (such as the trapezius muscle, which is innervated by accessory nerves as well as certain cervical spinal nerves).\nTwo pairs of cranial nerves; the olfactory nerves and the optic nerves are often considered structures of the CNS. This is because they do not synapse first on peripheral ganglia, but directly on CNS neurons. The olfactory epithelium is significant in that it consists of CNS tissue expressed in direct contact to the environment, allowing for administration of certain pharmaceuticals and drugs.\nBrain.\nAt the anterior end of the spinal cord lies the brain. The brain makes up the largest portion of the CNS. It is often the main structure referred to when speaking of the nervous system in general. The brain is the major functional unit of the CNS. While the spinal cord has certain processing ability such as that of spinal locomotion and can process reflexes, the brain is the major processing unit of the nervous system.\nBrainstem.\nThe brainstem consists of the medulla, the pons and the midbrain. The medulla can be referred to as an extension of the spinal cord, which both have similar organization and functional properties. The tracts passing from the spinal cord to the brain pass through here.\nRegulatory functions of the medulla nuclei include control of blood pressure and breathing. Other nuclei are involved in balance, taste, hearing, and control of muscles of the face and neck.\nThe next structure rostral to the medulla is the pons, which lies on the ventral anterior side of the brainstem. Nuclei in the pons include pontine nuclei which work with the cerebellum and transmit information between the cerebellum and the cerebral cortex.\nIn the dorsal posterior pons lie nuclei that are involved in the functions of breathing, sleep, and taste.\nThe midbrain, or mesencephalon, is situated above and rostral to the pons. It includes nuclei linking distinct parts of the motor system, including the cerebellum, the basal ganglia and both cerebral hemispheres, among others. Additionally, parts of the visual and auditory systems are located in the midbrain, including control of automatic eye movements.\nThe brainstem at large provides entry and exit to the brain for a number of pathways for motor and autonomic control of the face and neck through cranial nerves, Autonomic control of the organs is mediated by the tenth cranial nerve. A large portion of the brainstem is involved in such autonomic control of the body. Such functions may engage the heart, blood vessels, and pupils, among others.\nThe brainstem also holds the reticular formation, a group of nuclei involved in both arousal and alertness.\nCerebellum.\nThe cerebellum lies behind the pons. The cerebellum is composed of several dividing fissures and lobes. Its function includes the control of posture and the coordination of movements of parts of the body, including the eyes and head, as well as the limbs. Further, it is involved in motion that has been learned and perfected through practice, and it will adapt to new learned movements.\nDespite its previous classification as a motor structure, the cerebellum also displays connections to areas of the cerebral cortex involved in language and cognition. These connections have been shown by the use of medical imaging techniques, such as functional MRI and Positron emission tomography.\nThe body of the cerebellum holds more neurons than any other structure of the brain, including that of the larger cerebrum, but is also more extensively understood than other structures of the brain, as it includes fewer types of different neurons. It handles and processes sensory stimuli, motor information, as well as balance information from the vestibular organ.\nDiencephalon.\nThe two structures of the diencephalon worth noting are the thalamus and the hypothalamus. The thalamus acts as a linkage between incoming pathways from the peripheral nervous system as well as the optical nerve (though it does not receive input from the olfactory nerve) to the cerebral hemispheres. Previously it was considered only a \"relay station\", but it is engaged in the sorting of information that will reach cerebral hemispheres (neocortex).\nApart from its function of sorting information from the periphery, the thalamus also connects the cerebellum and basal ganglia with the cerebrum. In common with the aforementioned reticular system the thalamus is involved in wakefullness and consciousness, such as though the SCN.\nThe hypothalamus engages in functions of a number of primitive emotions or feelings such as hunger, thirst and maternal bonding. This is regulated partly through control of secretion of hormones from the pituitary gland. Additionally the hypothalamus plays a role in motivation and many other behaviors of the individual.\nCerebrum.\nThe cerebrum of cerebral hemispheres make up the largest visual portion of the human brain. Various structures combine to form the cerebral hemispheres, among others: the cortex, basal ganglia, amygdala and hippocampus. The hemispheres together control a large portion of the functions of the human brain such as emotion, memory, perception and motor functions. Apart from this the cerebral hemispheres stand for the cognitive capabilities of the brain.\nConnecting each of the hemispheres is the corpus callosum as well as several additional commissures.\nOne of the most important parts of the cerebral hemispheres is the cortex, made up of gray matter covering the surface of the brain. Functionally, the cerebral cortex is involved in planning and carrying out of everyday tasks.\nThe hippocampus is involved in storage of memories, the amygdala plays a role in perception and communication of emotion, while the basal ganglia play a major role in the coordination of voluntary movement.\nDifference from the peripheral nervous system.\nThis differentiates the CNS from the PNS, which consists of neurons, axons, and Schwann cells. Oligodendrocytes and Schwann cells have similar functions in the CNS and PNS, respectively. Both act to add myelin sheaths to the axons, which acts as a form of insulation allowing for better and faster proliferation of electrical signals along the nerves. Axons in the CNS are often very short, barely a few millimeters, and do not need the same degree of isolation as peripheral nerves. Some peripheral nerves can be over 1 meter in length, such as the nerves to the big toe. To ensure signals move at sufficient speed, myelination is needed.\nThe way in which the Schwann cells and oligodendrocytes myelinate nerves differ. A Schwann cell usually myelinates a single axon, completely surrounding it. Sometimes, they may myelinate many axons, especially when in areas of short axons. Oligodendrocytes usually myelinate several axons. They do this by sending out thin projections of their cell membrane, which envelop and enclose the axon.\nDevelopment.\nDuring early development of the vertebrate embryo, a longitudinal groove on the neural plate gradually deepens and the ridges on either side of the groove (the neural folds) become elevated, and ultimately meet, transforming the groove into a closed tube called the neural tube. The formation of the neural tube is called neurulation. At this stage, the walls of the neural tube contain proliferating neural stem cells in a region called the ventricular zone. The neural stem cells, principally radial glial cells, multiply and generate neurons through the process of neurogenesis, forming the rudiment of the CNS.\nThe neural tube gives rise to both brain and spinal cord. The anterior (or 'rostral') portion of the neural tube initially differentiates into three brain vesicles (pockets): the prosencephalon at the front, the mesencephalon, and, between the mesencephalon and the spinal cord, the rhombencephalon. (By six weeks in the human embryo) the prosencephalon then divides further into the telencephalon and diencephalon; and the rhombencephalon divides into the metencephalon and myelencephalon. The spinal cord is derived from the posterior or 'caudal' portion of the neural tube.\nAs a vertebrate grows, these vesicles differentiate further still. The telencephalon differentiates into, among other things, the striatum, the hippocampus and the neocortex, and its cavity becomes the first and second ventricles. Diencephalon elaborations include the subthalamus, hypothalamus, thalamus and epithalamus, and its cavity forms the third ventricle. The tectum, pretectum, cerebral peduncle and other structures develop out of the mesencephalon, and its cavity grows into the mesencephalic duct (cerebral aqueduct). The metencephalon becomes, among other things, the pons and the cerebellum, the myelencephalon forms the medulla oblongata, and their cavities develop into the fourth ventricle.\nEvolution.\nPlanaria.\nPlanarians, members of the phylum Platyhelminthes (flatworms), have the simplest, clearly defined delineation of a nervous system into a CNS and a PNS.\nTheir primitive brains, consisting of two fused anterior ganglia, and longitudinal nerve cords form the CNS. Like vertebrates, have a distinct CNS and PNS. The nerves projecting laterally from the CNS form their PNS.\nA molecular study found that more than 95% of the 116 genes involved in the nervous system of planarians, which includes genes related to the CNS, also exist in humans. \nArthropoda.\nIn arthropods, the ventral nerve cord, the subesophageal ganglia and the supraesophageal ganglia are usually seen as making up the CNS. Arthropoda, unlike vertebrates, have inhibitory motor neurons due to their small size. \nChordata.\nThe CNS of chordates differs from that of other animals in being placed dorsally in the body, above the gut and notochord/spine. The basic pattern of the CNS is highly conserved throughout the different species of vertebrates and during evolution. The major trend that can be observed is towards a progressive telencephalisation: the telencephalon of reptiles is only an appendix to the large olfactory bulb, while in mammals it makes up most of the volume of the CNS. In the human brain, the telencephalon covers most of the diencephalon and the mesencephalon. Indeed, the allometric study of brain size among different species shows a striking continuity from rats to whales, and allows us to complete the knowledge about the evolution of the CNS obtained through cranial endocasts.\nMammals.\nMammals \u2013 which appear in the fossil record after the first fishes, amphibians, and reptiles \u2013 are the only vertebrates to possess the evolutionarily recent, outermost part of the cerebral cortex known as the neocortex.\nThe neocortex of monotremes (the duck-billed platypus and several species of spiny anteaters) and of marsupials (such as kangaroos, koalas, opossums, wombats, and Tasmanian devils) lack the convolutions \u2013 gyri and sulci \u2013 found in the neocortex of most placental mammals (eutherians).\nWithin placental mammals, the size and complexity of the neocortex increased over time. The area of the neocortex of mice is only about 1/100 that of monkeys, and that of monkeys is only about 1/10 that of humans. In addition, rats lack convolutions in their neocortex (possibly also because rats are small mammals), whereas cats have a moderate degree of convolutions, and humans have quite extensive convolutions. Extreme convolution of the neocortex is found in dolphins, possibly related to their complex echolocation.\nClinical significance.\nDiseases.\nThere are many CNS diseases and conditions, including infections such as encephalitis and poliomyelitis, early-onset neurological disorders including ADHD and autism, late-onset neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and essential tremor, autoimmune and inflammatory diseases such as multiple sclerosis and acute disseminated encephalomyelitis, genetic disorders such as Krabbe's disease and Huntington's disease, as well as amyotrophic lateral sclerosis and adrenoleukodystrophy. Lastly, cancers of the central nervous system can cause severe illness and, when malignant, can have very high mortality rates. Symptoms depend on the size, growth rate, location and malignancy of tumors and can include alterations in motor control, hearing loss, headaches and changes in cognitive ability and autonomic functioning.\nSpecialty professional organizations recommend that neurological imaging of the brain be done only to answer a specific clinical question and not as routine screening."}
{"id": "7252", "revid": "2039422", "url": "https://en.wikipedia.org/wiki?curid=7252", "title": "Cell cycle", "text": "The cell cycle, or cell-division cycle, is the series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA (DNA replication) and some of its organelles, and subsequently the partitioning of its cytoplasm and other components into two daughter cells in a process called cell division.\nIn cells with nuclei (eukaryotes), (i.e., animal, plant, fungal, and protist cells), the cell cycle is divided into two main stages: interphase and the mitotic (M) phase (including mitosis and cytokinesis). During interphase, the cell grows, accumulating nutrients needed for mitosis, and replicates its DNA and some of its organelles. During the mitotic phase, the replicated chromosomes, organelles, and cytoplasm separate into two new daughter cells. To ensure the proper replication of cellular components and division, there are control mechanisms known as cell cycle checkpoints after each of the key steps of the cycle that determine if the cell can progress to the next phase.\nIn cells without nuclei (prokaryotes), (i.e., bacteria and archaea), the cell cycle is divided into the B, C, and D periods. The B period extends from the end of cell division to the beginning of DNA replication. DNA replication occurs during the C period. The D period refers to the stage between the end of DNA replication and the splitting of the bacterial cell into two daughter cells.\nThe cell-division cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of the cell division.\nPhases.\nThe eukaryotic cell cycle consists of four distinct phases: G1 phase, S phase (synthesis), G2 phase (collectively known as interphase) and M phase (mitosis and cytokinesis). M phase is itself composed of two tightly coupled processes: mitosis, in which the cell's nucleus divides, and cytokinesis, in which the cell's cytoplasm divides forming two daughter cells. Activation of each phase is dependent on the proper progression and completion of the previous one. Cells that have temporarily or reversibly stopped dividing are said to have entered a state of quiescence called G0 phase.\nAfter cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of cell division.\nG0 phase (quiescence).\nG0 is a resting phase where the cell has left the cycle and has stopped dividing. The cell cycle starts with this phase. Non-proliferative (non-dividing) cells in multicellular eukaryotes generally enter the quiescent G0 state from G1 and may remain quiescent for long periods of time, possibly indefinitely (as is often the case for neurons). This is very common for cells that are fully differentiated. Some cells enter the G0 phase semi-permanently and are considered post-mitotic, e.g., some liver, kidney, and stomach cells. Many cells do not enter G0 and continue to divide throughout an organism's life, e.g., epithelial cells.\nThe word \"post-mitotic\" is sometimes used to refer to both quiescent and senescent cells. Cellular senescence occurs in response to DNA damage and external stress and usually constitutes an arrest in G1. Cellular senescence may make a cell's progeny nonviable; it is often a biochemical alternative to the self-destruction of such a damaged cell by apoptosis.\nInterphase.\nInterphase is a series of changes that takes place in a newly formed cell and its nucleus before it becomes capable of division again. It is also called preparatory phase or intermitosis. Typically interphase lasts for at least 91% of the total time required for the cell cycle.\nInterphase proceeds in three stages, G1, S, and G2, followed by the cycle of mitosis and cytokinesis. The cell's nuclear DNA contents are duplicated during S phase.\nG1 phase (First growth phase or Post mitotic gap phase).\nThe first phase within interphase, from the end of the previous M phase until the beginning of DNA synthesis, is called G1 (G indicating \"gap\"). It is also called the growth phase. During this phase, the biosynthetic activities of the cell, which are considerably slowed down during M phase, resume at a high rate. The duration of G1 is highly variable, even among different cells of the same species. In this phase, the cell increases its supply of proteins, increases the number of organelles (such as mitochondria, ribosomes), and grows in size. In G1 phase, a cell has three options. \nThe deciding point is called check point (Restriction point). This check point is called the restriction point or START and is regulated by G1/S cyclins, which cause transition from G1 to S phase. Passage through the G1 check point commits the cell to division.\nS phase (DNA replication).\nThe ensuing S phase starts when DNA synthesis commences; when it is complete, all of the chromosomes have been replicated, i.e., each chromosome consists of two sister chromatids. Thus, during this phase, the amount of DNA in the cell has doubled, though the ploidy and number of chromosomes are unchanged. Rates of RNA transcription and protein synthesis are very low during this phase. An exception to this is histone production, most of which occurs during the S phase.\nG2 phase (growth).\nG2 phase occurs after DNA replication and is a period of protein synthesis and rapid cell growth to prepare the cell for mitosis.\nDuring this phase microtubules begin to reorganize to form a spindle (preprophase). Before proceeding to mitotic phase, cells must be checked at the G2 checkpoint for any DNA damage within the chromosomes. The G2 checkpoint is mainly regulated by the tumor protein p53. If the DNA is damaged, p53 will either repair the DNA or trigger the apoptosis of the cell. If p53 is dysfunctional or mutated, cells with damaged DNA may continue through the cell cycle, leading to the development of cancer.\nMitotic phase (chromosome separation).\nThe relatively brief \"M phase\" consists of nuclear division (karyokinesis). It is a relatively short period of the cell cycle. M phase is complex and highly regulated. The sequence of events is divided into phases, corresponding to the completion of one set of activities and the start of the next. These phases are sequentially known as:\nMitosis is the process by which a eukaryotic cell separates the chromosomes in its cell nucleus into two identical sets in two nuclei. During the process of mitosis the pairs of chromosomes condense and attach to microtubules that pull the sister chromatids to opposite sides of the cell.\nMitosis occurs exclusively in eukaryotic cells, but occurs in different ways in different species. For example, animal cells undergo an \"open\" mitosis, where the nuclear envelope breaks down before the chromosomes separate, while fungi such as \"Aspergillus nidulans\" and \"Saccharomyces cerevisiae\" (yeast) undergo a \"closed\" mitosis, where chromosomes divide within an intact cell nucleus.\nCytokinesis phase (separation of all cell components).\nMitosis is immediately followed by cytokinesis, which divides the nuclei, cytoplasm, organelles and cell membrane into two cells containing roughly equal shares of these cellular components. Mitosis and cytokinesis together define the division of the mother cell into two daughter cells, genetically identical to each other and to their parent cell. This accounts for approximately 10% of the cell cycle.\nBecause cytokinesis usually occurs in conjunction with mitosis, \"mitosis\" is often used interchangeably with \"M phase\". However, there are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei in a process called endoreplication. This occurs most notably among the fungi and slime molds, but is found in various groups. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development. Errors in mitosis can result in cell death through apoptosis or cause mutations that may lead to cancer.\nRegulation of eukaryotic cell cycle.\nRegulation of the cell cycle involves processes crucial to the survival of a cell, including the detection and repair of genetic damage as well as the prevention of uncontrolled cell division. The molecular events that control the cell cycle are ordered and directional; that is, each process occurs in a sequential fashion and it is impossible to \"reverse\" the cycle.\nRole of cyclins and CDKs.\nTwo key classes of regulatory molecules, cyclins and cyclin-dependent kinases (CDKs), determine a cell's progress through the cell cycle. Leland H. Hartwell, R. Timothy Hunt, and Paul M. Nurse won the 2001 Nobel Prize in Physiology or Medicine for their discovery of these central molecules. Many of the genes encoding cyclins and CDKs are conserved among all eukaryotes, but in general more complex organisms have more elaborate cell cycle control systems that incorporate more individual components. Many of the relevant genes were first identified by studying yeast, especially \"Saccharomyces cerevisiae\"; genetic nomenclature in yeast dubs many of these genes \"cdc\" (for \"cell division cycle\") followed by an identifying number, e.g. \"cdc25\" or \"cdc20\".\nCyclins form the regulatory subunits and CDKs the catalytic subunits of an activated heterodimer; cyclins have no catalytic activity and CDKs are inactive in the absence of a partner cyclin. When activated by a bound cyclin, CDKs perform a common biochemical reaction called phosphorylation that activates or inactivates target proteins to orchestrate coordinated entry into the next phase of the cell cycle. Different cyclin-CDK combinations determine the downstream proteins targeted. CDKs are constitutively expressed in cells whereas cyclins are synthesised at specific stages of the cell cycle, in response to various molecular signals.\nGeneral mechanism of cyclin-CDK interaction.\nUpon receiving a pro-mitotic extracellular signal, G1 cyclin-CDK complexes become active to prepare the cell for S phase, promoting the expression of transcription factors that in turn promote the expression of S cyclins and of enzymes required for DNA replication. The G1 cyclin-CDK complexes also promote the degradation of molecules that function as S phase inhibitors by targeting them for ubiquitination. Once a protein has been ubiquitinated, it is targeted for proteolytic degradation by the proteasome. However, results from a recent study of E2F transcriptional dynamics at the single-cell level argue that the role of G1 cyclin-CDK activities, in particular cyclin D-CDK4/6, is to tune the timing rather than the commitment of cell cycle entry.\nActive S cyclin-CDK complexes phosphorylate proteins that make up the pre-replication complexes assembled during G1 phase on DNA replication origins. The phosphorylation serves two purposes: to activate each already-assembled pre-replication complex, and to prevent new complexes from forming. This ensures that every portion of the cell's genome will be replicated once and only once. The reason for prevention of gaps in replication is fairly clear, because daughter cells that are missing all or part of crucial genes will die. However, for reasons related to gene copy number effects, possession of extra copies of certain genes is also deleterious to the daughter cells.\nMitotic cyclin-CDK complexes, which are synthesized but inactivated during S and G2 phases, promote the initiation of mitosis by stimulating downstream proteins involved in chromosome condensation and mitotic spindle assembly. A critical complex activated during this process is a ubiquitin ligase known as the anaphase-promoting complex (APC), which promotes degradation of structural proteins associated with the chromosomal kinetochore. APC also targets the mitotic cyclins for degradation, ensuring that telophase and cytokinesis can proceed.\nSpecific action of cyclin-CDK complexes.\nCyclin D is the first cyclin produced in the cells that enter the cell cycle, in response to extracellular signals (e.g. growth factors). Cyclin D levels stay low in resting cells that are not proliferating. Additionally, CDK4/6 and CDK2 are also inactive because CDK4/6 are bound by INK4 family members (e.g., p16), limiting kinase activity. Meanwhile, CDK2 complexes are inhibited by the CIP/KIP proteins such as p21 and p27, When it is time for a cell to enter the cell cycle, which is triggered by a mitogenic stimuli, levels of cyclin D increase. In response to this trigger, cyclin D binds to existing CDK4/6, forming the active cyclin D-CDK4/6 complex. Cyclin D-CDK4/6 complexes in turn mono-phosphorylates the retinoblastoma susceptibility protein (Rb) to pRb. The un-phosphorylated Rb tumour suppressor functions in inducing cell cycle exit and maintaining G0 arrest (senescence).\nIn the last few decades, a model has been widely accepted whereby pRB proteins are inactivated by cyclin D-Cdk4/6-mediated phosphorylation. Rb has 14+ potential phosphorylation sites. Cyclin D-Cdk 4/6 progressively phosphorylates Rb to hyperphosphorylated state, which triggers dissociation of pRB\u2013E2F complexes, thereby inducing G1/S cell cycle gene expression and progression into S phase.\nHowever, scientific observations from a recent study show that Rb is present in three types of isoforms: (1) un-phosphorylated Rb in G0 state; (2) mono-phosphorylated Rb, also referred to as \"hypo-phosphorylated' or 'partially' phosphorylated Rb in early G1 state; and (3) inactive hyper-phosphorylated Rb in late G1 state. In early G1 cells, mono-phosphorylated Rb exits as 14 different isoforms, one of each has distinct E2F binding affinity. Rb has been found to associate with hundreds of different proteins and the idea that different mono-phosphorylated Rb isoforms have different protein partners was very appealing. A recent report confirmed that mono-phosphorylation controls Rb's association with other proteins and generates functional distinct forms of Rb. All different mono-phosphorylated Rb isoforms inhibit E2F transcriptional program and are able to arrest cells in G1-phase. Importantly, different mono-phosphorylated forms of RB have distinct transcriptional outputs that are extended beyond E2F regulation.\nIn general, the binding of pRb to E2F inhibits the E2F target gene expression of certain G1/S and S transition genes including E-type cyclins. The partial phosphorylation of RB de-represses the Rb-mediated suppression of E2F target gene expression, begins the expression of cyclin E. The molecular mechanism that causes the cell switched to cyclin E activation is currently not known, but as cyclin E levels rise, the active cyclin E-CDK2 complex is formed, bringing Rb to be inactivated by hyper-phosphorylation. Hyperphosphorylated Rb is completely dissociated from E2F, enabling further expression of a wide range of E2F target genes are required for driving cells to proceed into S phase [1]. Recently, it has been identified that cyclin D-Cdk4/6 binds to a C-terminal alpha-helix region of Rb that is only distinguishable to cyclin D rather than other cyclins, cyclin E, A and B. This observation based on the structural analysis of Rb phosphorylation supports that Rb is phosphorylated in a different level through multiple Cyclin-Cdk complexes. This also makes feasible the current model of a simultaneous switch-like inactivation of all mono-phosphorylated Rb isoforms through one type of Rb hyper-phosphorylation mechanism. In addition, mutational analysis of the cyclin D- Cdk 4/6 specific Rb C-terminal helix shows that disruptions of cyclin D-Cdk 4/6 binding to Rb prevents Rb phosphorylation, arrests cells in G1, and bolsters Rb's functions in tumor suppressor. This cyclin-Cdk driven cell cycle transitional mechanism governs a cell committed to the cell cycle that allows cell proliferation. A cancerous cell growth often accompanies with deregulation of Cyclin D-Cdk 4/6 activity.\nThe hyperphosphorylated Rb dissociates from the E2F/DP1/Rb complex (which was bound to the E2F responsive genes, effectively \"blocking\" them from transcription), activating E2F. Activation of E2F results in transcription of various genes like cyclin E, cyclin A, DNA polymerase, thymidine kinase, etc. Cyclin E thus produced binds to CDK2, forming the cyclin E-CDK2 complex, which pushes the cell from G1 to S phase (G1/S, which initiates the G2/M transition). Cyclin B-cdk1 complex activation causes breakdown of nuclear envelope and initiation of prophase, and subsequently, its deactivation causes the cell to exit mitosis. A quantitative study of E2F transcriptional dynamics at the single-cell level by using engineered fluorescent reporter cells provided a quantitative framework for understanding the control logic of cell cycle entry, challenging the canonical textbook model. Genes that regulate the amplitude of E2F accumulation, such as Myc, determine the commitment in cell cycle and S phase entry. G1 cyclin-CDK activities are not the driver of cell cycle entry. Instead, they primarily tune the timing of E2F increase, thereby modulating the pace of cell cycle progression.\nInhibitors.\nEndogenous.\nTwo families of genes, the \"cip/kip\" (\"CDK interacting protein/Kinase inhibitory protein\") family and the INK4a/ARF (\"In\"hibitor of \"K\"inase 4/\"A\"lternative \"R\"eading \"F\"rame) family, prevent the progression of the cell cycle. Because these genes are instrumental in prevention of tumor formation, they are known as tumor suppressors.\nThe \"cip/kip\" family includes the genes p21, p27 and p57. They halt the cell cycle in G1 phase by binding to and inactivating cyclin-CDK complexes. p21 is activated by p53 (which, in turn, is triggered by DNA damage e.g. due to radiation). p27 is activated by Transforming Growth Factor \u03b2 (TGF \u03b2), a growth inhibitor.\nThe INK4a/ARF family includes p16INK4a, which binds to CDK4 and arrests the cell cycle in G1 phase, and p14ARF which prevents p53 degradation.\nSynthetic.\nSynthetic inhibitors of Cdc25 could also be useful for the arrest of cell cycle and therefore be useful as antineoplastic and anticancer agents.\nMany human cancers possess the hyper-activated Cdk 4/6 activities. Given the observations of cyclin D-Cdk 4/6 functions, inhibition of Cdk 4/6 should result in preventing a malignant tumor from proliferating. Consequently, scientists have tried to invent the synthetic Cdk4/6 inhibitor as Cdk4/6 has been characterized to be a therapeutic target for anti-tumor effectiveness. Three Cdk4/6 inhibitors - palbociclib, ribociclib, and abemaciclib - currently received FDA approval for clinical use to treat advanced-stage or metastatic, hormone-receptor-positive (HR-positive, HR+), HER2-negative (HER2-) breast cancer. For example, palbociclib is an orally active CDK4/6 inhibitor which has demonstrated improved outcomes for ER-positive/HER2-negative advanced breast cancer. The main side effect is neutropenia which can be managed by dose reduction.\nCdk4/6 targeted therapy will only treat cancer types where Rb is expressed. Cancer cells with loss of Rb have primary resistance to Cdk4/6 inhibitors.\nTranscriptional regulatory network.\nCurrent evidence suggests that a semi-autonomous transcriptional network acts in concert with the CDK-cyclin machinery to regulate the cell cycle. Several gene expression studies in \"Saccharomyces cerevisiae\" have identified 800\u20131200 genes that change expression over the course of the cell cycle. They are transcribed at high levels at specific points in the cell cycle, and remain at lower levels throughout the rest of the cycle. While the set of identified genes differs between studies due to the computational methods and criteria used to identify them, each study indicates that a large portion of yeast genes are temporally regulated.\nMany periodically expressed genes are driven by transcription factors that are also periodically expressed. One screen of single-gene knockouts identified 48 transcription factors (about 20% of all non-essential transcription factors) that show cell cycle progression defects. Genome-wide studies using high throughput technologies have identified the transcription factors that bind to the promoters of yeast genes, and correlating these findings with temporal expression patterns have allowed the identification of transcription factors that drive phase-specific gene expression. The expression profiles of these transcription factors are driven by the transcription factors that peak in the prior phase, and computational models have shown that a CDK-autonomous network of these transcription factors is sufficient to produce steady-state oscillations in gene expression).\nExperimental evidence also suggests that gene expression can oscillate with the period seen in dividing wild-type cells independently of the CDK machinery. Orlando \"et al.\" used microarrays to measure the expression of a set of 1,271 genes that they identified as periodic in both wild type cells and cells lacking all S-phase and mitotic cyclins (\"clb1,2,3,4,5,6\"). Of the 1,271 genes assayed, 882 continued to be expressed in the cyclin-deficient cells at the same time as in the wild type cells, despite the fact that the cyclin-deficient cells arrest at the border between G1 and S phase. However, 833 of the genes assayed changed behavior between the wild type and mutant cells, indicating that these genes are likely directly or indirectly regulated by the CDK-cyclin machinery. Some genes that continued to be expressed on time in the mutant cells were also expressed at different levels in the mutant and wild type cells. These findings suggest that while the transcriptional network may oscillate independently of the CDK-cyclin oscillator, they are coupled in a manner that requires both to ensure the proper timing of cell cycle events. Other work indicates that phosphorylation, a post-translational modification, of cell cycle transcription factors by Cdk1 may alter the localization or activity of the transcription factors in order to tightly control timing of target genes.\nWhile oscillatory transcription plays a key role in the progression of the yeast cell cycle, the CDK-cyclin machinery operates independently in the early embryonic cell cycle. Before the midblastula transition, zygotic transcription does not occur and all needed proteins, such as the B-type cyclins, are translated from maternally loaded mRNA.\nDNA replication and DNA replication origin activity.\nAnalyses of synchronized cultures of \"Saccharomyces cerevisiae\" under conditions that prevent DNA replication initiation without delaying cell cycle progression showed that origin licensing decreases the expression of genes with origins near their 3' ends, revealing that downstream origins can regulate the expression of upstream genes. This confirms previous predictions from mathematical modeling of a global causal coordination between DNA replication origin activity and mRNA expression, and shows that mathematical modeling of DNA microarray data can be used to correctly predict previously unknown biological modes of regulation.\nCheckpoints.\nCell cycle checkpoints are used by the cell to monitor and regulate the progress of the cell cycle. Checkpoints prevent cell cycle progression at specific points, allowing verification of necessary phase processes and repair of DNA damage. The cell cannot proceed to the next phase until checkpoint requirements have been met. Checkpoints typically consist of a network of regulatory proteins that monitor and dictate the progression of the cell through the different stages of the cell cycle.\nThere are several checkpoints to ensure that damaged or incomplete DNA is not passed on to daughter cells. Three main checkpoints exist: the G1/S checkpoint, the G2/M checkpoint and the metaphase (mitotic) checkpoint. Another checkpoint is the Go checkpoint, in which the cells are checked for maturity. If the cells fail to pass this checkpoint by not being ready yet, they will be discarded from dividing. \nG1/S transition is a rate-limiting step in the cell cycle and is also known as restriction point. This is where the cell checks whether it has enough raw materials to fully replicate its DNA (nucleotide bases, DNA synthase, chromatin, etc.). An unhealthy or malnourished cell will get stuck at this checkpoint.\nThe G2/M checkpoint is where the cell ensures that it has enough cytoplasm and phospholipids for two daughter cells. But sometimes more importantly, it checks to see if it is the right time to replicate. There are some situations where many cells need to all replicate simultaneously (for example, a growing embryo should have a symmetric cell distribution until it reaches the mid-blastula transition). This is done by controlling the G2/M checkpoint.\nThe metaphase checkpoint is a fairly minor checkpoint, in that once a cell is in metaphase, it has committed to undergoing mitosis. However that's not to say it isn't important. In this checkpoint, the cell checks to ensure that the spindle has formed and that all of the chromosomes are aligned at the spindle equator before anaphase begins.\nWhile these are the three \"main\" checkpoints, not all cells have to pass through each of these checkpoints in this order to replicate. Many types of cancer are caused by mutations that allow the cells to speed through the various checkpoints or even skip them altogether. Going from S to M to S phase almost consecutively. Because these cells have lost their checkpoints, any DNA mutations that may have occurred are disregarded and passed on to the daughter cells. This is one reason why cancer cells have a tendency to exponentially accrue mutations. Aside from cancer cells, many fully differentiated cell types no longer replicate so they leave the cell cycle and stay in G0 until their death. Thus removing the need for cellular checkpoints. An alternative model of the cell cycle response to DNA damage has also been proposed, known as the postreplication checkpoint.\nCheckpoint regulation plays an important role in an organism's development. In sexual reproduction, when egg fertilization occurs, when the sperm binds to the egg, it releases signalling factors that notify the egg that it has been fertilized. Among other things, this induces the now fertilized oocyte to return from its previously dormant, G0, state back into the cell cycle and on to mitotic replication and division.\np53 plays an important role in triggering the control mechanisms at both G1/S and G2/M checkpoints. In addition to p53, checkpoint regulators are being heavily researched for their roles in cancer growth and proliferation.\nFluorescence imaging of the cell cycle.\nPioneering work by Atsushi Miyawaki and coworkers developed the fluorescent ubiquitination-based cell cycle indicator (FUCCI), which enables fluorescence imaging of the cell cycle. Originally, a green fluorescent protein, mAG, was fused to hGem(1/110) and an orange fluorescent protein (mKO2) was fused to hCdt1(30/120). Note, these fusions are fragments that contain a nuclear localization signal and ubiquitination sites for degradation, but are not functional proteins. The green fluorescent protein is made during the S, G2, or M phase and degraded during the G0 or G1 phase, while the orange fluorescent protein is made during the G0 or G1 phase and destroyed during the S, G2, or M phase. A far-red and near-infrared FUCCI was developed using a cyanobacteria-derived fluorescent protein (smURFP) and a bacteriophytochrome-derived fluorescent protein (movie found at this link).\nRole in tumor formation.\nA disregulation of the cell cycle components may lead to tumor formation. As mentioned above, when some genes like the cell cycle inhibitors, RB, p53 etc. mutate, they may cause the cell to multiply uncontrollably, forming a tumor. Although the duration of cell cycle in tumor cells is equal to or longer than that of normal cell cycle, the proportion of cells that are in active cell division (versus quiescent cells in G0 phase) in tumors is much higher than that in normal tissue. Thus there is a net increase in cell number as the number of cells that die by apoptosis or senescence remains the same.\nThe cells which are actively undergoing cell cycle are targeted in cancer therapy as the DNA is relatively exposed during cell division and hence susceptible to damage by drugs or radiation. This fact is made use of in cancer treatment; by a process known as debulking, a significant mass of the tumor is removed which pushes a significant number of the remaining tumor cells from G0 to G1 phase (due to increased availability of nutrients, oxygen, growth factors etc.). Radiation or chemotherapy following the debulking procedure kills these cells which have newly entered the cell cycle.\nThe fastest cycling mammalian cells in culture, crypt cells in the intestinal epithelium, have a cycle time as short as 9 to 10 hours. Stem cells in resting mouse skin may have a cycle time of more than 200 hours. Most of this difference is due to the varying length of G1, the most variable phase of the cycle. M and S do not vary much.\nIn general, cells are most radiosensitive in late M and G2 phases and most resistant in late S phase.\nFor cells with a longer cell cycle time and a significantly long G1 phase, there is a second peak of resistance late in G1.\nThe pattern of resistance and sensitivity correlates with the level of sulfhydryl compounds in the cell. Sulfhydryls are natural substances that protect cells from radiation damage and tend to be at their highest levels in S and at their lowest near mitosis.\nHomologous recombination (HR) is an accurate process for repairing DNA double-strand breaks. HR is nearly absent in G1 phase, is most active in S phase, and declines in G2/M. Non-homologous end joining, a less accurate and more mutagenic process for repairing double strand breaks, is active throughout the cell cycle."}
{"id": "7253", "revid": "27335766", "url": "https://en.wikipedia.org/wiki?curid=7253", "title": "Cartesian", "text": "Cartesian means of or relating to the French philosopher Ren\u00e9 Descartes\u2014from his Latinized name \"Cartesius\". It may refer to:"}
{"id": "7255", "revid": "40147850", "url": "https://en.wikipedia.org/wiki?curid=7255", "title": "Connection (dance)", "text": "In partner dancing, connection is a physical communication method used by a pair of dancers to facilitate synchronized dance movement, in which one dancer (the \"lead\") directs the movements of the other dancer (the \"follower\") by means of non-verbal directions conveyed through a physical connection between the dancers. It is an essential technique in many types of partner dancing and is used extensively in partner dances that feature significant physical contact between the dancers, including the Argentine Tango, Lindy Hop, Balboa, East Coast Swing, West Coast Swing, Salsa, and Modern Jive.\nOther forms of communication, such as visual cues or spoken cues, sometimes aid in connecting with one's partner, but are often used in specific circumstances (e.g., practicing figures, or figures which are purposely danced without physical connection). Connection can be used to transmit power and energy as well as information and signals; some dance forms (and some dancers) primarily emphasize power or signaling, but most are probably a mixture of both.\nFollowing and leading in a partner dance is accomplished by maintaining a physical connection called the frame that allows the leader to transmit body movement to the follower, and for the follower to suggest ideas to the leader. A frame is a stable structural combination of both bodies maintained through the dancers' arms and/or legs.\nConnection occurs in both open and closed dance positions (also called \"open frame\" and \"closed frame\").\nIn closed position with body contact, connection is achieved by maintaining the frame. The follower moves to match the leader, maintaining the pressure between the two bodies as well as the position.\nWhen creating frame, tension is the primary means of establishing communication. Changes in tension are made to create rhythmic variations in moves and movements, and are communicated through points of contact. In an open position or a closed position without body contact, the hands and arms alone provide the connection, which may be one of three forms: tension, compression or neutral.\nIn swing dances, tension and compression may be maintained for a significant period of time. In other dances, such as Latin, tension and compression may be used as indications of upcoming movement. However, in both styles, tension and compression do not signal immediate movement: the follow must be careful not to move prior to actual movement by the lead. Until then, the dancers must match pressures without moving their hands. In some styles of Lindy Hop, the tension may become quite high without initiating movement.\nThe general rule for open connections is that moves of the leader's hands back, forth, left or right are originated through moves of the entire body. Accordingly, for the follower, a move of the connected hand is immediately transformed into the corresponding move of the body. Tensing the muscles and locking the arm achieves this effect but is neither comfortable nor correct. Such tension eliminates the subtler communication in the connection, and eliminates free movement up and down, such as is required to initiate many turns.\nInstead of just tensing the arms, connection is achieved by engaging the shoulder, upper body and torso muscles. Movement originates in the body's core. A leader leads by moving himself and maintaining frame and connection. Different forms of dance and different movements within each dance may call for differences in the connection. In some dances the separation distance between the partners remains pretty constant. In others e.g. Modern Jive moving closer together and further apart are fundamental to the dance, requiring flexion and extension of the arms, alternating compression and tension.\nThe connection between two partners has a different feel in every dance and with every partner. Good social dancers adapt to the conventions of the dance and the responses of their partners."}
{"id": "7256", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=7256", "title": "Cardiovascular system", "text": ""}
{"id": "7257", "revid": "915833", "url": "https://en.wikipedia.org/wiki?curid=7257", "title": "Caste", "text": "Caste is a form of social stratification characterized by endogamy, hereditary transmission of a style of life which often includes an occupation, ritual status in a hierarchy, and customary social interaction and exclusion based on cultural notions of purity and pollution. Its paradigmatic ethnographic example is the division of India's Hindu society into rigid social groups, with roots in India's ancient history and persisting to the present time. However, the economic significance of the caste system in India has been declining as a result of urbanization and affirmative action programs. A subject of much scholarship by sociologists and anthropologists, the Hindu caste system is sometimes used as an analogical basis for the study of caste-like social divisions existing outside Hinduism and India. The term \"caste\" is also applied to morphological groupings in female populations of ants and bees.\nEtymology.\nThe English word \"caste\" () derives from the Spanish and Portuguese \"casta\", which, according to the John Minsheu's Spanish dictionary (1569), means \"race, lineage, tribe or breed\". When the Spanish colonized the New World, they used the word to mean a \"clan or lineage\". It was, however, the Portuguese who first employed \"casta\" in the primary modern sense of the English word 'caste' when they applied it to the thousands of endogamous, hereditary Indian social groups they encountered upon their arrival in India in 1498. The use of the spelling \"caste\", with this latter meaning, is first attested in English in 1613.\nIn South Asia.\nIndia.\nModern India's caste system is based on the colonial superimposition of the Portuguese word \"casta\" on the four-fold theoretical classification called \"Varna\" and on natural social groupings called \"J\u0101ti\". From 1901 onwards, for the purposes of the Decennial Census, the British classified all J\u0101tis into one or the other of the \"Varna\" categories as described in ancient texts. Herbert Hope Risley, the Census Commissioner, noted that \"The principle suggested as a basis was that of classification by social precedence as recognized by native public opinion at the present day, and manifesting itself in the facts that particular castes are supposed to be the modern representatives of one or other of the castes of the theoretical Indian system.\"\n\"Varna\", as mentioned in ancient Hindu texts, describes society as divided into four categories: Brahmins (scholars and yajna priests), Kshatriyas (rulers and warriors), Vaishyas (farmers, merchants and artisans) and Shudras (workmen/service providers). The texts do not mention any hierarchy or a separate, untouchable category in \"Varna\" classifications. Scholars believe that the \"Varnas\" system was never truly operational in society and there is no evidence of it ever being a reality in Indian history. The practical division of the society had always been in terms of \"Jatis\" (birth groups), which are not based on any specific religious principle, but could vary from ethnic origins to occupations to geographic areas. The \"J\u0101tis\" have been endogamous social groups without any fixed hierarchy but subject to vague notions of rank articulated over time based on lifestyle and social, political or economic status. Many of India's major empires and dynasties like the Mauryas, Shalivahanas, Chalukyas, Kakatiyas among many others, were founded by people who would have been classified as Shudras, under the \"Varnas\" system. It is well established that by the 9th century, kings from all the four Varnas, including Brahmins and Vaishyas, had occupied the highest seat in the monarchical system in Hindu India, contrary to the Varna theory. In many instances, as in Bengal, historically the kings and rulers had been called upon, when required, to mediate on the ranks of \"J\u0101tis\", which might number in thousands all over the subcontinent and vary by region. In practice, the \"j\u0101tis\" may or may not fit into the \"Varna\" classes and many prominent \"Jatis\", for example the Jats and Yadavs, straddled two Varnas i.e. Kshatriyas and Vaishyas, and the \"Varna\" status of \"J\u0101tis\" itself was subject to articulation over time.\nStarting with the British colonial Census of 1901 led by Herbert Hope Risley, all the \"j\u0101tis\" were grouped under the theoretical \"varnas\" categories. According to political scientist Lloyd Rudolph, Risley believed that \"varna\", however ancient, could be applied to all the modern castes found in India, and \"[he] meant to identify and place several hundred million Indians within it.\" In an effort to arrange various castes in order of precedence functional grouping was based less on the occupation that prevailed in each case in the present day than on that which was traditional with it, or which gave rise to its differentiation from the rest of the community. \"This action virtually removed Indians from the progress of history and condemned them to an unchanging position and place in time. In one sense, it is rather ironic that the British, who continually accused the Indian people of having a static society, should then impose a construct that denied progress\" The terms \"varna\" (conceptual classification based on occupation) and \"j\u0101ti\" (groups) are two distinct concepts: while \"varna\" is a theoretical four-part division, \"j\u0101ti\" (community) refers to the thousands of actual endogamous social groups prevalent across the subcontinent. The classical authors scarcely speak of anything other than the \"varnas\", as it provided a convenient shorthand; but a problem arises when colonial Indologists sometimes confuse the two. Thus, starting with the 1901 Census, caste officially became India's essential institution, with an imprimatur from the British administrators, augmenting a discourse that had already dominated Indology. \"Despite India's acquisition of formal political independence, it has still not regained the power to know its own past and present apart from that discourse\".\nUpon independence from Britain, the Indian Constitution listed 1,108 castes across the country as Scheduled Castes in 1950, for positive discrimination. The Untouchable communities are sometimes called \"Scheduled Castes\", \"Dalit\" or \"Harijan\" in contemporary literature. In 2001, Dalits were 16.2% of India's population. Most of the 15 million bonded child workers are from the lowest castes. Independent India has witnessed caste-related violence. In 2005, government recorded approximately 110,000 cases of reported violent acts, including rape and murder, against Dalits. For 2012, the government recorded 651 murders, 3,855 injuries, 1,576 rapes, 490 kidnappings, and 214 cases of arson.\nThe socio-economic limitations of the caste system are reduced due to urbanization and affirmative action. Nevertheless, the caste system still exists in endogamy and patrimony, and thrives in the politics of democracy, where caste provides ready made constituencies to politicians. The globalization and economic opportunities from foreign businesses has influenced the growth of India's middle-class population. Some members of the Chhattisgarh Potter Caste Community (CPCC) are middle-class urban professionals and no longer potters unlike the remaining majority of traditional rural potter members. There is persistence of caste in Indian politics. Caste associations have evolved into caste-based political parties. Political parties and the state perceive caste as an important factor for mobilization of people and policy development.\nStudies by Bhatt and Beteille have shown changes in status, openness, mobility in the social aspects of Indian society. As a result of modern socio-economic changes in the country, India is experiencing significant changes in the dynamics And the economics of its social sphere. While arranged marriages are still the most common practice in India, the internet has provided a network for younger Indians to take control of their relationships through the use of dating apps. This remains isolated to informal terms, as marriage is not often achieved through the use of these apps. Hypergamy is still a common practice in India and Hindu culture. Men are expected to marry within their caste, or one below, with no social repercussions. If a woman marries into a higher caste, then her children will take the status of their father. If she marries down, her family is reduced to the social status of their son in law. In this case, the women are bearers of the egalitarian principle of the marriage. There would be no benefit in marrying a higher caste if the terms of the marriage did not imply equality. However, men are systematically shielded from the negative implications of the agreement.\nGeographical factors also determine adherence to the caste system. Many Northern villages are more likely to participate in exogamous marriage, due to a lack of eligible suitors within the same caste. Women in North India have been found to be less likely to leave or divorce their husbands since they are of a relatively lower caste system, and have higher restrictions on their freedoms. On the other hand, Pahari women, of the northern mountains, have much more freedom to leave their husbands without stigma. This often leads to better husbandry as his actions are not protected by social expectations.\nChiefly among the factors influencing the rise of exogamy is the rapid urbanisation in India experienced over the last century. It is well known that urban centers tend to be less reliant on agriculture and are more progressive as a whole. As India's cities boomed in population, the job market grew to keep pace. Prosperity and stability were now more easily attained by an individual, and the anxiety to marry quickly and effectively was reduced. Thus, younger, more progressive generations of urban Indians are less likely than ever to participate in the antiquated system of arranged endogamy.\nIndia has also implemented a form of Affirmative Action, locally known as \"reservation groups\". Quota system jobs, as well as placements in publicly funded colleges, hold spots for the 8% of India's minority, and underprivileged groups. As a result, in states such as Tamil Nadu or those in the north-east, where underprivileged populations predominate, over 80% of government jobs are set aside in quotas. In education, colleges lower the marks necessary for the Dalits to enter.\nNepal.\nThe Nepalese caste system resembles in some respects the Indian \"j\u0101ti\" system, with numerous \"j\u0101ti\" divisions with a \"varna\" system superimposed. Inscriptions attest the beginnings of a caste system during the Licchavi period. Jayasthiti Malla (1382\u20131395) categorized Newars into 64 castes (Gellner 2001). A similar exercise was made during the reign of Mahindra Malla (1506\u20131575). The Hindu social code was later set up in Gorkha by Ram Shah (1603\u20131636).\nPakistan.\nMcKim Marriott claims a social stratification that is hierarchical, closed, endogamous and hereditary is widely prevalent, particularly in western parts of Pakistan. Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes.\nSri Lanka.\nThe caste system in Sri Lanka is a division of society into strata, influenced by the textbook \"varnas\" and \"j\u0101ti\" system found in India. Ancient Sri Lankan texts such as the Pujavaliya, Sadharmaratnavaliya and Yogaratnakaraya and inscriptional evidence show that the above hierarchy prevailed throughout the feudal period. The repetition of the same caste hierarchy even as recently as the 18th century, in the British/Kandyan period Kadayimpoth \u2013 Boundary books as well indicates the continuation of the tradition right up to the end of Sri Lanka's monarchy.\nOutside South Asia.\nSoutheast Asia.\nIndonesia.\nBalinese caste structure has been described as being based either on three categories\u2014the noble triwangsa (thrice born), the middle class of \"dwij\u0101ti\" (twice born), and the lower class of \"ekaj\u0101ti\" (once born)--or on four castes\nThe Brahmana caste was further subdivided by these Dutch ethnographers into two: Siwa and Buda. The Siwa caste was subdivided into five: Kemenuh, Keniten, Mas, Manuba and Petapan. This classification was to accommodate the observed marriage between higher-caste Brahmana men with lower-caste women. The other castes were similarly further sub-classified by these 19th-century and early-20th-century ethnographers based on numerous criteria ranging from profession, endogamy or exogamy or polygamy, and a host of other factors in a manner similar to \"castas\" in Spanish colonies such as Mexico, and caste system studies in British colonies such as India.\nPhilippines.\nIn the Philippines, pre-colonial societies do not have a single social structure. The class structures can be roughly categorized into four types:\nEast Asia.\nChina and Mongolia.\nDuring the period of Yuan Dynasty, ruler Kublai Khan enforced a \"Four Class System\", which was a legal caste system. The order of four classes of people was maintained by the information of the descending order were:\nToday, the Hukou system is argued by various Western sources to be the current caste system of China.\nTibet.\nThere is significant controversy over the social classes of Tibet, especially with regards to the serfdom in Tibet controversy.\n has put forth the argument that pre-1950s Tibetan society was functionally a caste system, in contrast to previous scholars who defined the Tibetan social class system as similar to European feudal serfdom, as well as non-scholarly western accounts which seek to romanticize a supposedly 'egalitarian' ancient Tibetan society.\nJapan.\nIn Japan's history, social strata based on inherited position rather than personal merit, were rigid and highly formalized in a system called \"mibunsei\" (\u8eab\u5206\u5236). At the top were the Emperor and Court nobles (kuge), together with the Sh\u014dgun and daimy\u014d. Below them, the population was divided into four classes: samurai, peasants, craftsmen and merchants. Only samurai were allowed to bear arms. A samurai had a right to kill any peasants, craftsman or merchant who he felt were disrespectful. Merchants were the lowest caste because they did not produce any products. The castes were further sub-divided; for example, peasants were labelled as \"furiuri\", \"tanagari\", \"mizunomi-byakusho\" among others. As in Europe, the castes and sub-classes were of the same race, religion and culture.\nHowell, in his review of Japanese society notes that if a Western power had colonized Japan in the 19th century, they would have discovered and imposed a rigid four-caste hierarchy in Japan.\nDe Vos and Wagatsuma observe that Japanese society had a systematic and extensive caste system. They discuss how alleged caste impurity and alleged racial inferiority, concepts often assumed to be different, are superficial terms, and are due to identical inner psychological processes, which expressed themselves in Japan and elsewhere.\nEndogamy was common because marriage across caste lines was socially unacceptable.\nJapan had its own untouchable caste, shunned and ostracized, historically referred to by the insulting term \"Eta\", now called \"Burakumin\". While modern law has officially abolished the class hierarchy, there are reports of discrimination against the Buraku or Burakumin underclasses. The Burakumin are regarded as \"ostracised\". The burakumin are one of the main minority groups in Japan, along with the Ainu of Hokkaid\u014d and those of residents of Korean and Chinese descent.\nKorea.\nThe baekjeong (\ubc31\uc815) were an \"untouchable\" outcaste of Korea. The meaning today is that of butcher. It originates in the Khitan invasion of Korea in the 11th century. The defeated Khitans who surrendered were settled in isolated communities throughout Goryeo to forestall rebellion. They were valued for their skills in hunting, herding, butchering, and making of leather, common skill sets among nomads. Over time, their ethnic origin was forgotten, and they formed the bottom layer of Korean society.\nIn 1392, with the foundation of the Confucian Joseon dynasty, Korea systemised its own native class system. At the top were the two official classes, the Yangban, which literally means \"two classes\". It was composed of scholars (\"munban\") and warriors (\"muban\"). Scholars had a significant social advantage over the warriors. Below were the \"jung-in\" (\uc911\uc778-\u4e2d\u4eba: literally \"middle people\". This was a small class of specialized professions such as medicine, accounting, translators, regional bureaucrats, etc. Below that were the \"sangmin\" (\uc0c1\ubbfc-\u5e38\u6c11: literally 'commoner'), farmers working their own fields. Korea also had a serf population known as the \"nobi\". The nobi population could fluctuate up to about one third of the population, but on average the nobi made up about 10% of the total population. In 1801, the vast majority of government nobi were emancipated, and by 1858 the nobi population stood at about 1.5% of the total population of Korea. The hereditary nobi system was officially abolished around 1886\u201387 and the rest of the nobi system was abolished with the Gabo Reform of 1894, but traces remained until 1930.\nThe opening of Korea to foreign Christian missionary activity in the late 19th century saw some improvement in the status of the \"baekjeong\". However, everyone was not equal under the Christian congregation, and even so protests erupted when missionaries tried to integrate \"baekjeong\" into worship, with non-\"baekjeong\" finding this attempt insensitive to traditional notions of hierarchical advantage. Around the same time, the \"baekjeong\" began to resist open social discrimination. They focused on social and economic injustices affecting them, hoping to create an egalitarian Korean society. Their efforts included attacking social discrimination by upper class, authorities, and \"commoners\", and the use of degrading language against children in public schools.\nWith the Gabo reform of 1896, the class system of Korea was officially abolished. Following the collapse of the Gabo government, the new cabinet, which became the Gwangmu government after the establishment of the Korean Empire, introduced systematic measures for abolishing the traditional class system. One measure was the new household registration system, reflecting the goals of formal social equality, which was implemented by the loyalists' cabinet. Whereas the old registration system signified household members according to their hierarchical social status, the new system called for an occupation.\nWhile most Koreans by then had surnames and even bongwan, although still substantial number of cheonmin, mostly consisted of serfs and slaves, and untouchables did not. According to the new system, they were then required to fill in the blanks for surname in order to be registered as constituting separate households. Instead of creating their own family name, some cheonmins appropriated their masters' surname, while others simply took the most common surname and its bongwan in the local area. Along with this example, activists within and outside the Korean government had based their visions of a new relationship between the government and people through the concept of citizenship, employing the term \"inmin\" (\"people\") and later, \"kungmin\" (\"citizen\").\nNorth Korea.\nThe Committee for Human Rights in North Korea reported that \"Every North Korean citizen is assigned a heredity-based class and socio-political rank over which the individual exercises no control but which determines all aspects of his or her life.\" Called \"Songbun\", Barbara Demick describes this \"class structure\" as an updating of the hereditary \"caste system\", a combination of Confucianism and Stalinism. She claims that a bad family background is called \"tainted blood\", and that by law this \"tainted blood\" lasts three generations.\nWest Asia.\nYezidi society is hierarchical. The secular leader is a hereditary emir or prince, whereas a chief sheikh heads the religious hierarchy. The Yazidi are strictly endogamous; members of the three Yazidi castes, the murids, sheikhs and pirs, marry only within their group.\nIran.\nPre-Islamic Sassanid society was immensely complex, with separate systems of social organization governing numerous different groups within the empire. Historians believe society comprised four\nsocial classes:\nYemen.\nIn Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Estimates put their number at over 3.5 million residents who are discriminated, out of a total Yemeni population of around 22 million.\nAfrica.\nVarious sociologists have reported caste systems in Africa. The specifics of the caste systems have varied in ethnically and culturally diverse Africa, however the following features are common \u2013 it has been a closed system of social stratification, the social status is inherited, the castes are hierarchical, certain castes are shunned while others are merely endogamous and exclusionary. In some cases, concepts of purity and impurity by birth have been prevalent in Africa. In other cases, such as the \"Nupe\" of Nigeria, the \"Beni Amer\" of East Africa, and the \"Tira\" of Sudan, the exclusionary principle has been driven by evolving social factors.\nWest Africa.\nAmong the Igbo of Nigeria \u2013 especially Enugu, Anambra, Imo, Abia, Ebonyi, Edo and Delta states of the country \u2013 Obinna finds Osu caste system has been and continues to be a major social issue. The Osu caste is determined by one's birth into a particular family irrespective of the religion practised by the individual. Once born into Osu caste, this Nigerian person is an outcast, shunned and ostracized, with limited opportunities or acceptance, regardless of his or her ability or merit. Obinna discusses how this caste system-related identity and power is deployed within government, Church and indigenous communities.\nThe \"osu\" class systems of eastern Nigeria and southern Cameroon are derived from indigenous religious beliefs and discriminate against the \"Osus\" people as \"owned by deities\" and outcasts.\nThe Songhai economy was based on a caste system. The most common were metalworkers, fishermen, and carpenters. Lower caste participants consisted of mostly non-farm working immigrants, who at times were provided special privileges and held high positions in society. At the top were noblemen and direct descendants of the original Songhai people, followed by freemen and traders.\nIn a review of social stratification systems in Africa, Richter reports that the term caste has been used by French and American scholars to many groups of West African artisans. These groups have been described as inferior, deprived of all political power, have a specific occupation, are hereditary and sometimes despised by others. Richter illustrates caste system in Ivory Coast, with six sub-caste categories. Unlike other parts of the world, mobility is sometimes possible within sub-castes, but not across caste lines. Farmers and artisans have been, claims Richter, distinct castes. Certain sub-castes are shunned more than others. For example, exogamy is rare for women born into families of woodcarvers.\nSimilarly, the Mand\u00e9 societies in Gambia, Ghana, Guinea, Ivory Coast, Liberia, Senegal and Sierra Leone have social stratification systems that divide society by ethnic ties. The Mande class system regards the \"jonow\" slaves as inferior. Similarly, the Wolof in Senegal is divided into three main groups, the \"geer\" (freeborn/nobles), \"jaam\" (slaves and slave descendants) and the underclass \"neeno\". In various parts of West Africa, Fulani societies also have class divisions. Other castes include \"Griots\", \"Forgerons\", and \"Cordonniers\".\nTamari has described endogamous castes of over fifteen West African peoples, including the Tukulor, Songhay, Dogon, Senufo, Minianka, Moors, Manding, Soninke, Wolof, Serer, Fulani, and Tuareg. Castes appeared among the \"Malinke\" people no later than 14th century, and was present among the \"Wolof\" and \"Soninke\", as well as some \"Songhay\" and \"Fulani\" populations, no later than 16th century. Tamari claims that wars, such as the \"Sosso-Malinke\" war described in the \"Sunjata\" epic, led to the formation of blacksmith and bard castes among the people that ultimately became the Mali empire.\nAs West Africa evolved over time, sub-castes emerged that acquired secondary specializations or changed occupations. Endogamy was prevalent within a caste or among a limited number of castes, yet castes did not form demographic isolates according to Tamari. Social status according to caste was inherited by off-springs automatically; but this inheritance was paternal. That is, children of higher caste men and lower caste or slave concubines would have the caste status of the father.\nCentral Africa.\nEthel M. Albert in 1960 claimed that the societies in Central Africa were caste-like social stratification systems. Similarly, in 1961, Maquet notes that the society in Rwanda and Burundi can be best described as castes. The Tutsi, noted Maquet, considered themselves as superior, with the more numerous Hutu and the least numerous Twa regarded, by birth, as respectively, second and third in the hierarchy of Rwandese society. These groups were largely endogamous, exclusionary and with limited mobility.\nHorn of Africa.\nIn a review published in 1977, Todd reports that numerous scholars report a system of social stratification in different parts of Africa that resembles some or all aspects of caste system. Examples of such caste systems, he claims, are to be found in Ethiopia in communities such as the Gurage and Konso. He then presents the Dime of Southwestern Ethiopia, amongst whom there operates a system which Todd claims can be unequivocally labelled as caste system. The Dime have seven castes whose size varies considerably. Each broad caste level is a hierarchical order that is based on notions of purity, non-purity and impurity. It uses the concepts of defilement to limit contacts between caste categories and to preserve the purity of the upper castes. These caste categories have been exclusionary, endogamous and the social identity inherited. Alula Pankhurst has published a study of caste groups in SW Ethiopia.\nAmong the Kafa, there were also traditionally groups labeled as castes. \"Based on research done before the Derg regime, these studies generally presume the existence of a social hierarchy similar to the caste system. At the top of this hierarchy were the Kafa, followed by occupational groups including blacksmiths (Qemmo), weavers (Shammano), bards (Shatto), potters, and tanners (Manno). In this hierarchy, the Manjo were commonly referred to as hunters, given the lowest status equal only to slaves.\"\nThe Borana Oromo of southern Ethiopia in the Horn of Africa also have a class system, wherein the Wata, an acculturated hunter-gatherer group, represent the lowest class. Though the Wata today speak the Oromo language, they have traditions of having previously spoken another language before adopting Oromo.\nThe traditionally nomadic Somali people are divided into clans, wherein the Rahanweyn agro-pastoral clans and the occupational clans such as the Madhiban were traditionally sometimes treated as outcasts. As Gabboye, the Madhiban along with the Yibir and Tumaal (collectively referred to as \"sab\") have since obtained political representation within Somalia, and their general social status has improved with the expansion of urban centers.\nEurope.\nEuropean feudalism with its rigid aristocracy can also be consider as a caste system\nBasque region.\nFor centuries, through the modern times, the majority regarded Cagots who lived primarily in the Basque region of France and Spain as an inferior caste, the untouchables. While they had the same skin color and religion as the majority, in the churches they had to use segregated doors, drink from segregated fonts, and receive communion on the end of long wooden spoons. It was a closed social system. The socially isolated Cagots were endogamous, and chances of social mobility non-existent.\nUnited Kingdom.\nIn July 2013, the UK government announced its intention to amend the Equality Act 2010, to \"introduce legislation on caste, including any necessary exceptions to the caste provisions, within the framework of domestic discrimination law\". Section 9(5) of the Equality Act 2010 provides that \"a Minister may by order amend the statutory definition of race to include caste and may provide for exceptions in the Act to apply or not to apply to caste\".\nFrom September 2013 to February 2014, Meena Dhanda led a project on \"Caste in Britain\" for the UK Equality and Human Rights Commission (EHRC).\nAmericas.\nUnited States.\nIn W. Lloyd Warner's view, the historic relationship between Blacks and Whites in the US showed many caste-like features such as residential segregation and marriage restrictions. In her bestselling 2020 book \"\", journalist Isabel Wilkerson similarly uses caste as a means to understand the racial hierarchy of the United States. Discrimination based upon socio-economic factors are historically prevalent within the country.\nAccording to Gerald D. Berreman, in the two systems, there are rigid rules of avoidance and certain types of contacts are defined as contaminating. In India, there are complex religious features which make up the system, whereas in the United States race and color are the basis for differentiation. The caste systems in India and the United States have higher groups which desire to retain their positions for themselves and thus perpetuate the two systems.\nThe process of creating a homogenized society by social engineering in both India and the US has created other institutions that have made class distinctions among different groups evident. Anthropologist James C. Scott elaborates on how \"global capitalism is perhaps the most powerful force for homogenization, whereas the state may be the defender of local difference and variety in some instances\". The caste system further emphasizes differences between the socio-economic classes that arise as a product of capitalism, which makes social mobility more difficult. Parts of the United States are sometimes divided by race and class status despite the national narrative of integration."}
{"id": "7258", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=7258", "title": "Creation", "text": "Creation may refer to:"}
{"id": "7262", "revid": "7957594", "url": "https://en.wikipedia.org/wiki?curid=7262", "title": "CORAL", "text": "CORAL, short for Computer On-line Real-time Applications Language is a programming language originally developed in 1964 at the Royal Radar Establishment (RRE), Malvern, Worcestershire, in the United Kingdom. The R was originally for \"radar\", not \"real-time\". It was influenced primarily by JOVIAL, and thus ALGOL, but is not a subset of either.\nThe most widely-known version, CORAL 66, was subsequently developed by I. F. Currie and M. Griffiths under the auspices of the \"Inter-Establishment Committee for Computer Applications\" (IECCA). Its official definition, edited by Woodward, Wetherall, and Gorman, was first published in 1970.\nIn 1971, CORAL was selected by the Ministry of Defence as the language for future military applications and to support this, a standardization program was introduced to ensure CORAL compilers met the specifications. This process was later adopted by the US Department of Defense while defining Ada.\nOverview.\nCoral 66 is a general-purpose programming language based on ALGOL 60, with some features from Coral 64, JOVIAL, and Fortran. It includes structured record types (as in Pascal) and supports the packing of data into limited storage (also as in Pascal). Like Edinburgh IMP it allows inline (embedded) assembly language, and also offers good runtime checking and diagnostics. It is designed for real-time computing and embedded system applications, and for use on computers with limited processing power, including those limited to fixed-point arithmetic and those without support for dynamic storage allocation.\nThe language was an inter-service standard for British military programming, and was also widely adopted for civil purposes in the British control and automation industry. It was used to write software for both the Ferranti and General Electric Company (GEC) computers from 1971 onwards. Implementations also exist for the Interdata 8/32, PDP-11, VAX and Alpha platforms and HPE Integrity Servers; for the Honeywell, and for the Computer Technology Limited (CTL, later ITL) Modular-1; and for SPARC running Solaris, and Intel running Linux.\nQueen Elizabeth II sent the first email from a head of state from the Royal Signals and Radar Establishment over the ARPANET on March 26, 1976. The message read \"This message to all ARPANET users announces the availability on ARPANET of the Coral 66 compiler provided by the GEC 4080 computer at the Royal Signals and Radar Establishment, Malvern, England, ... Coral 66 is the standard real-time high level language adopted by the Ministry of Defence.\"\nAs Coral was aimed at a variety of real-time work, rather than general office data processing, there was no standardised equivalent to a stdio library. IECCA recommended a primitive input/output (I/O) package to accompany any compiler (in a document titled \"Input/Output of Character data in Coral 66 Utility Programs\"). Most implementers avoided this by producing Coral interfaces to extant Fortran and, later, C libraries.\nCORAL's most significant contribution to computing may have been enforcing quality control in commercial compilers. To have a CORAL compiler approved by IECCA, and thus allowing a compiler to be marketed as a CORAL 66 compiler, the candidate compiler had to compile and execute a standard suite of 25 test programs and 6 benchmark programs. The process was part of the British Standard (BS) 5905 approval process. This methodology was observed and adapted later by the United States Department of Defense for the certification of Ada compilers.\nSource code for a Coral 66 compiler (written in BCPL) has been recovered and the \"Official Definition of Coral 66\" document by Her Majesty's Stationery Office (HMSO) has been scanned; the Ministry of Defence patent office has issued a licence to the Edinburgh Computer History project to allow them to put both the code and the language reference online for non-commercial use.\nVariants.\nA variant of Coral 66 named PO-CORAL was developed during the late 1970s to early 1980s by the British General Post Office (GPO), together with GEC, STC and Plessey, for use on the System X digital telephone exchange control computers. This was later renamed BT-CORAL when British Telecom was spun off from the Post Office. Unique features of this language were the focus on real-time execution, message processing, limits on statement execution between waiting for input, and a prohibition on recursion to remove the need for a stack."}
{"id": "7263", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7263", "title": "Cardiovascular System", "text": ""}
{"id": "7264", "revid": "1008807899", "url": "https://en.wikipedia.org/wiki?curid=7264", "title": "Rhyming slang", "text": "Rhyming slang is a form of slang word construction in the English language. It is especially prevalent in the UK, Ireland and Australia. It was first used in the early 19th century in the East End of London; hence its alternative name, Cockney rhyming slang. In the United States, especially the criminal underworld of the West Coast between 1880 and 1920, rhyming slang has sometimes been known as Australian slang.\nThe construction of rhyming slang involves replacing a common word with a phrase of two or more words, the last of which rhymes with the original word; then, in almost all cases, omitting, from the end of the phrase, the secondary rhyming word (which is thereafter implied), making the origin and meaning of the phrase elusive to listeners not in the know.\nExamples.\nThe form is made clear with the following example. The rhyming phrase \"apples and pears\" was used to mean \"stairs\". Following the pattern of omission, \"and pears\" is dropped, thus the spoken phrase \"I'm going up the apples\" means \"I'm going up the stairs\".\nThe following are further common examples of these phrases:\nIn some examples the meaning is further obscured by adding a second iteration of rhyme and truncation to the original rhymed phrase. For example, the word \"Aris\" is often used to indicate the buttocks. This is the result of a double rhyme, starting with the original rough synonym \"arse\", which is rhymed with \"bottle and glass\", leading to \"bottle\". \"Bottle\" was then rhymed with \"Aristotle\" and truncated to \"Aris\".\nPhonetic \"versus\" phono-semantic forms.\nGhil'ad Zuckermann, a linguist and revivalist, has proposed a distinction between rhyming slang based on sound only, and phono-semantic rhyming slang, which includes a semantic link between the slang expression and its referent (the thing it refers to). An example of rhyming slang based only on sound is the Cockney \"tea leaf\" (thief). An example of phono-semantic rhyming slang is the Cockney \"sorrowful tale\" ((three months in) jail), in which case the person coining the slang term sees a semantic link, sometimes jocular, between the Cockney expression and its referent.\nMainstream usage.\nThe use of rhyming slang has spread beyond the purely dialectal and some examples are to be found in the mainstream British English lexicon, although many users may be unaware of the origin of those words.\nMost of the words changed by this process are nouns, but a few are adjectival e.g. \"bales\" of cotton (rotten), or the adjectival phrase \"on one's tod\" for \"on one's own\", after Tod Sloan, a famous jockey.\nHistory.\nRhyming slang is believed to have originated in the mid-19th century in the East End of London, with several sources suggesting some time in the 1840s. \"The Flash Dictionary\" of unknown authorship, published in 1921 by Smeeton (48mo), contains a few rhymes. John Camden Hotten's 1859 \"Dictionary of Modern Slang, Cant, and Vulgar Words\" likewise states that it originated in the 1840s (\"about twelve or fifteen years ago\"), but with \"chaunters\" and \"patterers\" in the Seven Dials area of London. The reference is to travelling salesmen of certain kinds, chaunters selling sheet music and patterers offered cheap, tawdry goods at fairs and markets up and down the country. Hotten's \"Dictionary\" included the first known \"Glossary of the Rhyming Slang\", which included later mainstays such as \"frog and toad\" (the main road) and \"apples and pears (stairs), as well as many more obscure examples, e.g. \"Battle of the Nile\" (a tile, a vulgar term for a hat), \"Duke of York\" (take a walk), and \"Top of Rome\" (home).\nIt remains a matter of speculation exactly how rhyming slang originated, for example, as a linguistic game among friends or as a cryptolect developed intentionally to confuse non-locals. If deliberate, it may also have been used to maintain a sense of community, or to allow traders to talk amongst themselves in marketplaces to facilitate collusion, without customers knowing what they were saying, or by criminals to confuse the police (see thieves' cant).\nThe English academic, lexicographer and radio personality Terence Dolan has suggested that rhyming slang was invented by Irish immigrants to London \"so the actual English wouldn't understand what they were talking about.\"\nDevelopment.\nMany examples of rhyming slang are based on locations in London, such as \"Peckham Rye\", meaning \"tie\", which dates from the late nineteenth century; \"Hampstead Heath\", meaning \"teeth\" (usually as \"Hampsteads\"), which was first recorded in 1887; and \"barnet\" (Barnet Fair), meaning \"hair\", which dates from the 1850s.\nIn the 20th century, rhyming slang began to be based on the names of celebrities \u2014 Gregory Peck (neck; cheque), Ruby Murray [as Ruby] (curry), Alan Whicker [as \"Alan Whickers\"] (knickers), Puff Daddy (caddy), Max Miller (pillow [pronounced ]), Meryl Streep (cheap), Nat King Cole (\"dole\"), Britney Spears (beers, tears), Henry Halls (balls) \u2014 and after pop culture references \u2014 Captain Kirk (work), Pop Goes the Weasel (diesel), Mona Lisa (pizza), Mickey Mouse (Scouse), Wallace and Gromit (vomit), Brady Bunch (lunch), Bugs Bunny (money), Scooby-Doo (clue), Winnie the Pooh (shoe), and \"Schindler's List\" (pissed). Some words have numerous definitions, such as dead (\"Father Ted\", \"gone to bed\", brown bread), door (Roger Moore, Andrea Corr, George Bernard Shaw, Rory O'Moore), cocaine (Kurt Cobain; [as \"Charlie\"] Bob Marley, Boutros Boutros-Ghali, Gianluca Vialli, oats and barley; [as \"line\"] Patsy Cline; [as \"powder\"] Niki Lauda), flares (\"Lionel Blairs\", \"Tony Blairs\", \"Rupert Bears\", \"Dan Dares\"), etc.\nMany examples have passed into common usage. Some substitutions have become relatively widespread in England in their contracted form. \"To have a butcher's\", meaning to have a look, originates from \"butcher's hook\", an S-shaped hook used by butchers to hang up meat, and dates from the late nineteenth century but has existed independently in general use from around the 1930s simply as \"butchers\". Similarly, \"use your loaf\", meaning \"use your head\", derives from \"loaf of bread\" and also dates from the late nineteenth century but came into independent use in the 1930s.\nConversely usages have lapsed, or been usurped (\"Hounslow Heath\" for teeth, was replaced by \"Hampsteads\" from the heath of the same name, stating \"c.\" 1887).\nIn some cases, false etymologies exist. For example, the term \"barney\" has been used to mean an altercation or fight since the late nineteenth century, although without a clear derivation. In the 2001 feature film \"Ocean's Eleven\", the explanation for the term is that it derives from Barney Rubble, the name of a cartoon character from the \"Flintstones\" television program many decades later in origin.\nRegional and international variations.\nRhyming slang is used mainly in London in England but can to some degree be understood across the country. Some constructions, however, rely on particular regional accents for the rhymes to work. For instance, the term \"Charing Cross\" (a place in London), used to mean \"horse\" since the mid-nineteenth century, does not work for a speaker without the lot\u2013cloth split, common in London at that time but not nowadays. A similar example is \"Joanna\" meaning \"piano\", which is based on the pronunciation of \"piano\" as \"pianna\" . Unique formations also exist in other parts of the United Kingdom, such as in the East Midlands, where the local accent has formed \"Derby Road\", which rhymes with \"cold\".\nOutside England, rhyming slang is used in many English-speaking countries in the Commonwealth of Nations, with local variations. For example, in Australian slang, the term for an English person is \"pommy\", which has been proposed as a rhyme on \"pomegranate\", pronounced \"Pummy Grant\", which rhymed with \"immigrant\".\nRhyming slang is continually evolving, and new phrases are introduced all the time; new personalities replace old ones\u2014pop culture introduces new words\u2014as in \"I haven't a Scooby\" (from Scooby Doo, the eponymous cartoon dog of the cartoon series) meaning \"I haven't a clue\".\nTaboo terms.\nRhyming slang is often used as a substitute for words regarded as taboo, often to the extent that the association with the taboo word becomes unknown over time. \"Berk\" (often used to mean \"foolish person\") originates from the most famous of all fox hunts, the \"Berkeley Hunt\" meaning \"cunt\"; \"cobblers\" (often used in the context \"what you said is rubbish\") originates from \"cobbler's awls\", meaning \"balls\" (as in testicles); and \"hampton\" (usually \"'ampton\") meaning \"prick\" (as in penis) originates from \"Hampton Wick\" (a place in London) \u2013 the second part \"wick\" also entered common usage as \"he gets on my wick\" (he is an annoying person).\nLesser taboo terms include \"pony and trap\" for \"crap\" (as in defecate, but often used to denote nonsense or low quality); to blow a raspberry (rude sound of derision) from raspberry tart for \"fart\"; \"D'Oyly Carte (an opera company) for \"fart\"; \"Jimmy Riddle\" (an American country musician) for \"piddle\" (as in urinate), \"J. Arthur Rank\" (a film mogul), \"Sherman tank\", \"Jodrell Bank\" or \"ham shank\" for \"wank\", \"Bristol Cities\" (contracted to 'Bristols') for \"titties\", etc. \"Taking the Mick\" or \"taking the Mickey\" is thought to be a rhyming slang form of \"taking the piss\", where \"Mick\" came from \"Mickey Bliss\".\nIn December 2004 Joe Pasquale, winner of the fourth series of ITV's \"I'm a Celebrity... Get Me Out of Here!\", became well known for his frequent use of the term \"Jacobs\", for Jacob's Crackers, a rhyming slang term for knackers i.e. testicles.\nIn popular culture.\nIn film.\nCary Grant's character teaches rhyming slang to his female companion in \"Mr. Lucky\" (1943), describing it as 'Australian rhyming slang'. Rhyming slang is also used and described in a scene of the 1967 film \"To Sir, with Love\" starring Sidney Poitier, where the English students tell their foreign teacher that the slang is a drag and something for old people. The closing song of the 1969 crime caper, \"The Italian Job\", (\"Getta Bloomin' Move On\" a.k.a. \"The Self Preservation Society\") contains many slang terms.\nRhyming slang has been used to lend authenticity to an East End setting. Examples include \"Lock, Stock and Two Smoking Barrels\" (1998) (wherein the slang is translated via subtitles in one scene); \"The Limey\" (1999); \"Sexy Beast\" (2000); \"Snatch\" (2000); \"Ocean's Eleven\" (2001); and \"Austin Powers in Goldmember\" (2002); \"It's All Gone Pete Tong\" (2004), after BBC radio disc jockey Pete Tong whose name is used in this context as rhyming slang for \"wrong\"; \"Green Street Hooligans\" (2005). In \"Margin Call\" (2011), Will Emerson, played by London-born actor Paul Bettany, asks a friend on the telephone, \"How's the trouble and strife?\" (\"wife\").\n\"Cockneys vs Zombies\" (2012) mocked the genesis of rhyming slang terms when a Cockney character calls zombies \"Trafalgars\" to even his Cockney fellows' puzzlement; he then explains it thus: \"Trafalgar square \u2013 fox and hare \u2013 hairy cheek \u2013 five day week \u2013 weak and feeble \u2013 pins and needles \u2013 needle and stitch \u2013 Abercrombie and Fitch \u2013 Abercrombie: zombie\".\nThe live-action Disney film \"Mary Poppins Returns\" song \"Trip A Little Light Fantastic\" involves Cockney rhyming slang in part of its lyrics, and is primarily spoken by the London lamplighters.\nTelevision.\nOne early US show to regularly feature rhyming slang was the Saturday morning children's show \"The Bugaloos\" (1970\u201372), with the character of Harmony (Wayne Laryea) often incorporating it in his dialogue.\nIn Britain, rhyming slang had a resurgence of popular interest beginning in the 1970s, resulting from its use in a number of London-based television programmes such as \"Steptoe and Son\" (1970\u201374); and \"Not On Your Nellie\" (1974\u201375), starring Hylda Baker as Nellie Pickersgill, alludes to the phrase \"not on your Nellie Duff\", rhyming slang for \"not on your puff\" i.e. not on your life. Similarly, \"The Sweeney\" (1975\u201378) alludes to the phrase \"Sweeney Todd\" for \"Flying Squad\", a rapid response unit of London's Metropolitan Police. In \"The Fall and Rise of Reginald Perrin\" (1976\u201379), a comic twist was added to rhyming slang by way of spurious and fabricated examples which a young man had laboriously attempted to explain to his father (e.g. 'dustbins' meaning 'children', as in 'dustbin lids'='kids'; 'Teds' being 'Ted Heath' and thus 'teeth'; and even 'Chitty Chitty' being 'Chitty Chitty Bang Bang', and thus 'rhyming slang'...). It was also featured in an episode of \"The Good Life\" in the first season (1975) where Tom and Barbara purchase a wood-burning range from a junk trader called Sam, who litters his language with phony slang in hopes of getting higher payment. He comes up with a fake story as to the origin of Cockney Rhyming slang and is caught out rather quickly. In \"The Jeffersons\" season 2 (1976) episode \"The Breakup: Part 2\", Mr. Bentley explains Cockney rhyming slang to George Jefferson, in that \"whistle and flute\" means \"suit\", \"apples and pears\" means \"stairs\", \"plates of meat\" means \"feet\".\nThe use of rhyming slang was also prominent in \"Mind Your Language\" (1977\u201379), \"Citizen Smith\" (1977\u201380), \"Minder\" (1979\u201394), \"Only Fools and Horses\" (1981\u201391), and \"EastEnders\" (1985-). \"Minder\" could be quite uncompromising in its use of obscure forms without any clarification. Thus the non-Cockney viewer was obliged to deduce that, say, \"iron\" was \"male homosexual\" ('iron'='iron hoof'='poof'). One episode in Series 5 of \"Steptoe and Son\" was entitled \"Any Old Iron\", for the same reason, when Albert thinks that Harold is 'on the turn'.\nMusic.\nIn popular music, Spike Jones and his City Slickers recorded \"So 'Elp Me\", based on rhyming slang, in 1950. The 1967 Kinks song \"Harry Rag\" was based on the usage of the name Harry Wragg as rhyming slang for \"fag\" (i.e. a cigarette). The idiom made a brief appearance in the UK-based DJ reggae music of the 1980s in the hit \"Cockney Translation\" by Smiley Culture of South London; this was followed a couple of years later by Domenick and Peter Metro's \"Cockney and Yardie\". London-based artists such as Audio Bullys and Chas &amp; Dave (and others from elsewhere in the UK, such as The Streets, who are from Birmingham) frequently use rhyming slang in their songs.\nBritish-born M.C. MF Doom released an ode entitled \"Rhymin' Slang\", after settling in the UK in 2010. The track was released on the 2012 album JJ Doom album \"Keys to the Kuffs\".\nAnother contributor was Lonnie Donegan who had a song called \"My Old Man's a Dustman\". In it he says his father has trouble putting on his boots \"He's got such a job to pull them up that he calls them daisy roots\".\nLiterature.\nIn modern literature, Cockney rhyming slang is used frequently in the novels and short stories of Kim Newman, for instance in the short story collections \"The Man from the Diogenes Club\" (2006) and \"Secret Files of the Diogenes Club\" (2007), where it is explained at the end of each book.\nIt is also parodied in \"Going Postal\" by Terry Pratchett, which features a geriatric Junior Postman by the name of Tolliver Groat, a speaker of 'Dimwell Arrhythmic Rhyming Slang', the only rhyming slang on the Disc which \"does not actually rhyme\". Thus, a wig is a 'prunes', from 'syrup of prunes', an obvious parody of the Cockney \"syrup\" from \"syrup of figs \u2013 wig\". There are numerous other parodies, though it has been pointed out that the result is even more impenetrable than a conventional rhyming slang and so may not be quite so illogical as it seems, given the assumed purpose of rhyming slang as a means of communicating in a manner unintelligible to all but the initiated.\nIn the book \"Goodbye to All That\" by Robert Graves, a beer is a \"broken square\" as Welch Fusiliers officers walk into a pub and order broken squares when they see men from the Black Watch. The Black Watch had a minor blemish on its record of otherwise unbroken squares. Fistfights ensued.\nIn Dashiell Hammett's \"The Dain Curse\", the protagonist exhibits familiarity with Cockney rhyming slang. referring to gambling at dice with the phrase \"rats and mice.\"\nSport.\nIn Scottish football, a number of clubs have nicknames taken from rhyming slang. Partick Thistle are known as the \"Harry Rags\", which is taken from the rhyming slang of their 'official' nickname \"the jags\". Rangers are known as the \"Teddy Bears\", which comes from the rhyming slang for \"the Gers\" (shortened version of Ran-gers). Heart of Midlothian are known as the \"Jambos\", which comes from \"Jam Tarts\" which is the rhyming slang for \"Hearts\" which is the common abbreviation of the Club's name. Hibernian are also referred to as \"The Cabbage\" which comes from Cabbage and Ribs being the rhyming slang for Hibs.\nIn rugby league, \"meat pie\" is used for try."}
{"id": "7265", "revid": "13541347", "url": "https://en.wikipedia.org/wiki?curid=7265", "title": "Canchim", "text": "The Canchim is a breed of beef cattle developed in Central Brazil by crossing European Charolais cattle with Indubrazil cattle already kept in Brazil where Asian Zebu type cattle are best suited to the tropical conditions. When compared with Zebu bulls, Canchim bulls produce the same number of calves, but heavier and of superior quality. Compared to European breeds, the Canchim bull produces calves with the same weight but in larger numbers. The fast-growing progeny, from crossbred zebu cows with Canchim bulls, can be slaughtered at 18 months old from feedlots after weaning, up to 24 months old from feedlots after grazing and at 30 months from grazing on the range.\nOrigin.\nZebu cattle (Bos Indicus), introduced to Brazil in the last century, were extensively crossbred with herds of native cattle. The Indian breed, well known for its ability to survive in the tropics, adapted quickly to Brazil, and soon populated large areas, considerably improving Brazilian beef cattle breeding. Zebu cattle were however found to be inferior to the European breeds in growth rate and yield of meat. It became clear that the beef cattle population required genetic improvement. Simply placing European beef cattle (Bos Taurus), highly productive in temperate climates, in Central Brazil, would not produce good results, due to their inability to adapt to a tropical environment. Besides the climate, other factors such as the high occurrence of parasites, diseases and the very low nutritional value of the native forage were problems.\nFormation of the breed.\nThe European breed used in the formation of Canchim cattle was Charolais. In 1922 the Ministry of Agriculture imported Charolais cattle to the State of Goias, where they remained till 1936, when they were transferred to S\u00e3o Carlos in the State of S\u00e3o Paulo, to the Canchim Farm of the Government Research Station, EMBRAPA. From this herd originated the dams and sires utilised in the program of crossbreeding.\nThe main Zebu breed which contributed to the formation to the Canchim was the Indubrazil, although Guzer\u00e1 and Nelore cattle were also used. Preference was given to the Indubrasil breed, due to the ease of obtaining large herds at reasonable prices, which would have been difficult with Gir, Nelore or Guzer\u00e1.\nThe alternative crossbreeding programs initiated in 1940 by Dr. Antonio Teixeira Viana had the objective of obtaining first, crossbreeds 5/8 Charolais and 3/8 Zebu and second, 3/8 Charolais x 5/8 Zebu, to evaluate which of the two was the most successful. The total number of Zebu cows utilized to produce the half-breeds was 368, of which 292 were Indubrasil, 44 Guzer\u00e1 and 32 Nelore. All the animals produced were reared exclusively on the range. Control of parasites was done every 15 days and the animals were weighed at birth and monthly. The females were weighed up to 30 months and the males up to 40 months.\nThe data collected during various years of work, permitted an evaluation of the various degrees of crossbreeding. The conclusion was that the 5/8 Charolais and 3/8 Zebu was the most suitable, presenting an excellent frame for meat, precocious, resistance to heat and parasites, and a uniform coat. The first crossbred animals, 5/8 Charolais and 3/8 Zebu, were born in 1953. Thus was born a new type of beef cattle for Central Brazil, with the name CANCHIM, derived from the name of a tree very common in the region where the breed was developed. It was not until 1971 that the Brazilian Association of Canchim Cattle Breeders (ABCCAN) was formed, and on 11 November 1972 the Herd Book was initiated. On 18 May 1983 the Ministry of Agriculture, recognized Canchim type cattle as a Breed.\nNew bloodlines.\nThe Canchim breed, being a synthetic breed, permits breeders, in the development of new crossbreeding systems, to use the breeds used to form the Canchim breed, besides the breed itself, in its development.\nThere are many Canchim breeders forming new blood lines. Today the Nelore breed totally dominates the Zebu breed in the formation of Canchim. American and French Charolais semen, from carefully selected bulls is also used and recommended by the ABCCAN to form new bloodlines."}
{"id": "7266", "revid": "157842", "url": "https://en.wikipedia.org/wiki?curid=7266", "title": "Christkindlmarkt", "text": ""}
{"id": "7271", "revid": "11112747", "url": "https://en.wikipedia.org/wiki?curid=7271", "title": "Communist Party of the Soviet Union", "text": "The Communist Party of the Soviet Union (CPSU) was the founding and ruling political party of the Soviet Union. The CPSU was the sole governing party of the Soviet Union until 1990 when the Congress of People's Deputies modified Article 6 of the 1977 Soviet Constitution, which had previously granted the CPSU a monopoly over the political system.\nThe party started in 1898 as the Bolsheviks, a majority faction from the Russian Social Democratic Labour Party, led by Vladimir Lenin, who seized power in the October Revolution of 1917. 74 years later, its activities were suspended on August 29, 1991 on Soviet territory, soon after a failed coup d'\u00e9tat by hard-line CPSU leaders against Soviet president and party general secretary Mikhail Gorbachev. It was outlawed entirely three months later on 6 November 1991 on Russian territory.\nThe CPSU was a communist party based on democratic centralism. This principle, conceived by Lenin, entails democratic and open discussion of policy issues within the party, followed by the requirement of total unity in upholding the agreed policies. The highest body within the CPSU was the Party Congress, which convened every five years. When the Congress was not in session, the Central Committee was the highest body. Because the Central Committee met twice a year, most day-to-day duties and responsibilities were vested in the Politburo, (previously the Presidium), the Secretariat and the Orgburo (until 1952). The party leader was the head of government and held the office of either General Secretary, Premier or head of state, or two of the three offices concurrently, but never all three at the same time. The party leader was the \"de facto\" chairman of the CPSU Politburo and chief executive of the Soviet Union. The tension between the party and the state (Council of Ministers of the Soviet Union) for the shifting focus of power was never formally resolved. \nAfter the founding of the Soviet Union in 1922, Lenin had introduced a mixed economy, commonly referred to as the New Economic Policy, which allowed for capitalist practices to resume under the Communist Party dictation in order to develop the necessary conditions for socialism to become a practical pursuit in the economically undeveloped country. In 1929, as Joseph Stalin became the leader of the party, Marxism\u2013Leninism, a fusion of the original ideas of German philosopher and economic theorist Karl Marx, and Lenin, became formalized as the party's guiding ideology and would remain so throughout the rest of its existence. The party pursued state socialism, under which all industries were nationalized, and a command economy was implemented. After recovering from the Second World War, reforms were implemented which decentralized economic planning and liberalized Soviet society in general under Nikita Khrushchev. By 1980, various factors, including the continuing Cold War, and ongoing nuclear arms race with the United States and other Western European powers and unaddressed inefficiencies in the economy, led to stagnant economic growth under Alexei Kosygin, and further with Leonid Brezhnev and growing disillusionment. After the younger, vigorous Mikhail Gorbachev assumed leadership in 1985 (following two short-term elderly leaders, Yuri Andropov and Konstantin Chernenko, who quickly died in succession), rapid steps were taken to transform the tottering Soviet economic system in the direction of a market economy once again. Gorbachev and his allies envisioned the introduction of an economy similar to Lenin's earlier New Economic Policy through a program of \"perestroika\", or restructuring, but their reforms, along with the institution of free multi-candidate elections led to a decline in the party's power, and after the dissolution of the Soviet Union, the banning of the party by later last RSFSR President Boris Yeltsin and subsequent first President of an evolving democratic and free-market economy of the successor Russian Federation.\nA number of causes contributed to CPSU's loss of control and the dissolution of the Soviet Union during the early 1990s. Some historians have written that Gorbachev's policy of \"glasnost\" (political openness) was the root cause, noting that it weakened the party's control over society. Gorbachev maintained that \"perestroika\" without \"glasnost\" was doomed to failure anyway. Others have blamed the economic stagnation and subsequent loss of faith by the general populace in communist ideology. In the final years of the CPSU's existence, the Communist Parties of the federal subjects of Russia were united into the Communist Party of the Russian Soviet Federative Socialist Republic (RSFSR). After the CPSU's demise, the Communist Parties of the Union Republics became independent and underwent various separate paths of reform. In Russia, the Communist Party of the Russian Federation emerged and has been regarded as the inheritor of the CPSU's old Bolshevik legacy into the present day.\nHistory.\nEarly years (1898\u20131924).\nThe origin of the CPSU was in the Bolshevik majority faction of the Russian Social Democratic Labour Party (RSDLP), which, under the leadership of Vladimir Lenin, took over the party in January 1912 at the Prague Party Conference. The split with the Menshevik minority was officially acknowledged in May 1917, when the party has been renamed the Russian Social Democratic Labour Party (Bolsheviks) \u2013 or RSDLP(b). Prior to the February Revolution, the first phase of the Russian Revolutions of 1917, the party worked underground as organized anti-Tsarist groups. By the time of the revolution, many of the party's central leaders, including Lenin, were in exile.\nWith Emperor (Tsar/Czar) Nicholas II (1868-1918, reigned 1894-1917), deposed in February 1917, a republic was established and administered by a provisional government, which was largely dominated by the interests of the military, former nobility, major capitalists business owners and democratic socialists. Alongside it, grassroots general assemblies spontaneously formed, called soviets, and a dual-power structure between the soviets and the provisional government was in place until such a time that their differences would be reconciled in a post-provisional government. Lenin was at this time in exile in Switzerland where he, with other dissidents in exile, managed to arrange with the Imperial German government safe passage through Germany in a sealed train back to Russia through the continent amidst the ongoing World War. In April, Lenin arrived in Petrograd (renamed former St. Petersburg) and condemned the provisional government, calling for the advancement of the revolution towards the transformation of the ongoing war into a war of the working class against capitalism. The rebellion proved not yet to be over, as tensions between the social forces aligned with the soviets (councils) and those with the provisional government now led by Alexander Kerensky (1881-1970, in power 1917), came into explosive tensions during that summer.\nThe Bolsheviks had rapidly increased their political presence from May onward through the popularity of their program, notably calling for an immediate end to the war, land reform for the peasants, and restoring food allocation to the urban population. This program was translated to the masses through simple slogans that patiently explained their solution to each crisis the revolution created. Up to July, these policies were disseminated through 41 publications, Pravda being the main paper, with a readership of 320,000. This was roughly halved after the repression of the Bolsheviks following the July Days demonstrations so that even by the end of August, the principal paper of the Bolsheviks had a print run of only 50,000 copies. Despite this, their ideas gained them increasing popularity in elections to the soviets.\nThe factions within the soviets became increasingly polarized in the later summer after armed demonstrations by soldiers at the call of the Bolsheviks and an attempted military coup by commanding Gen. Lavr Kornilov to eliminate the socialists from the provisional government. As the general consensus within the soviets moved leftward, less militant forces began to abandon them, leaving the Bolsheviks in a stronger position. By October, the Bolsheviks were demanding the full transfer of power to the soviets and for total rejection of the Kerensky led provisional government's legitimacy. The provisional government, insistent on maintaining the universally despised war effort on the Eastern Front because of treaty ties with its Allies and fears of Imperial German victory, had become socially isolated and had no enthusiastic support on the streets. On 7 November (25 October, old style), the Bolsheviks led an armed insurrection, which overthrew the Kerensky provisional government and left the soviets as the sole governing force in Russia.\nIn the aftermath of the October Revolution, the soviets united federally and the Russian Socialist Federative Soviet Republic, the world's first constitutionally socialist state, was established. The Bolsheviks were the majority within the soviets and began to fulfill their campaign promises by signing a damaging peace to end the war with the Germans in the Treaty of Brest-Litovsk and transferring estates and imperial lands to workers' and peasants' soviets. In this context, in 1918, RSDLP(b) became All-Russian Communist Party (bolsheviks). Outside of Russia, social-democrats who supported the Soviet government began to identify as communists, while those who opposed it retained the social-democratic label.\nIn 1921, as the Civil War was drawing to a close, Lenin proposed the New Economic Policy (NEP), a system of state capitalism that started the process of industrialization and post-war recovery. The NEP ended a brief period of intense rationing called \"war communism\" and began a period of a market economy under Communist dictation. The Bolsheviks believed at this time that Russia, being among the most economically undeveloped and socially backward countries in Europe, had not yet reached the necessary conditions of development for socialism to become a practical pursuit and that this would have to wait for such conditions to arrive under capitalist development as had been achieved in more advanced countries such as England and Germany. On 30 December 1922, the Russian SFSR joined former territories of the Russian Empire to form the Union of Soviet Socialist Republics (USSR), of which Lenin was elected leader. On 9 March 1923, Lenin suffered a stroke, which incapacitated him and effectively ended his role in government. He died on 21 January 1924, only thirteen months after the founding of the Soviet Union, of which he would become regarded as the founding father.\nStalin era (1924\u201353).\nAfter Lenin's death, a power struggle ensued between Joseph Stalin, the party's General Secretary, and Leon Trotsky, the Minister of Defence, each with highly contrasting visions for the future direction of the country. Trotsky sought to implement a policy of permanent revolution, which was predicated on the notion that the Soviet Union would not be able to survive in a socialist character when surrounded by hostile governments and therefore concluded that it was necessary to actively support similar revolutions in the more advanced capitalist countries. Stalin, however, argued that such a foreign policy would not be feasible with the capabilities then possessed by the Soviet Union and that it would invite the country's destruction by engaging in armed conflict. Rather, Stalin argued that the Soviet Union should, in the meantime, pursue peaceful coexistence and invite foreign investment in order to develop the country's economy and build socialism in one country.\nUltimately, Stalin gained the greatest support within the party, and Trotsky, who was increasingly viewed as a collaborator with outside forces in an effort to depose Stalin, was isolated and subsequently expelled from the party and exiled from the country in 1928. Stalin's policies henceforth would later become collectively known as Stalinism. In 1925, the name of the party was changed to the All-Union Communist Party (bolsheviks), reflecting that the republics outside of Russia proper were no longer part of an all-encompassing Russian state. The acronym was usually transliterated as VKP(b), or sometimes VCP(b). Stalin sought to formalize the party's ideological outlook into a philosophical hybrid of the original ideas of Lenin with orthodox Marxism into what would be called Marxism\u2013Leninism. Stalin's position as General Secretary became the top executive position within the party, giving Stalin significant authority over party and state policy.\nBy the end of the 1920s, diplomatic relations with western countries were deteriorating to the point that there was a growing fear of another allied attack on the Soviet Union. Within the country, the conditions of the NEP had enabled growing inequalities between increasingly wealthy strata and the remaining poor. The combination of these tensions led the party leadership to conclude that it was necessary for the government's survival to pursue a new policy that would centralize economic activity and accelerate industrialization. To do this, the first five-year plan was implemented in 1928. The plan doubled the industrial workforce, proletarianizing many of the peasants by removing them from their land and assembling them into urban centers. Peasants who remained in agricultural work were also made to have a similarly proletarian relationship to their labor through the policies of collectivization, which turned feudal-style farms into collective farms which would be in a cooperative nature under the direction of the state. These two shifts changed the base of Soviet society towards a more working-class alignment. The plan was fulfilled ahead of schedule in 1932.\nThe success of industrialization in the Soviet Union led western countries, such as the United States, to open diplomatic relations with the Soviet government. In 1933, after years of unsuccessful workers' revolutions (including a short-lived Bavarian Soviet Republic) and spiraling economic calamity, Adolf Hitler came to power in Germany, violently suppressing the revolutionary organizers and posing a direct threat to the Soviet Union that ideologically supported them. The threat of fascist sabotage and imminent attack greatly exacerbated the already existing tensions within the Soviet Union and the Communist Party. A wave of paranoia overtook Stalin and the party leadership and spread through Soviet society. Seeing potential enemies everywhere, leaders of the government security apparatuses began severe crackdowns known as the Great Purge. In total, hundreds of thousands of people, many of whom were posthumously recognized as innocent, were arrested and either sent to prison camps or executed. Also during this time, a campaign against religion was waged in which the Russian Orthodox Church, which had long been a political arm of tsarism before the revolution, was targeted for repression and organized religion was generally removed from public life and made into a completely private matter, with many churches, mosques and other shrines being repurposed or demolished.\nThe Soviet Union was the first to warn of the impending danger of invasion from Nazi Germany to the international community. The western powers, however, remained committed to maintaining peace and avoiding another war breaking out, many considering the Soviet Union's warnings to be an unwanted provocation. After many unsuccessful attempts to create an anti-fascist alliance among the western countries, including trying to rally international support for the Spanish Republic in its struggle against a fascist military coup supported by Germany and Italy, in 1939 the Soviet Union signed a non-aggression pact with Germany which would be broken in June 1941 when the German military invading the Soviet Union in the largest land invasion in history, beginning the Great Patriotic War.\nThe Communist International was dissolved in 1943 after it was concluded that such an organization had failed to prevent the rise of fascism and the global war necessary to defeat it. After the 1945 Allied victory of World War II, the Party held to a doctrine of establishing socialist governments in the post-war occupied territories that would be administered by Communists loyal to Stalin's administration. The party also sought to expand its sphere of influence beyond the occupied territories, using proxy wars and espionage and providing training and funding to promote Communist elements abroad, leading to the establishment of the Cominform in 1947.\nIn 1949, the Communists emerged victorious in the Chinese Civil War, causing an extreme shift in the global balance of forces and greatly escalating tensions between the Communists and the western powers, fueling the Cold War. In Europe, Yugoslavia, under the leadership of Josip Broz Tito, acquired the territory of Trieste, causing conflict both with the western powers and with the Stalin administration who opposed such a provocative move. Furthermore, the Yugoslav Communists actively supported the Greek Communists during their civil war, further frustrating the Soviet government. These tensions led to a Tito\u2013Stalin Split, which marked the beginning of international sectarian division within the world communist movement.\nPost-Stalin years (1953\u201385).\nAfter Stalin's death, Khrushchev rose to the top post by overcoming political adversaries, including Lavrentiy Beria and Georgy Malenkov, in a power struggle. In 1955, Khrushchev achieved the demotion of Malenkov and secured his own position as Soviet leader. Early in his rule and with the support of several members of the Presidium, Khrushchev initiated the Thaw, which effectively ended the Stalinist mass terror of the prior decades and reduced socio-economic oppression considerably. At the 20th Congress held in 1956, Khrushchev denounced Stalin's crimes, being careful to omit any reference to complicity by any sitting Presidium members. His economic policies, while bringing about improvements, were not enough to fix the fundamental problems of the Soviet economy. The standard of living for ordinary citizens did increase; 108 million people moved into new housing between 1956 and 1965.\nKhrushchev's foreign policies led to the Sino-Soviet split, in part a consequence of his public denunciation of Stalin. Khrushchev improved relations with Josip Broz Tito's League of Communists of Yugoslavia but failed to establish the close, party-to-party relations that he wanted. While the Thaw reduced political oppression at home, it led to unintended consequences abroad, such as the Hungarian Revolution of 1956 and unrest in Poland, where the local citizenry now felt confident enough to rebel against Soviet control. Khrushchev also failed to improve Soviet relations with the West, partially because of a hawkish military stance. In the aftermath of the Cuban Missile Crisis, Khrushchev's position within the party was substantially weakened. Shortly before his eventual ousting, he tried to introduce economic reforms championed by Evsei Liberman, a Soviet economist, which tried to implement market mechanisms into the planned economy.\nKhrushchev was ousted on 14 October 1964 in a Central Committee plenum that officially cited his inability to listen to others, his failure in consulting with the members of the Presidium, his establishment of a cult of personality, his economic mismanagement, and his anti-party reforms as the reasons he was no longer fit to remain as head of the party. He was succeeded in office by Leonid Brezhnev as First Secretary and Alexei Kosygin as Chairman of the Council of Ministers.\nThe Brezhnev era began with a rejection of Khrushchevism in virtually every arena except one: continued opposition to Stalinist methods of terror and political violence. Khrushchev's policies were criticized as voluntarism, and the Brezhnev period saw the rise of neo-Stalinism. While Stalin was never rehabilitated during this period, the most conservative journals in the country were allowed to highlight positive features of his rule.\nAt the 23rd Congress held in 1966, the names of the office of First Secretary and the body of the Presidium reverted to their original names: General Secretary and Politburo, respectively. At the start of his premiership, Kosygin experimented with economic reforms similar to those championed by Malenkov, including prioritizing light industry over heavy industry to increase the production of consumer goods. Similar reforms were introduced in Hungary under the name New Economic Mechanism; however, with the rise to power of Alexander Dub\u010dek in Czechoslovakia, who called for the establishment of \"socialism with a human face\", all non-conformist reform attempts in the Soviet Union were stopped.\nDuring his rule, Brezhnev supported \"d\u00e9tente\", a passive weakening of animosity with the West with the goal of improving political and economic relations. However, by the 25th Congress held in 1976, political, economic and social problems within the Soviet Union began to mount, and the Brezhnev administration found itself in an increasingly difficult position. The previous year, Brezhnev's health began to deteriorate. He became addicted to painkillers and needed to take increasingly more potent medications to attend official meetings. Because of the \"trust in cadres\" policy implemented by his administration, the CPSU leadership evolved into a gerontocracy. At the end of Brezhnev's rule, problems continued to amount; in 1979 he consented to the Soviet intervention in Afghanistan to save the embattled communist regime there and supported the oppression of the Solidarity movement in Poland. As problems grew at home and abroad, Brezhnev was increasingly ineffective in responding to the growing criticism of the Soviet Union by Western leaders, most prominently by US Presidents Jimmy Carter and Ronald Reagan, and UK Prime Minister Margaret Thatcher. The CPSU, which had wishfully interpreted the financial crisis of the 1970s as the beginning of the end of capitalism, found its country falling far behind the West in its economic development. Brezhnev died on 10 November 1982, and was succeeded by Yuri Andropov on 12 November.\nAndropov, a staunch anti-Stalinist, chaired the KGB during most of Brezhnev's reign. He had appointed several reformers to leadership positions in the KGB, many of whom later became leading officials under Gorbachev. Andropov supported increased openness in the press, particularly regarding the challenges facing the Soviet Union. Andropov was in office briefly, but he appointed a number of reformers, including Yegor Ligachev, Nikolay Ryzhkov and Mikhail Gorbachev, to important positions. He also supported a crackdown on absenteeism and corruption. Andropov had intended to let Gorbachev succeed him in office, but Konstantin Chernenko and his supporters suppressed the paragraph in the letter which called for Gorbachev's elevation. Andropov died on 9 February 1984 and was succeeded by Chernenko. Throughout his short leadership, Chernenko was unable to consolidate power, and effective control of the party organization remained in Gorbachev's control. Chernenko died on 10 March 1985 and was succeeded in office by Gorbachev on 11 March 1985.\nGorbachev and the party's demise (1985\u201391).\nThe Politburo elected Gorbachev as CPSU General Secretary on 11 March 1985, one day after Chernenko's death. When Gorbachev acceded to power, the Soviet Union was stagnating but was stable and might have continued largely unchanged into the 21st century if not for Gorbachev's reforms.\nGorbachev conducted a significant personnel reshuffling of the CPSU leadership, forcing old party conservatives out of office. In 1985 and early 1986 the new leadership of the party called for \"uskoreniye\" (). Gorbachev reinvigorated the party ideology, adding new concepts and updating older ones. Positive consequences of this included the allowance of \"pluralism of thought\" and a call for the establishment of \"socialist pluralism\" (literally, socialist democracy). Gorbachev introduced a policy of \"glasnost\" (, meaning \"openness\" or \"transparency\") in 1986, which led to a wave of unintended democratization. According to the British researcher of Russian affairs, Archie Brown, the democratization of the Soviet Union brought mixed blessings to Gorbachev; it helped him to weaken his conservative opponents within the party but brought out accumulated grievances which had been suppressed during the previous decades. \nIn reaction to these changes, a conservative movement gained momentum in 1987 in response to Boris Yeltsin's dismissal as First Secretary of the CPSU Moscow City Committee. On 13 March 1988, Nina Andreyeva, a university lecturer, wrote an article titled \"I Cannot Forsake My Principles\". The publication was planned to occur when both Gorbachev and his protege Alexander Yakovlev were visiting foreign countries. In their place, Yegor Ligachev led the party organization and told journalists that the article was \"a benchmark for what we need in our ideology today\". Upon Gorbachev's return, the article was discussed at length during a Politburo meeting; it was revealed that nearly half of its members were sympathetic to the letter and opposed further reforms which could weaken the party. The meeting lasted for two days, but on 5 April a Politburo resolution responded with a point-by-point rebuttal to Andreyeva's article.\nGorbachev convened the 19th Party Conference in June 1988. He criticized leading party conservatives - Ligachev, Andrei Gromyko and Mikhail Solomentsev. In turn, conservative delegates attacked Gorbachev and the reformers. According to Brown, there had not been as much open discussion and dissent at a party meeting since the early 1920s.\nDespite the deep-seated opposition to further reform, the CPSU remained hierarchical; the conservatives acceded to Gorbachev's demands in deference to his position as the CPSU General Secretary. The 19th Conference approved the establishment of the Congress of People's Deputies (CPD) and allowed for contested elections between the CPSU and independent candidates. Other organized parties were not allowed. The CPD was elected in 1989; one-third of the seats were appointed by the CPSU and other public organizations to sustain the Soviet one-party state. The elections were democratic, but most elected CPD members opposed any more radical reform. The elections featured the highest electoral turnout in Russian history; no election before or since had a higher participation rate. An organized opposition was established within the legislature under the name Inter-Regional Group of Deputies by dissident Andrei Sakharov. An unintended consequence of these reforms was the increased anti-CPSU pressure; in March 1990, at a session of the Supreme Soviet of the Soviet Union, the party was forced to relinquish its political monopoly of power, in effect turning the Soviet Union into a liberal democracy.\nThe CPSU's demise began in March 1990, when state bodies eclipsed party elements in power by . From then until the Soviet Union's disestablishment, Gorbachev ruled the country through the newly created post of President of the Soviet Union. Following this, the central party apparatus didn't play a practical role in Soviet affairs. Gorbachev had become independent from the Politburo and faced few constraints from party leaders. In the summer of 1990 the party convened the 28th Congress. A new Politburo was elected, previous incumbents (except Gorbachev and Vladimir Ivashko, the CPSU Deputy General Secretary) were removed. Later that year, the party began work on a new program with a working title, \"Towards a Humane, Democratic Socialism\". According to Brown, the program reflected Gorbachev's journey from an orthodox communist to a European social democrat. The freedoms of thought and organization which Gorbachev allowed led to a rise in nationalism in the Soviet republics, indirectly weakening the central authorities. In response to this, a referendum took place in 1991, in which most of the union republics voted to preserve the union in a different form. In reaction to this, conservative elements within the CPSU launched the August 1991 coup, which overthrew Gorbachev but failed to preserve the Soviet Union. When Gorbachev resumed control (21 August 1991) after the coup's collapse, he resigned from the CPSU on 24 August 1991 and operations were handed over to Ivashko. On 29 August 1991 the activity of the CPSU was suspended throughout the country, on 6 November Yeltsin banned the activities of the party in Russia and Gorbachev resigned from the presidency on 25 December; the following day the Soviet of Republics dissolved the Soviet Union.\nOn 30 November 1992, the Russian Constitutional Court not only upheld this decree but barred the CPSU from ever being refounded. It accepted Yeltsin's argument that the CPSU was not a true party, but a criminal organization that had ruled the Soviet Union as a dictatorship in violation of the Soviet Constitution.\nAfter the dissolution of the Soviet Union in 1991, Russian adherents to the CPSU tradition, particularly as it existed before Gorbachev, reorganized themselves within the Communist Party of the Russian Federation (CPRF). Today a wide range of parties in Russia present themselves as successors of CPSU. Several of them have used the name \"CPSU\". However, the CPRF is generally seen (due to its massive size) as the heir of the CPSU in Russia. Additionally, the CPRF was initially founded as the Communist Party of the Russian SFSR in 1990 (sometime before the abolition of the CPSU) and was seen by critics as a \"Russian-nationalist\" counterpart to the CPSU.\nGoverning style.\nThe style of governance in the party alternated between collective leadership and a cult of personality. Collective leadership split power between the Politburo, the Central Committee, and the Council of Ministers to hinder any attempts to create a one-man dominance over the Soviet political system. By contrast, Stalin's period as the leader was characterized by an extensive cult of personality. Regardless of leadership style, all political power in the Soviet Union was concentrated in the organization of the CPSU.\nDemocratic centralism.\nDemocratic centralism is an organizational principle conceived by Lenin. According to Soviet pronouncements, democratic centralism was distinguished from \"bureaucratic centralism\", which referred to high-handed formulae without knowledge or discussion. In democratic centralism, decisions are taken after discussions, but once the general party line has been formed, discussion on the subject must cease. No member or organizational institution may dissent on a policy after it has been agreed upon by the party's governing body; to do so would lead to expulsion from the party (formalized at the 10th Congress). Because of this stance, Lenin initiated a ban on factions, which was approved at the 10th Congress.\nLenin believed that democratic centralism safeguarded both party unity and ideological correctness. He conceived of the system after the events of 1917 when several socialist parties \"deformed\" themselves and actively began supporting nationalist sentiments. Lenin intended that the devotion to policy required by centralism would protect the parties from such revisionist ills and bourgeois defamation of socialism. Lenin supported the notion of a highly centralized vanguard party, in which ordinary party members elected the local party committee, the local party committee elected the regional committee, the regional committee elected the Central Committee, and the Central Committee elected the Politburo, Orgburo, and the Secretariat. Lenin believed that the party needed to be ruled from the center and have at its disposal power to mobilize party members at will. This system was later introduced in communist parties abroad through the Communist International (Comintern).\nVanguardism.\nA central tenet of Leninism was that of the vanguard party. In a capitalist society, the party was to represent the interests of the working class and all of those who were exploited by capitalism in general; however, it was not to become a part of that class. Lenin decided that the party's sole responsibility was to articulate and plan the long-term interests of the oppressed classes. It was not responsible for the daily grievances of those classes; that was the responsibility of the trade unions. According to Lenin, the Party and the oppressed classes could never become one because the Party was responsible for leading the oppressed classes to victory. The basic idea was that a small group of organized people could wield power disproportionate to their size with superior organizational skills. Despite this, until the end of his life, Lenin warned of the danger that the party could be taken over by bureaucrats, by a small clique, or by an individual. Toward the end of his life, he criticized the bureaucratic inertia of certain officials and admitted to problems with some of the party's control structures, which were to supervise organizational life.\nOrganization.\nCongress.\nThe Congress, nominally the highest organ of the party, was convened every five years. Leading up to the October Revolution and until Stalin's consolidation of power, the Congress was the party's main decision-making body. However, after Stalin's ascension, the Congresses became largely symbolic. CPSU leaders used Congresses as a propaganda and control tool. The most noteworthy Congress since the 1930s was the 20th Congress, in which Khrushchev denounced Stalin in a speech titled \"The Personality Cult and its Consequences\".\nDespite delegates to Congresses losing their powers to criticize or remove party leadership, the Congresses functioned as a form of elite-mass communication. They were occasions for the party leadership to express the party line over the next five years to ordinary CPSU members and the general public. The information provided was general, ensuring that party leadership retained the ability to make specific policy changes as they saw fit.\nThe Congresses also provided the party leadership with formal legitimacy by providing a mechanism for the election of new members and the retirement of old members who had lost favor. The elections at Congresses were all predetermined and the candidates who stood for seats to the Central Committee and the Central Auditing Commission were approved beforehand by the Politburo and the Secretariat. A Congress could also provide a platform for the announcement of new ideological concepts. For instance, at the 22nd Congress, Khrushchev announced that the Soviet Union would see \"communism in twenty years\" a position later retracted.\nA Conference, officially referred to as an All-Union Conference, was convened between Congresses by the Central Committee to discuss party policy and to make personnel changes within the Central Committee. 19 conferences were convened during the CPSU's existence. The 19th Congress held in 1952 removed the clause in the party's statute which stipulated that a party Conference could be convened. The clause was reinstated at the 23rd Congress, which was held in 1966.\nCentral Committee.\nThe Central Committee was a collective body elected at the annual party congress. It was mandated to meet at least twice a year to act as the party's supreme governing body. Membership of the Central Committee increased from 71 full members in 1934 to 287 in 1976. Central Committee members were elected to the seats because of the offices they held, not on their personal merit. Because of this, the Central Committee was commonly considered an indicator for Sovietologists to study the strength of the different institutions. The Politburo was elected by and reported to the Central Committee. Besides the Politburo, the Central Committee also elected the Secretariat and the General Secretarythe \"de facto\" leader of the Soviet Union. In 1919\u20131952, the Orgburo was also elected in the same manner as the Politburo and the Secretariat by the plenums of the Central Committee. In between Central Committee plenums, the Politburo and the Secretariat were legally empowered to make decisions on its behalf. The Central Committee or the Politburo and/or Secretariat on its behalf could issue nationwide decisions; decisions on behalf of the party were transmitted from the top to the bottom.\nUnder Lenin, the Central Committee functioned much as the Politburo did during the post-Stalin era, serving as the party's governing body. However, as the membership in the Central Committee increased, its role was eclipsed by the Politburo. Between Congresses, the Central Committee functioned as the Soviet leadership's source of legitimacy. The decline in the Central Committee's standing began in the 1920s; it was reduced to a compliant body of the Party leadership during the Great Purge. According to party rules, the Central Committee was to convene at least twice a year to discuss political mattersbut not matters relating to military policy. The body remained largely symbolic after Stalin's consolidation; leading party officials rarely attended meetings of the Central Committee.\nCentral Auditing Commission.\nThe Central Auditing Commission (CAC) was elected by the party Congresses and reported only to the party Congress. It had about as many members as the Central Committee. It was responsible for supervising the expeditious and proper handling of affairs by the central bodies of the Party; it audited the accounts of the Treasury and the enterprises of the Central Committee. It was also responsible for supervising the Central Committee apparatus, making sure that its directives were implemented and that Central Committee directives complied with the party Statute.\nStatute.\nThe Statute (also referred to as the Rules, Charter and Constitution) was the party's by-laws and controlled life within the CPSU. The 1st Statute was adopted at the 2nd Congress of the Russian Social Democratic Labour Partythe forerunner of the CPSU. How the Statute was to be structured and organized led to a schism within the party, leading to the establishment of two competing factions; Bolsheviks (literally \"majority\") and Mensheviks (literally \"minority\"). The 1st Statute was based upon Lenin's idea of a centralized vanguard party. The 4th Congress, despite a majority of Menshevik delegates, added the concept of democratic centralism to Article 2 of the Statute. The 1st Statute lasted until 1919 when the 8th Congress adopted the 2nd Statute. It was nearly five times as long as the 1st Statute and contained 66 articles. It was amended at the 9th Congress. At the 11th Congress, the 3rd Statute was adopted with only minor amendments being made. New statutes were approved at the 17th and 18th Congresses respectively. The last party statute, which existed until the dissolution of the CPSU, was adopted at the 22nd Congress.\nCentral Committee apparatus.\nGeneral Secretary.\nGeneral Secretary of the Central Committee was the title given to the overall leader of the party. The office was synonymous with the leader of the Soviet Union after Joseph Stalin's consolidation of power in the 1920s. Stalin used the office of General Secretary to create a strong power base for himself. The office was formally titled \"First Secretary\" between 1952 and 1966.\nPolitburo.\nThe Political Bureau (Politburo), known as the Presidium from 1952 to 1966, was the highest party organ when the Congress and the Central Committee were not in session. Until the 19th Conference in 1988, the Politburo alongside the Secretariat controlled appointments and dismissals nationwide. In the post-Stalin period, the Politburo controlled the Central Committee apparatus through two channels; the General Department distributed the Politburo's orders to the Central Committee departments and through the personnel overlap which existed within the Politburo and the Secretariat. This personnel overlap gave the CPSU General Secretary a way of strengthening his position within the Politburo through the Secretariat. Kirill Mazurov, Politburo member from 1965 to 1978, accused Brezhnev of turning the Politburo into a \"second echelon\" of power. He accomplished this by discussing policies before Politburo meetings with Mikhail Suslov, Andrei Kirilenko, Fyodor Kulakov and Dmitriy Ustinov among others, who held seats both in the Politburo and the Secretariat. Mazurov's claim was later verified by Nikolai Ryzhkov, the Chairman of the Council of Ministers under Gorbachev. Ryzhkov said that Politburo meetings lasted only 15 minutes because the people close to Brezhnev had already decided what was to be approved.\nThe Politburo was abolished and replaced by a Presidium in 1952 at the 19th Congress. In the aftermath the 19th Congress and the 1st Plenum of the 19th Central Committee, Stalin ordered the creation of the Bureau of the Presidium, which acted as the standing committee of the Presidium. On 6 March 1953, one day after Stalin's death, a new and smaller Presidium was elected, and the Bureau of the Presidium was abolished in a joint session with the Presidium of the Supreme Soviet and the Council of Ministers.\nUntil 1990, the CPSU General Secretary acted as the informal chairman of the Politburo. During the first decades of the CPSU's existence, the Politburo was officially chaired by the Chairman of the Council of People's Commissars; first by Lenin, then by Aleksey Rykov, Molotov, Stalin and Malenkov. After 1922, when Lenin was incapacitated, Lev Kamenev as Deputy Chairman of the Council of People's Commissars chaired the Politburo's meetings. This tradition lasted until Khrushchev's consolidation of power. In the first post-Stalin years, when Malenkov chaired Politburo meetings, Khrushchev as First Secretary signed all Central Committee documents into force. From 1954 until 1958, Khrushchev chaired the Politburo as First Secretary, but in 1958 he dismissed and succeeded Nikolai Bulganin as Chairman of the Council of Ministers. During this period, the informal position of Second Secretarylater formalized as Deputy General Secretarywas established. The Second Secretary became responsible for chairing the Secretariat in place of the General Secretary. When the General Secretary could not chair the meetings of the Politburo, the Second Secretary would take his place. This system survived until the dissolution of the CPSU in 1991.\nTo be elected to the Politburo, a member had to serve in the Central Committee. The Central Committee elected the Politburo in the aftermath of a party Congress. Members of the Central Committee were given a predetermined list of candidates for the Politburo having only one candidate for each seat; for this reason, the election of the Politburo was usually passed unanimously. The greater the power held by the sitting CPSU General Secretary, the higher the chance that the Politburo membership would be approved.\nSecretariat.\nThe Secretariat headed the CPSU's central apparatus and was solely responsible for the development and implementation of party policies. It was legally empowered to take over the duties and functions of the Central Committee when it was not in the plenum (did not hold a meeting). Many members of the Secretariat concurrently held a seat in the Politburo. According to a Soviet textbook on party procedures, the Secretariat's role was that of \"leadership of current work, chiefly in the realm of personnel selection and in the organization of the verification of fulfillment of party-state decisions\". \"Selections of personnel\" () in this instance meant the maintenance of general standards and the criteria for selecting various personnel. \"Verification of fulfillment\" () of party and state decisions meant that the Secretariat instructed other bodies.\nThe powers of the Secretariat were weakened under Mikhail Gorbachev, and the Central Committee Commissions took over the functions of the Secretariat in 1988. Yegor Ligachev, a Secretariat member, said that the changes completely destroyed the Secretariat's hold on power and made the body almost superfluous. Because of this, the Secretariat rarely met during the next two years. It was revitalized at the 28th Party Congress in 1990, and the Deputy General Secretary became the official head of the Secretariat.\nOrgburo.\nThe Organizational Bureau, or Orgburo, existed from 1919 to 1952 and was one of three leading bodies of the party when the Central Committee was not in session. It was responsible for \"organizational questions, the recruitment, and allocation of personnel, the coordination of activities of the party, government and social organizations (e.g., trade unions and youth organizations), improvement to the party's structure, the distribution of information and reports within the party\". The 19th Congress abolished the Orgburo and its duties and responsibilities were taken over by the Secretariat. At the beginning, the Orgburo held three meetings a week and reported to the Central Committee every second week. Lenin described the relation between the Politburo and the Orgburo as \"the Orgburo allocates forces, while the Politburo decides policy\". A decision of the Orgburo was implemented by the Secretariat. However, the Secretariat could make decisions in the Orgburo's name without consulting its members, but if one Orgburo member objected to a Secretariat resolution, the resolution would not be implemented. In the 1920s, if the Central Committee could not convene the Politburo and the Orgburo would hold a joint session in its place.\nControl Commission.\nThe Central Control Commission (CCC) functioned as the party's supreme court. The CCC was established at the 9th All-Russian Conference in September 1920, but rules organizing its procedure were not enacted before the 10th Congress. The 10th Congress formally established the CCC on all party levels and stated that it could only be elected at a party congress or a party conference. The CCC and the CCs were formally independent but had to make decisions through the party committees at their level, which led them in practice to lose their administrative independence. At first, the primary responsibility of the CCs was to respond to party complaints, focusing mostly on party complaints of factionalism and bureaucratism. At the 11th Congress, the brief of the CCs was expanded; it became responsible for overseeing party discipline. In a bid to further centralize the powers of the CCC, a Presidium of the CCC, which functioned in a similar manner to the Politburo in relation to the Central Committee, was established in 1923. At the 18th Congress, party rules regarding the CCC were changed; it was now elected by the Central Committee and was subordinate to the Central Committee.\nCCC members could not concurrently be members of the Central Committee. To create an organizational link between the CCC and other central-level organs, the 9th All-Russian Conference created the joint CC\u2013CCC plenums. The CCC was a powerful organ; the 10th Congress allowed it to expel full and candidate Central Committee members and members of their subordinate organs if two-thirds of attendants at a CC\u2013CCC plenum voted for such. At its first such session in 1921, Lenin tried to persuade the joint plenum to expel Alexander Shliapnikov from the party; instead of expelling him, Shliapnikov was given a severe reprimand.\nDepartments.\nThe leader of a department was usually given the title \"head\" (). In practice, the Secretariat had a major say in the running of the departments; for example, five of eleven secretaries headed their own departments in 1978. Normally, specific secretaries were given supervising duties over one or more departments. Each department established its own cellscalled sectionswhich specialized in one or more fields. During the Gorbachev era, a variety of departments made up the Central Committee apparatus. The Party Building and Cadre Work Department assigned party personnel in the nomenklatura system. The State and Legal Department supervised the armed forces, KGB, the Ministry of Internal Affairs, the trade unions, and the Procuracy. Before 1989, the Central Committee had several departments, but some were abolished that year. Among these departments was the Economics Department that was responsible for the economy as a whole, one for machine building, one for the chemical industry, etc. The party abolished these departments to remove itself from the day-to-day management of the economy in favor of government bodies and a greater role for the market, as a part of the perestroika process. In their place, Gorbachev called for the creations of commissions with the same responsibilities as departments, but giving more independence from the state apparatus. This change was approved at the 19th Conference, which was held in 1988. Six commissions were established by late 1988.\n\"Pravda\".\n\"Pravda\" (\"The Truth\") was the leading newspaper in the Soviet Union. The Organizational Department of the Central Committee was the only organ empowered to dismiss \"Pravda\" editors. In 1905, \"Pravda\" began as a project by members of the Ukrainian Social Democratic Labour Party. Leon Trotsky was approached about the possibility of running the new paper because of his previous work on Ukrainian newspaper \"Kyivan Thought\". The first issue of \"Pravda\" was published on 3 October 1908 in Lvov, where it continued until the publication of the sixth issue in November 1909, when the operation was moved to Vienna, Austria-Hungary. During the Russian Civil War, sales of \"Pravda\" were curtailed by \"Izvestia\", the government run newspaper. At the time, the average reading figure for \"Pravda\" was 130,000. This Vienna-based newspaper published its last issue in 1912 and was succeeded the same year by a new newspaper dominated by the Bolsheviks, also called \"Pravda\", which was headquartered in St. Petersburg. The paper's main goal was to promote Marxist\u2013Leninist philosophy and expose the lies of the bourgeoisie. In 1975, the paper reached a circulation of 10.6 million. It's currently owned by the Communist Party of the Russian Federation.\nHigher Party School.\nThe Higher Party School (HPS) was the organ responsible for teaching cadres in the Soviet Union. It was the successor of the Communist Academy, which was established in 1918. The HPS was established in 1939 as the Moscow Higher Party School and it offered its students a two-year training course for becoming a CPSU official. It was reorganized in 1956 to that it could offer more specialized ideological training. In 1956, the school in Moscow was opened for students from socialist countries outside the Soviet Union. The Moscow Higher Party School was the party school with the highest standing. The school itself had eleven faculties until a 1972 Central Committee resolution demanded a reorganization of the curriculum. The first regional HPS outside Moscow was established in 1946 and by the early 1950s there were 70 Higher Party Schools. During the reorganization drive of 1956, Khrushchev closed 13 of them and reclassified 29 as inter-republican and inter-oblast schools.\nLower-level organization.\nRepublican and local organization.\nThe lowest organ above the primary party organization (PPO) was the district level. Every two years, the local PPO would elect delegates to the district-level party conference, which was overseen by a secretary from a higher party level. The conference elected a Party Committee and First Secretary and re-declared the district's commitment to the CPSU's program. In between conferences, the \"raion\" party committeecommonly referred to as \"raikom\"was vested with ultimate authority. It convened at least six times a year to discuss party directives and to oversee the implementation of party policies in their respective districts, to oversee the implementation of party directives at the PPO-level, and to issue directives to PPOs. 75\u201380 percent of raikom members were full members, while the remaining 20\u201325 were non-voting, candidate members. Raikom members were commonly from the state sector, party sector, Komsomol or the trade unions.\nDay-to-day responsibility of the raikom was handed over to a Politburo, which usually composed of 12 members. The district-level First Secretary chaired the meetings of the local Politburo and the raikom, and was the direct link between the district and the higher party echelons. The First Secretary was responsible for the smooth running of operations. The raikom was headed by the local apparatthe local agitation department or industry department. A raikom usually had no more than 4 or 5 departments, each of which was responsible for overseeing the work of the state sector but would not interfere in their work.\nThis system remained identical at all other levels of the CPSU hierarchy. The other levels were cities, oblasts (regions) and republics. The district-level elected delegates to a conference held at least held every three years to elect the party committee. The only difference between the oblast and the district level was that the oblast had its own Secretariat and had more departments at its disposal. The oblast's party committee in turn elected delegates to the republican-level Congress, which was held every five years. The Congress then elected the Central Committee of the republic, which in turn elected a First Secretary and a Politburo. Until 1990, the Russian Soviet Federative Socialist Republic was the only republic that did not have its own republican branch, being instead represented by the CPSU Central Committee.\nPrimary party organizations.\nThe primary party organization (PPO) was the lowest level in the CPSU hierarchy. PPOs were organized cells consisting of three or more members. A PPO could exist anywhere; for example, in a factory or a student dormitory. They functioned as the party's \"eyes and ears\" at the lowest level and were used to mobilize support for party policies. All CPSU members had to be a member of a local PPO. The size of a PPO varied from three people to several hundred, depending upon its setting. In a large enterprise, a PPO usually had several hundred members. In such cases, the PPO was divided into bureaus based upon production-units. Each PPO was led by an executive committee and an executive committee secretary. Each executive committee is responsible for the PPO executive committee and its secretary. In small PPOs, members met periodically to mainly discuss party policies, ideology, or practical matters. In such a case, the PPO secretary was responsible for collecting party dues, reporting to higher organs, and maintaining the party records. A secretary could be elected democratically through a secret ballot, but that was not often the case; in 1979, only 88 out of the over 400,000 PPOs were elected in this fashion. The remainder were chosen by a higher party organ and ratified by the general meetings of the PPO. The PPO general meeting was responsible for electing delegates to the party conference at either the district- or town-level, depending on where the PPO was located.\nMembership.\nMembership of the party was not open. To become a party member, one had to be approved by various committees, and one's past was closely scrutinized. As generations grew up having known nothing before the Soviet Union, party membership became something one generally achieved after passing a series of stages. Children would join the Young Pioneers and, at the age of 14, might graduate to the Komsomol (Young Communist League). Ultimately, as an adult, if one had shown the proper adherence to party discipline \u2013 or had the right connections, one would become a member of the Communist Party itself. Membership of the party carried obligations as it expected Komsomol and CPSU members to pay dues and to carry out appropriate assignments and \"social tasks\" (\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0440\u0430\u0431\u043e\u0442\u0430).\nIn 1918, party membership was approximately 200,000. In the late 1920s under Stalin, the party engaged in an intensive recruitment campaign, the \"Lenin Levy\", resulting in new members referred to as the Lenin Enrolment, from both the working class and rural areas. This represented an attempt to \"proletarianize\" the party and an attempt by Stalin to strengthen his base by outnumbering the Old Bolsheviks and reducing their influence in the Party. In 1925, the party had 1,025,000 members in a Soviet population of 147 million. In 1927, membership had risen to 1,200,000. During the collectivization campaign and industrialization campaigns of the first five-year plan from 1929 to 1933, party membership grew rapidly to approximately 3.5 million members. However, party leaders suspected that the mass intake of new members had allowed \"social-alien elements\" to penetrate the party's ranks and document verifications of membership ensued in 1933 and 1935, removing supposedly unreliable members. Meanwhile, the party closed its ranks to new members from 1933 to November 1936. Even after the reopening of party recruiting, membership fell to 1.9 million by 1939. Nicholas DeWitt gives 2.307 million members in 1939, including candidate members, compared with 1.535 million in 1929 and 6.3 million in 1947. In 1986, the CPSU had over 19 million members,approximately 10% of the Soviet Union's adult population. Over 44% of party members were classified as industrial workers and 12% as collective farmers. The CPSU had party organizations in 14 of the Soviet Union's 15 republics. The Russian Soviet Federative Socialist Republic itself had no separate Communist Party until 1990 because the CPSU controlled affairs there directly.\nKomsomol.\nThe All-Union Leninist Communist Youth League, commonly referred to as Komsomol, was the party's youth wing. The Komsomol acted under the direction of the CPSU Central Committee. It was responsible for indoctrinating youths in communist ideology and organizing social events. It was closely modeled on the CPSU; nominally the highest body was the Congress, followed by the Central Committee, Secretariat and the Politburo. The Komsomol participated in nationwide policy-making by appointing members to the collegiums of the Ministry of Culture, the Ministry of Higher and Specialized Secondary Education, the Ministry of Education and the State Committee for Physical Culture and Sports. The organization's newspaper was the \"Komsomolskaya Pravda\". The First Secretary and the Second Secretary were commonly members of the Central Committee but were never elected to the Politburo. However, at the republican level, several Komsomol first secretaries were appointed to the Politburo.\nIdeology.\nMarxism\u2013Leninism.\nMarxism\u2013Leninism was the cornerstone of Soviet ideology. It explained and legitimized the CPSU's right to rule while explaining its role as a vanguard party. For instance, the ideology explained that the CPSU's policies, even if they were unpopular, were correct because the party was enlightened. It was represented as the only truth in Soviet society; the Party rejected the notion of multiple truths. Marxism\u2013Leninism was used to justify CPSU rule and Soviet policy, but it was not used as a means to an end. The relationship between ideology and decision-making was at best ambivalent; most policy decisions were made in the light of the continued, permanent development of Marxism\u2013Leninism. Marxism\u2013Leninism as the only truth could notby its very naturebecome outdated.\nDespite having evolved over the years, Marxism\u2013Leninism had several central tenets. The main tenet was the party's status as the sole ruling party. The 1977 Constitution referred to the party as \"The leading and guiding force of Soviet society, and the nucleus of its political system, of all state and public organizations, is the Communist Party of the Soviet Union\". State socialism was essential and from Stalin until Gorbachev, official discourse considered that private social and economic activity retarding the development of collective consciousness and the economy. Gorbachev supported privatization to a degree but based his policies on Lenin's and Bukharin's opinions of the New Economic Policy of the 1920s, and supported complete state ownership over the commanding heights of the economy. Unlike liberalism, Marxism\u2013Leninism stressed the role of the individual as a member of a collective rather than the importance of the individual. Individuals only had the right to freedom of expression if it safeguarded the interests of a collective. For instance, the 1977 Constitution stated that every person had the right to express his or her opinion, but the opinion could only be expressed if it was in accordance with the \"general interests of Soviet society\". The number of rights granted to an individual was decided by the state, and the state could remove these rights if it saw fit. Soviet Marxism\u2013Leninism justified nationalism; the Soviet media portrayed every victory of the state as a victory for the communist movement as a whole. Largely, Soviet nationalism was based upon ethnic Russian nationalism. Marxism\u2013Leninism stressed the importance of the worldwide conflict between capitalism and socialism; the Soviet press wrote about progressive and reactionary forces while claiming that socialism was on the verge of victory and that the \"correlations of forces\" were in the Soviet Union's favor. The ideology professed state atheism; Party members were not allowed to be religious.\nMarxism\u2013Leninism believed in the feasibility of a communist mode of production. All policies were justifiable if it contributed to the Soviet Union's achievement of that stage.\nLeninism.\nIn Marxist philosophy, Leninism is the body of political theory for the democratic organization of a revolutionary vanguard party and the achievement of a dictatorship of the proletariat as a political prelude to the establishment of the socialist mode of production developed by Lenin. Since Karl Marx barely, if ever wrote about how the socialist mode of production would function, these tasks were left for Lenin to solve. Lenin's main contribution to Marxist thought is the concept of the vanguard party of the working class. He conceived the vanguard party as a highly knit, centralized organization that was led by intellectuals rather than by the working class itself. The CPSU was open only to a small number of workers because the workers in Russia still had not developed class consciousness and needed to be educated to reach such a state. Lenin believed that the vanguard party could initiate policies in the name of the working class even if the working class did not support them. The vanguard party would know what was best for the workers because the party functionaries had attained consciousness.\nLenin, in light of the Marx's theory of the state (which views the state as an oppressive organ of the ruling class), had no qualms of forcing change upon the country. He viewed the dictatorship of the proletariat, rather than the dictatorship of the bourgeoisie, to be the dictatorship of the majority. The repressive powers of the state were to be used to transform the country, and to strip of the former ruling class of their wealth. Lenin believed that the transition from the capitalist mode of production to the socialist mode of production would last for a long period. According to some authors, Leninism was by definition authoritarian. In contrast to Marx, who believed that the socialist revolution would comprise and be led by the working class alone, Lenin argued that a socialist revolution did not necessarily need to be led or to comprise the working class alone. Instead, he said that a revolution needed to be led by the oppressed classes of society, which in the case of Russia was the peasant class.\nStalinism.\nStalinism, while not an ideology \"per se\", refers to Stalin's thoughts and policies. Stalin's introduction of the concept \"Socialism in One Country\" in 1924 was an important moment in Soviet ideological discourse. According to Stalin, the Soviet Union did not need a socialist world revolution to construct a socialist society. Four years later, Stalin initiated his \"Second Revolution\" with the introduction of state socialism and central planning. In the early 1930s, he initiated the collectivization of Soviet agriculture by de-privatizing agriculture and creating peasant cooperatives rather than making it the responsibility of the state. With the initiation of his \"Second Revolution\", Stalin launched the \"Cult of Lenin\"a cult of personality centered upon himself. The name of the city of Petrograd was changed to Leningrad, the town of Lenin's birth was renamed Ulyanov (Lenin's birth-name), the Order of Lenin became the highest state award and portraits of Lenin were hung in public squares, workplaces and elsewhere. The increasing bureaucracy which followed the introduction of a state socialist economy was at complete odds with the Marxist notion of \"the withering away of the state\". Stalin explained the reasoning behind it at the 16th Congress held in 1930;\nWe stand for the strengthening of the dictatorship of the proletariat, which represents the mightiest and most powerful authority of all forms of State that have ever existed. The highest development of the State power for the withering away of State power \u2014this is the Marxian formula. Is this contradictory? Yes, it is contradictory. But this contradiction springs from life itself and reflects completely Marxist dialectic.\nAt the 1939 18th Congress, Stalin abandoned the idea that the state would wither away. In its place, he expressed confidence that the state would exist, even if the Soviet Union reached communism, as long as it was encircled by capitalism. Two key concepts were created in the latter half of his rule; the \"two camps\" theory and the \"capitalist encirclement\" theory. The threat of capitalism was used to strengthen Stalin's personal powers and Soviet propaganda began making a direct link with Stalin and stability in society, saying that the country would crumble without the leader. Stalin deviated greatly from classical Marxism on the subject of \"subjective factors\"; Stalin said that Party members of all ranks had to profess fanatic adherence to the Party's line and ideology, if not, those policies would fail.\nConcepts.\nDictatorship of the proletariat.\nLenin, supporting Marx's theory of the state, believed democracy to be unattainable anywhere in the world before the proletariat seized power. According to Marxist theory, the state is a vehicle for oppression and is headed by a ruling class. He believed that by his time, the only viable solution was dictatorship since the war was heading into a final conflict between the \"progressive forces of socialism and the degenerate forces of capitalism\". The Russian Revolution was by 1917, already a failure according to its original aim, which was to act as an inspiration for a world revolution. The initial anti-statist posture and the active campaigning for direct democracy was replaced because of Russia's level of development withaccording to their own assessments dictatorship. The reasoning was Russia's lack of development, its status as the sole socialist state in the world, its encirclement by imperialist powers, and its internal encirclement by the peasantry.\nMarx and Lenin did not care if a bourgeois state was ruled in accordance with a republican, parliamentary or a constitutional monarchical system since this did not change the overall situation. These systems, even if they were ruled by a small clique or ruled through mass participation, were all dictatorships of the bourgeoisie who implemented policies in defense of capitalism. However, there was a difference; after the failures of the world revolutions, Lenin argued that this did not necessarily have to change under the dictatorship of the proletariat. The reasoning came from practical considerations; the majority of the country's inhabitants were not communists, neither could the Party reintroduce parliamentary democracy because that was not in synchronization with its ideology and would lead to the Party losing power. He, therefore, concluded that the form of government has nothing to do with the nature of the dictatorship of the proletariat.\nBukharin and Trotsky agreed with Lenin; both said that the revolution had destroyed the old but had failed to create anything new. Lenin had now concluded that the dictatorship of the proletariat would not alter the relationship of power between men, but would rather \"transform their productive relations so that, in the long run, the realm of necessity could be overcome and, with that, genuine social freedom realized\". From 1920 to 1921, Soviet leaders and ideologists began differentiating between socialism and communism; hitherto the two terms had been used interchangeably and used to explain the same things. From then, the two terms had different meanings; Russia was in transition from capitalism to socialismreferred to interchangeably under Lenin as the dictatorship of the proletariat, socialism was the intermediate stage to communism and communism was considered the last stage of social development. By now, the party leaders believed that because of Russia's backward state, universal mass participation and true democracy could only take form in the last stage.\nIn early Bolshevik discourse, the term \"dictatorship of the proletariat\" was of little significance, and the few times it was mentioned it was likened to the form of government which had existed in the Paris Commune. However, with the ensuing Russian Civil War and the social and material devastation that followed, its meaning altered from commune-type democracy to rule by iron-discipline. By now, Lenin had concluded that only a proletarian regime as oppressive as its opponents could survive in this world. The powers previously bestowed upon the Soviets were now given to the Council of People's Commissars, the central government, which was, in turn, to be governed by \"an army of steeled revolutionary Communists [by Communists he referred to the Party]\". In a letter to Gavril Myasnikov in late 1920, Lenin explained his new interpretation of the term \"dictatorship of the proletariat\":\nDictatorship means nothing more nor less than authority untrammeled by any laws, absolutely unrestricted by any rules whatever, and based directly on force. The term 'dictatorship' \"has no other meaning but this\".\nLenin justified these policies by claiming that all states were class states by nature and that these states were maintained through class struggle. This meant that the dictatorship of the proletariat in the Soviet Union could only be \"won and maintained by the use of violence against the bourgeoisie\". The main problem with this analysis is that the Party came to view anyone opposing or holding alternate views of the party as bourgeois. Its worst enemy remained the moderates, which were considered to be \"the real agents of the bourgeoisie in the working-class movement, the labor lieutenants of the capitalist class\". The term \"bourgeoisie\" became synonymous with \"opponent\" and with people who disagreed with the Party in general. These oppressive measures led to another reinterpretation of the dictatorship of the proletariat and socialism in general; it was now defined as a purely economic system. Slogans and theoretical works about democratic mass participation and collective decision-making were now replaced with texts which supported authoritarian management. Considering the situation, the Party believed it had to use the same powers as the bourgeoisie to transform Russia; there was no alternative. Lenin began arguing that the proletariat, like the bourgeoisie, did not have a single preference for a form of government and because of that, the dictatorship was acceptable to both the Party and the proletariat. In a meeting with Party officials, Lenin statedin line with his economist view of socialismthat \"Industry is indispensable, democracy is not\", further arguing that \"we [the Party] do not promise any democracy or any freedom\".\nAnti-imperialism.\nThe Marxist theory on imperialism was conceived by Lenin in his book, \"\" (published in 1917). It was written in response to the theoretical crisis within Marxist thought, which occurred due to capitalism's recovery in the 19th century. According to Lenin, imperialism was a specific stage of development of capitalism; a stage he referred to as state monopoly capitalism. The Marxist movement was split on how to solve capitalism's resurgence after the great depression of the late 19th century. Eduard Bernstein from the Social Democratic Party of Germany (SDP) considered capitalism's revitalization as proof that it was evolving into a more humane system, adding that the basic aims of socialists were not to overthrow the state but to take power through elections. Karl Kautsky, also from the SDP, held a highly dogmatic view; he said that there was no crisis within Marxist theory. Both of them denied or belittled the role of class contradictions in society after the crisis. In contrast, Lenin believed that the resurgence was the beginning of a new phase of capitalism; this stage was created because of a strengthening of class contradiction, not because of its reduction.\nLenin did not know when the imperialist stage of capitalism began; he said it would be foolish to look for a specific year, however, said it began at the beginning of the 20th century (at least in Europe). Lenin believed that the economic crisis of 1900 accelerated and intensified the concentration of industry and banking, which led to the transformation of the finance capital connection to industry into the monopoly of large banks. In \"Imperialism: the Highest Stage of Capitalism\", Lenin wrote; \"the twentieth century marks the turning point from the old capitalism to the new, from the domination of capital in general to the domination of finance capital\". Lenin defines imperialism as the monopoly stage of capitalism.\nThe1986 Party Program claimed the tsarist regime collapsed because the contradictions of imperialism, which he held to be the gap \"between the social nature of production and the private capitalist form of appropriation\" manifesting itself in wars, economic recessions, and exploitation of the working class, were strongest in Russia. Imperialism was held to have caused the Russo-Japanese War and the First World War, with the 1905 Russian Revolution presented as \"the first people's revolution of the imperialist epoch\" and the October Revolution is said to have been rooted in \"the nationwide movement against imperialist war and for peace.\"\nPeaceful coexistence.\n\"Peaceful coexistence\" was an ideological concept introduced under Khrushchev's rule. While the concept has been interpreted by fellow communists as proposing an end to the conflict between the systems of capitalism and socialism, Khrushchev saw it as a continuation of the conflict in every area except in the military field. The concept said that the two systems were developed \"by way of diametrically opposed laws\", which led to \"opposite principles in foreign policy\".\nPeaceful coexistence was steeped in Leninist and Stalinist thought. Lenin believed that international politics were dominated by class struggle; in the 1940s Stalin stressed the growing polarization which was occurring in the capitalist and socialist systems. Khrushchev's peaceful coexistence was based on practical changes which had occurred; he accused the old \"two camp\" theory of neglecting the non-aligned movement and the national liberation movements. Khrushchev considered these \"grey areas\", in which the conflict between capitalism and socialism would be fought. He still stressed that the main contradiction in international relations were those of capitalism and socialism. The Soviet Government under Khrushchev stressed the importance of peaceful coexistence, saying that it had to form the basis of Soviet foreign policy. Failure to do, they believed, would lead to nuclear conflict. Despite this, Soviet theorists still considered peaceful coexistence to be a continuation of the class struggle between the capitalist and socialist worlds, but not based on armed conflict. Khrushchev believed that the conflict, in its current phase, was mainly economic.\nThe emphasis on peaceful coexistence did not mean that the Soviet Union accepted a static world with clear lines. It continued to uphold the creed that socialism was inevitable and they sincerely believed that the world had reached a stage in which the \"correlations of forces\" were moving towards socialism. With the establishment of socialist regimes in Eastern Europe and Asia, Soviet foreign policy planners believed that capitalism had lost its dominance as an economic system.\nSocialism in One Country.\nThe concept of \"Socialism in One Country\" was conceived by Stalin in his struggle against Leon Trotsky and his concept of permanent revolution. In 1924, Trotsky published his pamphlet \"Lessons of October\", in which he stated that socialism in the Soviet Union would fail because of the backward state of economic development unless a world revolution began. Stalin responded to Trotsky's pamphlet with his article, \"October and Comrade Trotsky's Theory of Permanent Revolution\". In it, Stalin stated that he did not believe an inevitable conflict between the working class and the peasants would take place, and that \"socialism in one country is completely possible and probable\". Stalin held the view common among most Bolsheviks at the time; there was a possibility of real success for socialism in the Soviet Union despite the country's backwardness and international isolation. While Grigoriy Zinoviev, Lev Kamenev and Nikolai Bukharintogether with Stalinopposed Trotsky's theory of permanent revolution, their views on the way socialism could be built diverged.\nAccording to Bukharin, Zinoviev and Kamenev supported the resolution of the 14th Conference held in 1925, which stated that \"we cannot complete the building of socialism due to our technological backwardness\". Despite this cynical attitude, Zinoviev and Kamenev believed that a defective form of socialism could be constructed. At the 14th Conference, Stalin reiterated his position that socialism in one country was feasible despite the capitalist blockade of the Soviet Union. After the conference, Stalin wrote \"Concerning the Results of the XIV Conference of the RCP(b)\", in which he stated that the peasantry would not turn against the socialist system because they had a self-interest in preserving it. Stalin said the contradictions which arose within the peasantry during the socialist transition could \"be overcome by our own efforts\". He concluded that the only viable threat to socialism in the Soviet Union was a military intervention.\nIn late 1925, Stalin received a letter from a Party official which stated that his position of \"Socialism in One Country\" was in contradiction with Friedrich Engels' writings on the subject. Stalin countered that Engels' writings reflected \"the era of pre-monopoly capitalism, the pre-imperialist era when there were not yet the conditions of an uneven, abrupt development of the capitalist countries\". From 1925, Bukharin began writing extensively on the subject and in 1926, Stalin wrote \"On Questions of Leninism\", which contains his best-known writings on the subject. With the publishing of \"Leninism\", Trotsky began countering Bukharin's and Stalin's arguments, writing that socialism in one country was only possible only in the short term, and said that without a world revolution it would be impossible to safeguard the Soviet Union from the \"restoration of bourgeois relations\". Zinoviev disagreed with Trotsky and Bukharin, and Stalin; he maintained Lenin's position from 1917 to 1922 and continued to say that only a defective form of socialism could be constructed in the Soviet Union without a world revolution. Bukharin began arguing for the creation of an autarkic economic model, while Trotsky said that the Soviet Union had to participate in the international division of labor to develop. In contrast to Trotsky and Bukharin, in 1938, Stalin said that a world revolution was impossible and that Engels was wrong on the matter. At the 18th Congress, Stalin took the theory to its inevitable conclusion, saying that the communist mode of production could be conceived in one country. He rationalized this by saying that the state could exist in a communist society as long as the Soviet Union was encircled by capitalism. However, with the establishment of socialist regimes in Eastern Europe, Stalin said that socialism in one country was only possible in a large country like the Soviet Union and that to survive, the other states had to follow the Soviet line.\nReasons for demise.\nWestern view.\nThere were few if any, who believed that the Soviet Union was on the verge of collapse by 1985. The economy was stagnating, but stable enough for the Soviet Union to continue into the 21st century. The political situation was calm because of twenty years of systematic repression against any threat to the country and one-party rule, and the Soviet Union was in its peak of influence in world affairs. The immediate causes for the Soviet Union's dissolution were the policies and thoughts of Mikhail Gorbachev, the CPSU General Secretary. His policies of \"perestroika\" and \"glasnost\" tried to revitalize the Soviet economy and the social and political culture of the country. Throughout his rule, he put more emphasis on democratizing the Soviet Union because he believed it had lost its moral legitimacy to rule. These policies led to the collapse of the communist regimes in Eastern Europe and indirectly destabilized Gorbachev's and the CPSU's control over the Soviet Union. Archie Brown said:\nThe expectations of, again most notably, Lithuanians, Estonians, and Latvians were enormously enhanced by what they saw happening in the 'outer empire' [Eastern Europe], and they began to believe that they could remove themselves from the 'inner empire'. In truth, a democratized Soviet Union was incompatible with denial of the Baltic states' independence for, to the extent that those Soviet republics became democratic, their opposition to remaining in a political entity whose center was Moscow would become increasingly evident. Yet, it was not preordained that the entire Soviet Union would break up.\nHowever, Brown said that the system did not need to collapse or to do so in the way it did. The democratization from above weakened the Party's control over the country and put it on the defensive. Brown added that a different leader than Gorbachev would probably have oppressed the opposition and continued with economic reform. Nonetheless, Gorbachev accepted that the people sought a different road and consented to the Soviet Union's dissolution in 1991. He said that because of its peaceful collapse, the fall of Soviet communism is \"one of the great success stories of 20th-century politics\". According to Lars T. Lih, the Soviet Union collapsed because people stopped believing in its ideology. He wrote:\nWhen in 1991 the Soviet Union collapsed not with a bang but a whimper, this unexpected outcome was partly the result of the previous disenchantments of the narrative of class leadership. The Soviet Union had always been based on the fervent belief in this narrative in its various permutations. When the binding power of the narrative dissolved, the Soviet Union itself dissolved.\nAccording to the Communist Party of China.\nThe first research into the collapse of the Soviet Union and the Eastern Bloc were very simple and did not take into account several factors. However, these examinations became more advanced by the 1990s, and unlike most Western scholarship, which focuses on the role of Gorbachev and his reform efforts, the Communist Party of China (CPC) examined \"core (political) life and death issues\" so that it could learn from them and not make the same mistakes. Following the CPSU's demise and the Soviet Union's collapse, the CPC's analysis began examining systematic causes. Several leading CPC officials began hailing Khrushchev's rule, saying that he was the first reformer and that if he had continued after 1964, the Soviet Union would not have witnessed the Era of Stagnation began under Brezhnev and continued under Yuri Andropov and Konstantin Chernenko. The main economic failure was that the political leadership did not pursue any reforms to tackle the economic malaise that had taken hold, dismissing certain techniques as capitalist, and never disentangling the planned economy from socialism. Xu Zhixin from the CASS Institute of Eastern Europe, Russia, and Central Asia, argued that Soviet planners laid too much emphasis on heavy industry, which led to shortages of consumer goods. Unlike his counterparts, Xu argued that the shortages of consumer goods were not an error but \"was a consciously planned feature of the system\". Other CPSU failures were pursuing the policy of state socialism, the high spending on the military-industrial complex, a low tax base, and the subsidizing of the economy. The CPC argued that when Gorbachev came to power and introduced his economic reforms, they were \"too little, too late, and too fast\".\nWhile most CPC researchers criticize the CPSU's economic policies, many have criticized what they see as \"Soviet totalitarianism\". They accuse Joseph Stalin of creating a system of mass terror, intimidation, annulling the democracy component of democratic centralism and emphasizing centralism, which led to the creation of an inner-party dictatorship. Other points were Russian nationalism, a lack of separation between the Party and state bureaucracies, suppression of non-Russian ethnicities, distortion of the economy through the introduction of over-centralization and the collectivization of agriculture. According to CPC researcher Xiao Guisen, Stalin's policies led to \"stunted economic growth, tight surveillance of society, a lack of democracy in decision-making, an absence of the rule of law, the burden of bureaucracy, the CPSU's alienation from people's concerns, and an accumulation of ethnic tensions\". Stalin's effect on ideology was also criticized; several researchers accused his policies of being \"leftist\", \"dogmatist\" and a deviation \"from true Marxism\u2013Leninism.\" He is criticized for initiating the \"bastardization of Leninism\", of deviating from true democratic centralism by establishing a one-man rule and destroying all inner-party consultation, of misinterpreting Lenin's theory of imperialism and of supporting foreign revolutionary movements only when the Soviet Union could get something out of it. Yu Sui, a CPC theoretician, said that \"the collapse of the Soviet Union and CPSU is a punishment for its past wrongs!\" Similarly, Brezhnev, Mikhail Suslov, Alexei Kosygin and Konstantin Chernenko have been criticized for being \"dogmatic, ossified, inflexible, [for having a] bureaucratic ideology and thinking\", while Yuri Andropov is depicted by some of having the potential of becoming a new Khrushchev if he had not died early.\nWhile the CPC concur with Gorbachev's assessment that the CPSU needed internal reform, they do not agree on how it was implemented, criticizing his idea of \"humanistic and democratic socialism\", of negating the leading role of the CPSU, of negating Marxism, of negating the analysis of class contradictions and class struggle, and of negating the \"ultimate socialist goal of realizing communism\". Unlike the other Soviet leaders, Gorbachev is criticized for pursuing the wrong reformist policies and for being too flexible and too rightist. The CPC Organization Department said, \"What Gorbachev in fact did was not to transform the CPSU by correct principles\u2014indeed the Soviet Communist Party \"needed transformation\"\u2014but instead he, step-by-step, and ultimately, eroded the ruling party's dominance in ideological, political and organizational aspects\".\nThe CPSU was also criticized for not taking enough care in building the primary party organization and not having inner-party democracy. Others, more radically, concur with Milovan \u0110ilas assessment, saying that a new class was established within the central party leadership of the CPSU and that a \"corrupt and privileged class\" had developed because of the nomenklatura system. Others criticized the special privileges bestowed on the CPSU elite, the nomenklatura systemwhich some said had decayed continuously since Stalin's ruleand the relationship between the Soviet military and the CPSU. Unlike in China, the Soviet military was a state institution whereas in China it is a Party (and state) institution. The CPC criticizes the CPSU of pursuing Soviet imperialism in its foreign policies."}
{"id": "7272", "revid": "39324413", "url": "https://en.wikipedia.org/wiki?curid=7272", "title": "Christianity and homosexuality", "text": "Within Christianity, there are a variety of views on sexual orientation and homosexuality. Even within a denomination, individuals and groups may hold different views, and not all members of a denomination necessarily support their church's views on homosexuality.\nHistory.\nThe history of Christianity and homosexuality has been much debated. Some maintain early Christian Churches deplored same-sex relationships, while others maintain they accepted these relationships on the level of their heterosexual counterparts. These disagreements concern, in some cases, the translations of certain terms, or the meaning and context of some passages.\nThis article focuses on the twentieth and twenty-first centuries, covering how the extent to which the Bible mentions the subject, whether or not it is condemned, and whether the various passages apply today, have become contentious topics. Significant debate has arisen over the proper interpretation of the Levitical code; the story of Sodom and Gomorrah; and various Pauline passages, and whether these verses condemn same-sex sexual activities.\nChristian denominational positions.\nCatholic Church.\nThe Catholic Church views as sinful any sexual act not related to procreation by a couple joined under the Sacrament of Matrimony. The Church states that \"homosexual tendencies\" are \"objectively disordered\", but does not consider the tendency itself to be sinful but rather a temptation toward sin. The Church, however, considers \"homosexual acts\" to be \"grave sins\", \"intrinsically disordered\", and \"contrary to the natural law\", and \"under no circumstances can they be approved\".\nThe Catechism of the Catholic Church states \"men and women who have deep-seated homosexual tendencies ... must be accepted with respect, compassion, and sensitivity.\" \"Every sign of unjust discrimination in their regard should be avoided.\" They oppose criminal penalties against homosexuality. The Catholic Church requires those who are attracted to people of the same (or opposite) sex to practice chastity, because it teaches that sexuality should only be practiced within marriage, which includes chaste sex as permanent, procreative, heterosexual, and monogamous. The Vatican distinguishes between \"deep-seated homosexual tendencies\" and the \"expression of a transitory problem\", in relation to ordination to the priesthood; saying in a 2005 document that homosexual tendencies \"must be clearly overcome at least three years before ordination to the diaconate.\" A 2011 report based on telephone surveys of self-identified American Catholics conducted by the Public Religion Research Institute found that 56% believe that sexual relations between two people of the same sex are not sinful.\nIn January 2018 German bishop Franz-Josef Bode of the Roman Catholic Diocese of Osnabr\u00fcck, and in February 2018 German Roman Catholic cardinal Reinhard Marx, chairman of the German Bishops' Conference said in interviews with German journalists that blessing of same-sex unions is possible in Roman Catholic churches in Germany. In Austria blessing of same sex unions is allowed in two churches located in the Roman Catholic Diocese of Linz.\nThe Eastern Orthodox churches, like the Catholic Church, condemn homosexual acts.\nOrthodox churches.\nThe Orthodox Church shares a long history of Church teachings and canon law with the Catholic Church and has a similar conservative stance on homosexuality. Some Orthodox Church jurisdictions, such as the Orthodox Church in America, have taken the approach of welcoming people with \"homosexual feelings and emotions,\" while encouraging them to work towards \"overcoming its harmful effects in their lives,\" while not allowing the sacraments to people who seek to justify homosexual activity. Other Orthodox Churches, such as those in Eastern Europe and Greece, view homosexuality less favourably. The Greek Orthodox Archdiocese lists homosexuality along with fornication, adultery, and more because of the thinking that homosexuality breaks up the institution of marriage and family. The issue of gay marriage seems to be strongly rejected, even in Greece, where half of Orthodox Christians don't believe that homosexuality should be discouraged.\nProtestant churches.\nLiberal position.\nCertain other Christian denominations do not view monogamous same-sex relationships as sinful or immoral, and may bless such unions and consider them marriages. These include the United Church of Canada, the Presbyterian Church (USA), the United Church of Christ, all German Lutheran, reformed and united churches in EKD, all Swiss reformed churches, the Protestant Church in the Netherlands, the United Protestant Church in Belgium, the United Protestant Church of France, the Church of Denmark, the Church of Sweden, the Church of Iceland, the Church of Norway, and the Uniting Church in Australia. The Evangelical Lutheran Church of Finland also allows prayer for same-sex couples. The Metropolitan Community Church was founded specifically to serve the Christian LGBT community. The Global Alliance of Affirming Apostolic Pentecostals (GAAAP), traces its roots back to 1980, making it the oldest LGBT-affirming Apostolic Pentecostal denomination in existence. Another such organization is the Affirming Pentecostal Church International, currently the largest affirming Pentecostal organization, with churches in the US, UK, Central and South America, Europe and Africa.\nLGBT-affirming denominations regard homosexuality as a natural occurrence. The United Church of Christ celebrates gay marriage, and some parts of the Anglican and Lutheran churches allow for the blessing of gay unions. The United Church of Canada also allows same-sex marriage, and views sexual orientation as a gift from God. Within the Anglican Communion, there are openly gay clergy; for example, Gene Robinson is an openly gay Bishop in the US Episcopal Church. Within the Lutheran communion, there are openly gay clergy, too; for example, bishop Eva Brunne is an openly lesbian Bishop in the Church of Sweden. Such religious groups and denominations interpret scripture and doctrine in a way that leads them to accept that homosexuality is morally acceptable, and a natural occurrence. For example, in 1988 the United Church of Canada, that country's largest Protestant denomination, affirmed that \"a) All persons, regardless of their sexual orientation, who profess Jesus Christ and obedience to Him, are welcome to be or become full members of the Church\"; and \"b) All members of the Church are eligible to be considered for the Ordered Ministry.\" In 2000, the Church's General Assembly further affirmed that \"human sexual orientations, whether heterosexual or homosexual, are a gift from God and part of the marvelous diversity of creation.\"\nIn addition, some Christian denominations such as the Moravian Church, believe that the Bible speaks negatively of homosexual acts but, as research on the matter continues, the Moravian Church seeks to establish a policy on homosexuality and the ordination of homosexuals. In 2014, Moravian Church in Europe allowed blessings of same-sex unions.\nLiberal Quakers, those in membership of Britain Yearly Meeting and Friends General Conference in the US approve of same-sex marriage and union. Quakers were the first Christian group in the United Kingdom to advocate for equal marriage and Quakers in Britain formally recognised same-sex relationships in 1963.\nThe United Methodist Church elected a lesbian bishop in 2016, and on 7 May 2018, the Council of Bishops proposed the One Church Plan, which would allow individual pastors and regional church bodies to decide whether to ordain LGBT clergy and perform same-sex weddings. On 26 February 2019, a special session of the General Conference rejected the One Church Plan and voted to strengthen its official opposition to same-sex marriages and ordaining openly LGBT clergy.\nVarious positions.\nChurches within Lutheranism hold stances on the issue ranging from labeling homosexual acts as sinful, to acceptance of homosexual relationships. For example, the Lutheran Church\u2013Missouri Synod, the Lutheran Church of Australia, and the Wisconsin Evangelical Lutheran Synod recognize homosexual behavior as intrinsically sinful and seek to minister to those who are struggling with homosexual inclinations. However, the Church of Sweden, the Church of Denmark, the Church of Norway or lutheran churches of Evangelical Church in Germany conducts same-sex marriages, while the Evangelical Lutheran Church in America and Evangelical Lutheran Church in Canada opens the ministry of the church to gay pastors and other professional workers living in committed relationships. The Ethiopian Mekane Yesus Lutheran Church, however, has taken a stand that marriage is inherently between a man and a woman, and has formally broken fellowship with the ELCA, a doctrinal stand that has cost the Ethiopian church ELCA financial support.\nConservative position.\nSome mainline Protestant denominations, such as the African Methodist churches, the Reformed Church in America, and the Presbyterian Church in America have a conservative position on the subject.\nThe Seventh-day Adventist Church \"recognizes that every human being is valuable in the sight of God, and seeks to minister to all men and women [including homosexuals] in the spirit of Jesus,\" while maintaining that homosexual sex itself is forbidden in the Bible. \"Jesus affirmed the dignity of all human beings and reached out compassionately to persons and families suffering the consequences of sin. He offered caring ministry and words of solace to struggling people, while differentiating His love for sinners from His clear teaching about sinful practices.\"\nThe Anglican Church reassures people with same sex attraction they are loved by God and are welcomed as full members of the Body of Christ. The Church leadership has a variety of views in regard to homosexual expression and ordination. Some expressions of sexuality are considered sinful including \"promiscuity, prostitution, incest, pornography, paedophilia, predatory sexual behaviour, and sadomasochism (all of which may be heterosexual and homosexual). The Church is concerned with pressures on young people to engage sexually and encourages abstinence. Most of the Anglican Communion does not approve of homosexual activity, with the exception of the Episcopal Church, the Scottish Episcopal Church and the Anglican Church of Canada, which is facing a possible exclusion from international Anglican bodies over the issue.\nConservative Quakers, those within Friends United Meeting and the Evangelical Friends International believe that sexual relations are condoned only in marriage, which they define to be between a man and a woman.\nConfessional Lutheran churches teach that it is sinful to have homosexual desires, even if they do not lead to homosexual activity. The Doctrinal statement issued by the Wisconsin Evangelical Lutheran Synod states that making a distinction between homosexual orientation and the act of homosexuality is confusing:\nHowever, confessional Lutherans also warn against selective morality which harshly condemns homosexuality while treating other sins more lightly.\nEvangelical churches.\nThe positions of the evangelical churches are varied. They range from liberal to conservative, through moderate. Some evangelical denominations have adopted neutral positions, leaving the choice to local churches to decide for same-sex marriage. \nEvangelical Conservative position.\nMany American Evangelical and Fundamentalist Christians regard homosexual acts as sinful and think they should not be accepted by society. They tend to interpret biblical verses on homosexual acts to mean that the heterosexual family was created by God to be the bedrock of civilization and that same-sex relationships contradict God\u2019s design for marriage and violate his will. Christians who oppose homosexual relationships sometimes argue that same-gender sexual activity is unnatural.\nIt is in the fundamentalist and conservative positions, that there are anti-gay activists on TV or radio who claim that homosexuality is the cause of many social problems, such as terrorism. Some evangelical churches in Uganda strongly oppose homosexuality and homosexuals. They have campaigned for laws criminalizing homosexuality. The generalization and use of prejudices to spread hatred of homosexual people are frequent. \nIn opposing interpretations of the Bible that are supportive of homosexual relationships, conservative Christians have argued for the reliability of the Bible, and the meaning of texts related to homosexual acts, while often seeing what they call the diminishing of the authority of the Bible by many homosexual authors as being ideologically driven.\nAs an alternative to a school-sponsored Day of Silence opposing bullying of LGBT students, conservative Christians organized a Golden Rule Initiative, where they passed out cards saying \"As a follower of Christ, I believe that all people are created in the image of God and therefore deserve love and respect.\" Others created a Day of Dialogue to oppose what they believe is the silencing of Christian students who make public their opposition to homosexuality.\nOn 29 August 2017, the Council on Biblical Manhood and Womanhood released a manifesto on human sexuality known as the \"Nashville Statement\". The statement was signed by 150 evangelical leaders, and includes 14 points of belief.\nSex scandals.\nSome evangelical pastors with antigay speeches have been outed.\nThere was Pastor Ted Haggard, founder of nondenominational charismatic megachurch New Life Church in Colorado Springs, USA. Married with five children, Ted was an anti-gay activist and said he wanted to ban homosexuality from the church. In 2006, he was dismissed from his position as senior pastor after a male prostitute claimed to have had sex with him for three years. After denying the relationship, the pastor admitted that the allegations were accurate.\nThere was also Baptist Pastor George Alan Rekers of the Southern Baptist Convention in the United States and psychologist member of the National Association for Research &amp; Therapy of Homosexuality. Married and father of children, the antigay activist was recognized with a gay escort, hired for a trip to Europe, in 2010. According to him, he had hired the gay escort to carry his luggage.\nLiberal position.\nInternational.\nThere are some international evangelical denominations that are gay-friendly, such as the Alliance of Baptists and Affirming Pentecostal Church International.\nU.S..\nA 2014 survey reported that 43% of white evangelical American Christians between the ages of 18 and 33 supported same-sex marriage. Some evangelical churches accept homosexuality and celebrate gay weddings. Pastors have also been involved in changing the traditional position of their church. In 2014, the New Heart Community Church of La Mirada, a Baptist church in the suburbs of Los Angeles was expelled from the Southern Baptist Convention for this purpose. In 2015, GracePointe Church in Franklin in the suburbs of Nashville made this decision. It lost over half of her weekly attendance (from 1,000 to 482).\nModerate position.\nSome churches have a moderate position. Although they do not approve homosexual practices, they show sympathy and respect for homosexuals.\nU.S..\nReflecting this position, some pastors, for example, showed moderation during public statements. For example, in 2008, Baptist pastor Rick Warren of Saddleback Church in Lake Forest said that he had developed good relationships with several gay people, without having to compromise his beliefs about the definition of marriage between a man and a woman present in the bible. \nPastor Joel Osteen of Lakewood Church in Houston said in 2013 he found it unfortunate that several Christian ministers focus on the homosexuality by forgetting the other sins described in the Bible. He said that Jesus did not come to condemn people, but to save them. Other pastors also share this view.\nPastor Andy Stanley of North Point Community Church in Alpharetta, mentioned in 2015 that the church should be the safest place on the planet for students to talk about anything, including same-sex attraction.\nThere is also a movement of people who consider themselves \"gay Evangelicals\". Composed mainly of young people, the movement is positioned against liberals and conservatives. Recognizing themselves as gay or bisexual, these young people believe that their attraction to same-sex people, while present, does not allow them to have homosexual relationships. They say that their Christian conversion did not instantly change their sexual desires. They insist that the church should always reject homosexual practices, but that it should welcome gay people.\nThere are also believers gathered in Christian organizations of evangelical orientation, such as \"Your Other Brothers\" or \"Voice of the Voiceless\", who claim they have not been attracted to homosexuality since their new birth, without having recourse to a conversion therapy. They insist on the importance of welcoming and loving homosexuals, but believe that sexuality should be reserved for heterosexual marriage.\nIn Australia.\nBrian Houston of Hillsong Church said that gays are welcome in the church, but they cannot take up leadership positions.\nFrance.\nThe French Evangelical Alliance, a member of the European Evangelical Alliance and the World Evangelical Alliance, adopted on 12 October 2002, through its National Council, a document entitled \"Foi, esp\u00e9rance et homosexualit\u00e9\" (\"Faith, Hope and Homosexuality \"), in which homophobia, hatred and rejection of homosexuals are condemned, but which denies homosexual practices and full church membership of unrepentant homosexuals and those who approve of these practices. In 2015, the Conseil national des \u00e9vang\u00e9liques de France (French National Council of Evangelicals) reaffirmed its position on the issue by opposing marriage of same-sex couples, while not rejecting homosexuals, but wanting to offer them more than a blessing; an accompaniment and a welcome.\nThe French evangelical pastor Philippe Auzenet, a chaplain of the association Oser en parler, regularly intervenes on the subject in the media. It promotes dialogue and respect, as well as sensitization in order to better understand homosexuals. He also said in 2012 that Jesus would go to a gay bar, because he was going to all people with love.\nNigeria.\nPhilip Igbinijesu, a pastor of the Lagos Word Assembly, an Evangelical church, said in a message to his church that the Nigerian law on homosexuality (inciting denunciation) was hateful. He recalled that homosexuals are creatures of God and that they should be treated with respect.\nRestorationist churches.\nRestorationist churches, like Jehovah's Witnesses, Seventh-Day Adventist, and The Church of Jesus Christ of Latter-day Saints generally teach that homosexuals are 'broken' and can be 'fixed'. The Community of Christ, a branch of Mormonism, fully accepts LGBT persons, performs weddings for gay and lesbian couples, and ordains LGBT members. Within the Stone-Campbell aligned restorationist churches the views are divergent. The churches of Christ (A Capella) and the Independent Christian Churches/Churches of Christ mostly adhere to a very conservative ideology; socially, politically, and religiously and are generally not accepting of openly LGBT members and will not perform weddings for gay and lesbian couples. The Disciples of Christ, is fully accepting of LGBT persons, often performs weddings for gay and lesbian couples, and ordains LGBT members. The United Church of Christ is an officially \"open and affirming\" church. Other Restorationist churches such as The Brethren (see Mennonite) and Millerite churches, have taken mixed positions but are increasingly accepting with some of their congregations fully accepting LGBT persons in all aspects of religious and political life.\nViews supportive of homosexuality.\nIn the 20th century, theologians like J\u00fcrgen Moltmann, Hans K\u00fcng, John Robinson, Bishop David Jenkins, Don Cupitt, and Bishop Jack Spong challenged traditional theological positions and understandings of the Bible; following these developments some have suggested that passages have been mistranslated or that they do not refer to what we understand as \"homosexuality.\" Clay Witt, a minister in the Metropolitan Community Church, explains how theologians and commentators like John Shelby Spong, George Edwards and Michael England interpret injunctions against certain sexual acts as being originally intended as a means of distinguishing religious worship between Abrahamic and the surrounding pagan faiths, within which homosexual acts featured as part of idolatrous religious practices: \"England argues that these prohibitions should be seen as being directed against sexual practices of fertility cult worship. As with the earlier reference from Strong\u2019s, he notes that the word 'abomination' used here is directly related to idolatry and idolatrous practices throughout the Hebrew Testament. Edwards makes a similar suggestion, observing that 'the context of the two prohibitions in and suggest that what is opposed is not same-sex activity outside the cult, as in the modern secular sense, but within the cult identified as Canaanite'\".\nIn 1986, the Evangelical and Ecumenical Women\u2019s Caucus (EEWC), then known as the Evangelical Women's Caucus International, passed a resolution stating: \"Whereas homosexual people are children of God, and because of the biblical mandate of Jesus Christ that we are all created equal in God's sight, and in recognition of the presence of the lesbian minority in EWCI, EWCI takes a firm stand in favor of civil rights protection for homosexual persons.\"\nSome Christians believe that Biblical passages have been mistranslated or that these passages do not refer to LGBT orientation as currently understood. Liberal Christian scholars, like conservative Christian scholars, accept earlier versions of the texts that make up the Bible in Hebrew or Greek. However, within these early texts there are many terms that modern scholars have interpreted differently from previous generations of scholars. There are concerns with copying errors, forgery, and biases among the translators of later Bibles. They consider some verses such as those they say support slavery or the inferior treatment of women as not being valid today, and against the will of God present in the context of the Bible. They cite these issues when arguing for a change in theological views on sexual relationships to what they say is an earlier view. They differentiate among various sexual practices, treating rape, prostitution, or temple sex rituals as immoral and those within committed relationships as positive regardless of sexual orientation. They view certain verses, which they believe refer only to homosexual rape, as not relevant to consensual homosexual relationships.\nYale professor John Boswell has argued that a number of Early Christians entered into homosexual relationships, and that certain Biblical figures had homosexual relationships, such as Ruth and her mother-in-law Naomi, Daniel and the court official Ashpenaz, and David and King Saul's son Jonathan. Boswell has also argued that adelphopoiesis, a rite bonding two men, was akin to a religiously sanctioned same-sex union. Having partaken in such a rite, a person was prohibited from entering into marriage or taking monastic vows, and the choreography of the service itself closely parallelled that of the marriage rite. His views have not found wide acceptance, and opponents have argued that this rite sanctified a Platonic brotherly bond, not a homosexual union. He also argued that condemnation of homosexuality began only in the 12th century. Boswell's critics point out that many earlier doctrinal sources condemn homosexuality as a sin even if they do not prescribe a specific punishment, and that Boswell's arguments are based on sources which reflected a general trend towards harsher penalties, rather than a change in doctrine, from the 12th century onwards.\nDesmond Tutu, the former Anglican Archbishop of Cape Town and a Nobel Peace Prize winner, has described homophobia as a \"crime against humanity\" and \"every bit as unjust\" as apartheid: \"We struggled against apartheid in South Africa, supported by people the world over, because black people were being blamed and made to suffer for something we could do nothing about; our very skins. It is the same with sexual orientation. It is a given. ... We treat them [gays and lesbians] as pariahs and push them outside our communities. We make them doubt that they too are children of God \u2013 and this must be nearly the ultimate blasphemy. We blame them for what they are.\"\nModern gay Christian leader Justin R. Cannon promotes what he calls \"Inclusive Orthodoxy\" ('orthodoxy' in this sense is not to be confused with the Eastern Orthodox Church). He explains on his ministry website: \"Inclusive Orthodoxy is the belief that the Church can and must be inclusive of LGBT individuals without sacrificing the Gospel and the Apostolic teachings of the Christian faith.\" Cannon's ministry takes a unique and distinct approach from modern liberal Christians while still supporting homosexual relations. His ministry affirms the divine inspiration of the Bible, the authority of Tradition, and says \"...that there is a place within the full life and ministry of the Christian Church for lesbian, gay, bisexual, and transgender Christians, both those who are called to lifelong celibacy and those who are partnered.\"\nToday, many religious people are becoming more affirming of same-sex relationships, even in denominations with official stances against homosexuality. In the United States, people in denominations who are against same-sex relationships are liberalizing quickly, though not as quickly as those in more affirming groups. This social change is creating tension within many denominations, and even schisms and mass walk-outs among Mormons and other conservative groups.\nPope Francis voiced support for same-sex civil unions during an interview in a documentary film, \"Francesco\", which was premiered at the Rome Film Festival on 21 October 2020. \nStudies in the US show more LGBT individuals identify as Protestant than Catholic.\nHomosexual Christians and organizations.\nGeorge Barna, a conservative Christian author and researcher, conducted a survey in the United States in 2009 that found gay and lesbian people having a Christian affiliation were more numerous than had been presumed. He characterized some of his leading conclusions from the data as follows: \"People who portray gay adults as godless, hedonistic, Christian bashers are not working with the facts. A substantial majority of gays cite their faith as a central facet of their life, consider themselves to be Christian, and claim to have some type of meaningful personal commitment to Jesus Christ active in their life today.\" Barna also found that LGBT people were more likely to interpret faith as an individual rather than a collective experience.\nCandace Chellew-Hodge, liberal Christian lesbian founder of the online magazine \"Whosoever\", responded to the findings: \"All in all, I'm grateful for Barna even wandering into the subject of gay and lesbian religious belief. I think his study is important and can go a long way to dispelling the old \"gays vs. God\" dichotomy that too often gets played out in the media. However, his overall message is still harmful: Gays and lesbians are Christians \u2013 they're just not as good as straight ones.\" She argued that Barna had formulated his report with undue irony and skepticism, and that he had failed to take into account the reasons for the data which enkindled his \"arri\u00e8re pens\u00e9e.\" The reason why far fewer homosexuals attend church, she argued, is that there are far fewer churches who will accept them. Equally, gays and lesbians do not see the Bible as unequivocally true because they are forced by its use against them to read it more closely and with less credulity, leading them to note its myriad contradictions.\nOrganizations for homosexual Christians exist across a wide range of beliefs and traditions. The interdenominational Q Christian Fellowship (formerly Gay Christian Network) has some members who affirm same-sex relationships and others who commit themselves to celibacy, groups it refers to as \"Side A\" and \"Side B\", respectively. According to founder Justin Lee, \"We're just trying to get people together who experience attraction to the same sex, however they have handled that, and who love Jesus and say, OK, you are welcome here, and then let's pray together and figure out where God wants us to take it.\"\nSome organizations cater exclusively to homosexual Christians who do not want to have gay sex, or attraction; the goals of these organizations vary. Some Christian groups focus on simply refraining from gay sex, such as Courage International and North Star. Other groups additionally encourage gay members to reduce or eliminate same-sex attractions. Love Won Out and the now-defunct Exodus International are examples of such ministries. These groups are sometimes referred to as ex-gay organizations, though many no longer use the term. Alan Chambers, the president of Exodus, says the term incorrectly implies a complete change in sexual orientation, though the group Parents and Friends of Ex-Gays and Gays continues to use the term. In addition, individual Christians identifying as gay who want to subscribe to the conservative ethic are becoming more vocal themselves.\nGay Christian writer and actor Peterson Toscano argues that organizations promoting orientation change are a \"ruse.\" An organization he co-founded, Beyond Ex-Gay, supports people who feel they have been wounded by such organizations.\nOther groups support or advocate for gay Christians and their relationships. For example, in the United States, IntegrityUSA represents the interests of lesbian and gay Christians in the Episcopal Church, while United Methodists have the Reconciling Ministries Network and evangelical Christians have \"Evangelicals Concerned\". In 2014 the United Church of Christ filed a lawsuit challenging North Carolina\u2019s ban on same-sex marriage, which is America\u2019s first faith-based challenge to same-sex marriage bans; the Alliance of Baptists joined the lawsuit later that year.\nIn Europe, lesbian and gay evangelical Christians have a European forum. Working within the worldwide Anglican Communion on a range of discrimination issues, including those of LGBT clergy and people in the church, is \"Inclusive Church\". The longest standing group for lesbian and gay Christians in the UK, founded in 1976, is the non-denominational Lesbian and Gay Christian Movement; specifically aimed to meet the needs of lesbian and gay evangelicals, there is the \"Evangelical Fellowship for Lesbian and Gay Christians\"; specifically working within the Church of England is \"Changing Attitude\", which also takes an international focus in working for gay, lesbian, bisexual and transgender affirmation within the Anglican Communion.\nSociologist Richard N. Pitt argues that these organizations are only available to LGBT members of liberal denominations, as opposed to those in conservative denominations. His review of the literature on gay Christians suggests that these organizations not only represent the interests of Christians who attend their churches, but (like gay-friendly and gay-affirming churches) also give these members useful responses to homophobic and heterosexist rhetoric. His research shows that those LGBT Christians who stay at homophobic churches \"kill the messenger\" by attacking the minister's knowledge about homosexuality, personal morality, focus on sin instead of forgiveness, and motivations for preaching against homosexuality."}
{"id": "7273", "revid": "1013122763", "url": "https://en.wikipedia.org/wiki?curid=7273", "title": "Chadic languages", "text": "The Chadic languages form a branch of the Afroasiatic language family. They are spoken in parts of the Sahel. They include 150 languages spoken across northern Nigeria, southern Niger, southern Chad, Central African Republic and northern Cameroon. The most widely spoken Chadic language is Hausa, a lingua franca of much of inland Eastern West Africa.\nComposition.\nNewman (1977) classified the languages into the four groups which have been accepted in all subsequent literature. Further subbranching, however, has not been as robust; Blench (2006), for example, only accepts the A/B bifurcation of East Chadic. Kujarg\u00e9 has been added from Blench (2008), who suggests Kujarg\u00e9 may have split off before the breakup of Proto-Chadic and then subsequently became influenced by East Chadic. Subsequent work by Lovestrand argues strongly that Kujarge is a valid member of East Chadic. The placing of Luri as a primary split of West Chadic is erroneous. Caron (2004) shows that this language is South Bauchi and part of the Polci cluster.\nOrigin.\nModern genetic studies of Northwestern Cameroonian Chadic-speaking populations have observed high frequencies of the Y-Chromosome Haplogroup R1b in these populations (the R1b-V88 variant). This paternal marker is common in parts of West Eurasia, but otherwise rare in Africa. Cruciani et al. (2010) thus proposed that the Proto-Chadic speakers during the mid-Holocene (~7,000 years ago) migrated from the Levant to the Central Sahara, and from there settled in the Lake Chad Basin. \nHowever, a more recent study in 2018 found that haplogroup R1b-V88 entered Chad much more recently during \"Baggarization\" (the migration of Baggara Arabs to the Sahel in the 17th century AD), finding no evidence of ancient Eurasian gene flow. \nLoanwords.\nChadic languages contain many Nilo-Saharan loanwords from either the Songhay or Maban branches, pointing to early contact between Chadic and Nilo-Saharan speakers as Chadic was migrating west.\nAlthough Adamawa languages are spoken adjacently to Chadic languages, interaction between Chadic and Adamawa is limited.\nPronouns.\nPronouns in Proto-Chadic, as compared to pronouns in Proto-Afroasiatic (Vossen &amp; Dimmendaal 2020:351):\nComparative vocabulary.\nSample basic vocabulary in different Chadic branches listed in order from west to east, with reconstructions of other Afroasiatic branches also given for comparison:"}
{"id": "7274", "revid": "1013091058", "url": "https://en.wikipedia.org/wiki?curid=7274", "title": "Cushitic languages", "text": "The Cushitic languages are a branch of the Afroasiatic language family. They are spoken primarily in the Horn of Africa (Djibouti, Eritrea, Ethiopia, Somalia, Somaliland, Puntland) as well as the Nile Valley (Sudan and Egypt), and parts of the African Great Lakes region (Tanzania and Kenya). Speakers of Cushitic languages and the descendants of speakers of Cushitic languages are referred to as Cushitic peoples. The phylum was first designated as \"Cushitic\" in 1858. Major Cushitic languages include Oromo, Somali, Beja, Agaw, Afar, Saho and Sidamo.\nBased on onomastic evidence, the Medjay and the Blemmyes of northern Nubia are believed to have spoken Cushitic languages related to the modern Beja language. Less certain are hypotheses which propose that Cushitic languages were spoken by the people of the C-Group culture in northern Nubia, or the people of the Kerma culture in southern Nubia.\nMajor and official languages.\nThe Cushitic languages with the greatest number of total speakers are Oromo (25 million), Somali (16.2 million), Beja (3.2 million), Sidamo (3 million), and Afar (2 million). Oromo is the working language of the Oromia Region in Ethiopia. Somali is one of two official languages of Somalia, and as such is the only Cushitic language accorded official language status at the country level. It also serves as a language of instruction in Djibouti, and as the working language of the Somali Region in Ethiopia. Beja, Afar, Blin and Saho, the languages of the Cushitic branch of Afroasiatic that are spoken in Eritrea, are languages of instruction in the Eritrean elementary school curriculum. The constitution of Eritrea also recognizes the equality of all natively spoken languages. Additionally, Afar is a language of instruction in Djibouti, as well as the working language of the Afar Region in Ethiopia.\nOrigin.\nThere is some evidence of a Proto-Cushitic language as far back as the Early Holocene.\nTypological characteristics.\nPhonology.\nMost Cushitic languages have a simple five-vowel system with phonemic length (); a notable exception are the Agaw languages, which do not contrast vowel length, but have one or two additional central vowels. The consonant inventory of many Cushitic languages includes glottalic consonants, e.g. in Oromo, which has the ejectives and the implosive . Less common are pharyngeal consonants , which appear e.g. in Somali or the Saho\u2013Afar languages.\nPitch accent is found most Cushitic languages, and plays a prominent role in morphology and syntax.\nGrammar.\nNouns.\nNouns are inflected for case and number. All nouns are further grouped into two gender categories, masculine gender and feminine gender. In many languages, gender is overtly marked directly on the noun (e.g. in Awngi, where all female nouns carry the suffix \"-a\").\nThe case system of many Cushitic languages is characterized by marked nominative alignment, which is typologically quite rare and predominantly found in languages of Africa. In marked nominative languages, the noun appears in unmarked \"absolutive\" case when cited in isolation, or when used as predicative noun and as object of a transitive verb; on the other hand, it is explicitly marked for nominative case when it functions as subject in a transitive or intransitive sentence.\nPossession is usually expressed by genitive case marking of the possessor. South Cushitic\u2014which has no case marking for subject and object\u2014follows the opposite strategy: here, the possessed noun is marked for construct case, e.g. Iraqw \"af\u00e9-r mar'i\" \"doors\" (lit. \"mouths of houses\"), where \"afee\" \"mouth\" is marked for construct case.\nMost nouns are by default unmarked for number, but can be explicitly marked for singular (\"singulative\") and plural number. E.g. in Bilin, \"d\u0259mmu\" \"cat(s)\" is number-neutral, from which singular \"d\u0259mmura\" \"a single cat\" and plural \"d\u0259mmura\" \"several cats\" can be formed. Plural formation is very diverse, and employs ablaut (i.e. changes of root vowels or consonants), suffixes and reduplication.\nVerbs.\nVerbs are inflected for person/number and tense/aspect. Many languages also have a special form of the verb in negative clauses.\nMost languages distinguish seven person/number categories: first, second, third person, singular and plural number, with a masculine/feminine gender distinction in third person singular. The most common conjugation type employs suffixes. Some languages also have a prefix conjugation: in Beja and the Saho\u2013Afar languages, the prefix conjugation is still a productive part of the verb paradigm, whereas in most other languages, e.g. Somali, it is restricted to only a few verbs. It is generally assumed that historically, the suffix conjugation developed from the older prefix conjugation, by combining the verb stem with a suffixed auxiliary verb. The following table gives an example for the suffix and prefix conjugations in affirmative present tense in Somali.\nSyntax.\nBasic word order is verb final, the most common order being subject\u2013object\u2013verb (SOV). The subject or object can also follow the verb to indicate focus.\nClassification.\nOverview.\nThe Cushitic languages usually include the following branches:\nThese classifications have not been without contention, and many other classifications have been proposed over the years.\nBeja.\nBeja constitutes the only member of the Northern Cushitic subgroup. As such, Beja contains a number of linguistic innovations that are unique to it, as is also the situation with the other subgroups of Cushitic (e.g. idiosyncratic features in Agaw or Central Cushitic). Hetzron (1980) argues that Beja therefore may comprise an independent branch of the Afroasiatic family. However, this suggestion has been rejected by most other scholars. The characteristics of Beja that differ from those of other Cushitic languages are instead generally acknowledged as normal branch variation.\nJoseph Hal\u00e9vy (1873) identified linguistic similarities shared between Beja and other neighboring Cushitic languages (viz. Afar, Agaw, Oromo and Somali). Leo Reinisch subsequently grouped Beja with Saho-Afar, Somali and Oromo in a Lowland Cushitic sub-phylum, representing one half of a two-fold partition of Cushitic. Moreno (1940) proposed a bipartite classification of Beja similar to that of Reinisch, but lumped Beja with both Lowland Cushitic and Central Cushitic. Around the same period, Enrico Cerulli (c. 1950) asserted that Beja constituted an independent sub-group of Cushitic. During the 1960s, Archibald N. Tucker (1960) posited an \"orthodox\" branch of Cushitic that comprised Beja, East Cushitic and Agaw, and a \"fringe\" branch of Cushitic that included other languages in the phylum. Although also similar to Reinisch's paradigm, Tucker's orthodox-fringe dichotomy was predicated on a different typological approach. Andrzej Zaborski (1976) suggested, on the basis of genetic features, that Beja constituted the only member of the North Cushitic sub-phylum. Due to its linguistic innovations, Robert Hetzron (1980) argued that Beja may constitute an independent branch of the Afroasiatic family. Hetzron's suggestion was arrived at independently, and was largely ignored or rejected by almost all linguists (Zaborski 1984 &amp; 1997; Tosco 2000; Morin 2001). Appleyard (2004) later also demonstrated that the innovations in Beja, which Hetzron had identified, were centered on a typological argument involving a presumed change in syntax and also consisted of only five differing Cushitic morphological features. Marcello Lamberti (1991) elucidated Cerulli's traditional classification of Beja, juxtaposing the language as the North Cushitic branch alongside three other independent Cushitic sub-phyla, Lowland Cushitic, Central Cushitic and Sidama. Didier Morin (2001) assigned Beja to Lowland Cushitic on the grounds that the language shared lexical and phonological features with the Afar and Saho idioms, and also because the languages were historically spoken in adjacent speech areas. However, among linguists specializing in the Cushitic languages, Cerulli's traditional paradigm is accepted as the standard classification for Beja.\nOther divergent languages.\nThere are also a few poorly-classified languages, including Yaaku, Dahalo, Aasax, Kw'adza, Boon, the Cushitic element of Mbugu (Ma'a) and Ongota. There is a wide range of opinions as to how the languages are interrelated.\nThe positions of the Dullay languages and of Yaaku are uncertain. They have traditionally been assigned to an East Cushitic subbranch along with Highland (Sidamic) and Lowland East Cushitic. However, Hayward thinks that East Cushitic may not be a valid node and that its constituents should be considered separately when attempting to work out the internal relationships of Cushitic.\nThe Afroasiatic identity of Ongota has also been broadly questioned, as is its position within Afroasiatic among those who accept it, because of the \"mixed\" appearance of the language and a paucity of research and data. Harold C. Fleming (2006) proposes that Ongota is a separate branch of Afroasiatic. Bonny Sands (2009) thinks the most convincing proposal is by Sav\u00e0 and Tosco (2003), namely that Ongota is an East Cushitic language with a Nilo-Saharan substratum. In other words, it would appear that the Ongota people once spoke a Nilo-Saharan language but then shifted to speaking a Cushitic language while retaining some characteristics of their earlier Nilo-Saharan language.\nHetzron (1980) and Ehret (1995) have suggested that the South Cushitic languages (Rift languages) are a part of Lowland East Cushitic, the only one of the six groups with much internal diversity.\nCushitic was formerly seen as also including the Omotic languages, then called West Cushitic. However, this view has been abandoned. Omotic is generally agreed to be an independent branch of Afroasiatic, primarily due to the work of Harold C. Fleming (1974) and Lionel Bender (1975); some linguists like Paul Newman (1980) challenge Omotic's classification within the Afroasiatic family itself.\nExtinct languages.\nA number of extinct populations have been proposed to have spoken Afroasiatic languages of the Cushitic branch. Marianne Bechhaus-Gerst (2000) proposed that the peoples of the Kerma Culture \u2013 which inhabited the Nile Valley in present-day Sudan immediately before the arrival of the first Nubian speakers \u2013 spoke Cushitic languages. She argues that the Nilo-Saharan Nobiin language today contains a number of key pastoralism related loanwords that are of proto-Highland East Cushitic origin, including the terms for sheep/goatskin, hen/cock, livestock enclosure, butter and milk. However, more recent linguistic research indicates that the people of the Kerma culture (who were based in southern Nubia) instead spoke Nilo-Saharan languages of the Eastern Sudanic branch, and that the peoples of the C-Group culture to their north (in northern Nubia) and other groups in northern Nubia (such as the Medjay and Belmmyes) spoke Cushitic languages with the latter being related to the modern Beja language. The linguistic affinity of the ancient A-Group culture of northern Nubia\u2014the predecessor of the C-Group culture\u2014is unknown, but Rilly (2019) suggests that it is unlikely to have spoken a language of the Northern East Sudanic branch of Nilo-Saharan, and may have spoken a Cushitic language, another Afro-Asiatic language, or a language belonging to another (non-Northern East Sudanic) branch of the Nilo-Saharan family. Rilly also criticizes proposals (by Behrens and Bechaus-Gerst) of significant early Afro-Asiatic influence on Nobiin, and considers evidence of substratal influence on Nobiin from an earlier now extinct Eastern Sudanic language to be stronger.\nLinguistic evidence indicates that Cushitic languages were spoken in Lower Nubia, an ancient region which straddles present day Southern Egypt and Northern Sudan, before the arrival of North Eastern Sudanic languages from Upper Nubia\nJulien Cooper (2017) states that in antiquity, Cushitic languages were spoken in Lower Nubia (the northernmost part of modern day Sudan). He also states that Eastern Sudanic speaking populations from southern and west Nubia gradually replaced the earlier Cushitic speaking populations of this region.\nIn Handbook of Ancient Nubia, Claude Rilly (2019) states that Cushitic languages once dominated Lower Nubia along with the Ancient Egyptian language. He mentions historical records of the Blemmyes, a Cushitic speaking tribe which controlled Lower Nubia and some cities in Upper Egypt. He mentions the linguistic relationship between the modern Beja language and the ancient Blemmyan language, and that the Blemmyes can be regarded as a particular tribe of the Medjay.\nAdditionally, historiolinguistics indicate that the makers of the Savanna Pastoral Neolithic (Stone Bowl Culture) in the Great Lakes area likely spoke South Cushitic languages.\nChristopher Ehret (1998) proposed on the basis of loanwords that South Cushitic languages (called \"Tale\" and \"Bisha\" by Ehret) were spoken in an area closer to Lake Victoria than are found today.\nAlso, historically, the Southern Nilotic languages have undergone extensive contact with a \"missing\" branch of East Cushitic that Heine (1979) refers to as \"Baz\".\nReconstruction.\nChristopher Ehret proposed a reconstruction of Proto-Cushitic in 1987, but did not base this on individual branch reconstructions. Grover Hudson (1989) has done some preliminary work on Highland East Cushitic, David Appleyard (2006) has proposed a reconstruction of Proto-Agaw, and Roland Kie\u00dfling and Maarten Mous (2003) have jointly proposed a reconstruction of West Rift Southern Cushitic. No reconstruction been published for Lowland East Cushitic, though Paul D. Black wrote his (unpublished) dissertation on the topic in 1974. No comparative work has yet brought these branch reconstructions together.\nComparative vocabulary.\nBasic vocabulary.\nSample basic vocabulary of Cushitic languages from Vossen &amp; Dimmendaal (2020:318) (with PSC denoting Proto-Southern Cushitic):\nNumerals.\nComparison of numerals in individual Cushitic languages:"}
{"id": "7277", "revid": "12978", "url": "https://en.wikipedia.org/wiki?curid=7277", "title": "Celestial dynamics", "text": ""}
{"id": "7279", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7279", "title": "Chapter 11, Title 11, United States Code", "text": "Chapter 11 of the United States Bankruptcy Code (Title 11 of the United States Code) permits reorganization under the bankruptcy laws of the United States. Such reorganization, known as \"Chapter 11 bankruptcy\", is available to every business, whether organized as a corporation, partnership or sole proprietorship, and to individuals, although it is most prominently used by corporate entities. In contrast, Chapter 7 governs the process of a liquidation bankruptcy, though liquidation may also occur under Chapter 11; while Chapter 13 provides a reorganization process for the majority of private individuals.\nChapter 11 overview.\nWhen a business is unable to service its debt or pay its creditors, the business or its creditors can file with a federal bankruptcy court for protection under either Chapter 7 or Chapter 11.\nIn Chapter 7, the business ceases operations, a trustee sells all of its assets, and then distributes the proceeds to its creditors. Any residual amount is returned to the owners of the company.\nIn Chapter 11, in most instances the debtor remains in control of its business operations as a debtor in possession, and is subject to the oversight and jurisdiction of the court.\nA Chapter 11 bankruptcy will result in one of three outcomes for the debtor: reorganization, conversion to Chapter 7 bankruptcy, or dismissal. In order for a chapter 11 debtor to reorganize, the debtor must file (and the court must confirm) a plan of reorganization. In effect, the plan is a compromise between the major stakeholders in the case, including the debtor and its creditors. Most chapter 11 cases aim to confirm a plan, but that may not always be possible.\nIf the judge approves the reorganization plan and the creditors all agree, then the plan can be confirmed. Section 1129 of the Bankruptcy Code requires the bankruptcy court reach certain conclusions prior to confirming or approving the plan and making it binding on all parties in the case, most notably that the plan complies with applicable law and was proposed in good faith. The court must also find that the reorganization plan is feasible in that, unless the plan provides otherwise, the plan is not likely to be followed by further reorganization or liquidation.\nIn a Chapter 11 bankruptcy, the debtor corporation is typically recapitalized so that it emerges from bankruptcy with more equity and less debt, a process through which some of the debtor corporation's debts may be discharged. Determinations as to which debts are discharged, and how equity and other entitlements are distributed to various groups of investors, often on valuation of the reorganized business. Bankruptcy valuation is often highly contentious because it is both subjective and important to case outcomes. The methods of valuation used in bankruptcy have changed over time, generally tracking methods used in investment banking, Delaware corporate law, and corporate and academic finance, but with a significant time lag.\nFeatures of Chapter 11 reorganization.\nChapter 11 retains many of the features present in all, or most, bankruptcy proceedings in the United States. It provides additional tools for debtors as well. Most importantly, empowers the trustee to operate the debtor's business. In Chapter 11, unless a separate trustee is appointed for cause, the debtor, as debtor in possession, acts as trustee of the business.\nChapter 11 affords the debtor in possession a number of mechanisms to restructure its business. A debtor in possession can acquire financing and loans on favorable terms by giving new lenders first priority on the business's earnings. The court may also permit the debtor in possession to reject and cancel contracts. Debtors are also protected from other litigation against the business through the imposition of an automatic stay. While the automatic stay is in place, creditors are stayed from any collection attempts or activities against the debtor in possession, and most litigation against the debtor is stayed, or put on hold, until it can be resolved in bankruptcy court, or resumed in its original venue. An example of proceedings that are not necessarily stayed automatically are family law proceedings against a spouse or parent. Further, creditors may file with the court seeking relief from the automatic stay.\nIf the business is insolvent, its debts exceed its assets and the business is unable to pay debts as they come due, the bankruptcy restructuring may result in the company's owners being left with nothing; instead, the owners' rights and interests are ended and the company's creditors are left with ownership of the newly reorganized company.\nAll creditors are entitled to be heard by the court. The court is ultimately responsible for determining whether the proposed plan of reorganization complies with bankruptcy laws.\nOne controversy that has broken out in bankruptcy courts concerns the proper amount of disclosure that the court and other parties are entitled to receive from the members of the creditor's committees that play a large role in many proceedings.\nChapter 11 plan.\nChapter 11 usually results in reorganization of the debtor's business or personal assets and debts, but can also be used as a mechanism for liquidation. Debtors may \"emerge\" from a chapter 11 bankruptcy within a few months or within several years, depending on the size and complexity of the bankruptcy. The Bankruptcy Code accomplishes this objective through the use of a bankruptcy plan. The debtor in possession typically has the first opportunity to propose a plan during the period of exclusivity. This period allows the debtor 120 days from the date of filing for chapter 11 to propose a plan of reorganization before any other party in interest may propose a plan. If the debtor proposes a plan within the 120-day exclusivity period, a 180-day exclusivity period from the date of filing for chapter 11 is granted in order to allow the debtor to gain confirmation of the proposed plan. With some exceptions, the plan may be proposed by any party in interest. Interested creditors then vote for a plan.\nConfirmation.\nIf the judge approves the reorganization plan and the creditors all agree, then the plan can be confirmed. If at least one class of creditors objects and votes against the plan, it may nonetheless be confirmed if the requirements of cramdown are met. In order to be confirmed over the creditors' objection, the plan must not discriminate against that class of creditors, and the plan must be found fair and equitable to that class. Upon confirmation, the plan becomes binding and identifies the treatment of debts and operations of the business for the duration of the plan. If a plan cannot be confirmed, the court may either convert the case to a liquidation under chapter 7, or, if in the best interests of the creditors and the estate, the case may be dismissed resulting in a return to the status quo before bankruptcy. If the case is dismissed, creditors will look to non-bankruptcy law in order to satisfy their claims.\nIn order to proceed to the confirmation hearing, a disclosure statement must be approved by the bankruptcy court. Once the disclosure statement is approved, the plan proponent will solicit votes from the classes of creditors. Solicitation is the process by which creditors vote on the proposed confirmation plan. This process can be complicated if creditors fail or refuse to vote. In which case, the plan proponent might tailor his or her efforts in obtaining votes, or the plan itself. The plan may be modified before confirmation, so long as the modified plan meets all the requirements of Chapter 11.\nAutomatic stay.\nLike other forms of bankruptcy, petitions filed under chapter 11 invoke the automatic stay of \u00a7\u00a0362. The automatic stay requires all creditors to cease collection attempts, and makes many post-petition debt collection efforts void or voidable. Under some circumstances, some creditors, or the United States Trustee, can request the court convert the case into a liquidation under chapter 7, or appoint a trustee to manage the debtor's business. The court will grant a motion to convert to chapter 7 or appoint a trustee if either of these actions is in the best interest of all creditors. Sometimes a company will liquidate under chapter 11 (perhaps in a 363 sale), in which the pre-existing management may be able to help get a higher price for divisions or other assets than a chapter 7 liquidation would be likely to achieve.\nExecutory contracts.\nIn the new millennium, airlines have fallen under intense scrutiny for what many see as abusing Chapter 11 bankruptcy as a tool for escaping labor contracts, usually 30-35% of an airline's operating cost. Every major US airline has filed for Chapter 11 since 2002. In the space of 2 years (2002\u20132004) US Airways filed for bankruptcy twice leaving the AFL-CIO, pilot unions and other airline employees claiming the rules of Chapter 11 have helped turn the United States into a corporatocracy.\nPriority.\nChapter 11 follows the same priority scheme as other bankruptcy chapters. The priority structure is defined primarily by \u00a7\u00a0507 of the Bankruptcy Code ().\nAs a general rule, administrative expenses (the actual, necessary expenses of preserving the bankruptcy estate, including expenses such as employee wages, and the cost of litigating the chapter 11 case) are paid first. Secured creditors\u2014creditors who have a security interest, or collateral, in the debtor's property\u2014will be paid before unsecured creditors. Unsecured creditors' claims are prioritized by \u00a7\u00a0507. For instance the claims of suppliers of products or employees of a company may be paid before other unsecured creditors are paid. Each priority level must be paid in full before the next lower priority level may receive payment.\nSection 1110.\nSection 1110 () generally provides a secured party with an interest in an aircraft the ability to take possession of the equipment within 60 days after a bankruptcy filing unless the airline cures all defaults. More specifically, the right of the lender to take possession of the secured equipment is not hampered by the automatic stay provisions of the Bankruptcy Code.\nSubchapter V.\nIn August 2019, the Small Business Reorganization Act of 2019 (\u201cSBRA\u201d) added Subchapter V to Chapter 11 of the Bankruptcy Code. Subchapter V, which took effect in February 2020, is reserved exclusively for the small business debtor with the purpose of expediting bankruptcy procedure and economically resolving small business bankruptcy cases.\nSubchapter V retains many of the advantages of a traditional Chapter 11 case without the unnecessary procedural burdens and costs. It seeks to increase the debtor's ability to negotiate a successful reorganization and retain control of the business and increase oversight and ensure a quick reorganization.\nA Subchapter V case contrasts from a traditional Chapter 11 in several key aspects: It's earmarked only for the \u201csmall business debtor\u201d (as defined by the Bankruptcy Code), so, only a debtor can file a plan of reorganization. The SBRA requires the U.S. Trustee appoint a \u201csubchapter V trustee\u201d to every Subchapter V case to supervise and control estate funds, and facilitate the development of a consensual plan. It also eliminates automatic appointment of an official committee of unsecured creditors and abolishes quarterly fees usually paid to the U.S. Trustee throughout the case. Most notably, Subchapter V allows the small business owner to retain their equity in the business so long as the reorganization plan does not discriminate unfairly and is fair and equitable with respect to each class of claims or interests.\nConsiderations.\nThe reorganization and court process may take an inordinate amount of time, limiting the chances of a successful outcome and sufficient debtor-in-possession financing may be unavailable during an economic recession. A preplanned, pre-agreed approach between the debtor and its creditors (sometimes called a pre-packaged bankruptcy) may facilitate the desired result. A company undergoing Chapter 11 reorganization is effectively operating under the \"protection\" of the court until it emerges. An example is the airline industry in the United States; in 2006 over half the industry's seating capacity was on airlines that were in Chapter 11. These airlines were able to stop making debt payments, break their previously agreed upon labor union contracts, freeing up cash to expand routes or weather a price war against competitors \u2014 all with the bankruptcy court's approval.\nStudies on the impact of forestalling the creditors' rights to enforce their security reach different conclusions.\nDeadlines.\nWithin 60 days of filing for Chapter 11 bankruptcy, the debtor must submit a written disclosure statement with the court containing information on assets, liabilities and business affairs.\nStatistics.\nFrequency.\nChapter 11 cases dropped by 60% from 1991 to 2003. One 2007 study found this was because businesses were turning to bankruptcy-like proceedings under state law, rather than the federal bankruptcy proceedings, including those under chapter 11. Insolvency proceedings under state law, the study stated, are currently faster, less expensive, and more private, with some states not even requiring court filings. However, a 2005 study claimed the drop may have been due to an increase in the incorrect classification of many bankruptcies as \"consumer cases\" rather than \"business cases\".\nCases involving more than US$50 million in assets are almost always handled in federal bankruptcy court, and not in bankruptcy-like state proceeding.\nLargest cases.\nThe largest bankruptcy in history was of the US investment bank Lehman Brothers Holdings Inc., which listed $639 billion in assets as of its Chapter 11 filing in 2008. The 16 largest corporate bankruptcies as of 13 December 2011\nEnron, Lehman Brothers, MF Global and Refco have all ceased operations while others were acquired by other buyers or emerged as a new company with a similar name.\n\u2021 The Enron assets were taken from the 10-Q filed on November 11, 2001. The company announced that the annual financials were under review at the time of filing for Chapter 11."}
{"id": "7280", "revid": "983935047", "url": "https://en.wikipedia.org/wiki?curid=7280", "title": "Conjugation", "text": "Conjugation or conjugate may refer to:"}
{"id": "7283", "revid": "31870463", "url": "https://en.wikipedia.org/wiki?curid=7283", "title": "Controversy", "text": "Controversy is a state of prolonged public dispute or debate, usually concerning a matter of conflicting opinion or point of view. The word was coined from the Latin \"controversia\", as a composite of \"controversus\" \u2013 \"turned in an opposite direction\".\nLegal.\nIn the theory of law, a controversy differs from a legal case; while legal cases include all suits, criminal as well as civil, a controversy is a purely civil proceeding.\nor example, the Case or Controversy Clause of Article Three of the United States Constitution (, Clause 1) states that \"the judicial Power shall extend\u00a0...\u00a0to Controversies to which the United States shall be a Party\". This clause has been deemed to impose a requirement that United States federal courts are not permitted to cases that do not pose an actual controversy\u2014that is, an actual dispute between adverse parties which is capable of being resolved by the [court]. In addition to setting out the scope of the jurisdiction of the federal judiciary, it also prohibits courts from issuing advisory opinions, or from hearing cases that are either unripe, meaning that the controversy has not arisen yet, or moot, meaning that the controversy has already been resolved.\nBenford's law.\nBenford's law of controversy, as expressed by the astrophysicist and science fiction author Gregory Benford in 1980, states: \"Passion is inversely proportional to the amount of real information available.\" In other words, it claims that the less factual information is available on a topic, the more controversy can arise around that topic \u2013 and the more facts are available, the less controversy can arise. Thus, for example, controversies in physics would be limited to subject areas where experiments cannot be carried out yet, whereas controversies would be inherent to politics, where communities must frequently decide on courses of action based on insufficient information.\nPsychological bases.\nControversies are frequently thought to be a result of a lack of confidence on the part of the disputants \u2013 as implied by Benford's law of controversy, which only talks about lack of information (\"passion is inversely proportional to the amount of real information available\"). For example, in analyses of the political controversy over anthropogenic climate change, which is exceptionally virulent in the United States, it has been proposed that those who are opposed to the scientific consensus do so because they don't have enough information about the topic. A study of 1540 US adults found instead that levels of scientific literacy correlated with the strength of opinion on climate change, but not on which side of the debate that they stood.\nThe puzzling phenomenon of two individuals being able to reach different conclusions after being exposed to the same facts has been frequently explained (particularly by Daniel Kahneman) by reference to a 'bounded rationality' \u2013 in other words, that most judgments are made using fast acting heuristics that work well in every day situations, but are not amenable to decision making about complex subjects such as climate change. Anchoring has been particularly identified as relevant in climate change controversies as individuals are found to be more positively inclined to believe in climate change if the outside temperature is higher, if they have been primed to think about heat, and if they are primed with higher temperatures when thinking about the future temperature increases from climate change.\nIn other controversies \u2013 such as that around the HPV vaccine, the same evidence seemed to license inference to radically different conclusions. Kahan et al. explained this by the cognitive biases of biased assimilation and a credibility heuristic.\nSimilar effects on reasoning are also seen in non-scientific controversies, for example in the gun control debate in the United States. As with other controversies, it has been suggested that exposure to empirical facts would be sufficient to resolve the debate once and for all. In computer simulations of cultural communities, beliefs were found to polarize within isolated sub-groups, based on the mistaken belief of the community's unhindered access to ground truth. Such confidence in the group to find the ground truth is explicable through the success of wisdom of the crowd based inferences. However, if there is no access to the ground truth, as there was not in this model, the method will fail.\nBayesian decision theory allows these failures of rationality to be described as part of a statistically optimized system for decision making. Experiments and computational models in multisensory integration have shown that sensory input from different senses is integrated in a statistically optimal way, in addition, it appears that the kind of inferences used to infer single sources for multiple sensory inputs uses a Bayesian inference about the causal origin of the sensory stimuli. As such, it appears neurobiologically plausible that the brain implements decision-making procedures that are close to optimal for Bayesian inference.\nBrocas and Carrillo propose a model to make decisions based on noisy sensory inputs, beliefs about the state of the world are modified by Bayesian updating, and then decisions are made based on beliefs passing a threshold. They show that this model, when optimized for single-step decision making, produces belief anchoring and polarization of opinions \u2013 exactly as described in the global warming controversy context \u2013 in spite of identical evidence presented, the pre-existing beliefs (or evidence presented first) has an overwhelming effect on the beliefs formed. In addition, the preferences of the agent (the particular rewards that they value) also cause the beliefs formed to change \u2013 this explains the biased assimilation (also known as confirmation bias) shown above. This model allows the production of controversy to be seen as a consequence of a decision maker optimized for single-step decision making, rather than as a result of limited reasoning in the bounded rationality of Daniel Kahneman."}
{"id": "7284", "revid": "2584239", "url": "https://en.wikipedia.org/wiki?curid=7284", "title": "Centromere", "text": "The centromere is the specialized DNA sequence of a chromosome that links a pair of sister chromatids (a dyad). During mitosis, spindle fibers attach to the centromere via the kinetochore. Centromeres were first thought to be genetic loci that direct the behavior of chromosomes.\nThe physical role of the centromere is to act as the site of assembly of the kinetochores \u2013 a highly complex multiprotein structure that is responsible for the actual events of chromosome segregation \u2013 i.e. binding microtubules and signalling to the cell cycle machinery when all chromosomes have adopted correct attachments to the spindle, so that it is safe for cell division to proceed to completion and for cells to enter anaphase.\nThere are, broadly speaking, two types of centromeres. \"Point centromeres\" bind to specific proteins that recognize particular DNA sequences with high efficiency. Any piece of DNA with the point centromere DNA sequence on it will typically form a centromere if present in the appropriate species. The best characterised point centromeres are those of the budding yeast, \"Saccharomyces cerevisiae\". \"Regional centromeres\" is the term coined to describe most centromeres, which typically form on regions of preferred DNA sequence, but which can form on other DNA sequences as well. The signal for formation of a regional centromere appears to be epigenetic. Most organisms, ranging from the fission yeast \"Schizosaccharomyces pombe\" to humans, have regional centromeres.\nRegarding mitotic chromosome structure, centromeres represent a constricted region of the chromosome (often referred to as the primary constriction) where two identical sister chromatids are most closely in contact. When cells enter mitosis, the sister chromatids (the two copies of each chromosomal DNA molecule resulting from DNA replication in chromatin form) are linked along their length by the action of the cohesin complex. It is now believed that this complex is mostly released from chromosome arms during prophase, so that by the time the chromosomes line up at the mid-plane of the mitotic spindle (also known as the metaphase plate), the last place where they are linked with one another is in the chromatin in and around the centromere.\nPosition.\nEach chromosome has two arms, labeled \"p\" (the shorter of the two) and \"q\" (the longer). Many remember that the short arm 'p' is named for the French word \"petit\" meaning 'small', although this explanation was shown to be apocryphal. They can be connected in either metacentric, submetacentric, acrocentric or telocentric manner.\nMetacentric.\nThese are X-shaped chromosomes, with the centromere in the middle so that the two arms of the chromosomes are almost equal.\nA chromosome is metacentric if its two arms are roughly equal in length. In a normal human karyotype, five chromosomes are considered metacentric: chromosomes 1, 3, 16, 19, and 20. In some cases, a metacentric chromosome is formed by balanced translocation: the fusion of two acrocentric chromosomes to form one metacentric chromosome.\nSubmetacentric.\nIf the arms' lengths are unequal, the chromosome is said to be submetacentric. They are L-shaped.\nAcrocentric.\nIf the p (short) arm is so short that it is hard to observe, but still present, then the chromosome is acrocentric (the \"acro-\" in acrocentric refers to the Greek word for \"peak\"). The human genome includes five acrocentric chromosomes: 13, 14, 15, 21, 22. The Y chromosome is also acrocentric.\nIn an acrocentric chromosome the p arm contains genetic material including repeated sequences such as nucleolar organizing regions, and can be translocated without significant harm, as in a balanced Robertsonian translocation. The domestic horse genome includes one metacentric chromosome that is homologous to two acrocentric chromosomes in the conspecific but undomesticated Przewalski's horse. This may reflect either fixation of a balanced Robertsonian translocation in domestic horses or, conversely, fixation of the fission of one metacentric chromosome into two acrocentric chromosomes in Przewalski's horses. A similar situation exists between the human and great ape genomes, with a reduction of two acrocentric chromosomes in the great apes to one metacentric chromosome in humans (see aneuploidy and the human chromosome 2).\nStrikingly, harmful translocations in disease context, especially unbalanced translocations in blood cancers, more frequently involve acrocentric chromosomes than non-acrocentric chromosomes. Although the cause is not known, this probably relates to the physical location of acrocentric chromosomes within the nucleus. Acrocentric chromosomes are usually located in and around the nucleolus, so in the center of the nucleus, where chromosomes tend to be less densely packed than chromosomes in the nuclear periphery. Consistently, chromosomal regions that are less densely packed are also more prone to chromosomal translocations in cancers.\nTelocentric.\nA telocentric chromosome's centromere is located at the terminal end of the chromosome. A telocentric chromosome has therefore only one arm. Telomeres may extend from both ends of the chromosome, their shape is similar to letter \"i\" during anaphase. For example, the standard house mouse karyotype has only telocentric chromosomes. Humans do not possess telocentric chromosomes.\nSubtelocentric.\nIf the chromosome's centromere is located closer to its end than to its center, it may be described as subtelocentric.\nCentromere number.\nAcentric.\nIf a chromosome lacks a centromere, it is said acentric. The macronucleus of ciliates for example contains hundreds of acentric chromosomes. Chromosome-breaking events can also generate acentric chromosomes or acentric fragments.\nDicentric.\nA dicentric chromosome is an abnormal chromosome with two centromeres. It is formed through the fusion of two chromosome segments, each with a centromere, resulting in the loss of acentric fragments (lacking a centromere) and the formation of dicentric fragments. The formation of dicentric chromosomes has been attributed to genetic processes, such as Robertsonian translocation and paracentric inversion. Dicentric chromosomes have important roles in the mitotic stability of chromosomes and the formation of pseudodicentric chromosomes.\nMonocentric.\nThe monocentric chromosome is a chromosome that has only one centromere in a chromosome and forms a narrow constriction.\nMonocentric centromeres are the most common structure on highly repetitive DNA in plants and animals.\nHolocentric.\nDifferent than monocentric chromosomes in holocentric chromosomes, the entire length of the chromosome acts as the centromere. In holocentric chromosomes there is not one primary constriction but the centromere has many CenH3 loci spread over the whole chromosome. Examples of this type of centromere can be found scattered throughout the plant and animal kingdoms, with the most well-known example being the nematode \"Caenorhabditis elegans\".\nSequence.\nThere are two types of centromeres. In regional centromeres, DNA sequences contribute to but do not define function. Regional centromeres contain large amounts of DNA and are often packaged into heterochromatin. In most eukaryotes, the centromere's DNA sequence consists of large arrays of repetitive DNA (e.g. satellite DNA) where the sequence within individual repeat elements is similar but not identical. In humans, the primary centromeric repeat unit is called \u03b1-satellite (or alphoid), although a number of other sequence types are found in this region.\nPoint centromeres are smaller and more compact. DNA sequences are both necessary and sufficient to specify centromere identity and function in organisms with point centromeres. In budding yeasts, the centromere region is relatively small (about 125 bp DNA) and contains two highly conserved DNA sequences that serve as binding sites for essential kinetochore proteins.\nInheritance.\nSince centromeric DNA sequence is not the key determinant of centromeric identity in metazoans, it is thought that epigenetic inheritance plays a major role in specifying the centromere. The daughter chromosomes will assemble centromeres in the same place as the parent chromosome, independent of sequence. It has been proposed that histone H3 variant CENP-A (Centromere Protein A) is the epigenetic mark of the centromere. The question arises whether there must be still some original way in which the centromere is specified, even if it is subsequently propagated epigenetically. If the centromere is inherited epigenetically from one generation to the next, the problem is pushed back to the origin of the first metazoans.\nStructure.\nThe centromeric DNA is normally in a heterochromatin state, which is essential for the recruitment of the cohesin complex that mediates sister chromatid cohesion after DNA replication as well as coordinating sister chromatid separation during anaphase. In this chromatin, the normal histone H3 is replaced with a centromere-specific variant, CENP-A in humans. The presence of CENP-A is believed to be important for the assembly of the kinetochore on the centromere. CENP-C has been shown to localise almost exclusively to these regions of CENP-A associated chromatin. In human cells, the histones are found to be most enriched for H4K20me3 and H3K9me3 which are known heterochromatic modifications. In Drosophila, Islands of retroelements are major components of the centromeres.\nIn the yeast \"Schizosaccharomyces pombe\" (and probably in other eukaryotes), the formation of centromeric heterochromatin is connected to RNAi. In nematodes such as \"Caenorhabditis elegans\", some plants, and the insect orders Lepidoptera and Hemiptera, chromosomes are \"holocentric\", indicating that there is not a primary site of microtubule attachments or a primary constriction, and a \"diffuse\" kinetochore assembles along the entire length of the chromosome.\nCentromeric aberrations.\nIn rare cases, neocentromeres can form at new sites on a chromosome as a result of a repositioning of the centromere. This phenomenon is most well known from human clinical studies and there are currently over 90 known human neocentromeres identified on 20 different chromosomes. The formation of a neocentromere must be coupled with the inactivation of the previous centromere, since chromosomes with two functional centromeres (Dicentric chromosome) will result in chromosome breakage during mitosis. In some unusual cases human neocentromeres have been observed to form spontaneously on fragmented chromosomes. Some of these new positions were originally euchromatic and lack alpha satellite DNA altogether. Neocentromeres lack the repetitive structure seen in normal centromeres which suggest that centromere formation is mainly controlled epigenetically. Over time a neocentromere can accumulate repetitive elements and mature into what is known as an evolutionary new centromere. There are several well known examples in primate chromosomes where the centromere position is different from the human centromere of the same chromosome and is thought to be evolutionary new centromeres. Centromere repositioning and the formation of evolutionary new centromeres has been suggested to be a mechanism of speciation.\nCentromere proteins are also the autoantigenic target for some anti-nuclear antibodies, such as anti-centromere antibodies.\nDysfunction and disease.\nIt has been known that centromere misregulation contributes to mis-segregation of chromosomes, which is strongly related to cancer and abortion. Notably, overexpression of many centromere genes have been linked to cancer malignant phenotypes. Overexpression of these centromere genes can increase genomic instability in cancers. Elevated genomic instability on one hand relates to malignant phenotypes; on the other hand, it makes the tumor cells more vulnerable to specific adjuvant therapies such as certain chemotherapies and radiotherapy. Instability of centromere repetitive DNA was recently shown in cancer and aging.\nEtymology and pronunciation.\nThe word \"centromere\" () uses combining forms of \"centro- and -mere\", yielding \"central part\", describing the centromere's location at the center of the chromosome."}
{"id": "7287", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=7287", "title": "Castello", "text": "Castello may refer to:"}
{"id": "7288", "revid": "20585603", "url": "https://en.wikipedia.org/wiki?curid=7288", "title": "Common preference", "text": "Common preference is an \"everyone wins\" situation in a number of places:"}
{"id": "7291", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7291", "title": "CuteFTP", "text": "CuteFTP is a series of FTP (file transfer protocol) client applications distributed and supported since 1996 by GlobalSCAPE, who later bought the rights to the software. Both a Windows-based or Mac-based interface were made for both home and professional use.\nCuteFTP is used to transfer files between computers and File Transfer Protocol (FTP) servers to publish web pages, download digital images, music, multi-media files and software, and transfer files of any size or type between home and office. Since 1999, CuteFTP Pro and CuteFTP Mac Pro have also been available alongside CuteFTP Home with free trial periods.\nIt was originally developed by Alex Kunadze, a Russian programmer."}
{"id": "7292", "revid": "300", "url": "https://en.wikipedia.org/wiki?curid=7292", "title": "Carsons rule", "text": ""}
{"id": "7293", "revid": "734812", "url": "https://en.wikipedia.org/wiki?curid=7293", "title": "Commodore 64", "text": "The Commodore 64, also known as the C64 or the CBM 64, is an 8-bit home computer introduced in January 1982 by Commodore International (first shown at the Consumer Electronics Show, January 7\u201310, 1982, in Las Vegas). It has been listed in the Guinness World Records as the highest-selling single computer model of all time, with independent estimates placing the number sold between 12.5 and 17 million units. This claim is in spite of the Commodore 64 having three different Kernal ROM versions, two different SID sound chip versions, a few different motherboard versions and two different cases during its lifetime. Volume production started in early 1982, marketing in August for . Preceded by the Commodore VIC-20 and Commodore PET, the C64 took its name from its of RAM. With support for multicolor sprites and a custom chip for waveform generation, the C64 could create superior visuals and audio compared to systems without such custom hardware.\nThe C64 dominated the low-end computer market (except in the UK and Japan, lasting only about six months in Japan) for most of the 1980s. For a substantial period (1983\u20131986), the C64 had between 30% and 40% share of the US market and two million units sold per year, outselling IBM PC compatibles, Apple computers, and the Atari 8-bit family of computers. Sam Tramiel, a later Atari president and the son of Commodore's founder, said in a 1989 interview, \"When I was at Commodore we were building C64s a month for a couple of years.\" In the UK market, the C64 faced competition from the BBC Micro and the ZX Spectrum, but the C64 was still the second most popular computer in the UK after the ZX Spectrum. The Commodore 64 failed to make any impact in Japan. The Japanese market was dominated by Japanese computers, such as the NEC PC-8801, Sharp X1, Fujitsu FM-7, and MSX.\nPart of the Commodore 64's success was its sale in regular retail stores instead of only electronics or computer hobbyist specialty stores. Commodore produced many of its parts in-house to control costs, including custom integrated circuit chips from MOS Technology. It has been compared to the Ford Model T automobile for its role in bringing a new technology to middle-class households via creative and affordable mass-production. Approximately 10,000 commercial software titles have been made for the Commodore\u00a064, including development tools, office productivity applications, and video games. C64 emulators allow anyone with a modern computer, or a compatible video game console, to run these programs today. The C64 is also credited with popularizing the computer demoscene and is still used today by some computer hobbyists. In 2011, 17 years after it was taken off the market, research showed that brand recognition for the model was still at 87%.\nHistory.\nIn January 1981, MOS Technology, Inc., Commodore's integrated circuit design subsidiary, initiated a project to design the graphic and audio chips for a next-generation video game console. Design work for the chips, named MOS Technology VIC-II (Video Integrated Circuit for graphics) and MOS Technology SID (Sound Interface Device for audio), was completed in November 1981. Commodore then began a game console project that would use the new chips\u2014called the \"Ultimax\" or the \"Commodore MAX Machine\", engineered by Yash Terakura from Commodore Japan. This project was eventually cancelled after just a few machines were manufactured for the Japanese market. At the same time, Robert \"Bob\" Russell (system programmer and architect on the VIC-20) and Robert \"Bob\" Yannes (engineer of the SID) were critical of the current product line-up at Commodore, which was a continuation of the Commodore PET line aimed at business users. With the support of Al Charpentier (engineer of the VIC-II) and Charles Winterble (manager of MOS Technology), they proposed to Commodore CEO Jack Tramiel a true low-cost sequel to the VIC-20. Tramiel dictated that the machine should have of random-access memory (RAM). Although 64-Kbit dynamic random-access memory (DRAM) chips cost over at the time, he knew that 64K DRAM prices were falling and would drop to an acceptable level before full production was reached. The team was able to quickly design the computer because, unlike most other home-computer companies, Commodore had its own semiconductor fab to produce test chips; because the fab was not running at full capacity, development costs were part of existing corporate overhead. The chips were complete by November, by which time Charpentier, Winterble, and Tramiel had decided to proceed with the new computer; the latter set a final deadline for the first weekend of January, to coincide with the 1982 Consumer Electronics Show (CES).\nThe product was code named the VIC-40 as the successor to the popular VIC-20. The team that constructed it consisted of Yash Terakura, Shiraz Shivji, Bob Russell, Bob Yannes, and David A. Ziembicki. The design, prototypes, and some sample software were finished in time for the show, after the team had worked tirelessly over both Thanksgiving and Christmas weekends. The machine used the same case, same-sized motherboard, and same Commodore BASIC 2.0 in ROM as the VIC-20. BASIC also served as the user interface shell and was available immediately on startup at the codice_1 prompt. When the product was to be presented, the VIC-40 product was renamed C64. The C64 made an impressive debut at the January 1982 Consumer Electronics Show, as recalled by Production Engineer David A. Ziembicki: \"All we saw at our booth were Atari people with their mouths dropping open, saying, 'How can you do that for $595?'\" The answer was vertical integration; due to Commodore's ownership of MOS Technology's semiconductor fabrication facilities, each C64 had an estimated production cost of .\nReception.\nIn July 1983, \"BYTE\" magazine stated that \"the 64 retails for $595. At that price it promises to be one of the hottest contenders in the under-$1000 personal computer market.\" It described the SID as \"a true music synthesizer\u00a0... the quality of the sound has to be heard to be believed\", while criticizing the use of Commodore BASIC\u00a02.0, the floppy disk performance which is \"even slower than the Atari 810 drive\", and Commodore's quality control. \"BYTE\" gave more details, saying the C64 had \"inadequate Commodore BASIC 2.0. An 8K-byte interpreted BASIC\" which they assumed was because \"Obviously, Commodore feels that most home users will be running prepackaged software-there is no provision for using graphics (or sound as mentioned above) from within a BASIC program except by means of POKE commands.\" This was one of very few warnings about C64 BASIC published in any computer magazines. \n \"Creative Computing\" said in December 1984 that the 64 was \"the overwhelming winner\" in the category of home computers under $500. Despite criticizing its \"slow disk drive, only two cursor directional keys, zero manufacturer support, non-standard interfaces, etc.\", the magazine said that at the 64's price of less than $200 \"you can't get another system with the same features: 64K, color, sprite graphics, and barrels of available software\". The Tandy/Radio Shack Color Computer was the runner up. However, this was only one of twelve categories being voted on, depending on the price and what people wanted to do with a computer. The same article also said \"Although there was no single best all-around system, we noted that one system stood out because it was mentioned in so many categories. Although many systems were mentioned in two categories, just two systems were mentioned in three categories, and only one in four categories--the Apple Macintosh.\" Apart from this, the Apple II was the winner in the category of home computer over $500, which was the category the Commodore 64 was in when it was first released at the price of $595.\nMarket war: 1982\u20131983.\nCommodore had a reputation for announcing products that never appeared, so sought to quickly ship the C64. Production began in spring 1982 and volume shipments began in August. The C64 faced a wide range of competing home computers, but with a lower price and more flexible hardware, it quickly outsold many of its competitors.\nIn the United States the greatest competitors were the Atari 8-bit 400, the Atari 800, and the Apple\u00a0II. The Atari 400 and 800 had been designed to accommodate previously stringent FCC emissions requirements and so were expensive to manufacture. Though similar in specifications, the two computers represented differing design philosophies; as an open architecture system, upgrade capability for the Apple II was granted by internal expansion slots, whereas the C64's comparatively closed architecture had only a single external ROM cartridge port for bus expansion. However, the Apple II used its expansion slots for interfacing to common peripherals like disk drives, printers, and modems; the C64 had a variety of ports integrated into its motherboard which were used for these purposes, usually leaving the cartridge port free. Commodore's was not a completely closed system, however; the company had published detailed specifications for most of their models since the Commodore PET and VIC-20 days, and the C64 was no exception. Initial C64 sales were nonetheless relatively slow due to a lack of software, reliability issues with early production models, particularly high failure rates of the PLA chip, which used a new production process, and a shortage of 1541 disk drives, which also suffered rather severe reliability issues. During 1983, however, a trickle of software turned into a flood and sales began rapidly climbing, especially with price cuts from $600 to just $300 ($ to $ in ).\nCommodore sold the C64 not only through its network of authorized dealers, but also through department stores, discount stores, toy stores and college bookstores. The C64 had a built-in RF modulator and thus could be plugged into any television set. This allowed it (like its predecessor, the VIC-20) to compete directly against video game consoles such as the Atari 2600. Like the Apple IIe, the C64 could also output a composite video signal, avoiding the RF modulator altogether. This allowed the C64 to be plugged into a specialized monitor for a sharper picture. Unlike the IIe, the C64's NTSC output capability also included separate luminance/chroma signal output equivalent to (and electrically compatible with) S-Video, for connection to the Commodore 1702 monitor, providing even better video quality than a composite signal.\nAggressive pricing of the C64 is considered to have been a major catalyst in the video game crash of 1983. In January 1983, Commodore offered a $100 rebate in the United States on the purchase of a C64 to anyone that traded in another video game console or computer. To take advantage of this rebate, some mail-order dealers and retailers offered a Timex Sinclair 1000 (TS1000) for as little as $10 with purchase of a C64. This deal meant that the consumer could send the TS1000 to Commodore, collect the rebate, and pocket the difference; Timex Corporation departed the computer market within a year. Commodore's tactics soon led to a price war with the major home computer manufacturers. The success of the VIC-20 and C64 contributed significantly to the exit from the field of Texas Instruments and other smaller competitors.\nThe price war with Texas Instruments was seen as a personal battle for Commodore president Jack Tramiel. Commodore dropped the C64's list price by $200 within two months of its release. In June 1983 the company lowered the price to $300, and some stores sold the computer for $199. At one point, the company was selling as many C64s as all computers sold by the rest of the industry combined. Meanwhile, TI lost money by selling the 99/4A for $99. TI's subsequent demise in the home computer industry in October 1983 was seen as revenge for TI's tactics in the electronic calculator market in the mid-1970s, when Commodore was almost bankrupted by TI.\nAll four machines had similar memory configurations which were standard in 1982\u201383: 48\u00a0KB for the Apple II+ (upgraded within months of C64's release to 64\u00a0KB with the Apple IIe) and 48\u00a0KB for the Atari 800. At upwards of $1,200, the Apple\u00a0II was about twice as expensive, while the Atari 800 cost $899. One key to the C64's success was Commodore's aggressive marketing tactics, and they were quick to exploit the relative price/performance divisions between its competitors with a series of television commercials after the C64's launch in late 1982. The company also published detailed documentation to help developers, while Atari initially kept technical information secret.\nAlthough many early C64 games were inferior Atari 8-bit ports, by late 1983 the growing installed base caused developers to create new software with better graphics and sound. It was the only non-discontinued, widely available home computer by then, with more than 500,000 sold during the Christmas season; because of production problems in Atari's supply chain, by the start of 1984 \"the Commodore 64 largely has [the low-end] market to itself right now\", \"The Washington Post\" reported.\n1984\u20131987.\nWith sales booming and the early reliability issues with the hardware addressed, software for the C64 began to grow in size and ambition during 1984. This growth shifted to the primary focus of most US game developers. The two holdouts were Sierra, who largely skipped over the C64 in favor of Apple and PC compatible machines, and Broderbund, who were heavily invested in educational software and developed primarily around the Apple II. In the North American market, the disk format had become nearly universal while cassette and cartridge-based software all-but disappeared. So most US-developed games by this point grew large enough to require multi-loading.\nAt a mid-1984 conference of game developers and experts at Origins Game Fair, Dan Bunten, Sid Meier), and a representative of Avalon Hill said that they were developing games for the C64 first as the most promising market. By 1985, games were an estimated 60 to 70% of Commodore 64 software.\"Computer Gaming World\" stated in January 1985 that companies such as Epyx that survived the video game crash did so because they \"jumped on the Commodore bandwagon early.\" Over 35% of SSI's 1986 sales were for the C64, ten points higher than for the Apple\u00a0II. The C64 was even more important for other companies, which often found that more than half the sales for a title ported to six platforms came from the C64 version. That year, \"Computer Gaming World\" published a survey of ten game publishers that found that they planned to release forty-three Commodore 64 games that year, compared to nineteen for Atari and forty-eight for Apple\u00a0II, and Alan Miller stated that Accolade developed first for the C64 because \"it will sell the most on that system\".\nIn Europe, the primary competitors to the C64 were British-built computers: the Sinclair ZX Spectrum, the BBC Micro and the Amstrad CPC 464. In the UK, the 48K Spectrum had not only been released a few months ahead of the C64's early 1983 debut, but it was also selling for \u00a3175, less than half the C64's \u00a3399 price. The Spectrum quickly became the market leader and Commodore had an uphill struggle against it in the marketplace. The C64 did however go on to rival the Spectrum in popularity in the latter half of the 1980s. Adjusted to the size of population, the popularity of Commodore\u00a064 was the highest in Finland at roughly 3 units per 100 inhabitants, where it was subsequently marketed as \"the Computer of the Republic\".\nRumors spread in late 1983 that Commodore would discontinue the C64. By early 1985 the C64's price was $149; with an estimated production cost of $35\u201350, its profitability was still within the industry-standard markup of two to three times. Commodore sold about one million C64s in 1985 and a total of 3.5 million by mid-1986. Although the company reportedly attempted to discontinue the C64 more than once in favor of more expensive computers such as the Commodore 128, demand remained strong. In 1986, Commodore introduced the 64C, a redesigned 64, which \"Compute!\" saw as evidence that\u2014contrary to C64 owners' fears that the company would abandon them in favor of the Amiga and 128\u2014\"the 64 refuses to die.\" Its introduction also meant that Commodore raised the price of the C64 for the first time, which the magazine cited as the end of the home-computer price war. Software sales also remained strong; MicroProse, for example, in 1987 cited the Commodore and IBM PC markets as its top priorities.\n1988\u20131994.\nBy 1988 PC compatibles were the largest and fastest-growing home and entertainment software markets, displacing former leader Commodore. Commodore 64 software sales were almost unchanged in the third quarter of 1988 year over year while the overall market grew 42%, but the company was still selling 1 to 1.5 million units worldwide each year of what \"Computer Chronicles\" that year called \"the Model T of personal computers\". Epyx CEO David Shannon Morse cautioned that \"there are no new 64 buyers, or very few. It's a consistent group that's not growing... it's going to shrink as part of our business.\" One computer gaming executive stated that the Nintendo Entertainment System's enormous popularityseven million sold in 1988, almost as many as the number of C64s sold in its first five yearshad stopped the C64's growth. Trip Hawkins reinforced that sentiment, stating that Nintendo was \"the last hurrah of the 8-bit world.\"\nSSI exited the Commodore 64 market in 1991, after most competitors. \"Ultima VI\", released in 1991, was the last major C64 game release from a North American developer, and the Simpsons Arcade Game, published by Ultra Games, was the last arcade conversion. The latter was a somewhat uncommon example of a US-developed arcade port as after the early years of the C64, most arcade conversions were produced by UK developers and converted to NTSC and disk format for the US market, American developers instead focusing on more computer-centered game genres such as RPGs and simulations. In the European market, disk software was rarer and cassettes were the most common distribution method; this led to a higher prevalence of arcade titles and smaller, lower budget games that could fit entirely in the computer's memory without requiring multiloads. European programmers also tended to exploit advanced features of the C64's hardware more than their US counterparts.\nIn the United States, demand for 8- and 16-bit computers all but ceased as the 1990s began and PC compatibles completely dominated the computer market. However, the C64 continued to be popular in the UK and other European countries. The machine's eventual demise was not due to lack of demand or the cost of the C64 itself (still profitable at a retail price point between \u00a344 and \u00a350), but rather because of the cost of producing the disk drive. In March 1994, at CeBIT in Hanover, Germany, Commodore announced that the C64 would be finally discontinued in 1995, noting that the Commodore 1541 cost more than the C64 itself.\nHowever, only one month later in April 1994, the company filed for bankruptcy. Claims of sales of 17, 22 and 30 million of C64 units sold worldwide have been made. Company sales records, however, indicate that the total number was about 12.5 million. Based on that figure, the Commodore 64 was still the third most popular computing platform into the 21st century until the Raspberry Pi family replaced it. While 360,000 C64s were sold in 1982, about 1.3 million were sold in 1983, followed by a large spike in 1984 when 2.6 million were sold. After that, sales held steady at between 1.3 and 1.6 million a year for the remainder of the decade and then dropped off after 1989. North American sales peaked between 1983 and 1985 and gradually tapered off afterward, while European sales remained quite strong into the early 1990smuch to the embarrassment of Commodore officials who wished to rid themselves of the aging machine.\nThe computer's designers claimed that \"The freedom that allowed us to do the C-64 project will probably never exist again in that environment\"; by spring 1983 most had left to found Ensoniq.\nC64 family.\nCommodore MAX.\nIn 1982, Commodore released the Commodore MAX Machine in Japan. It was called the Ultimax in the United States and VC-10 in Germany. The MAX was intended to be a game console with limited computing capability and was based on a cut-down version of the hardware family later used in the C64. The MAX was discontinued months after its introduction because of poor sales in Japan.\nCommodore Educator 64.\n1983 saw Commodore attempt to compete with the Apple\u00a0II's hold on the US education market with the Educator 64, essentially a C64 and \"greenscale\" monochrome monitor in a PET case. Schools preferred the all-in-one metal construction of the PET over the standard C64's separate components, which could be easily damaged, vandalized, or stolen. Schools did not prefer the Educator 64 to the wide range of software and hardware options the Apple\u00a0IIe was able to offer, and it was produced in limited quantities.\nSX-64.\nAlso in 1983, Commodore released the SX-64, a portable version of the C64. The SX-64 has the distinction of being the first \"full-color\" portable computer. While earlier computers using this form factor only incorporate monochrome (\"green screen\") displays, the base SX-64 unit features a color cathode ray tube (CRT) and one integrated 1541 floppy disk drive. While, in the advertisements for the computer it claimed it would have dual 1541 drives, but when the SX-64 was released there was only one and the other became a floppy disk storage slot. Also, unlike most other C64s, the SX-64 does not have a datasette connector so an external cassette was not an option.\nCommodore C128.\nTwo designers at Commodore, Fred Bowen and Bil Herd, were determined to rectify the problems of the Plus/4. They intended that the eventual successors to the C64\u2014the Commodore 128 and 128D computers (1985)\u2014were to build upon the C64, avoiding the Plus/4's flaws. The successors had many improvements such as a BASIC with graphics and sound commands (like almost all home computers not made by Commodore ), 80-column display ability, and full CP/M compatibility. The decision to make the Commodore 128 plug compatible with the C64 was made quietly by Bowen and Herd, software and hardware designers respectively, without the knowledge or approval by the management in the post Jack Tramiel era. The designers were careful not to reveal their decision until the project was too far along to be challenged or changed and still make the impending Consumer Electronics Show (CES) in Las Vegas. Upon learning that the C128 was designed to be compatible with the C64, Commodore's marketing department independently announced that the C128 would be 100% compatible with the C64, thereby raising the bar for C64 support. In a case of malicious compliance, the 128 design was altered to include a separate \"64 mode\" using a complete C64 environment to try to ensure total compatibility.\nCommodore 64C.\nThe C64's designers intended the computer to have a new, wedge-shaped case within a year of release, but the change did not occur. In 1986, Commodore released the 64C computer, which is functionally identical to the original. The exterior design was remodeled in the sleeker style of the Commodore 128. The 64C uses new versions of the SID, VIC-II, and I/O chips being deployed. Models with the C64E board had the graphic symbols printed on the top of the keys, instead of the normal location on the front. The sound chip (SID) was changed to use the MOS 8580 chip, with the core voltage reduced from 12V to 9V. The most significant changes include different behavior in the filters and in the volume control, which result in some music/sound effects sounding differently than intended, and in digitally-sampled audio being almost inaudible, respectively (though both of these can mostly be corrected-for in software). The 64\u00a0KB RAM memory went from eight chips to two chips. BASIC and the KERNAL went from two separate chips into one 16\u00a0KB ROM chip. The PLA chip and some TTL chips were integrated into a DIL 64-pin chip. The \"252535-01\" PLA integrated the color RAM as well into the same chip. The smaller physical space made it impossible to put in some internal expansions like a floppy-speeder. In the United States, the 64C was often bundled with the third-party GEOS graphical user interface (GUI)-based operating system, as well as the software needed to access Quantum Link. The 1541 drive received a matching face-lift, resulting in the 1541C. Later, a smaller, sleeker 1541-II model was introduced, along with the 3.5-inch microfloppy 1581.\nCommodore 64 Games System.\nIn 1990, the C64 was repackaged in the form of a game console, called the C64 Games System (C64GS), with most external connectivity removed. A simple modification to the 64C's motherboard was made to allow cartridges to be inserted from above. A modified ROM replaced the BASIC interpreter with a boot screen to inform the user to insert a cartridge. Designed to compete with the Nintendo Entertainment System and the Sega Master System, it suffered from very low sales compared to its rivals. It was another commercial failure for Commodore, and it was never released outside Europe.\nCommodore 65.\nIn 1990, an advanced successor to the C64, the Commodore 65 (also known as the \"C64DX\"), was prototyped, but the project was canceled by Commodore's chairman Irving Gould in 1991. The C65's specifications were impressive for an 8-bit computer, bringing specs comparable to the 16-bit Apple\u00a0IIGS. For example, it could display 256 colors on the screen, while OCS based Amigas could only display 64 in HalfBrite mode (32 colors and half-bright transformations). Although no specific reason was given for the C65's cancellation, it would have competed in the marketplace with Commodore's lower end Amigas and the Commodore CDTV.\nSoftware.\nIn 1982, the C64's graphics and sound capabilities were rivaled only by the Atari 8-bit family and appeared exceptional when compared with the widely publicized Atari VCS and Apple\u00a0II. The C64 is often credited with starting the computer subculture known as the demoscene (see Commodore 64 demos). It is still being actively used in the demoscene, especially for music (its SID sound chip even being used in special sound cards for PCs, and the Elektron SidStation synthesizer). Even though other computers quickly caught up with it, the C64 remained a strong competitor to the later video game consoles Nintendo Entertainment System (NES) and Sega Master System, thanks in part to its by-then established software base, especially outside North America, where it comprehensively outsold the NES.\nBecause of lower incomes and the domination of the Sinclair Spectrum in the UK, almost all British C64 software used cassette tapes. Few cassette C64 programs were released in the US after 1983 and, in North America, the diskette was the principal method of software distribution. The cartridge slot on the C64 was also mainly a feature used in the computer's first two years on the market and became rapidly obsolete once the price and reliability of 1541 drives improved. A handful of PAL region games used bank switched cartridges to get around the 16\u00a0KB memory limit.\nBASIC.\nAs is common for home computers of the early 1980s, the C64 comes with a BASIC interpreter, in ROM. KERNAL, I/O, and tape/disk drive operations are accessed via custom BASIC language commands. The disk drive has its own interfacing microprocessor and ROM (firmware) I/O routines, much like the earlier CBM/PET systems and the Atari 400 and Atari 800. This means that no memory space is dedicated to running a disk operating system, as was the case with earlier systems such as the Apple\u00a0II and TRS-80.\nCommodore BASIC 2.0 is used instead of the more advanced BASIC\u00a04.0 from the PET series, since C64 users were not expected to need the disk-oriented enhancements of BASIC\u00a04.0. The company did not expect many to buy a disk drive, and using BASIC\u00a02.0 simplified VIC-20 owners' transition to the 64. \"The choice of BASIC\u00a02.0 instead of 4.0 was made with some soul-searching, not just at random. The typical user of a C64 is not expected to need the direct disk commands as much as other extensions, and the amount of memory to be committed to BASIC were to be limited. We chose to leave expansion space for color and sound extensions instead of the disk features. As a result, you will have to handle the disk in the more cumbersome manner of the 'old days'.\"\nThe version of Microsoft BASIC is not very comprehensive and does not include specific commands for sound or graphics manipulation, instead requiring users to use the \"PEEK and POKE\" commands to access the graphics and sound chip registers directly. To provide extended commands, including graphics and sound, Commodore produced two different cartridge-based extensions to BASIC\u00a02.0: Simons' BASIC and Super Expander 64. Other languages available for the C64 include Pascal, C, Logo, Forth, and FORTRAN. Compilers for BASIC\u00a02.0 such as Petspeed\u00a02 (from Commodore), Blitz (from Jason Ranheim), and Turbo Lightning (from Ocean Software) were produced. Most commercial C64 software was written in assembly language, either cross developed on a larger computer, or directly on the C64 using a machine code monitor or an assembler. This maximized speed and minimized memory use. Some games, particularly adventures, used high level scripting languages and sometimes mixed BASIC and machine language.\nAlternative operating systems.\nMany third party operating systems have been developed for the C64. As well as the original GEOS, two third-party GEOS-compatible systems have been written: Wheels and GEOS megapatch. Both of these require hardware upgrades to the original C64. Several other operating systems are or have been available, including WiNGS OS, the Unix-like LUnix, operated from a command-line, and the embedded systems OS Contiki, with full GUI. Other less well-known OSes include ACE, Asterix, DOS/65, and GeckOS. A version of CP/M was released, but this requires the addition of an external Z80 processor to the expansion bus. Furthermore, the Z80 processor is underclocked to be compatible with the C64's memory bus, so performance is poor compared to other CP/M implementations. C64 CP/M and C128 CP/M both suffer a lack of software; although most commercial CP/M software can run on these systems, software media is incompatible between platforms. The low usage of CP/M on Commodores means that software houses saw no need to invest in mastering versions for the Commodore disk format. The C64 CP/M cartridge is also not compatible with anything except the early 326298 motherboards.\nNetworking software.\nDuring the 1980s, the Commodore 64 was used to run bulletin board systems using software packages such as Punter BBS, Bizarre 64, Blue Board, C-Net, Color 64, CMBBS, C-Base, DMBBS, Image BBS, EBBS, and The Deadlock Deluxe BBS Construction Kit, often with sysop-made modifications. These boards sometimes were used to distribute cracked software. As late as December 2013, there were 25 such Bulletin Board Systems in operation, reachable via the Telnet protocol. There were major commercial online services, such as Compunet (UK), CompuServe (US later bought by America Online), The Source (US), and Minitel (France) among many others. These services usually required custom software which was often bundled with a modem and included free online time as they were billed by the minute. Quantum Link (or Q-Link) was a US and Canadian online service for Commodore 64 and 128 personal computers that operated from November 5, 1985, to November 1, 1994. It was operated by Quantum Computer Services of Vienna, Virginia, which in October 1991 changed its name to America Online and continued to operate its AOL service for the IBM PC compatible and Apple Macintosh. Q-Link was a modified version of the PlayNET system, which Control Video Corporation (CVC, later renamed Quantum Computer Services) licensed.\nOnline gaming.\nThe first graphical character-based interactive environment is \"Club Caribe\". First released as \"Habitat\" in 1988, \"Club Caribe\" was introduced by LucasArts for Q-Link customers on their Commodore 64 computers. Users could interact with one another, chat and exchange items. Although the game's open world was very basic, its use of online avatars (already well-established off-line by \"Ultima\" and other games) and the combination of chat and graphics was revolutionary. Online graphics in the late 1980s were severely restricted by the need to support modem data transfer rates as low as 300 bits per second. Habitat's graphics were stored locally on floppy disk, eliminating the need for network transfer.\nHardware.\nCPU and memory.\nThe C64 uses an 8-bit MOS Technology 6510 microprocessor. It's almost identical to the 6502 but with three state buses, a different pinout, slightly different clock signals and other minor changes for this specific application. It also has six I/O lines on otherwise unused legs on the 40-pin IC package. These are used for two purposes in the C64: to bank-switch the machine's read-only memory (ROM) in and out of the processor's address space and to operate the datasette tape recorder. The C64 has of 8-bit-wide dynamic RAM, of 4-bit-wide static color RAM for text mode and are available to built-in Commodore BASIC 2.0 on startup. There is of ROM, made up of the BASIC interpreter, the KERNAL, and the character ROM. As the processor could only address at a time, the ROM was mapped into memory, and only of RAM (plus 4\u00a0KB in between the ROMs) were available at startup. Most \"breadbox\" Commodore 64s used 4164 DRAM, with eight chips to total up 64K of system RAM. Late breadbox models and all C64Cs used 41464 DRAM (64K\u00d74) chips which stored 32\u00a0KB per chip, so only two were required. Since 4164 DRAMs are 64K\u00d71, eight chips are needed to make an entire byte, and the computer will not function without all of them present. Thus, the first chip contains Bit 0 for the entire memory space, the second chip contains Bit 1, and so forth. This also makes detecting faulty RAM easy, as a bad chip will display random characters on the screen and the character displayed can be used to determine the faulty RAM.\nThe C64 performs a RAM test on power up and if a RAM error is detected, the amount of free BASIC memory will be lower than the normal 38911 figure. If the faulty chip is in lower memory, then an codice_2 error is displayed rather than the usual BASIC startup banner. The color RAM at $D800 uses a separate 2114 SRAM chip and is gated directly to the VIC-II.\nThe C64 uses a somewhat complicated memory banking scheme; the normal power-on default is to have the BASIC ROM mapped in at $A000-$BFFF and the screen editor/KERNAL\u00a0ROM at $E000-$FFFF. RAM underneath the system ROMs can be written to, but not read back without swapping out the ROMs. Memory location $01 contains a register with control bits for enabling/disabling the system ROMS as well as the I/O area at $D000. If the KERNAL\u00a0ROM is swapped out, BASIC will be removed at the same time it, and it is not possible to have BASIC active without the KERNAL (as BASIC often calls KERNAL routines and part of the ROM code for BASIC is in fact located in the KERNAL ROM, this makes sense).\nThe character ROM is normally not visible to the CPU. It has two mirrors at $1000 and $9000, but only the VIC-II can see them, the CPU will see RAM in those locations. The character ROM may be mapped into $D000-$DFFF where it is then visible to the CPU. Since doing so necessitates swapping out the I/O registers, interrupts must be disabled first. Graphics memory and data cannot be placed at $1000 or $9000 as the VIC-II will see the character ROM there instead.\nBy removing I/O from the memory map, $D000-$DFFF becomes free RAM. The color RAM at $D800 is swapped out along with the I/O registers and this area can be used for static graphics data such as character sets since the VIC-II cannot see the I/O registers (or color RAM via the CPU mapping). If all ROMs and the I/O area are swapped out, the entire 64k RAM space is available aside for locations $0/$1.\n$C000-$CFFF is free RAM and not used by BASIC or KERNAL routines; because of this, it is an ideal location to store short machine language programs that can be accessed from BASIC. The cassette buffer at $0334-$03FF can also be used to store short machine language routines provided that a Datasette is not used, which will overwrite the buffer.\nC64 cartridges map into assigned ranges in the CPU's address space and the most common cartridge auto starting requires the presence of a special string at $8000 which contains \"CBM80\" followed by the address where program execution begins. A few early C64 cartridges released in 1982 use Ultimax mode (or MAX mode), a leftover feature of the failed MAX Machine. These cartridges map into $F000 and displace the KERNAL\u00a0ROM. If Ultimax mode is used, the programmer will have to provide code for handling system interrupts. The cartridge port has 14 address lines, which allows 16\u00a0KB of ROM to be accessed. Disk and tape software normally load at the start of BASIC memory ($0801) and use a small BASIC stub (e.g., 10 SYS(2064)) to jump to the start of the program. Although no Commodore 8-bit machine except the C128 can automatically boot from a floppy disk, some software intentionally overwrites certain BASIC vectors in the process of loading so that execution begins automatically rather than requiring the user to type RUN at the BASIC prompt following loading.\nAround 300 cartridges were released for the C64, mostly in the machine's first 2\u00bd years on the market, after which most software outgrew the 16\u00a0KB cartridge limit. In the final years of the C64, larger software companies such as Ocean Software began releasing games on bank-switched cartridges to overcome this 16\u00a0KB cartridge limit.\nCommodore did not include a reset button on any of their computers until the CBM-II line, but there were third-party cartridges with a reset button on them. It is possible to trigger a soft reset by jumping to the CPU reset routine at $FCE2 (64738). A few programs use this as an \"exit\" feature, although it does not clear memory.\nThe KERNAL\u00a0ROM went through three separate revisions, mostly designed to fix bugs. The initial version is only found on 326298 motherboards, used in the first production models, and cannot detect if an NTSC or PAL VIC-II is present. The second revision is found on all C64s made from late 1982 through 1985. The third and last KERNAL\u00a0ROM revision was introduced on the 250466 motherboard (late breadbin models with 41464 RAM) and is found in all C64Cs. The 6510 CPU is clocked at (NTSC) and (PAL), lower than some competing systems (for example, the Atari 800 is clocked at ). A small performance boost can be gained by disabling the VIC-II's video output via a register write. This feature is often used by tape and disk fastloaders as well as the KERNAL cassette routine to keep a standard CPU cycle timing not modified by the VIC-II's sharing of the bus.\nThe Restore key is gated directly to the CPU's NMI line and will generate an NMI if pressed. The KERNAL handler for the NMI checks if Run/Stop is also pressed, if not, it ignores the NMI and simply exits back out. Run/Stop-Restore normally functions as a soft reset in BASIC that restores all I/O registers to their power on default state, but does not clear memory or reset pointers, so any BASIC programs in memory will be left untouched. Machine language software usually disables Run/Stop-Restore by remapping the NMI vector to a dummy RTI instruction. The NMI can be used for an extra interrupt thread by programs as well, but runs the risk of a system lockup or undesirable side effects if the Restore key is accidentally pressed, as this will trigger an inadvertent activation of the NMI thread.\nJoysticks, mice, and paddles.\nThe C64 retained the DE-9 joystick Atari joystick port from the VIC-20 and added another; any Atari specification game controller can be used on a C64. The joysticks are read from the registers at $DC00 and $DC01, and most software is designed to use a joystick in port 2 for control rather than port 1, as the upper bits of $DC00 are used by the keyboard and an I/O conflict can result. Although it is possible to use Sega game pads on a C64, it is not recommended as the slightly different signal generated by them can damage the CIA chip. The SID chip's register $D419 is used to control paddles and is an analog input. Atari paddles are electrically compatible with the C64, but have different resistance values than Commodore's paddles, which means most software will not work properly with them. However, only a handful of games, mostly ones released early in the computer's life cycle, can use paddles. In 1986, Commodore released two mice for the C64 and C128, the 1350 and 1351. The 1350 is a digital device, read from the joystick registers (and can be used with any program supporting joystick input); while the 1351 is a true, analog potentiometer based, mouse, read with the SID's analog-to-digital converter.\nGraphics.\nThe graphics chip, VIC-II, features 16 colors, eight hardware sprites per scanline (enabling up to 112 sprites per PAL screen), scrolling capabilities, and two bitmap graphics modes.\nText Modes.\nThe standard text mode features 40 columns, like most Commodore PET models; the built-in character encoding is not standard ASCII but PETSCII, an extended form of ASCII-1963. The KERNAL\u00a0ROM sets the VIC-II to a dark blue background on power up with a light blue text and border. Unlike the PET and VIC-20, the C64 uses \"fat\" double-width text as some early VIC-IIs had poor video quality that resulted in a fuzzy picture. Most screenshots show borders around the screen, which is a feature of the VIC-II chip. By utilizing interrupts to reset various hardware registers on precise timings it was possible to place graphics within the borders and thus use the full screen.\nThe C64 has a resolution of 320\u00d7200 pixels, consisting of a 40\u00d725 grid of 8\u00d78 character blocks. The C64 has 255 predefined character blocks, called PETSCII. The character set can be copied into RAM and altered by a programmer.\nThere are two colour modes, high resolution, with two colours available per character block (one foreground and one background) and multicolour with four colours per character block (three foreground and one background). In multicolour mode, attributes are shared between pixel pairs, so the effective visible resolution is 160\u00d7200 pixels. This is necessary since only 16\u00a0KB of memory is available for the VIC-II video processor.\nAs the C64 has a bitmapped screen, it is possible to draw each pixel individually. This is, however, \"very\" slow. Most programmers used techniques developed for earlier non-bitmapped systems, like the Commodore Pet and TRS-80. A programmer redraws the character set and the video processor fills the screen block by block from the top left corner to the bottom right corner.\nTwo different types of animation are used: Character block animation and hardware sprites.\nCharacter block animation.\nThe user draws a series of characters of a man walking say, two in the middle of the block, and another two walking in and out of the block. Then the user sequences them so the character walks into the block and out again. Drawing a series of these and the user gets a man walking across the screen. By timing the redraw to occur when the television screen blanks out to restart drawing the screen there will be no flicker. For this to happen, the user programs the VIC-II that it generates a raster interrupt when the video flyback occurs. This is the technique used in the classic \"Space Invaders\" arcade game.\nHorizontal and vertical pixelwise scrolling of up to one character block is supported by two hardware scroll registers. Depending on timing, hardware scrolling affects the entire screen or just selected lines of character blocks. On a non-emulated C64, scrolling is glasslike and blur-free.\nHardware sprites.\nA sprite is a movable character which moves over an area of the screen, draws over the background and then redraws it after it moves. Note this is very different from character block animation, where the user is just flipping character blocks. On the C64, the VIC-II video processor handles most of the legwork in sprite emulation, the programmer simply defines the sprite and where they want it to go.\nThe C64 has two types of sprites, respecting their colour mode limitations. Hires sprites have one colour (one background and one foreground) and multicolour sprites three (one background and three foreground). Colour modes can be split or windowed on a single screen. Sprites can be doubled in size vertically and horizontally up to four times their size, but the pixel attributes are the same - the pixels become \"fatter\". There can be 8 sprites in total and 8 in a horizontal line. Sprites can move with glassy smoothness in front of and behind screen characters and other sprites.\nSprite-sprite and sprite-background collisions are detected in hardware and the VIC-II can be programmed to trigger an interrupt accordingly.\nSound.\nThe SID chip has three channels, each with its own ADSR envelope generator and filter capabilities. Ring modulation makes use of channel N\u00b03, to work with the other two channels. Bob Yannes developed the SID chip and later co-founded synthesizer company Ensoniq. Yannes criticized other contemporary computer sound chips as \"primitive, obviously...designed by people who knew nothing about music\". Often the game music has become a hit of its own among C64 users. Well-known composers and programmers of game music on the C64 are Rob Hubbard, Jeroen Tel, Tim Follin, David Whittaker, Chris H\u00fclsbeck, Ben Daglish, Martin Galway, Kjell Nordb\u00f8 and David Dunn among many others. Due to the chip's three channels, chords are often played as arpeggios, coining the C64's characteristic lively sound. It was also possible to continuously update the master volume with sampled data to enable the playback of 4-bit digitized audio. As of 2008, it became possible to play four channel 8-bit audio samples, 2 SID channels and still use filtering.\nThere are two versions of the SID chip: the 6581 and the 8580. The MOS Technology 6581 was used in the original (\"breadbox\") C64s, the early versions of the 64C, and the Commodore 128. The 6581 was replaced with the MOS Technology 8580 in 1987. While the 6581 sound quality is a little crisper and many Commodore 64 fans say they prefer its sound, it lacks some versatility available in the 8580 \u2013 for example, the 8580 can mix all available waveforms on each channel, whereas the 6581 can only mix waveforms in a channel in a much more limited fashion. The main difference between the 6581 and the 8580 is the supply voltage. The 6581 uses a supply\u2014the 8580, a supply. A modification can be made to use the 6581 in a newer 64C board (which uses the chip). The SID chip's distinctive sound has allowed it to retain a following long after its host computer was discontinued. A number of audio enthusiasts and companies have designed SID-based products as add-ons for the C64, x86 PCs, and standalone or Musical Instrument Digital Interface (MIDI) music devices such as the Elektron SidStation. These devices use chips taken from excess stock, or removed from used computers. In 2007, Timbaland's extensive use of the SidStation led to the plagiarism controversy for \"Block Party\" and \"Do It\" (written for Nelly Furtado).\nHardware revisions.\nCommodore made many changes to the C64's hardware during its lifetime, sometimes causing compatibility issues. The computer's rapid development, and Commodore and Tramiel's focus on cost cutting instead of product testing, resulted in several defects that caused developers like Epyx to complain and required many revisions to fix; Charpentier said that \"not coming a little close to quality\" was one of the company's mistakes.\nCost reduction was the reason for most of the revisions. Reducing manufacturing costs was vitally important to Commodore's survival during the price war and leaner years of the 16-bit era. The C64's original (NMOS based) motherboard went through two major redesigns, (and numerous sub-revisions) exchanging positions of the VIC-II, SID and PLA chips. Initially, a large portion of the cost was eliminated by reducing the number of discrete components, such as diodes and resistors, which enabled the use of a smaller printed circuit board. There were 16 total C64 motherboard revisions, aimed at simplifying and reducing manufacturing costs. Some board revisions were exclusive to PAL regions. All C64 motherboards were manufactured in Hong Kong.\nIC locations changed frequently on each motherboard revision, as did the presence or lack thereof of the metal RF shield around the VIC-II. PAL boards often had aluminized cardboard instead of a metal shield. The SID and VIC-II are socketed on all boards; however, the other ICs may be either socketed or soldered. The first production C64s, made in 1982 to early 1983, are known as \"silver label\" models due to the case sporting a silver-colored \"Commodore\" logo. The power LED had a separate silver badge around it reading \"64\". These machines also have only a 5-pin video cable and cannot output S-video. In late 1982, Commodore introduced the familiar \"rainbow badge\" case, but many machines produced into early 1983 also used silver label cases until the existing stock of them was used up. In the spring of 1983, the original 326298 board was replaced by the 250407 motherboard which sported an 8-pin video connector and added S-video support for the first time. This case design was used until the C64C appeared in 1986. All ICs switched to using plastic shells while the silver label C64s had some ceramic ICs, notably the VIC-II. The case is made from ABS plastic which may become brown with time. This can be reversed by using the public domain chemical mix \"Retr0bright\".\nICs.\nThe VIC-II was manufactured with 5 micrometer NMOS technology and was clocked at either (PAL) or (NTSC). Internally, the clock was divided down to generate the dot clock (about 8\u00a0MHz) and the two-phase system clocks (about 1\u00a0MHz; the exact pixel and system clock speeds are slightly different between NTSC and PAL machines). At such high clock rates, the chip generated a lot of heat, forcing MOS Technology to use a ceramic dual in-line package called a \"CERDIP\". The ceramic package was more expensive, but it dissipated heat more effectively than plastic.\nAfter a redesign in 1983, the VIC-II was encased in a plastic dual in-line package, which reduced costs substantially, but it did not totally eliminate the heat problem. Without a ceramic package, the VIC-II required the use of a heat sink. To avoid extra cost, the metal RF shielding doubled as the heat sink for the VIC, although not all units shipped with this type of shielding. Most C64s in Europe shipped with a cardboard RF shield, coated with a layer of metal foil. The effectiveness of the cardboard was highly questionable and, worse still, it acted as an insulator, blocking airflow which trapped heat generated by the SID, VIC, and PLA chips. The SID was originally manufactured using NMOS at 7 micrometers and in some areas 6 micrometers. The prototype SID and some very early production models featured a ceramic dual in-line package, but unlike the VIC-II, these are extremely rare as the SID was encased in plastic when production started in early 1982.\nMotherboard.\nIn 1986, Commodore released the last revision to the classic C64 motherboard. It was otherwise identical to the 1984 design, except for the two 64 kilobit \u00d7 4 bit DRAM chips that replaced the original eight 64 kilobit \u00d7 1\u00a0bit ICs. After the release of the Commodore 64C, MOS Technology began to reconfigure the original C64's chipset to use HMOS production technology. The main benefit of using HMOS was that it required less voltage to drive the IC, which consequently generates less heat. This enhanced the overall reliability of the SID and VIC-II. The new chipset was renumbered to 85xx to reflect the change to HMOS.\nIn 1987, Commodore released a 64C variant with a highly redesigned motherboard commonly known as a \"short board\". The new board used the new HMOS chipset, featuring a new 64-pin PLA chip. The new \"SuperPLA\", as it was dubbed, integrated many discrete components and transistor\u2013transistor logic (TTL) chips. In the last revision of the 64C motherboard, the 2114 4-bit-wide color RAM was integrated into the SuperPLA.\nPower supply.\nThe C64 used an external power supply, a conventional transformer with multiple tappings (as opposed to switch mode, the type now used on PC power supplies). It was encased in an epoxy resin gel, which discouraged tampering but tended to increase the heat level during use. The design saved space within the computer's case and allowed international versions to be more easily manufactured. The 1541-II and 1581 disk drives, along with various third-party clones, also come with their own external power supply \"bricks\", as did most peripherals leading to a \"spaghetti\" of cables and the use of numerous double adapters by users.\nCommodore power supplies often failed before expected. The computer reportedly had a 30% return rate in late 1983, compared to the 5-7% the industry considered acceptable. \"Creative Computing\" reported four working computers out of seven C64s. Malfunctioning power bricks were particularly notorious for damaging the RAM chips. Due to their higher density and single supply (+5V), they had less tolerance for an overvoltage condition.\nThe original PSU included on early 1982-83 machines had a 5-pin connector that could accidentally be plugged into the video output of the computer. To prevent the user from making this damaging mistake, Commodore changed the plug design on 250407 motherboards to a 3-pin connector in 1984. Commodore later changed the design yet again, omitting the resin gel in order to reduce costs. The follow-on model, the Commodore 128, used a larger, improved power supply that included a fuse. The power supply that came with the Commodore REU was similar to that of the Commodore 128's unit, providing an upgrade for customers who purchased that accessory.\nSpecifications.\nInternal hardware.\nCreative Micro Designs also produced a 2\u00a0MB REU for the C64 and C128, called the 1750\u00a0XL. The technology actually supported up to 16\u00a0MB, but 2\u00a0MB was the biggest one officially made. Expansions of up to 16\u00a0MB were also possible via the CMD SuperCPU.\nInput/output (I/O) ports and power supply.\nThe is used to supply power via a charge pump to the SID sound generator chip, provide via a rectifier to the cassette motor, a \"0\" pulse for every positive half wave to the time-of-day (TOD) input on the CIA chips, and directly to the user-port. Thus, as a minimum, a square wave is required. But a sine wave is preferred.\nMemory map.\nNote that even if an I/O chip like the VIC-II only uses 64 positions in the memory address space, it will occupy 1,024 addresses because some address bits are left undecoded.\nManufacturing cost.\nVertical integration was the key to keeping Commodore 64 production costs low. At the introduction in 1982, the production cost was US$135 and the retail price US$595. In 1985, the retail price went down to US$149 (US$ today) and the production costs were believed to be somewhere between US$35\u201350 ( Commodore would not confirm this cost figure. Dougherty of the Berkeley Softworks estimated the costs of the Commodore 64 parts based on his experience at Mattel and Imagic.\nTo lower costs TTL chips were replaced with less expensive custom chips and ways to increase the yields on the sound and graphics chips were found. The video chip 6567 had the ceramic package replaced with plastic but heat dissipation demanded a redesign of the chip and the development of a plastic package that can dissipate heat as well as ceramic.\nClones.\nClones are computers that imitate C64 functions. In the middle of 2004, after an absence from the marketplace of more than 10 years, PC manufacturer Tulip Computers BV (owners of the Commodore brand since 1997) announced the C64 Direct-to-TV (C64DTV), a joystick-based TV game based on the C64 with 30 video games built into ROM. Designed by Jeri Ellsworth, a self-taught computer designer who had earlier designed the modern C-One C64 implementation, the C64DTV was similar in concept to other mini-consoles based on the Atari 2600 and Intellivision, which had gained modest success earlier in the decade. The product was advertised on QVC in the United States for the 2004 holiday season. By \"hacking\" the circuit board, it is possible to attach C1541 floppy disk drives, a second joystick, and PS/2 keyboards to these units, which gives the DTV devices nearly all the capabilities of a full Commodore 64. The DTV hardware is also used in the mini-console \"Hummer\", sold at RadioShack in mid-2005.\nIn 2015, a Commodore 64 compatible motherboard was produced by Individual Computers. Dubbed the \"C64 Reloaded\", it is a modern redesign of the Commodore 64 motherboard revision 250466 with a few new features. The motherboard itself is designed to be placed in an empty C64 or C64C case already owned by the user. Produced in limited quantities, models of this Commodore 64 \"clone\" sport either machined or ZIF sockets in which the custom C64 chips would be placed. The board also contains jumpers to accept different revisions of the VIC-II and SID chips, as well as the ability to jumper between the analogue video system modes PAL and NTSC. The motherboard contains several innovations, including selection via the RESTORE key of multiple KERNAL and character ROMs, built-in reset toggle on the power switch, and an S-video socket to replace the original TV modulator. The motherboard is powered by a DC-to-DC converter that uses a single power input of from a mains adapter to power the unit rather than the original and failure prone Commodore 64 power supply brick.\nNewer compatible hardware.\nAs of 2008, C64 enthusiasts still develop new hardware, including Ethernet cards, specially adapted hard disks and flash card interfaces (sd2iec).\nBrand reuse.\nIn 1998, the C64 brand was reused for the \"Web.it Internet Computer\", a low-powered (even for the time) Internet-oriented, all-in-one x86 PC running Windows 3.1. Despite its \"Commodore 64\" nameplate, the \"C64 Web.it\" is not directly compatible with the original (except via included emulation software), nor does it share its appearance. PC clones branded as C64x sold by Commodore USA, LLC, a company licensing the Commodore trademark, began shipping in June 2011. The C64x has a case resembling the original C64 computer, but- as with the \"Web.it\"- it is based on x86 architecture and is not compatible with the Commodore 64 on either hardware or software levels.\nVirtual Console.\nSeveral Commodore 64 games were released on the Nintendo Wii's Virtual Console service in Europe and North America only. The games were unlisted from the service as of August 2013 for unknown reasons.\nTHEC64 and THEC64 Mini.\nTHEC64 Mini is an unofficial Linux-based console that emulates the Commodore 64, released in 2018 by UK-based Retro Games. The console takes the form of a decorative half-scale Commodore 64 with two USB and one HDMI port, plus a mini USB connection to power the system. The console's decorative keyboard is non-functional \u2013 the system is controlled via the included THEC64 joystick, or a separate USB keyboard. It is possible to load new software ROMs into the console, which uses emulator x64 (as part of VICE) to run software, and has a built-in graphical operating system.\nThe full-size THEC64 was released in 2019 in Europe and Australia, and was scheduled for release in November 2020 in the North American market. The console and built-in keyboard are built to scale with the original Commodore 64, including a functional keyboard. Enhancements include VIC-20 emulation, four USB ports, and an upgraded joystick.\nNeither product features any of Commodore's trademarks \u2013 the Commodore key on the original keyboard is replaced with a THEC64 key, and Retro Games can call neither product a \"C64\" \u2013 although the system ROMs are licensed from Cloanto Corporation. The consoles can be switched between \"carousel mode\" for accessing the built-in game library, and \"classic mode\" in which the machine operates similarly to a traditional Commodore 64. USB storage can be used to hold disk, cartridge and tape images for use with the machine.\nEmulators.\nCommodore 64 emulators include the open source VICE, Hoxs64, and CCS64. An iPhone app was also released with a compilation of C64 ports."}
{"id": "7294", "revid": "8183", "url": "https://en.wikipedia.org/wiki?curid=7294", "title": "Cartography", "text": "Cartography (; from Greek \u03c7\u03ac\u03c1\u03c4\u03b7\u03c2 \"chart\u0113s\", \"papyrus, sheet of paper, map\"; and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd \"graphein\", \"write\") is the study and practice of making and using maps. Combining science, aesthetics, and technique, cartography builds on the premise that reality (or an imagined reality) can be modeled in ways that communicate spatial information effectively.\nThe fundamental objectives of traditional cartography are to:\nModern cartography constitutes many theoretical and practical foundations of geographic information systems and geographic information science.\nHistory.\nAncient times.\nWhat is the earliest known map is a matter of some debate, both because the term \"map\" is not well-defined and because some artifacts that might be maps might actually be something else. A wall painting that might depict the ancient Anatolian city of \u00c7atalh\u00f6y\u00fck (previously known as Catal Huyuk or \u00c7atal H\u00fcy\u00fck) has been dated to the late 7th millennium BCE. Among the prehistoric alpine rock carvings of Mount Bego (France) and Valcamonica (Italy), dated to the 4th millennium BCE, geometric patterns consisting of dotted rectangles and lines are widely interpreted in archaeological literature as a depiction of cultivated plots. Other known maps of the ancient world include the Minoan \"House of the Admiral\" wall painting from c. 1600 BCE, showing a seaside community in an oblique perspective, and an engraved map of the holy Babylonian city of Nippur, from the Kassite period (14th12th centuries BCE). The oldest surviving world maps are from 9th century BCE Babylonia. One shows Babylon on the Euphrates, surrounded by Assyria, Urartu and several cities, all, in turn, surrounded by a \"bitter river\" (Oceanus). Another depicts Babylon as being north of the center of the world.\nThe ancient Greeks and Romans created maps from the time of Anaximander in the 6th century BCE. In the 2nd century CE, Ptolemy wrote his treatise on cartography, Geographia. This contained Ptolemy's world map \u2013 the world then known to Western society \"(Ecumene)\". As early as the 8th century, Arab scholars were translating the works of the Greek geographers into Arabic.\nIn ancient China, geographical literature dates to the 5th century BCE. The oldest extant Chinese maps come from the State of Qin, dated back to the 4th century BCE, during the Warring States period. In the book of the \"Xin Yi Xiang Fa Yao\", published in 1092 by the Chinese scientist Su Song, a star map on the equidistant cylindrical projection. Although this method of charting seems to have existed in China even before this publication and scientist, the greatest significance of the star maps by Su Song is that they represent the oldest existent star maps in printed form.\nEarly forms of cartography of India included depictions of the pole star and surrounding constellations. These charts may have been used for navigation.\nMiddle Ages and Renaissance.\n (\"maps of the world\") are the medieval European maps of the world. About 1,100 of these are known to have survived: of these, some 900 are found illustrating manuscripts and the remainder exist as stand-alone documents.\nThe Arab geographer Muhammad al-Idrisi produced his medieval atlas \"Tabula Rogeriana (Book of Roger)\" in 1154. By combining the knowledge of Africa, the Indian Ocean, Europe, and the Far East (which he learned through contemporary accounts from Arab merchants and explorers) with the information he inherited from the classical geographers, he was able to write detailed descriptions of a multitude of countries. Along with the substantial text he had written, he created a world map influenced mostly by the Ptolemaic conception of the world, but with significant influence from multiple Arab geographers. It remained the most accurate world map for the next three centuries. The map was divided into seven climatic zones, with detailed descriptions of each zone. As part of this work, a smaller, circular map was made depicting the south on top and Arabia in the center. Al-Idrisi also made an estimate of the circumference of the world, accurate to within 10%.\nIn the Age of Exploration, from the 15th century to the 17th century, European cartographers both copied earlier maps (some of which had been passed down for centuries) and drew their own, based on explorers' observations and new surveying techniques. The invention of the magnetic compass, telescope and sextant enabled increasing accuracy. In 1492, Martin Behaim, a German cartographer, made the oldest extant globe of the Earth.\nIn 1507, Martin Waldseem\u00fcller produced a globular world map and a large 12-panel world wall map (\"Universalis Cosmographia\") bearing the first use of the name \"America\". Portuguese cartographer Diego Ribero was the author of the first known planisphere with a graduated Equator (1527). Italian cartographer Battista Agnese produced at least 71 manuscript atlases of sea charts. Johannes Werner refined and promoted the Werner projection. This was an equal-area, heart-shaped world map projection (generally called a cordiform projection) which was used in the 16th and 17th centuries. Over time, other iterations of this map type arose; most notable are the sinusoidal projection and the Bonne projection. The Werner projection places its standard parallel at the North Pole; a sinusoidal projection places its standard parallel at the equator; and the Bonne projection is intermediate between the two.\nIn 1569, mapmaker Gerardus Mercator first published a map based on his Mercator projection, which uses equally-spaced parallel vertical lines of longitude and parallel latitude lines spaced farther apart as they get farther away from the equator. By this construction, courses of constant bearing are conveniently represented as straight lines for navigation. The same property limits its value as a general-purpose world map because regions are shown as increasingly larger than they actually are the further from the equator they are. Mercator is also credited as the first to use the word \"atlas\" to describe a collection of maps. In the later years of his life, Mercator resolved to create his Atlas, a book filled with many maps of different regions of the world, as well as a chronological history of the world from the Earth's creation by God until 1568. He was unable to complete it to his satisfaction before he died. Still, some additions were made to the Atlas after his death and new editions were published after his death.\nIn the Renaissance, maps were used to impress viewers and establish the owner's reputation as sophisticated, educated, and worldly. Because of this,\u00a0 towards the end of the Renaissance, maps were displayed with equal importance of painting, sculptures, and other pieces of art. In the sixteenth century, maps were becoming increasingly available to consumers through the introduction of printmaking, with about 10% of Venetian homes having some sort of map by the late 1500s.\nThere were three main functions of maps in the Renaissance:\nIn medieval times, written directions of how to get somewhere were more common than the use of maps. With the Renaissance, cartography began to be seen as a metaphor for power. Political leaders could lay claim on territories through the use of maps and this was greatly aided by the religious and colonial expansion of Europe. The most commonly mapped places during the Renaissance were the Holy Land and other religious places.\nIn the late 1400s to the late 1500s, Rome, Florence, and Venice dominated map making and trade. It started in Florence in the mid to late 1400s. Map trade quickly shifted to Rome and Venice but then was overtaken by atlas makers in the late 16th century. Map publishing in Venice was completed with humanities and book publishing in mind, rather than just informational use.\nPrinting technology.\nThere were two main printmaking technologies in the Renaissance: woodcut and copper-plate intaglio, referring to the medium used to transfer the image onto paper.\nIn woodcut, the map image is created as a relief chiseled from medium-grain hardwood. The areas intended to be printed are inked and pressed against the sheet. Being raised from the rest of the block, the map lines cause indentations in the paper that can often be felt on the back of the map. There are advantages to using relief to make maps. For one, a printmaker doesn't need a press because the maps could be developed as rubbings. Woodblock is durable enough to be used many times before defects appear. Existing printing presses can be used to create the prints rather than having to create a new one. On the other hand, it is hard to achieve fine detail with the relief technique. Inconsistencies in linework are more apparent in woodcut than in intaglio. To improve quality in the late fifteenth century, a style of relief craftsmanship developed using fine chisels to carve the wood, rather than the more commonly used knife.\nIn intaglio, lines are engraved into workable metals, typically copper but sometimes brass. The engraver spreads a thin sheet of wax over the metal plate and uses ink to draw the details. Then, the engraver traces the lines with a stylus to etch them into the plate beneath. The engraver can also use styli to prick holes along the drawn lines, trace along them with colored chalk, and then engrave the map.\u00a0Lines going in the same direction are carved at the same time, and then the plate is turned to carve lines going in a different direction. To print from the finished plate, ink is spread over the metal surface and scraped off such that it remains only in the etched channels. Then the plate is pressed forcibly against the paper so that the ink in the channels is transferred to the paper. The pressing is so forceful that it leaves a \"plate mark\" around the border of the map at the edge of the plate, within which the paper is depressed compared to the margins. Copper and other metals were expensive at the time, so the plate was often reused for new maps or melted down for other purposes.\nWhether woodcut or intaglio, the printed map is hung out to dry. Once dry, it is usually placed in another press of flatten the paper. Any type of paper that was available at the time could be used to print the map on, but thicker paper was more durable.\nBoth relief and intaglio were used about equally by the end of the fifteenth century.\nLettering.\nLettering in mapmaking is important for denoting information. Fine lettering is difficult in woodcut, where it often turned out square and blocky, contrary to the stylized, rounded writing style popular in Italy at the time. To improve quality, mapmakers developed fine chisels to carve the relief. Intaglio lettering did not suffer the troubles of a coarse medium and so was able to express the looping cursive that came to be known as cancellaresca. There were custom-made reverse punches that were also used in metal engraving alongside freehand lettering.\nColor.\nThe first use of color in map making cannot be narrowed down to one reason. There are arguments that color started as a way to indicate information on the map, with aesthetics coming second. There are also arguments that color was first used on maps for aesthetics but then evolved into conveying information. Either way, many maps of the Renaissance left the publisher without being colored, a practice that continued all the way into the 1800s. However, most publishers accepted orders from their patrons to have their maps or atlases colored if they wished. Because all coloring was done by hand, the patron could request simple, cheap color, or more expensive, elaborate color, even going so far as silver or gold gilding. The simplest coloring was merely outlines, such as of borders and along rivers. Wash color meant painting regions with inks or watercolors. Limning meant adding silver and gold leaf to the map to illuminate lettering, heraldic arms, or other decorative elements.\nEarly-Modern Period.\nThe Early Modern Period saw the convergence of cartographical techniques across Eurasia and the exchange of mercantile mapping techniques via the Indian Ocean.\nIn the early seventeenth century, the Selden map was created by a Chinese cartographer. Historians have put its date of creation around 1620, but there is debate in this regard. This map's significance draws from historical misconceptions of East Asian cartography, the main one being that East Asians didn't do cartography until Europeans arrived. The map's depiction of trading routes, a compass rose, and scale bar points to the culmination of many map-making techniques incorporated into Chinese mercantile cartography.\nIn 1689, representatives of the Russian tsar and Qing Dynasty met near the border town of Nerchinsk, which was near the disputed border of the two powers, in eastern Siberia. The two parties, with the Qing negotiation party bringing Jesuits as intermediaries, managed to work a treaty which placed the Amur River as the border between the Eurasian powers, and opened up trading relations between the two. This treaty's significance draws from the interaction between the two sides, and the intermediaries who were drawn from a wide variety of nationalities.\nThe Enlightenment.\nMaps of the Enlightenment period practically universally used copper plate intaglio, having abandoned the fragile, coarse woodcut technology. Use of map projections evolved, with the double hemisphere being very common and Mercator's prestigious navigational projection gradually making more appearances.\nDue to the paucity of information and the immense difficulty of surveying during the period, mapmakers frequently plagiarized material without giving credit to the original cartographer. For example, a famous map of North America known as the \"Beaver Map\" was published in 1715 by Herman Moll. This map is a close reproduction of a 1698 work by Nicolas de Fer. De Fer, in turn, had copied images that were first printed in books by Louis Hennepin, published in 1697, and Fran\u00e7ois Du Creux, in 1664. By the late 18th century, mapmakers often credited the original publisher with something along the lines of, \"After [the original cartographer]\" in the map's title or cartouche.\nModern period.\nIn cartography, technology has continually changed in order to meet the demands of new generations of mapmakers and map users. The first maps were produced manually, with brushes and parchment; so they varied in quality and were limited in distribution. The advent of magnetic devices, such as the compass and much later, magnetic storage devices, allowed for the creation of far more accurate maps and the ability to store and manipulate them digitally.\nAdvances in mechanical devices such as the printing press, quadrant and vernier, allowed the mass production of maps and the creation of accurate reproductions from more accurate data. Hartmann Schedel was one of the first cartographers to use the printing press to make maps more widely available. Optical technology, such as the telescope, sextant and other devices that use telescopes, allowed accurate land surveys and allowed mapmakers and navigators to find their latitude by measuring angles to the North Star at night or the Sun at noon.\nAdvances in photochemical technology, such as the lithographic and photochemical processes, make possible maps with fine details, which do not distort in shape and which resist moisture and wear. This also eliminated the need for engraving, which further speeded up map production.\nIn the 20th century, aerial photography, satellite imagery, and remote sensing provided efficient, precise methods for mapping physical features, such as coastlines, roads, buildings, watersheds, and topography. The United States Geological Survey has devised multiple new map projections, notably the Space Oblique Mercator for interpreting satellite ground tracks for mapping the surface. The use of satellites and space telescopes now allows researchers to map other planets and moons in outer space. Advances in electronic technology ushered in another revolution in cartography: ready availability of computers and peripherals such as monitors, plotters, printers, scanners (remote and document) and analytic stereo plotters, along with computer programs for visualization, image processing, spatial analysis, and database management, democratized and greatly expanded the making of maps. The ability to superimpose spatially located variables onto existing maps created new uses for maps and new industries to explore and exploit these potentials. See also digital raster graphic.\nIn the early years of the new millennium, three key technological advances transformed cartography: the removal of Selective Availability in the Global Positioning System (GPS) in May 2000, which improved locational accuracy for consumer-grade GPS receivers to within a few metres; the invention of OpenStreetMap in 2004, a global digital counter-map that allowed anyone to contribute and use new spatial data without complex licensing agreements; and the launch of Google Earth in 2005 as a development of the virtual globe EarthViewer 3D (2004), which revolutionised access to satellite and aerial imagery. These advances brought more accuracy to geographical and location-based data and widened the range of applications for cartography, for example in the development of satnav devices.\nThese days most commercial-quality maps are made using software of three main types: CAD, GIS and specialized illustration software. Spatial information can be stored in a database, from which it can be extracted on demand. These tools lead to increasingly dynamic, interactive maps that can be manipulated digitally.\nField-rugged computers, GPS, and laser rangefinders make it possible to create maps directly from measurements made on site.\nDeconstruction.\nThere are technical and cultural aspects to producing maps. In this sense, maps can sometimes be said to be biased. The study of bias, influence, and agenda in making a map is what comprise a map's deconstruction. A central tenet of deconstructionism is that maps have power. Other assertions are that maps are inherently biased and that we search for metaphor and rhetoric in maps.\nIt is claimed that the Europeans promoted an \"epistemological\" understanding of the map as early as the 17th century. An example of this understanding is that \"[European reproduction of terrain on maps] reality can be expressed in mathematical terms; that systematic observation and measurement offer the only route to cartographic truth\u2026\". 17th-century map-makers were careful and precise in their strategic approaches to maps based on a scientific model of knowledge. Popular belief at the time was that this scientific approach to cartography was immune to the social atmosphere.\nA common belief is that science heads in a direction of progress, and thus leads to more accurate representations of maps. In this belief European maps must be superior to others, which necessarily employed different map-making skills. \"There was a 'not cartography' land where lurked an army of inaccurate, heretical, subjective, valuative, and ideologically distorted images. Cartographers developed a 'sense of the other' in relation to nonconforming maps.\"\nAlthough cartography has been a target of much criticism in recent decades, a cartographer's 'black box' always seemed to be naturally defended to the point where it overcame the criticism. However, to later scholars in the field, it was evident that cultural influences dominate map-making. For instance, certain abstracts on maps and the map-making society itself describe the social influences on the production of maps. This social play on cartographic knowledge \"\u2026produces the 'order' of [maps'] features and the 'hierarchies of its practices.'\"\nDepictions of Africa are a common target of deconstructionism. According to deconstructionist models, cartography was used for strategic purposes associated with imperialism and as instruments and representations of power during the conquest of Africa. The depiction of Africa and the low latitudes in general on the Mercator projection has been interpreted as imperialistic and as symbolic of subjugation due to the diminished proportions of those regions compared to higher latitudes where the European powers were concentrated.\nMaps furthered imperialism and colonization of Africa in practical ways by showing basic information like roads, terrain, natural resources, settlements, and communities. Through this, maps made European commerce in Africa possible by showing potential commercial routes and made natural resource extraction possible by depicting locations of resources. Such maps also enabled military conquests and made them more efficient, and imperial nations further used them to put their conquests on display. These same maps were then used to cement territorial claims, such as at the Berlin Conference of 1884\u20131885.\nBefore 1749, maps of the African continent had African kingdoms drawn with assumed or contrived boundaries, with unknown or unexplored areas having drawings of animals, imaginary physical geographic features, and descriptive texts. In 1748, Jean B. B. d'Anville created the first map of the African continent that had blank spaces to represent the unknown territory. This was revolutionary in cartography and the representation of power associated with map making.\nMap types.\nGeneral vs. thematic cartography.\nIn understanding basic maps, the field of cartography can be divided into two general categories: general cartography and thematic cartography. General cartography involves those maps that are constructed for a general audience and thus contain a variety of features. General maps exhibit many reference and location systems and often are produced in a series. For example, the 1:24,000 scale topographic maps of the United States Geological Survey (USGS) are a standard as compared to the 1:50,000 scale Canadian maps. The government of the UK produces the classic 1:50,000 (replacing the older 1\u00a0inch to 1 mile) \"Ordnance Survey\" maps of the entire UK and with a range of correlated larger- and smaller-scale maps of great detail. Many private mapping companies have also produced thematic map series.\nThematic cartography involves maps of specific geographic themes, oriented toward specific audiences. A couple of examples might be a dot map showing corn production in Indiana or a shaded area map of Ohio counties, divided into numerical choropleth classes. As the volume of geographic data has exploded over the last century, thematic cartography has become increasingly useful and necessary to interpret spatial, cultural and social data.\nA third type of map is known as an \"orienteering,\" or special purpose map. This type of map falls somewhere between thematic and general maps. They combine general map elements with thematic attributes in order to design a map with a specific audience in mind. Oftentimes, the type of audience an orienteering map is made for is in a particular industry or occupation. An example of this kind of map would be a municipal utility map.\nTopographic vs. topological.\nA topographic map is primarily concerned with the topographic description of a place, including (especially in the 20th and 21st centuries) the use of contour lines showing elevation. Terrain or relief can be shown in a variety of ways (see Cartographic relief depiction). In the present era, one of the most widespread and advanced methods used to form topographic maps is to use computer software to generate digital elevation models which show shaded relief. Before such software existed, cartographers had to draw shaded relief by hand. One cartographer who is respected as a master of hand-drawn shaded relief is the Swiss professor Eduard Imhof whose efforts in hill shading were so influential that his method became used around the world despite it being so labor-intensive.\nA topological map is a very general type of map, the kind one might sketch on a napkin. It often disregards scale and detail in the interest of clarity of communicating specific route or relational information. Beck's London Underground map is an iconic example. Although the most widely used map of \"The Tube,\" it preserves little of reality: it varies scale constantly and abruptly, it straightens curved tracks, and it contorts directions. The only topography on it is the River Thames, letting the reader know whether a station is north or south of the river. That and the topology of station order and interchanges between train lines are all that is left of the geographic space. Yet those are all a typical passenger wishes to know, so the map fulfills its purpose.\nMap design.\nModern technology, including advances in Printing, the advent of Geographic information systems and Graphics software, and the Internet, has vastly simplified the process of map creation and increased the palette of design options available to cartographers. This has led to a decreased focus on production skill, and an increased focus on quality design, the attempt to craft maps that are both aesthetically pleasing and practically useful for their intended purposes.\nMap purpose and audience.\nA map has a purpose and an audience. Its purpose may be as broad as teaching the major physical and political features of the entire world, or as narrow as convincing a neighbor to move a fence. The audience may be as broad as the general public or as narrow as a single person. Mapmakers use design principles to guide them in constructing a map that is effective for its purpose and audience.\nCartographic process.\nThe cartographic process spans many stages, starting from conceiving the need for a map and extending all the way through its consumption by an audience. Conception begins with a real or imagined environment. As the cartographer gathers information about the subject, they consider how that information is structured and how that structure should inform the map's design. Next, the cartographers experiments with generalization, symbolization, typography, and other map elements to find ways to portray the information so that the map reader can interpret the map as intended. Guided by these experiments, the cartographer settles on a design and creates the map, whether in physical or electronic form. Once finished, the map is delivered to its audience. The map reader interpret the symbols and patterns on the map to draw conclusions and perhaps to take action. By the spatial perspectives they provide, maps help shape how we view the world.\nAspects of map design.\nDesigning a map involves bringing together a number of elements and making a large number of decisions. The elements of design fall into several broad topics, each of which has its own theory, its own research agenda, and its own best practices. That said, there are synergistic effects between these elements, meaning that the overall design process is not just working on each element one at a time, but an iterative feedback process of adjusting each to achieve the desired gestalt.\nCartographic errors.\nSome maps contain deliberate errors or distortions, either as propaganda or as a \"watermark\" to help the copyright owner identify infringement if the error appears in competitors' maps. The latter often come in the form of nonexistent, misnamed, or misspelled \"trap streets\". Other names and forms for this are paper townsites, fictitious entries, and copyright easter eggs.\nAnother motive for deliberate errors is cartographic \"vandalism\": a mapmaker wishing to leave his or her mark on the work. Mount Richard, for example, was a fictitious peak on the Rocky Mountains' continental divide that appeared on a Boulder County, Colorado map in the early 1970s. It is believed to be the work of draftsman Richard Ciacci. The fiction was not discovered until two years later.\nSandy Island (New Caledonia) is an example of a fictitious location that stubbornly survives, reappearing on new maps copied from older maps while being deleted from other new editions.\nProfessional and learned societies.\nProfessional and learned societies include:"}
{"id": "7295", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=7295", "title": "Consumption", "text": "Consumption may refer to:\nin biology:\nin social sciences:"}
{"id": "7296", "revid": "992698846", "url": "https://en.wikipedia.org/wiki?curid=7296", "title": "Cardiac glycoside", "text": "Cardiac glycosides are a class of organic compounds that increase the output force of the heart and decrease its rate of contractions by acting on the cellular sodium-potassium ATPase pump. Their beneficial medical uses are as treatments for congestive heart failure and cardiac arrhythmias; however, their relative toxicity prevents them from being widely used. Most commonly found as secondary metabolites in several plants such as foxglove plants, these compounds nevertheless have a diverse range of biochemical effects regarding cardiac cell function and have also been suggested for use in cancer treatment.\nClassification.\nGeneral structure.\nThe general structure of a cardiac glycoside consists of a steroid molecule attached to a sugar (glycoside) and an R group. The steroid nucleus consists of four fused rings to which other functional groups such as methyl, hydroxyl, and aldehyde groups can be attached to influence the overall molecule's biological activity. Cardiac glycosides also vary in the groups attached at either end of the steroid. Specifically, different sugar groups attached at the sugar end of the steroid can alter the molecule's solubility and kinetics; however, the lactone moiety at the R group end only serves a structural function.\nIn particular, the structure of the ring attached at the R end of the molecule allows it to be classified as either a cardenolide or bufadienolide. Cardenolides differ from bufadienolides due to the presence of an \u201cenolide,\u201d a five-membered ring with a single double bond, at the lactone end. Bufadienolides, on the other hand, contain a \u201cdienolide,\u201d a six-membered ring with two double bonds, at the lactone end. While compounds of both groups can be used to influence the cardiac output of the heart, cardenolides are more commonly used medicinally, primarily due to the widespread availability of the plants from which they are derived.\nClassification.\nCardiac glycosides can be more specifically categorized based on the plant they are derived from, as in the following list. For example, cardenolides have been primarily derived from the foxglove plants \"Digitalis purpurea\" and \"Digitalis lanata\", while bufadienolides have been derived from the venom of the cane toad \"Bufo marinus\", from which they receive the \u201cbufo\u201d portion of their name. Below is a list of organisms from which cardiac glycosides can be derived.\nMechanism of action.\nCardiac glycosides affect the sodium-potassium ATPase pump in cardiac muscle cells to alter their function. Normally, these sodium-potassium pumps move potassium ions in and sodium ions out. Cardiac glycosides, however, inhibit this pump by stabilizing it in the E2-P transition state, so that sodium cannot be extruded: intracellular sodium concentration therefore increases. With regard to potassium ion movement, because both cardiac glycosides and potassium compete for binding to the ATPase pump, changes in extracellular potassium concentration can potentially lead to altered drug efficacy. Nevertheless, by carefully controlling the dosage, such adverse effects can be avoided. Continuing on with the mechanism, raised intracellular sodium levels inhibit the function of a second membrane ion exchanger, NCX, which is responsible for pumping calcium ions out of the cell and sodium ions in at a ratio of /. Thus, calcium ions are also not extruded and will begin to build up inside the cell as well.\nThe disrupted calcium homeostasis and increased cytoplasmic calcium concentrations cause increased calcium uptake into the sarcoplasmic reticulum (SR) via the SERCA2 transporter. Raised calcium stores in the SR allow for greater calcium release on stimulation, so the myocyte can achieve faster and more powerful contraction by cross-bridge cycling. The refractory period of the AV node is increased, so cardiac glycosides also function to decrease heart rate. For example, the ingestion of digoxin leads to increased cardiac output and decreased heart rate without significant changes in blood pressure; this quality allows it to be widely used medicinally in the treatment of cardiac arrhythmias.\nClinical significance.\nCardiac glycosides have long served as the main medical treatment to congestive heart failure and cardiac arrhythmia, due to their effects of increasing the force of muscle contraction while reducing heart rate. Heart failure is characterized by an inability to pump enough blood to support the body, possibly due to a decrease in the volume of the blood or its contractile force. Treatments for the condition thus focus on lowering blood pressure, so that the heart does not have to exert as much force to pump the blood, or directly increasing the heart's contractile force, so that the heart can overcome the higher blood pressure. Cardiac glycosides, such as the commonly used digoxin and digitoxin, deal with the latter, due to their positive inotropic activity. On the other hand, cardiac arrhythmia are changes in heart rate, whether faster (tachycardia) or slower (bradycardia). Medicinal treatments for this condition work primarily to counteract tachycardia or atrial fibrillation by slowing down heart rate, as done by cardiac glycosides.\nNevertheless, due to questions of toxicity and dosage, cardiac glycosides have been replaced with synthetic drugs such as ACE inhibitors and beta blockers and are no longer used as the primary medical treatment for such conditions. Depending on the severity of the condition, though, they may still be used in conjunction with other treatments.\nToxicity.\nFrom ancient times, humans have used cardiac-glycoside-containing plants and their crude extracts as arrow coatings, homicidal or suicidal aids, rat poisons, heart tonics, diuretics and emetics, primarily due to the toxic nature of these compounds. Thus, though cardiac glycosides have been used for their medicinal function, their toxicity must also be recognized. For example, in 2008 US poison centers reported 2,632 cases of digoxin toxicity, and 17 cases of digoxin-related deaths. Because cardiac glycosides affect the cardiovascular, neurologic, and gastrointestinal systems, these three systems can be used to determine the effects of toxicity. The effect of these compounds on the cardiovascular system presents a reason for concern, as they can directly affect the function of the heart through their inotropic and chronotropic effects. In terms of inotropic activity, excessive cardiac glycoside dosage results in cardiac contractions with greater force, as further calcium is released from the SR of cardiac muscle cells. Toxicity also results in changes to heart chronotropic activity, resulting in multiple kinds of dysrhythmia and potentially fatal ventricular tachycardia. These dysrhythmias are an effect of an influx of sodium and decrease of resting membrane potential threshold in cardiac muscle cells. When taken beyond a narrow dosage range specific to each particular cardiac glycoside, these compounds can rapidly become dangerous. In sum, they interfere with fundamental processes that regulate membrane potential. They are toxic to the heart, the brain, and the gut at doses that are not difficult to reach. In the heart, the most common negative effect is Premature ventricular contraction."}
{"id": "7297", "revid": "40019", "url": "https://en.wikipedia.org/wiki?curid=7297", "title": "Ca plus plus antagonist", "text": ""}
{"id": "7298", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7298", "title": "Cyclic AMP", "text": ""}
{"id": "7299", "revid": "40426870", "url": "https://en.wikipedia.org/wiki?curid=7299", "title": "Colonialism", "text": "Colonialism is where one country assumes political control over another, often by establishing colonies and generally with the aim of economic dominance. In the process of colonisation, colonisers may impose their religion, language, economics, and other cultural practices on indigenous peoples. The foreign administrators rule the territory in pursuit of their interests, seeking to benefit from the colonised region's people and resources. It is associated but distinct to imperialism.\nColonialism is strongly associated with the European colonial period starting with the 15th century when some European states established colonising empires. At first, European colonising countries followed policies of mercantilism, aiming to strengthen the home-country economy, so agreements usually restricted the colony to trading only with the metropole (mother country). By the mid-19th century, however, the British Empire gave up mercantilism and trade restrictions and adopted the principle of free trade, with few restrictions or tariffs. Christian missionaries were active in practically all of the European-controlled colonies because the metropoles were Christian. Historian Philip Hoffman calculated that by 1800, before the Industrial Revolution, Europeans already controlled at least 35% of the globe, and by 1914, they had gained control of 84% of the globe.\nIn the aftermath of World War II colonial powers were forced to retreat between 1945 and 1975, when nearly all colonies gained independence, entering into changed colonial, so-called postcolonial and neocolonialist relations. Postcolonialism and neocolonialism has continued or shifted relations and ideologies of colonialism, justifying its continuation with concepts such as development and new frontiers, as in exploring outer space for colonization.\nDefinitions.\n\"Collins English Dictionary\" defines colonialism as \"the policy and practice of a power in extending control over weaker peoples or areas\". \"Webster's Encyclopedic Dictionary\" defines colonialism as \"the system or policy of a nation seeking to extend or retain its authority over other people or territories\". The \"Merriam-Webster Dictionary\" offers four definitions, including \"something characteristic of a colony\" and \"control by one power over a dependent area or people\". Etymologically, the word \"colony\" comes from the Latin \"col\u014dnia\"\u2014\"a place for agriculture\".\nThe \"Stanford Encyclopedia of Philosophy\" uses the term \"to describe the process of European settlement and political control over the rest of the world, including the Americas, Australia, and parts of Africa and Asia\". It discusses the distinction between colonialism, imperialism and conquest and states that \"[t]he difficulty of defining colonialism stems from the fact that the term is often used as a synonym for imperialism. Both colonialism and imperialism were forms of conquest that were expected to benefit Europe economically and strategically,\" and continues \"given the difficulty of consistently distinguishing between the two terms, this entry will use \"colonialism\" broadly to refer to the project of European political domination from the sixteenth to the twentieth centuries that ended with the national liberation movements of the 1960s\".\nIn his preface to J\u00fcrgen Osterhammel's \"Colonialism: A Theoretical Overview\", Roger Tignor says \"For Osterhammel, the essence of colonialism is the existence of colonies, which are by definition governed differently from other territories such as protectorates or informal spheres of influence.\" In the book, Osterhammel asks, \"How can 'colonialism' be defined independently from 'colony?'\" He settles on a three-sentence definition:\nTypes of colonialism.\nHistorians often distinguish between various overlapping forms of colonialism, which are classified into four types: settler colonialism, exploitation colonialism, surrogate colonialism, and internal colonialism.\nSocio-cultural evolution.\nAs colonialism often played out in pre-populated areas, sociocultural evolution included the formation of various ethnically hybrid populations. Colonialism gave rise to culturally and ethnically mixed populations such as the mestizos of the Americas, as well as racially divided populations such as those found in French Algeria or in Southern Rhodesia. In fact, everywhere where colonial powers established a consistent and continued presence, hybrid communities existed.\nNotable examples in Asia include the Anglo-Burmese, Anglo-Indian, Burgher, Eurasian Singaporean, Filipino mestizo, Kristang and Macanese peoples. In the Dutch East Indies (later Indonesia) the vast majority of \"Dutch\" settlers were in fact Eurasians known as Indo-Europeans, formally belonging to the European legal class in the colony (see also Indos in pre-colonial history and Indos in colonial history).\nHistory.\nPremodern.\nActivity that could be called colonialism has a long history, starting at least as early as the Ancient Egyptians. Phoenicians, Greeks and Romans founded colonies in antiquity. Phoenicia had an enterprising maritime trading-culture that spread across the Mediterranean from 1550 BC to 300 BC; later the Persian empire and various Greek city-states continued on this line of setting up colonies. The Romans would soon follow, setting up \"coloniae\" throughout the Mediterranean, in Northern Africa, and in Western Asia. Beginning in the 7th century, Arabs colonized a substantial portion of the Middle East, Northern Africa, and parts of Asia and Europe. From the 9th century Vikings (Norsemen) established colonies in Britain, Ireland, Iceland, Greenland, North America, present-day Russia and Ukraine, France (Normandy) and Sicily. In the 9th century a new wave of Mediterranean colonisation began, with competitors such as the Venetians, Genovese and Amalfians infiltrating the wealthy previously Byzantine or Eastern Roman islands and lands. European Crusaders set up colonial regimes in Outremer (in the Levant, 1097-1291) and in the Baltic littoral (12th century onwards). Venice began to dominate Dalmatia and reached its greatest nominal colonial extent at the conclusion of the Fourth Crusade in 1204, with the declaration of the acquisition of three octaves of the Byzantine Empire.\nModern.\nModern colonialism started with the Portuguese Prince Henry the Navigator (1394-1460), initiating the Age of Exploration and establishing African trading posts (1445 onwards). Spain (initially the Crown of Castile) and soon after Portugal encountered the Americas (1492 onwards) through sea travel and built trading posts or conquered large extents of land. For some people, it is this building of colonies across oceans that differentiates colonialism from other types of expansionism. Madrid and Lisbon divided the areas of these \"new\" lands between the Spanish Empire and the Portuguese Empire in 1494; other would-be colonial powers paid little heed to the theoretical demarcation.\nThe 17th century saw the birth of the French colonial empire and the Dutch Empire, as well as the English overseas possessions, which later became the British Empire. It also saw the establishment of a Danish colonial empire and some Swedish overseas colonies.\nA first wave of independence movements started with the American Revolutionary War (1775\u20131783), initiating a new phase for the British Empire. The Spanish Empire largely collapsed in the Americas with the Latin American wars of independence ( onwards). However, empire-builders established several new colonies after this time, including in the German colonial empire and the Belgian colonial empire. In the late-19th century, many European powers became involved in the Scramble for Africa.\nThe Russian Empire, Ottoman Empire and Austrian Empire existed at the same time as the above empires but did not expand over oceans. Rather, these empires expanded through the more traditional route of the conquest of neighbouring territories. There was, though, some Russian colonisation of the Americas across the Bering Strait. From the 1860s, the Empire of Japan modelled itself on European colonial empires and expanded its territories in the Pacific and on the Asian mainland. Argentina and the Empire of Brazil fought for hegemony in South America. The United States of America gained overseas territories after the 1898 Spanish\u2013American War - hence the coining of the term \"American Empire\".\nAfter the First World War of 1914-1918, the victorious allies divided up the German colonial empire and much of the Ottoman Empire between themselves as League of Nations mandates, grouping these territories into three classes according to how quickly it was deemed that they could prepare for independence. The empires of Russia and Austria collapsed in 1917-1918. Nazi Germany set up short-lived colonial systems (\"Reichskommissariate\", \"Generalgouvernement\") in Eastern Europe in the early 1940s.\nAfter World War II (1939-1945) decolonisation progressed rapidly, due to a number of reasons. First, the Japanese victories in the Pacific War of 1941-1945 had showed Indians and other subject peoples that the colonial powers were not invincible. Second, World War II had significantly weakened all the overseas colonial powers economically.\nDozens of independence movements and global political solidarity projects such as the Non-Aligned Movement proved instrumental in the decolonisation efforts of former colonies. These included significant wars of independence fought in Indonesia, Vietnam, Algeria, and Kenya. Eventually, the European powers\u2014pressured by the United States and Soviets\u2014resigned themselves to decolonisation.\nIn 1962 the United Nations set up a Special Committee on Decolonisation, often called the Committee of 24, to encourage this process.\nThe status and cost of European colonization at the turn of the 20th century.\nThe world's colonial population at the outbreak of the First World War (1914) - a high point for colonialism - totalled about 560\u00a0million people, of whom 70% lived in British possessions, 10% in French possessions, 9% in Dutch possessions, 4% in Japanese possessions, 2% in German possessions, 2% in American possessions, 2% in Portuguese possessions, 1% in Belgian possessions and half of 1% in Italian possessions. The domestic domains of the colonial powers had a total population of about 370\u00a0million people. Outside Europe, few areas had remained without coming under formal colonial tutorship - and even Siam, China, Nepal, Japan, Afghanistan, Persia and Abyssinia had felt varying degrees of Western colonial-style influence - concessions, unequal treaties, extraterritoriality and the like.\nAsking whether colonies paid, economic historian Grover Clark (1891-1938) argues an emphatic \"No!\" He reports that in every case the support cost, especially the military system necessary to support and defend colonies, outran the total trade they produced. Apart from the British Empire, they did not provide favoured destinations for the immigration of surplus metropole populations. The question of whether colonies paid is, however, a complicated one when recognizing the multiplicity of interests involved. In some cases colonial powers paid a lot in military costs while private investors pocketed the benefits. In other cases the colonial powers managed to move the burden of administrative costs to the colonies themselves by imposing taxes.\nNeocolonialism.\nThe word \"neocolonialism\" has been used to refer to a variety of contexts since the decolonisation that took place after World War II. Generally it does not refer to a type of direct colonisation - rather to colonialism or colonial-style exploitation by other means. Specifically, neocolonialism may refer to the theory that former or existing economic relationships, such as the General Agreement on Tariffs and Trade and the Central American Free Trade Agreement, or the operations of companies (such as Royal Dutch Shell in Nigeria and Brunei) fostered by former colonial powers were or are used to maintain control of former colonies and dependencies after the colonial independence movements of the post\u2013World War II period.\nThe term \"neocolonialism\" became popular in ex-colonies in the late-20th century.\nList of colonies.\nOther non-European colonialist countries.\nOmani colonies.\nOmani Empire\nImpact of colonialism and colonisation.\nThe impacts of colonisation are immense and pervasive. Various effects, both immediate and protracted, include the spread of virulent diseases, unequal social relations, detribalization, exploitation, enslavement, medical advances, the creation of new institutions, abolitionism, improved infrastructure, and technological progress. Colonial practices also spur the spread of colonist languages, literature and cultural institutions, while endangering or obliterating those of native peoples. The native cultures of the colonised peoples can also have a powerful influence on the imperial country.\nEconomy, trade and commerce.\nEconomic expansion, sometimes described as the colonial surplus, has accompanied imperial expansion since ancient times. Greek trade networks spread throughout the Mediterranean region while Roman trade expanded with the primary goal of directing tribute from the colonised areas towards the Roman metropole. According to Strabo, by the time of emperor Augustus, up to 120 Roman ships would set sail every year from Myos Hormos in Roman Egypt to India. With the development of trade routes under the Ottoman Empire,\nAztec civilisation developed into an extensive empire that, much like the Roman Empire, had the goal of exacting tribute from the conquered colonial areas. For the Aztecs, a significant tribute was the acquisition of sacrificial victims for their religious rituals.\nOn the other hand, European colonial empires sometimes attempted to channel, restrict and impede trade involving their colonies, funneling activity through the metropole and taxing accordingly.\nDespite the general trend of economic expansion, the economic performance of former European colonies varies significantly. In \"Institutions as a Fundamental Cause of Long-run Growth\", economists Daron Acemoglu, Simon Johnson and James A. Robinson compare the economic influences of the European colonists on different colonies and study what could explain the huge discrepancies in previous European colonies, for example, between West African colonies like Sierra Leone and Hong Kong and Singapore.\nAccording to the paper, economic institutions are the determinant of the colonial success because they determine their financial performance and order for the distribution of resources. At the same time, these institutions are also consequences of political institutions \u2013 especially how de facto and de jure political power is allocated. To explain the different colonial cases, we thus need to look first into the political institutions that shaped the economic institutions.\nFor example, one interesting observation is \"the Reversal of Fortune\" \u2013 the less developed civilisations in 1500, like North America, Australia, and New Zealand, are now much richer than those countries who used to be in the prosperous civilisations in 1500 before the colonists came, like the Mughals in India and the Incas in the Americas. One explanation offered by the paper focuses on the political institutions of the various colonies: it was less likely for European colonists to introduce economic institutions where they could benefit quickly from the extraction of resources in the area. Therefore, given a more developed civilisation and denser population, European colonists would rather keep the existing economic systems than introduce an entirely new system; while in places with little to extract, European colonists would rather establish new economic institutions to protect their interests. Political institutions thus gave rise to different types of economic systems, which determined the colonial economic performance.\nEuropean colonisation and development also changed gendered systems of power already in place around the world. In many pre-colonialist areas, women maintained power, prestige, or authority through reproductive or agricultural control. For example, in certain parts of sub-Saharan Africa women maintained farmland in which they had usage rights. While men would make political and communal decisions for a community, the women would control the village's food supply or their individual family's land. This allowed women to achieve power and autonomy, even in patrilineal and patriarchal societies.\nThrough the rise of European colonialism came a large push for development and industrialisation of most economic systems. However, when working to improve productivity, Europeans focused mostly on male workers. Foreign aid arrived in the form of loans, land, credit, and tools to speed up development, but were only allocated to men. In a more European fashion, women were expected to serve on a more domestic level. The result was a technologic, economic, and class-based gender gap that widened over time.\nWithin a colony, the presence of extractive colonial institutions in a given area has been found have effects on the modern day economic development, institutions and infrastructure of these areas.\nSlavery and indentured servitude.\nEuropean nations entered their imperial projects with the goal of enriching the European metropoles. Exploitation of non-Europeans and of other Europeans to support imperial goals was acceptable to the colonisers. Two outgrowths of this imperial agenda were the extension of slavery and indentured servitude. In the 17th century, nearly two-thirds of English settlers came to North America as indentured servants.\nEuropean slave traders brought large numbers of African slaves to the Americas by sail. Spain and Portugal had brought African slaves to work in African colonies such as Cape Verde and S\u00e3o Tom\u00e9 and Pr\u00edncipe, and then in Latin America, by the 16th century. The British, French and Dutch joined in the slave trade in subsequent centuries. The European colonial system took approximately 11\u00a0million Africans to the Caribbean and to North and South America as slaves.\nAbolitionists in Europe and Americas protested the inhumane treatment of African slaves, which led to the elimination of the slave trade (and later, of most forms of slavery) by the late 19th century. One (disputed) school of thought points to the role of abolitionism in the American Revolution: while the British colonial metropole started to move towards outlawing slavery, slave-owning elites in the Thirteen Colonies saw this as one of the reasons to fight for their post-colonial independence and for the right to develop and continue a largely slave-based economy.\nBritish colonising activity in New Zealand from the early 19th century played a part in ending slave-taking and slave-keeping among the indigenous M\u0101ori.\nOn the other hand, British colonial administration in Southern Africa, when it officially abolished slavery in the 1830s, caused rifts in society which arguably perpetuated slavery in the Boer Republics and fed into the philosophy of \"apartheid\".\nThe labour shortages that resulted from abolition inspired European colonisers in Queensland, British Guaiana and Fiji (for example) to develop new sources of labour, re-adopting a system of indentured servitude. Indentured servants consented to a contract with the European colonisers. Under their contract, the servant would work for an employer for a term of at least a year, while the employer agreed to pay for the servant's voyage to the colony, possibly pay for the return to the country of origin, and pay the employee a wage as well. The employees became \"indentured\" to the employer because they owed a debt back to the employer for their travel expense to the colony, which they were expected to pay through their wages. In practice, indentured servants were exploited through terrible working conditions and burdensome debts imposed by the employers, with whom the servants had no means of negotiating the debt once they arrived in the colony.\nIndia and China were the largest source of indentured servants during the colonial era. Indentured servants from India travelled to British colonies in Asia, Africa and the Caribbean, and also to French and Portuguese colonies, while Chinese servants travelled to British and Dutch colonies. Between 1830 and 1930, around 30\u00a0million indentured servants migrated from India, and 24\u00a0million returned to India. China sent more indentured servants to European colonies, and around the same proportion returned to China.\nFollowing the Scramble for Africa, an early but secondary focus for most colonial regimes was the suppression of slavery and the slave trade. By the end of the colonial period they were mostly successful in this aim, though slavery persists in Africa and in the world at large with much the same practices of \"de facto\" servility despite legislative prohibition.\nMilitary innovation.\nConquering forces have throughout history applied innovation in order to gain an advantage over the armies of the people they aim to conquer. Greeks developed the phalanx system, which enabled their military units to present themselves to their enemies as a wall, with foot soldiers using shields to cover one another during their advance on the battlefield. Under Philip II of Macedon, they were able to organise thousands of soldiers into a formidable battle force, bringing together carefully trained infantry and cavalry regiments. Alexander the Great exploited this military foundation further during his conquests.\nThe Spanish Empire held a major advantage over Mesoamerican warriors through the use of weapons made of stronger metal, predominantly iron, which was able to shatter the blades of axes used by the Aztec civilisation and others. The use of gunpowder weapons cemented the European military advantage over the peoples they sought to subjugate in the Americas and elsewhere.\nThe end of empire.\nThe populations of some colonial territories, such as Canada, enjoyed relative peace and prosperity as part of a European power, at least among the majority; however, minority populations such as First Nations peoples and French-Canadians experienced marginalisation and resented colonial practises. Francophone residents of Quebec, for example, were vocal in opposing conscription into the armed services to fight on behalf of Britain during World War I, resulting in the Conscription crisis of 1917. Other European colonies had much more pronounced conflict between European settlers and the local population. Rebellions broke out in the later decades of the imperial era, such as India's Sepoy Rebellion of 1857.\nThe territorial boundaries imposed by European colonisers, notably in central Africa and South Asia, defied the existing boundaries of native populations that had previously interacted little with one another. European colonisers disregarded native political and cultural animosities, imposing peace upon people under their military control. Native populations were often relocated at the will of the colonial administrators.\nThe Partition of British India in August 1947 led to the Independence of India and the creation of Pakistan. These events also caused much bloodshed at the time of the migration of immigrants from the two countries. Muslims from India and Hindus and Sikhs from Pakistan migrated to the respective countries they sought independence to.\nPost-independence population movement.\nIn a reversal of the migration patterns experienced during the modern colonial era, post-independence era migration followed a route back towards the imperial country. In some cases, this was a movement of settlers of European origin returning to the land of their birth, or to an ancestral birthplace. 900,000 French colonists (known as the \"Pied-Noirs\") resettled in France following Algeria's independence in 1962. A significant number of these migrants were also of Algerian descent. 800,000 people of Portuguese origin migrated to Portugal after the independence of former colonies in Africa between 1974 and 1979; 300,000 settlers of Dutch origin migrated to the Netherlands from the Dutch West Indies after Dutch military control of the colony ended.\nAfter WWII 300,000 Dutchmen from the Dutch East Indies, of which the majority were people of Eurasian descent called Indo Europeans, repatriated to the Netherlands. A significant number later migrated to the US, Canada, Australia and New Zealand.\nGlobal travel and migration in general developed at an increasingly brisk pace throughout the era of European colonial expansion. Citizens of the former colonies of European countries may have a privileged status in some respects with regard to immigration rights when settling in the former European imperial nation. For example, rights to dual citizenship may be generous, or larger immigrant quotas may be extended to former colonies.\nIn some cases, the former European imperial nations continue to foster close political and economic ties with former colonies. The Commonwealth of Nations is an organisation that promotes cooperation between and among Britain and its former colonies, the Commonwealth members. A similar organisation exists for former colonies of France, the Francophonie; the Community of Portuguese Language Countries plays a similar role for former Portuguese colonies, and the Dutch Language Union is the equivalent for former colonies of the Netherlands.\nMigration from former colonies has proven to be problematic for European countries, where the majority population may express hostility to ethnic minorities who have immigrated from former colonies. Cultural and religious conflict have often erupted in France in recent decades, between immigrants from the Maghreb countries of north Africa and the majority population of France. Nonetheless, immigration has changed the ethnic composition of France; by the 1980s, 25% of the total population of \"inner Paris\" and 14% of the metropolitan region were of foreign origin, mainly Algerian.\nIntroduced diseases.\nEncounters between explorers and populations in the rest of the world often introduced new diseases, which sometimes caused local epidemics of extraordinary virulence. For example, smallpox, measles, malaria, yellow fever, and others were unknown in pre-Columbian America.\nHalf the native population of Hispaniola in 1518 was killed by smallpox. Smallpox also ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlan alone, including the emperor, and Peru in the 1530s, aiding the European conquerors. Measles killed a further two million Mexican natives in the 17th century. In 1618\u20131619, smallpox wiped out 90% of the Massachusetts Bay Native Americans. Smallpox epidemics in 1780\u20131782 and 1837\u20131838 brought devastation and drastic depopulation among the Plains Indians. Some believe that the death of up to 95% of the Native American population of the New World was caused by Old World diseases. Over the centuries, the Europeans had developed high degrees of immunity to these diseases, while the indigenous peoples had no time to build such immunity.\nSmallpox decimated the native population of Australia, killing around 50% of indigenous Australians in the early years of British colonisation. It also killed many New Zealand M\u0101ori. As late as 1848\u201349, as many as 40,000 out of 150,000 Hawaiians are estimated to have died of measles, whooping cough and influenza. Introduced diseases, notably smallpox, nearly wiped out the native population of Easter Island. In 1875, measles killed over 40,000 Fijians, approximately one-third of the population. The Ainu population decreased drastically in the 19th century, due in large part\nto infectious diseases brought by Japanese settlers pouring into Hokkaido.\nConversely, researchers have hypothesised that a precursor to syphilis may have been carried from the New World to Europe after Columbus's voyages. The findings suggested Europeans could have carried the nonvenereal tropical bacteria home, where the organisms may have mutated into a more deadly form in the different conditions of Europe. The disease was more frequently fatal than it is today; syphilis was a major killer in Europe during the Renaissance. The first cholera pandemic began in Bengal, then spread across India by 1820. Ten thousand British troops and countless Indians died during this pandemic. Between 1736 and 1834 only some 10% of East India Company's officers survived to take the final voyage home. Waldemar Haffkine, who mainly worked in India, who developed and used vaccines against cholera and bubonic plague in the 1890s, is considered the first microbiologist.\nCountering disease.\nAs early as 1803, the Spanish Crown organised a mission (the Balmis expedition) to transport the smallpox vaccine to the Spanish colonies, and establish mass vaccination programs there. By 1832, the federal government of the United States established a smallpox vaccination program for Native Americans. Under the direction of Mountstuart Elphinstone a program was launched to propagate smallpox vaccination in India. From the beginning of the 20th century onwards, the elimination or control of disease in tropical countries became a driving force for all colonial powers. The sleeping sickness epidemic in Africa was arrested due to mobile teams systematically screening millions of people at risk. In the 20th century, the world saw the biggest increase in its population in human history due to lessening of the mortality rate in many countries due to medical advances. The world population has grown from 1.6\u00a0billion in 1900 to over seven billion today.\nColonialism and the history of thought.\nColonial botany.\nColonial botany refers to the body of works concerning the study, cultivation, marketing and naming of the new plants that were acquired or traded during the age of European colonialism. Notable examples of these plants included sugar, nutmeg, tobacco, cloves, cinnamon, Peruvian bark, peppers and tea. This work was a large part of securing financing for colonial ambitions, supporting European expansion and ensuring the profitability of such endeavors. Vasco de Gama and Christopher Columbus were seeking to establish routes to trade spices, dyes and silk from the Moluccas, India and China by sea that would be independent of the established routes controlled by Venetian and Middle Eastern merchants. Naturalists like Hendrik van Rheede, Georg Eberhard Rumphius, and Jacobus Bontius compiled data about eastern plants on behalf of the Europeans. Though Sweden did not possess an extensive colonial network, botanical research based on Carl Linnaeus identified and developed techniques to grow cinnamon, tea and rice locally as an alternative to costly imports.\nUniversalism.\nThe conquest of vast territories brings multitudes of diverse cultures under the central control of the imperial authorities. From the time of Ancient Greece and Ancient Rome, this fact has been addressed by empires adopting the concept of universalism, and applying it to their imperial policies towards their subjects far from the imperial capitol. The capitol, the metropole, was the source of ostensibly enlightened policies imposed throughout the distant colonies.\nThe empire that grew from Greek conquest, particularly by Alexander the Great, spurred the spread of Greek language, religion, science and philosophy throughout the colonies. While most Greeks considered their own culture superior to all others (the word barbarian is derived from mutterings that sounded to Greek ears like \"bar-bar\"), Alexander was unique in promoting a campaign to win the hearts and minds of the Persians. He adopted Persian customs of clothing and otherwise encouraged his men to go native by adopting local wives and learning their mannerisms. Of note is that he radically departed from earlier Greek attempts at colonisation, characterised by the murder and enslavement of the local inhabitants and the settling of Greek citizens from the polis.\nRoman universalism was characterised by cultural and religious tolerance and a focus on civil efficiency and the rule of law. Roman law was imposed on both Roman citizens and colonial subjects. Although Imperial Rome had no public education, Latin spread through its use in government and trade. Roman law prohibited local leaders to wage war between themselves, which was responsible for the 200 year long Pax Romana, at the time the longest period of peace in history. The Roman Empire was tolerant of diverse cultures and religious practises, even allowing them on a few occasions to threaten Roman authority.\nColonialism and geography.\nSettlers acted as the link between indigenous populations and the imperial hegemony, thus bridging the geographical, ideological and commercial gap between the colonisers and colonised. While the extent in which geography as an academic study is implicated in colonialism is contentious, geographical tools such as cartography, shipbuilding, navigation, mining and agricultural productivity were instrumental in European colonial expansion. Colonisers' awareness of the Earth's surface and abundance of practical skills provided colonisers with a knowledge that, in turn, created power.\nAnne Godlewska and Neil Smith argue that \"empire was 'quintessentially a geographical project. Historical geographical theories such as environmental determinism legitimised colonialism by positing the view that some parts of the world were underdeveloped, which created notions of skewed evolution. Geographers such as Ellen Churchill Semple and Ellsworth Huntington put forward the notion that northern climates bred vigour and intelligence as opposed to those indigenous to tropical climates (See The Tropics) viz a viz a combination of environmental determinism and Social Darwinism in their approach.\nPolitical geographers also maintain that colonial behaviour was reinforced by the physical mapping of the world, therefore creating a visual separation between \"them\" and \"us\". Geographers are primarily focused on the spaces of colonialism and imperialism; more specifically, the material and symbolic appropriation of space enabling colonialism.\nMaps played an extensive role in colonialism, as Bassett would put it \"by providing geographical information in a convenient and standardised format, cartographers helped open West Africa to European conquest, commerce, and colonisation\". However, because the relationship between colonialism and geography was not scientifically objective, cartography was often manipulated during the colonial era. Social norms and values had an effect on the constructing of maps. During colonialism map-makers used rhetoric in their formation of boundaries and in their art. The rhetoric favoured the view of the conquering Europeans; this is evident in the fact that any map created by a non-European was instantly regarded as inaccurate. Furthermore, European cartographers were required to follow a set of rules which led to ethnocentrism; portraying one's own ethnicity in the centre of the map. As J.B. Harley put it, \"The steps in making a map \u2013 selection, omission, simplification, classification, the creation of hierarchies, and 'symbolisation' \u2013 are all inherently rhetorical.\"\nA common practice by the European cartographers of the time was to map unexplored areas as \"blank spaces\". This influenced the colonial powers as it sparked competition amongst them to explore and colonise these regions. Imperialists aggressively and passionately looked forward to filling these spaces for the glory of their respective countries. The \"Dictionary of Human Geography\" notes that cartography was used to empty 'undiscovered' lands of their Indigenous meaning and bring them into spatial existence via the imposition of \"Western place-names and borders, [therefore] priming 'virgin' (putatively empty land, 'wilderness') for colonisation (thus sexualising colonial landscapes as domains of male penetration), reconfiguring alien space as absolute, quantifiable and separable (as property).\"\nDavid Livingstone stresses \"that geography has meant different things at different times and in different places\" and that we should keep an open mind in regards to the relationship between geography and colonialism instead of identifying boundaries. Geography as a discipline was not and is not an objective science, Painter and Jeffrey argue, rather it is based on assumptions about the physical world. Comparison of exogeographical representations of ostensibly tropical environments in science fiction art support this conjecture, finding the notion of the tropics to be an artificial collection of ideas and beliefs that are independent of geography.\nColonialism and imperialism.\nA colony is a part of an empire and so colonialism is closely related to imperialism. Assumptions are that colonialism and imperialism are interchangeable, however Robert J. C. Young suggests that imperialism is the concept while colonialism is the practice. Colonialism is based on an imperial outlook, thereby creating a consequential relationship. Through an empire, colonialism is established and capitalism is expanded, on the other hand a capitalist economy naturally enforces an empire. In the next section Marxists make a case for this mutually reinforcing relationship.\nMarxist view of colonialism.\nMarxism views colonialism as a form of capitalism, enforcing exploitation and social change. Marx thought that working within the global capitalist system, colonialism is closely associated with uneven development. It is an \"instrument of wholesale destruction, dependency and systematic exploitation producing distorted economies, socio-psychological disorientation, massive poverty and neocolonial dependency\". Colonies are constructed into modes of production. The search for raw materials and the current search for new investment opportunities is a result of inter-capitalist rivalry for capital accumulation. Lenin regarded colonialism as the root cause of imperialism, as imperialism was distinguished by monopoly capitalism via colonialism and as Lyal S. Sunga explains: \"Vladimir Lenin advocated forcefully the principle of self-determination of peoples in his \"Theses on the Socialist Revolution and the Right of Nations to Self-Determination\" as an integral plank in the programme of socialist internationalism\" and he quotes Lenin who contended that \"The right of nations to self-determination implies exclusively the right to independence in the political sense, the right to free political separation from the oppressor nation. Specifically, this demand for political democracy implies complete freedom to agitate for secession and for a referendum on secession by the seceding nation.\" Non Russian marxists within the RSFSR and later the USSR, like Sultan Galiev and Vasyl Shakhrai, meanwhile, between 1918 and 1923 and then after 1929, considered the Soviet Regime a renewed version of the Russian imperialism and colonialism.\nIn his critique of colonialism in Africa, the Guyanese historian and political activist Walter Rodney states:\nAccording to Lenin, the new imperialism emphasised the transition of capitalism from free trade to a stage of monopoly capitalism to finance capital. He states it is, \"connected with the intensification of the struggle for the partition of the world\". As free trade thrives on exports of commodities, monopoly capitalism thrived on the export of capital amassed by profits from banks and industry. This, to Lenin, was the highest stage of capitalism. He goes on to state that this form of capitalism was doomed for war between the capitalists and the exploited nations with the former inevitably losing. War is stated to be the consequence of imperialism. As a continuation of this thought G.N. Uzoigwe states, \"But it is now clear from more serious investigations of African history in this period that imperialism was essentially economic in its fundamental impulses.\"\nLiberalism, capitalism and colonialism.\nClassical liberals were generally in abstract opposition to colonialism and imperialism, including Adam Smith, Fr\u00e9d\u00e9ric Bastiat, Richard Cobden, John Bright, Henry Richard, Herbert Spencer, H.R. Fox Bourne, Edward Morel, Josephine Butler, W.J. Fox and William Ewart Gladstone. Their philosophies found the colonial enterprise, particularly mercantilism, in opposition to the principles of free trade and liberal policies. Adam Smith wrote in \"The Wealth of Nations\" that Britain should grant independence to all of its colonies and also argued that it would be economically beneficial for British people in the average, although the merchants having mercantilist privileges would lose out.\nScientific thought in colonialism, race and gender.\nDuring the colonial era, the global process of colonisation served to spread and synthesize the social and political belief systems of the \"mother-countries\" which often included a belief in a certain natural racial superiority of the race of the mother-country. Colonialism also acted to reinforce these same racial belief systems within the \"mother-countries\" themselves. Usually also included within the colonial belief systems was a certain belief in the inherent superiority of male over female, however this particular belief was often pre-existing amongst the pre-colonial societies, prior to their colonisation.\nPopular political practices of the time reinforced colonial rule by legitimising European (and/ or Japanese) male authority, and also legitimising female and non-mother-country race inferiority through studies of Craniology, Comparative Anatomy, and Phrenology. Biologists, naturalists, anthropologists, and ethnologists of the 19th century were focused on the study of colonised indigenous women, as in the case of Georges Cuvier's study of Sarah Baartman. Such cases embraced a natural superiority and inferiority relationship between the races based on the observations of naturalists' from the mother-countries. European studies along these lines gave rise to the perception that African women's anatomy, and especially genitalia, resembled those of mandrills, baboons, and monkeys, thus differentiating colonised Africans from what were viewed as the features of the evolutionarily superior, and thus rightfully authoritarian, European woman.\nIn addition to what would now be viewed as pseudo-scientific studies of race, which tended to reinforce a belief in an inherent mother-country racial superiority, a new supposedly \"science-based\" ideology concerning gender roles also then emerged as an adjunct to the general body of beliefs of inherent superiority of the colonial era. Female inferiority across all cultures was emerging as an idea supposedly supported by craniology that led scientists to argue that the typical brain size of the female human was, on the average, slightly smaller than that of the male, thus inferring that therefore female humans must be less developed and less evolutionarily advanced than males. This finding of relative cranial size difference was later simply attributed to the general typical size difference of the human male body versus that of the typical human female body.\nWithin the former European colonies, non-Europeans and women sometimes faced invasive studies by the colonial powers in the interest of the then prevailing pro-colonial scientific ideology of the day. Such seemingly flawed studies of race and gender coincided with the era of colonialism and the initial introduction of foreign cultures, appearances, and gender roles into the now gradually widening world-views of the scholars of the mother-countries.\n\"The Other\".\n\"The Other\", or \"othering\", is the process of creating a separate entity to persons or groups who are labelled as different or non-normal due to the repetition of characteristics. Othering is the creation of those who discriminate, to distinguish, label, categorise those who do not fit in the societal norm. Several scholars in recent decades developed the notion of the \"other\" as an epistemological concept in social theory. For example, postcolonial scholars, believed that colonising powers explained an \"other\" who were there to dominate, civilise, and extract resources through colonisation of land.\nPolitical geographers explain how colonial/imperial powers (countries, groups of people etc.) \"othered\" places they wanted to dominate to legalise their exploitation of the land. During and after the rise of colonialism the Western powers perceived the East as the \"other\", being different and separate from their societal norm. This viewpoint and separation of culture had divided the Eastern and Western culture creating a dominant/subordinate dynamic, both being the \"other\" towards themselves.\nPost-colonialism.\nPost-colonialism (or post-colonial theory) can refer to a set of theories in philosophy and literature that grapple with the legacy of colonial rule. In this sense, one can regard post-colonial literature as a branch of postmodern literature concerned with the political and cultural independence of peoples formerly subjugated in colonial empires.\nMany practitioners take Edward Sa\u00efd's book \"Orientalism\" (1978) as the theory's founding work (although French theorists such as Aim\u00e9 C\u00e9saire (1913\u20132008) and Frantz Fanon (1925\u20131961) made similar claims decades before Sa\u00efd). Sa\u00efd analyzed the works of Balzac, Baudelaire and Lautr\u00e9amont, arguing that they helped to shape a societal fantasy of European racial superiority.\nWriters of post-colonial fiction interact with the traditional colonial discourse, but modify or subvert it; for instance by retelling a familiar story from the perspective of an oppressed minor character in the story. Gayatri Chakravorty Spivak's \"Can the Subaltern Speak?\" (1998) gave its name to Subaltern Studies.\nIn \"A Critique of Postcolonial Reason\" (1999), Spivak argued that major works of European metaphysics (such as those of Kant and Hegel) not only tend to exclude the subaltern from their discussions, but actively prevent non-Europeans from occupying positions as fully human subjects. Hegel's \"Phenomenology of Spirit\" (1807), famous for its explicit ethnocentrism, considers Western civilisation as the most accomplished of all, while Kant also had some traces of racialism in his work.\nColonistics.\nThe field of colonistics studies colonialism from such viewpoints as those of economics, sociology and psychology.\nEffects of Colonialism on the Colonisers.\nIn his 1955 essay, \"Discourse on Colonialism\" (French: \"Discours sur le colonialisme\"), French poet Aim\u00e9 C\u00e9saire evaluates the effects of racist, sexist, and capitalist attitudes and motivations on the civilisations that attempted to colonise other civilisations. In explaining his position, he says \"I admit that it is a good thing to place different civilisations in contact with each other that it is an excellent thing to blend different worlds; that whatever its own particular genius may be, a civilisation that withdraws into itself atrophies; that for civilisations, exchange is oxygen.\" However, he contends that colonisation is a harmful and counterproductive means of interacting with and learning from neighbouring civilisations.\nTo illustrate his point, he explains that colonisation relies on racist and xenophobic frameworks that dehumanise the targets of colonisation and justify their extreme and brutal mistreatment. Every time an immoral act perpetrated by colonisers onto the colonised is justified by racist, sexist, otherwise xenophobic, or capitalist motivations to subjugate a group of people, the colonising civilisation \"acquires another dead weight, a universal regression takes place, a gangrene sets in, a centre of infection begins to spread.\" C\u00e9saire argues the result of this process is that \"a poison [is] instilled into the veins of Europe and, slowly but surely, the continent proceeds toward \"savagery\".\" C\u00e9saire is indicating that the racist and xenophobic justifications for colonisation\u2014motivated by capitalist desires\u2014ultimately result in the moral and cultural degradation of the colonising nation. Thusly, colonisation is damaging to the civilisations that participate as perpetrators in a way that is internally harmful.\nBritish public opinion about the British Empire.\nThe 2014 YouGov survey found that British people are mostly proud of colonialism and the British Empire:\nColonial migrations.\nNations and regions outside Europe with significant populations of European ancestry\nNumbers of European settlers in the colonies (1500\u20131914).\nBy 1914, Europeans had migrated to the colonies in the millions. Some intended to remain in the colonies as temporary settlers, mainly as military personnel or on business. Others went to the colonies as immigrants. British people were by far the most numerous population to migrate to the colonies: 2.5\u00a0million settled in Canada; 1.5\u00a0million in Australia; 750,000 in New Zealand; 450,000 in the Union of South Africa; and 200,000 in India. French citizens also migrated in large numbers, mainly to the colonies in the north African Maghreb region: 1.3\u00a0million settled in Algeria; 200,000 in Morocco; 100,000 in Tunisia; while only 20,000 migrated to French Indochina. Dutch and German colonies saw relatively scarce European migration, since Dutch and German colonial expansion focused on commercial goals rather than settlement. Portugal sent 150,000 settlers to Angola, 80,000 to Mozambique, and 20,000 to Goa. During the Spanish Empire, approximately 550,000 Spanish settlers migrated to Latin America."}
{"id": "7300", "revid": "1152308", "url": "https://en.wikipedia.org/wiki?curid=7300", "title": "Colonial", "text": "Colonial or The Colonial may refer to:"}
{"id": "7301", "revid": "17954829", "url": "https://en.wikipedia.org/wiki?curid=7301", "title": "Casablanca", "text": "Casablanca (; ) is the largest city of Morocco. Located in the central-western part of Morocco bordering the Atlantic Ocean, it is the second largest city in the Maghreb region and the eighth-largest in the Arab world. Casablanca is Morocco's chief port and one of the largest financial centers in Africa. According to the 2019 population estimate, the city has a population of about 3.71 million in the urban area and over 4.27 million in the Greater Casablanca. Casablanca is considered the economic and business center of Morocco, although the national political capital is Rabat.\nThe leading Moroccan companies and many international corporations doing business in the country have their headquarters and main industrial facilities in Casablanca. Recent industrial statistics show Casablanca holds its recorded position as the primary industrial zone of the nation. The Port of Casablanca is one of the largest artificial ports in the world, and the second largest port of North Africa, after Tanger-Med east of Tangier. Casablanca also hosts the primary naval base for the Royal Moroccan Navy.\nEtymology.\nThe original name of Casablanca was \"Anfa\" (Neo-Tifinagh: \u2d30\u2d4f\u2d3c\u2d30), in Berber language, by at least the seventh century BC. After the Portuguese took control of the city in the 15th century AD, they rebuilt it, changing the name to \"Casa Branca\" (), meaning 'white house' in Portuguese. The present name, which is the Spanish version (), came when the Kingdom of Portugal came under Spanish control through the Iberian Union. During the French protectorate in Morocco, the name remained Casablanca (). In 1755 an earthquake destroyed most of the town. It was rebuilt by Sultan Mohammed ben Abdallah who changed the name into the local Arabic \"Ad-d\u0101r al-Bayda\"' (\u0627\u0644\u062f\u0627\u0631 \u0627\u0644\u0628\u064a\u0636\u0627\u0621), although occasionally \"Casablanca\" is written in Arabic (\u0643\u0627\u0632\u0627\u0628\u0644\u0627\u0646\u0643\u0627, \"K\u0101z\u0101bl\u0101nk\u0101\"). The city is still nicknamed \"Casa\" by many locals and outsiders to the city. In many other cities with a different dialect, it is called \"Ad-d\u0101r al-Bay\u1e0d\u0101\", instead.\nHistory.\nEarly history.\nThe area which is today Casablanca was founded and settled by Berbers by at least the seventh century BC. It was used as a port by the Phoenicians and later the Romans. In his book Description of Africa, Leo Africanus refers to ancient Casablanca as \"Anfa\", a great city founded in the Berber kingdom of Barghawata in 744\u00a0AD. He believed Anfa was the most \"prosperous city on the Atlantic Coast because of its fertile land.\" Barghawata rose as an independent state around this time, and continued until it was conquered by the Almoravids in 1068. Following the defeat of the Barghawata in the 12th century, Arab tribes of Hilal and Sulaym descent settled in the region, mixing with the local Berbers, which led to widespread Arabization. During the 14th century, under the Merinids, Anfa rose in importance as a port. The last of the Merinids were ousted by a popular revolt in 1465.\nPortuguese conquest and Spanish influence.\nIn the early 15th century, the town became an independent state once again, and emerged as a safe harbour for pirates and privateers, leading to it being targeted by the Portuguese, who bombarded the town which led to its destruction in 1468. The Portuguese used the ruins of Anfa to build a military fortress in 1515. The town that grew up around it was called Casa Branca, meaning \"white house\" in Portuguese.\nBetween 1580 and 1640, the Crown of Portugal was integrated to the Crown of Spain, so Casablanca and all other areas occupied by the Portuguese were under Spanish control, though maintaining an autonomous Portuguese administration. As Portugal broke ties with Spain in 1640, Casablanca came under fully Portuguese control once again. The Europeans eventually abandoned the area completely in 1755 following an earthquake which destroyed most of the town.\nThe town was finally reconstructed by Sultan Mohammed ben Abdallah (1756\u20131790), the grandson of Moulay Ismail and an ally of George Washington, with the help of Spaniards from the nearby emporium. The town was called \"ad-D\u0101r al-Bay\u1e0d\u0101\u02bc\" (\u0627\u0644\u062f\u0627\u0631 \u0627\u0644\u0628\u064a\u0636\u0627\u0621), the Arabic translation of the Portuguese \"Casa Branca\".\nColonial struggle.\nIn the 19th century, the area's population began to grow as it became a major supplier of wool to the booming textile industry in Britain and shipping traffic increased (the British, in return, began importing gunpowder tea, used in Morocco's national drink, mint tea). By the 1860s, around 5,000 residents were there, and the population grew to around 10,000 by the late 1880s. Casablanca remained a modestly sized port, with a population reaching around 12,000 within a few years of the French conquest and arrival of French colonialists in 1906. By 1921, this rose to 110,000, largely through the development of shanty towns.\nFrench rule and influence.\nThe Treaty of Algeciras of 1906 formalized French preeminence in Morocco and included three measures that directly impacted Casablanca: that French officers would control operations at the customs office and seize revenue as collateral for loans given by France, that the French holding company \"La Compagnie Marocaine\" would develop the port of Casablanca, and that a French-and-Spanish-trained police force would be assembled to patrol the port.\nTo build the port's breakwater, narrow-gauge track was laid in June 1907 for a small Decauville locomotive to connect the port to a quarry in Roches Noires, passing through the sacred Sidi Belyout graveyard. In resistance to this and the measures of the 1906 Treaty of Algeciras, tribesmen of the Chaouia attacked the locomotive, killing 9 Compagnie Marocaine laborers\u20143 French, 3 Italians, and 3 Spanish.\nIn response, the French bombarded the city with multiple gunboats and landed troops inside the town, causing severe damage and 15,000 dead and wounded. In the immediate aftermath of the bombardment and the deployment of French troops, the European homes and the \"Mellah\", or Jewish quarter, were sacked, and the latter was also set ablaze.\nThe bombardment and military invasion of the city effectively began the French military conquest of Morocco, although French control of Casablanca was not formalized until the French \"Protectorat\" was established by the Treaty of Fes March 1912.\nGeneral Hubert Lyautey assigned the planning of the new colonial port city to Henri Prost. As he did in other Moroccan cities, Prost designed a European \"ville nouvelle\" outside the walls of the medina. In Casablanca, he also designed a new \"ville indig\u00e8ne\" to house Moroccans arriving from other cities.\nEuropeans formed almost half the population of Casablanca.\nWorld War II.\nAfter Philippe P\u00e9tain of France signed the armistice with the Nazis, he ordered French troops in France's colonial empire to defend French territory against any aggressors\u2014Allied or otherwise\u2014applying a policy of \"asymmetrical neutrality\" in favor of the Germans. French colonists in Morocco generally supported P\u00e9tain, while politically conscious Moroccans tended to favor de Gaulle and the Allies.\nOperation Torch, which started on 8 November 1942, was the British-American invasion of French North Africa during the North African campaign of World War II. The Western Task Force, composed of American units led by Major General George S. Patton and Rear Admiral Henry Kent Hewitt, carried out the invasions of Mehdia, Fedhala, and Asfi. American forces captured Casablanca from Vichy control when France surrendered November 11, 1942, but the Naval Battle of Casablanca continued until American forces sank German submarine U-173 on November 16.\nCasablanca was the site of the Nouasseur Air Base, a large American air base used as the staging area for all American aircraft for the European Theater of Operations during World War II. The air field has since become Mohammed V International Airport.\nAnfa Conference.\nCasablanca hosted the Anfa Conference (also called the Casablanca Conference) in January 1943. Prime Minister Winston Churchill and President Franklin D. Roosevelt discussed the progress of the war. Also in attendance were the Free France generals Charles de Gaulle and Henri Giraud, though they played minor roles and didn't participate in the military planning.\nIt was at this conference that the Allies adopted the doctrine of \"unconditional surrender,\" meaning that the Axis powers would be fought until their defeat. Roosevelt also met privately with Sultan Muhammad V and expressed his support for Moroccan independence after the war. This became a turning point, as Moroccan nationalists were emboldened to openly seek complete independence.\nToward independence.\nDuring the 1940s and 1950s, Casablanca was a major centre of anti-French rioting.\nApril 7, 1947, a massacre of working class Moroccans, carried out by Senegalese Tirailleurs in the service of the French colonial army, was instigated just as Sultan Muhammed V was due to make a speech in Tangier appealing for independence.\nRiots in Casablanca took place from December 7\u20138, 1952, in response to the assassination of the Tunisian labor unionist Farhat Hached by \"La Main Rouge\"\u2014the clandestine militant wing of French intelligence. Then, on 25 December 1953 (Christmas Day), Muhammad Zarqtuni orchestrated a bombing of Casablanca's Central Market in response to the forced exile of Sultan Muhammad V and the royal family on August 20 (Eid al-Adha) of that year.\nSince independence.\nMorocco gained independence from France in 1956.\nCasablanca Group.\nJanuary 4\u20137, 1961, the city hosted an ensemble of progressive African leaders during the Casablanca Conference of 1961. Among those received by King Muhammad V were Gamal Abd An-Nasser, Kwame Nkrumah, Modibo Ke\u00efta, and Ahmed S\u00e9kou Tour\u00e9, Ferhat Abbas.\nJewish emigration.\nCasablanca was a major departure point for Jews leaving Morocco through Operation Yachin, an operation conducted by Mossad to secretly migrate Moroccan Jews to Israel between November 1961 and spring 1964.\n1965 riots.\nThe 1965 student protests organized by the National Union of Popular Forces-affiliated National Union of Moroccan Students, which spread to cities around the country and devolved into riots, started on March 22, 1965, in front of Lyc\u00e9e Mohammed V in Casablanca. The protests started as a peaceful march to demand the right to public higher education for Morocco, but expanded to include concerns of laborers, the unemployed, and other marginalized segments of society, and devolved into vandalism and rioting. The riots were violently repressed by security forces with tanks and armored vehicles; Moroccan authorities reported a dozen deaths while the UNFP reported more than 1,000.\nKing Hassan II blamed the events on teachers and parents, and declared in a speech to the nation on March 30, 1965: \"There is no greater danger to the State than a so-called intellectual. It would have been better if you were all illiterate.\u201d\n1981 riots.\nOn June 6, 1981, the Casablanca Bread Riots took place. Hassan II appointed the French-trained interior minister Driss Basri as hardliner, who would later become a symbol of the Years of Lead, with quelling the protests. The government stated that 66 people were killed and 100 were injured, while opposition leaders put the number of dead at 637, saying that many of these were killed by police and army gunfire.\n\"Mudawana\".\nIn March 2000, more than 60 women's groups organized demonstrations in Casablanca proposing reforms to the legal status of women in the country. About 40,000 women attended, calling for a ban on polygamy and the introduction of divorce law (divorce being a purely religious procedure at that time). Although the counter-demonstration attracted half a million participants, the movement for change started in 2000 was influential on King Mohammed VI, and he enacted a new \"mudawana\", or family law, in early 2004, meeting some of the demands of women's rights activists.\nOn 16 May 2003, 33 civilians were killed and more than 100 people were injured when Casablanca was hit by a multiple suicide bomb attack carried out by Moroccans and claimed by some to have been linked to al-Qaeda. Twelve suicide bombers struck five locations in the city.\nAnother series of suicide bombings struck the city in early 2007. These events illustrated some of the persistent challenges the city faces in addressing poverty and integrating disadvantaged neighborhoods and populations. One initiative to improve conditions in the city's disadvantaged neighborhoods was the creation of the Sidi Moumen Cultural Center.\nAs calls for reform spread through the Arab world in 2011, Moroccans joined in, but concessions by the ruler led to acceptance. However, in December, thousands of people demonstrated in several parts of the city, especially the city center near la Fontaine, desiring more significant political reforms.\nGeography.\nCasablanca is located on the Atlantic coast of the Chaouia Plains, which have historically been the breadbasket of Morocco. Apart from the Atlantic coast, the Bouskoura forest is the only natural attraction in the city. The forest was planted in the 20th century and consists mostly of eucalyptus, palm, and pine trees. It is located halfway to the city's international airport.\nThe only watercourse in Casablanca is \"oued Bouskoura\", a small seasonal creek that until 1912 reached the Atlantic Ocean near the actual port. Most of oued Bouskoura's bed has been covered due to urbanization and only the part south of El Jadida road can now be seen. The closest permanent river to Casablanca is Oum Rabia, to the south-east.\nClimate.\nCasablanca has a hot-summer Mediterranean climate (K\u00f6ppen climate classification \"Csa\"). The cool Canary Current off the Atlantic coast moderates temperature variation, which results in a climate remarkably similar to that of coastal Los Angeles, with similar temperature ranges. The city has an annual average of 72 days with significant precipitation, which amounts to per year. The highest and lowest temperatures ever recorded in the city are and , respectively. The highest amount of rainfall recorded in a single day is on 30 November 2010.\nEconomy.\nThe Grand Casablanca region is considered the locomotive of the development of the Moroccan economy. It attracts 32% of the country's production units and 56% of industrial labor. The region uses 30% of the national electricity production. With MAD 93\u00a0billion, the region contributes to 44% of the industrial production of the kingdom. About 33% of national industrial exports, MAD 27\u00a0billion, comes from the Grand Casablanca; 30% of the Moroccan banking network is concentrated in Casablanca.\nOne of the most important Casablancan exports is phosphate. Other industries include fishing, fish canning, sawmills, furniture production, building materials, glass, textiles, electronics, leather work, processed food, spirits, soft drinks, and cigarettes.\nThe Casablanca and Mohammedia seaports activity represent 50% of the international commercial flows of Morocco. Almost the entire Casablanca waterfront is under development, mainly the construction of huge entertainment centres between the port and Hassan II Mosque, the Anfa Resort project near the business, entertainment and living centre of Megarama, the shopping and entertainment complex of Morocco Mall, as well as a complete renovation of the coastal walkway. The Sindbad park is planned to be totally renewed with rides, games and entertainment services.\nRoyal Air Maroc has its head office at the Casablanca-Anfa Airport. In 2004, it announced that it was moving its head office from Casablanca to a location in Province of Nouaceur, close to Mohammed V International Airport. The agreement to build the head office in Nouaceur was signed in 2009.\nThe biggest CBD of Casablanca and Maghreb is in the North of the town in Sidi Maarouf near the mosque of Hassan II and the biggest project of skycrapers of Maghreb and Africa Casablanca Marina.\nAdministrative divisions.\nCasablanca is a commune, part of the region of Casablanca-Settat. The commune is divided into eight districts or prefectures, which are themselves divided into 16 subdivisions or arrondissements and one municipality. The districts and their subdivisions are:\nNeighborhoods.\nThe list of neighborhoods is indicative and not complete:\nDemographics.\nThe commune of Casablanca recorded a population of 3,359,818 in the 2014 Moroccan census. About 98% live in urban areas. Around 25% of them are under 15 and 9% are over 60 years old. The population of the city is about 11% of the total population of Morocco. Grand Casablanca is also the largest urban area in the Maghreb. 99.9% of the population of Morocco are Arab and Berber Muslims.\nDuring the French protectorate in Morocco, European Christians formed almost half the population of Casablanca. Since independence in 1956, the European population has decreased substantially. The city also is still home to a small community of Moroccan Christians, as well as a small group of foreign Roman Catholic and Protestant residents.\nJudaism in Casablanca.\nJews have a long history in Casablanca. A Sephardic Jewish community was in Anfa up to the destruction of the city by the Portuguese in 1468. Jews were slow to return to the town, but by 1750, the Rabbi Elijah Synagogue was built as the first Jewish synagogue in Casablanca. It was destroyed along with much of the town in the 1755 Lisbon earthquake.\nApproximately 28,000 Moroccan Jews immigrated to the State of Israel between 1948 and 1951, many through Casablanca. Casablanca then became a departure point in Operation Yachin, the covert Mossad-organized migration operation from 1961 to 1964. In 2018 it was estimated that there were only 2,500 Moroccan Jews left in Casablanca, while according to the World Jewish Congress there were only 1,000 Moroccan Jews remaining.\nToday, the Jewish cemetery of Casablanca is one of the major cemeteries of the city, and many synagogues remain in service, but the city's Jewish community has dwindled. The Moroccan Jewish Museum is a museum established in the city in 1997.\nEducation.\nColleges and universities.\nPublic: University of Hassan II Casablanca\nPrivate:\nPrimary and secondary schools.\nInternational schools:\nPlaces of worship.\nMost of the city's places of worship are Muslim mosques. Some of the city's synagogues, such as Ettedgui Synagogue, also remain. There are also Christian churches; some remain in use \u2014 particularly by the West African migrant community \u2014 while many of the churches built during the colonial period have been repurposed, such as Church of the Sacred Heart.\nSports.\nAssociation football.\nCasablanca is home to two popular football clubs: Wydad Casablanca and Raja Casablanca\u2014which are rivals. Raja's symbol is an eagle and Wydad's symbol is a star and crescent, a symbol of Islam. These two popular clubs have produced some of Morocco's best players, such as: Salaheddine Bassir, Abdelmajid Dolmy, Baddou Zaki, Aziz Bouderbala, and Noureddine Naybet. Other football teams on top of these two major teams based in the city of Casablanca include Rachad Bernoussi, TAS de Casablanca, Majd Al Madina, and Racing Casablanca.\nTennis.\nCasablanca hosts The Grand Prix Hassan II, a professional men's tennis tournament of the ATP tour. It first began in 1986, and is played on clay courts type at Complexe Al Amal.\nNotable winners of the Hassan II Grand-Prix are Thomas Muster in 1990, Hicham Arazi in 1997, Younes El Aynaoui in 2002, and Stanislas Wawrinka in 2010.\nHosting.\nCasablanca staged the 1961 Pan Arab Games, the 1983 Mediterranean Games, and games during the 1988 Africa Cup of Nations. Morocco was scheduled to host the 2015 African Nations Cup, but decided to decline due to Ebola fears. Morocco was expelled and the tournament was held in Equatorial Guinea.\nVenues.\nThe Grand Stade de Casablanca is the proposed title of the planned football stadium to be built in the city. Once completed in 2014, it will be used mostly for football matches and will serve as the home of Raja Casablanca, Wydad Casablanca, and the Morocco national football team. The stadium was designed with a capacity of 93,000 spectators, making it one of the highest-capacity stadiums in Africa. Once completed, it will replace the Stade Mohamed V. The initial idea of the stadium was for the 2010 FIFA World Cup, for which Morocco lost their bid to South Africa. Nevertheless, the Moroccan government supported the decision to go ahead with the plans. It will be completed in 2025. The idea of the stadium was also for the 2026 FIFA World Cup, for which Morocco lost their bid to Canada, Mexico and United States. It is now hoping for the 2030 FIFA World Cup which Morocco is co-bidding with either African neighbors Tunisia and Algeria or two European nations Spain and Portugal.\nCulture.\nMusic.\nHaja El Hamdaouia, one of the most iconic figures in aita music, was born in Casablanca. Nass El Ghiwane, led by Larbi Batma, came out of Hay Mohammadi in Casablanca. Naima Samih of Derb Sultan gained prominence through the program \"Mawahib\" (). Abdelhadi Belkhayat and Abdelwahab Doukkali are musicians specializing in traditional Moroccan Arabic popular music. Zina Daoudia, Abdelaziz Stati, Abdellah Daoudi, and Said Senhaji are notable Moroccan chaabi musicians.\nAbdelakabir Faradjallah founded Attarazat Addahabia, a Moroccan funk band, in 1968. Fadoul, another funk band, formed in the 1970s.\nHoba Hoba Spirit also formed in Casablanca, and is still based there. Casablanca has a thriving hiphop scene, with artists such as El Grande Toto, Don Big, 7liwa, and Issam Harris.\nCasablanca hosts numerous music festivals, such as Jazzablanca and L'Boulevard, as well as a museum dedicated to Andalusi music, \"Dar ul-Aala\".\nLiterature.\nThe French writer Antoine de Saint-Exup\u00e9ry is associated with Casablanca.\nDriss Chra\u00efbi's novel \"The Simple Past\" takes place in Casablanca. Mohamed Zafzaf lived in Maarif.\nLamalif, a radical leftist political and cultural magazine, was based in Casablanca.\nCasablanca's International Book Fair is held at the fair grounds opposite Hassan II Mosque annually in February.\nTheater.\nTayeb Saddiki, described as the father of Moroccan theater, grew up in Casablanca and made his career there. Hanane el-Fadili and Hassan El Fad are popular comedians from Casablanca. Gad Elmaleh is another comedian from Casablanca, though he has made his career abroad.\nArt.\nThe \u00c9cole des Beaux-Arts of Casablanca was founded in 1919 by a French Orientalist painter named \u00c9douard Brindeau de Jarny, who started his career teaching drawing at Lyc\u00e9e Lyautey. The Casablanca School\u2014a Modernist art movement and collective including artists such as Farid Belkahia, Mohamed Melihi, and Mohammed Chab\u00e2a\u2014developed out of the \u00c9cole des Beaux-Arts of Casablanca in the late 1960s.\nThe Academy of Traditional Arts, part of the Hassan II Mosque complex, was founded October 31, 2012.\nL'Uzine is a community-based art and culture space in Casablanca.\nRebel Spirit published \"The Casablanca Guide\" (, ) a comic book about life in Casablanca.\nSbagha Bagha is a street art festival during which murals are created on the sides of apartment buildings.\nPhotography.\nPostcard companies such as L\u00e9on &amp; L\u00e9vy were active in Casablanca. Gabriel Veyre also worked and eventually died in Casablanca.\nMarcelin Flandrin (1889-1957), a French military photographer, settled in Casablanca and recorded much of the early colonial period in Morocco with his photography. With his staged nude postcard photos taken in Casablanca's colonial brothel quarter, Flandrin was also responsible for disseminating the orientalist image of Moroccan women as sexual objects.\nCasablanca has a thriving street photography scene. Yoriyas is prominent among photographers capturing the economic capital's street scenes, and has attracted international attention.\nFilm.\nIn the first half of the 20th century, Casablanca had many movie theaters, such as Cinema Rialto, Cinema Lynx and Cinema Vox\u2014the largest in Africa at the time it was built.\nThe 1942 American film \"Casablanca\" is set in Casablanca and has had a lasting impact on the city's image, despite being filmed in the US. \"Salut Casa!\" was a propaganda film brandishing France's purported colonial triumph in its \"mission civilizatric\"e in the city.\n\"Love in Casablanca\" (1991), starring Abdelkarim Derqaoui and Muna Fettou, is one of the first Moroccan films to deal with Morocco's complex realities and depict life in Casablanca with verisimilitude. Nour-Eddine Lakhmari's \"Casanegra\" (2008) depicts the harsh realities of Casablanca's working classes. The films \"Ali Zaoua\" (2000), \"Horses of\" God (2012), and \"Razzia\" (2017) of Nabil Ayouch\u2014a French director of Moroccan heritage\u2014deal with street crime, terrorism, and social issues in Casablanca, respectively. The events in Meryem Benm'Barek-Alo\u00efsi's 2018 film Sofia revolve around an illegitimate pregnancy in Casablanca. Hicham Lasri and Said Naciri also from Casablanca.\nArchitecture.\nCasablanca's architecture and urban development are historically significant. The city is home to many notable buildings in a variety of styles, including traditional Moroccan architecture, various colonial architectural styles, Art Nouveau, Art Deco, Neo-Mauresque, Streamline Moderne, Modernism, Brutalism, and more. During the French Protectorate, the French government described Casablanca as a \"laboratory of urbanism.\"\nThe work of the \"Groupe des Architectes Modernes Marocains\" (GAMMA) on public housing projects\u2014such as Carri\u00e8res Centrales in Hay Mohammadi\u2014in a style described as vernacular modernism influenced modernist architecture around the world.\nCasam\u00e9moire and MAMMA. are two organizations dedicated to the preservation and appreciation of the city's architectural heritage.\nTransport.\nRapid transit.\nThe Casablanca Tramway is the rapid transit tram system in Casablanca. As of 2019, the network consists of two lines covering , with 71 stops; further lines (T3 and T4) are under construction.\nSince the 1970s, Casablanca had planned to build a metro system to offer some relief to the problems of traffic congestion and poor air quality. However, the city council voted to abandon the metro project in 2014 due to high costs, and decided to continue expanding the already operating tram system instead.\nAir.\nCasablanca's main airport is Mohammed V International Airport, Morocco's busiest airport. Regular domestic flights serve Marrakech, Rabat, Agadir, Oujda, Tangier, Al Hoceima, and Laayoune, as well as other cities.\nCasablanca is well-served by international flights to Europe, especially French and Spanish airports, and has regular connections to North American, Middle Eastern and sub-Saharan African destinations. New York City, Montreal, Paris, Washington D.C., London and Dubai are important primary destinations.\nThe older, smaller Casablanca-Anfa Airport to the west of the city, served certain destinations including Damascus, and Tunis, and was largely closed to international civilian traffic in 2006. It has been closed and destroyed to build the \"Casablanca Finance City\", the new heart of the city of Casablanca. Casablanca Tit Mellil Airport is located in the nearby community of Tit Mellil.\nCoach buses.\n\"Compagnie de Transports au Maroc\" (CTM) offers private intercity coach buses on various lines run servicing most notable Moroccan towns, as well as a number of European cities. These run from the CTM Bus Station on Leo Africanus Street near the Central Market in downtown Casablanca. Supratours, an affiliate of ONCF, also offers coach bus service at a slightly lower cost, departing from a station on Wilad Zian Street. There is another bus station farther down on the same street called the Wilad Zian Bus Station; this station is the country's largest bus station, serving over 800 buses daily, catering more to Morocco's lower income population.\nTaxis.\nRegistered taxis in Casablanca are coloured red and known as \"petit taxis\" (small taxis), or coloured white and known as \"grands taxis\" (big taxis). As is standard Moroccan practice, \"petits taxis,\" typically small-four door Dacia Logan, Peugeot 207, or similar cars, provide metered cab service in the central metropolitan areas. \"Grands taxis,\" generally older Mercedes-Benz sedans, provide shared mini-bus like service within the city on predefined routes, or shared intercity service. \"Grands taxis\" may also be hired for private service by the hour or day.\nTrains.\nCasablanca is served by three main railway stations run by the national rail service, the ONCF.\n is the main intercity station, from which trains run south to Marrakech or El Jadida and north to Mohammedia and Rabat, and then on either to Tangier or Meknes, Fes, Taza and Oujda/Nador. It also serves as the southern terminus of the Al-Boraq high speed line from Tangier. A dedicated airport shuttle service to Mohammed V International Airport also has its primary in-city stop at this station, for connections on to further destinations.\n serves primarily commuter trains such as the Train Navette Rapide (TNR or Aouita) operating on the Casablanca \u2013 Kenitra rail corridor, with some connecting trains running on to Gare de Casa-Voyageurs. The station provides a direct interchange between train and shipping services, and is located near several port-area hotels. It is the nearest station to the old town of Casablanca, and to the modern city centre, around the landmark Casablanca Twin Center. Casa-Port station is being rebuilt in a modern and enlarged configuration. During the construction, the station is still operational. From 2013, it will provide a close connection from the rail network to the city's new tram network.\nCasa-Oasis was originally a suburban commuter station which was fully redesigned and rebuilt in the early 21st century, and officially reopened in 2005 as a primary city rail station. Owing to its new status, all southern intercity train services to and from Casa-Voyageurs now call at Casa-Oasis. ONCF stated in 2005 that the refurbishment and upgrading of Casa-Oasis to intercity standards was intended to relieve passenger congestion at Casa-Voyageurs station.\nTourism.\nAlthough Mohammed V International Airport receives most international flights into Morocco, international tourism in Casablanca is not as developed as it is in cities like Marrakesh. Casablanca, however, attracts fewer tourists than those of cities such as Fes and Marrakech.\nThe Hassan II Mosque, which is the second largest mosque in Africa and the seventh largest in the world, is the city's main tourist attraction. Visitors also come to see the city's rich architectural heritage.\nPopular sites for national tourism include shopping centers such as Morocco Mall, Anfa Place, the Marina Shopping Center, and the Tachfine Center. Additional sites include the Corniche and the beach of Ain Diab, and parks such as the Arab League Park or the Sindibad theme park.\nTwin towns \u2013 sister cities.\nCasablanca is twinned with:"}
{"id": "7303", "revid": "32695559", "url": "https://en.wikipedia.org/wiki?curid=7303", "title": "Cross", "text": "A cross is a geometrical figure consisting of two intersecting lines or bars, usually perpendicular to each other. The lines usually run vertically and horizontally. \nA cross of oblique lines, in the shape of the Latin letter X, is also termed a saltire in heraldic terminology.\nThroughout centuries the cross in its various shapes and forms was a symbol of various beliefs. In pre-Christian times it was a pagan religious symbol throughout Europe and western Asia. In ancient times, the effigy of a man hanging on a cross was set up in the fields to protect the crops. The cross was even considered a male symbol of the phallic Tree of Life; thus it often appeared in conjunction with the female-genital circle or oval, to signify the sacred marriage, as in Egyptian amulet Nefer with male cross and female orb, considered as an amulet of blessedness, a charm of sexual harmony.\nName.\nThe word \"cross\" is recorded in 10th-century Old English as \"cros\", exclusively for the instrument of Christ's crucifixion, replacing the native Old English word \"rood\". The word's history is complicated; it appears to have entered English from Old Irish, possibly via Old Norse, ultimately from the Latin (or its accusative and its genitive ), \"stake, cross\". The English verb \"to cross\" arises from the noun , first in the sense \"to make the sign of the cross\"; the generic meaning \"to intersect\" develops in the 15th century.\nThe Latin word was, however, influenced by popular etymology by a native Germanic word reconstructed as *\"krukjo\" (English \"crook\", Old English , Old Norse , Old High German ). This word, by conflation with Latin , gave rise to Old French (modern French ), the term for a shepherd's crook, adopted in English as \"crosier\".\nLatin referred to the gibbet where criminals were executed, a stake or pole, with or without , on which the condemned were impaled or hanged, but more particularly a cross or the pole of a carriage. The derived verb means \"to put to death on the cross\" or, more frequently, \"to put to the rack, to torture, torment\", especially in reference to mental troubles. \n In the Roman world, replaced as the name of some cross-like instruments for lethal and temporary punishment, ranging from a forked cross to a gibbet or gallows.\nThe field of etymology is of no help in any effort to trace a supposed original meaning of \"crux\". A \"crux\" can be of various shapes: from a single beam used for impaling or suspending () to the various composite kinds of cross () made from more beams than one. The latter shapes include not only the traditional \u2020-shaped cross (the ), but also the T-shaped cross (the or tau cross), which the descriptions in antiquity of the execution cross indicate as the normal form in use at that time, and the X-shaped cross (the \"crux decussata\" or saltire).\nThe Greek equivalent of Latin \"crux\" \"stake, gibbet\" is , found in texts of four centuries or more before the gospels and always in the plural number to indicate a stake or pole. From the first century BC, it is used to indicate an instrument used in executions. The Greek word is used in descriptions in antiquity of the execution cross, which indicate that its normal shape was similar to the Greek letter tau (\u03a4).\nHistory.\nPre-Christian.\nDue to the simplicity of the design (two intersecting lines), cross-shaped incisions make their appearance from deep prehistory; as petroglyphs in European cult caves, dating back to the beginning of the Upper Paleolithic, and throughout prehistory to the Iron Age.\nAlso of prehistoric age are numerous variants of the simple cross mark, including the \"crux gammata\" with curving or angular lines, and the Egyptian \"crux ansata\" with a loop.\nSpeculation has associated the cross symbol \u2013 even in the prehistoric period \u2013 with astronomical or cosmological symbology involving \n\"four elements\" (Chevalier, 1997) or the cardinal points, or the unity of a vertical axis mundi or celestial pole with the horizontal world (Koch, 1955). Speculation of this kind became especially popular in the mid- to late-19th century in the context of comparative mythology seeking to tie Christian mythology to ancient cosmological myths. Influential works in this vein included \nG. de Mortillet (1866), L. M\u00fcller (1865), W. W. Blake (1888), Ansault (1891), etc.\nIn the European Bronze Age the cross symbol appeared to carry a religious meaning, perhaps as a symbol of consecration, especially pertaining to burial.\nThe cross sign occurs trivially in tally marks, and develops into a number symbol independently in the Roman numerals (X \"ten\"), the Chinese rod numerals (\u5341 \"ten\") and the Brahmi numerals (\"four\", whence the numeral 4).\nIn the Phoenician alphabet and derived scripts, the cross symbol represented the phoneme /t/, i.e. the letter taw, which is the historical predecessor of Latin T. The letter name \"taw\" means \"mark\", presumably continuing the Egyptian hieroglyph \"two crossed sticks\" (Gardiner Z9).\nAccording to W. E. Vine's \"Expository Dictionary of New Testament Words\", worshippers of Tammuz in Chaldea and thereabouts used the cross as symbol of that god.\nChristian cross.\nThe shape of the cross (\"crux\", \"stauros\" \"stake, gibbet\"), as represented by the letter T, came to be used as a \"seal\" or symbol of Early Christianity by the 2nd century. Clement of Alexandria in the early 3rd century calls it (\"the Lord's sign\") he repeats the idea, current as early as the Epistle of Barnabas, that the number 318 (in Greek numerals, \u03a4\u0399\u0397) in Genesis 14:14 was a foreshadowing (a \"type\") of the cross (the letter Tau) and of Jesus (the letters Iota Eta). Clement's contemporary Tertullian rejects the accusation that Christians are \"crucis religiosi\" (i.e. \"adorers of the gibbet\"), and returns the accusation by likening the worship of pagan idols to the worship of poles or stakes. \nIn his book \"De Corona\", written in 204, Tertullian tells how it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross.\nWhile early Christians used the T-shape to represent the cross in writing and gesture, the use of the Greek cross and Latin cross, i.e. crosses with intersecting beams, appears in Christian art towards the end of Late Antiquity. An early example of the cruciform halo, used to identify Christ in paintings, is found in the \"Miracles of the Loaves and Fishes\" mosaic of Sant'Apollinare Nuovo, Ravenna (6th century). \nThe Patriarchal cross, a Latin cross with an additional horizontal bar, first appears in the 10th century.\nA wide variation of cross symbols is introduced for the purposes of heraldry beginning in the age of the Crusades.\nCross-like marks and graphemes.\nThe cross mark is used to mark a position, or as a check mark, but also to mark deletion.\nDerived from Greek Chi are the Latin letter X, Cyrillic Kha and possibly runic Gyfu.\nEgyptian hieroglyphs involving cross shapes include \"ankh\" \"life\", \"ndj\" \"protect\" and \"nfr\" \"good; pleasant, beautiful\".\nSumerian cuneiform had a simple cross-shaped character, consisting of a horizontal and a vertical wedge (\ud808\ude26), read as \"ma\u0161\" \"tax, yield, interest\"; the superposition of two diagonal wedges results in a decussate cross (\ud808\ude7d), read as \"pap\" \"first, pre-eminent\" (the superposition of these two types of crosses results in the eight-pointed star used as the sign for \"sky\" or \"deity\" (\ud808\udc2d), DINGIR).\nThe cuneiform script has other, more complex, cruciform characters, consisting of an arrangement of boxes or the fourfold arrangement of other characters, including the archaic cuneiform characters LAK-210, LAK-276, LAK-278, LAK-617 and the classical sign EZEN (\ud808\udca1).\nPhoenician \"t\u0101w\" is still cross-shaped in Paleo-Hebrew alphabet and in some Old Italic scripts (Raetic and Lepontic), and its descendant T becomes again cross-shaped in the Latin minuscule t. \nThe plus sign (+) is derived from Latin t via a simplification of a ligature for \"et\" \"and\" (introduced by Johannes Widmann in the late 15th century).\nThe letter Aleph is cross-shaped in Aramaic and paleo-Hebrew.\nEgyptian hieroglyphs with cross-shapes include Gardiner Z9 \u2013 Z11 (\"crossed sticks\", \"crossed planks\").\nOther, unrelated cross-shaped letters include Brahmi \"ka\" (predecessor of the Devanagari letter \u0915) and Old Turkic (Orkhon) \"d\u00b2\" and Old Hungarian \"b\", and Katakana \u30ca \"na\" and \u30e1\"me\".\nThe multiplication sign (\u00d7), often attributed to William Oughtred (who first used it in an appendix to the 1618 edition of John Napier's \"Descriptio\") apparently had been in occasional use since the mid 16th century.\nOther typographical symbols resembling crosses include the dagger or \"obelus\" (\u2020), \nthe Chinese (\u5341, Kangxi radical 24) and Roman (X ten).\nUnicode has a variety of cross symbols in the \"Dingbat\" block (U+2700\u2013U+27BF) : \nThe Miscellaneous Symbols block (U+2626 to U+262F) adds three specific Christian cross variants, viz. the Patriarchal cross (\u2626), \nCross of Lorraine (\u2628) and \"Cross of Jerusalem\" (implemented as Cross potent, \u2629).\nCross-like emblems.\nThe following is a list of cross symbols, \"except\" for variants of the Christian cross and Heraldic crosses, for which see the dedicated lists at Christian cross variants and Crosses in heraldry, respectively.\nPhysical gestures.\nCross shapes are made by a variety of physical gestures. Crossing the fingers of one hand is a common invocation of the symbol. The sign of the cross associated with Christian genuflection is made with one hand: in Eastern Orthodox tradition the sequence is head-heart-right shoulder-left shoulder, while in Oriental Orthodox, Catholic and Anglican tradition the sequence is head-heart-left-right.\nCrossing the index fingers of both hands represents and a charm against evil in European folklore. Other gestures involving more than one hand include the \"cross my heart\" movement associated with making a promise and the Tau shape of the referee's \"time out\" hand signal.\nIn Chinese-speaking cultures, crossed index fingers represent the number 10."}
{"id": "7304", "revid": "5641591", "url": "https://en.wikipedia.org/wiki?curid=7304", "title": "Coordination complex", "text": "A coordination complex consists of a central atom or ion, which is usually metallic and is called the \"coordination centre\", and a surrounding array of bound molecules or ions, that are in turn known as \"ligands\" or complexing agents. Many metal-containing compounds, especially those of transition metals (d block elements), are coordination complexes. \nNomenclature and terminology.\nCoordination complexes are so pervasive that their structures and reactions are described in many ways, sometimes confusingly. The atom within a ligand that is bonded to the central metal atom or ion is called the donor atom. In a typical complex, a metal ion is bonded to several donor atoms, which can be the same or different. A polydentate (multiple bonded) ligand is a molecule or ion that bonds to the central atom through several of the ligand's atoms; ligands with 2, 3, 4 or even 6 bonds to the central atom are common. These complexes are called chelate complexes; the formation of such complexes is called chelation, complexation, and coordination.\nThe central atom or ion, together with all ligands, comprise the coordination sphere. The central atoms or ion and the donor atoms comprise the first coordination sphere.\nCoordination refers to the \"coordinate covalent bonds\" (dipolar bonds) between the ligands and the central atom. Originally, a complex implied a reversible association of molecules, atoms, or ions through such weak chemical bonds. As applied to coordination chemistry, this meaning has evolved. Some metal complexes are formed virtually irreversibly and many are bound together by bonds that are quite strong.\nThe number of donor atoms attached to the central atom or ion is called the coordination number. The most common coordination numbers are 2, 4, and especially 6. A hydrated ion is one kind of a complex ion (or simply a complex), a species formed between a central metal ion and one or more surrounding ligands, molecules or ions that contain at least one lone pair of electrons.\nIf all the ligands are monodentate, then the number of donor atoms equals the number of ligands. For example, the cobalt(II) hexahydrate ion or the hexaaquacobalt(II) ion\u00a0[Co(H2O)6]2+ is a hydrated-complex ion that consists of six water molecules attached to a metal ion\u00a0Co. The oxidation state and the coordination number reflect the number of bonds formed between the metal ion and the ligands in the complex ion. However, the coordination number of\u00a0Pt(en) is 4 (rather than 2) since it has two bidentate ligands, which contain four donor atoms in total.\nAny donor atom will give a pair of electrons. There are some donor atoms or groups which can offer more than one pair of electrons. Such are called bidentate (offers two pairs of electrons) or polydentate (offers more than two pairs of electrons). In some cases an atom or a group offers a pair of electrons to two similar or different central metal atoms or acceptors\u2014by division of the electron pair\u2014into a three-center two-electron bond. These are called bridging ligands.\nHistory.\nCoordination complexes have been known since the beginning of modern chemistry. Early well-known coordination complexes include dyes such as Prussian blue. Their properties were first well understood in the late 1800s, following the 1869 work of Christian Wilhelm Blomstrand. Blomstrand developed what has come to be known as the \"complex ion chain theory.\" The theory claimed that the reason coordination complexes form is because in solution, ions would be bound via ammonia chains. He compared this effect to the way that various carbohydrate chains form.\nFollowing this theory, Danish scientist Sophus Mads J\u00f8rgensen made improvements to it. In his version of the theory, J\u00f8rgensen claimed that when a molecule dissociates in a solution there were two possible outcomes: the ions would bind via the ammonia chains Blomstrand had described or the ions would bind directly to the metal.\nIt was not until 1893 that the most widely accepted version of the theory today was published by Alfred Werner. Werner's work included two important changes to the Blomstrand theory. The first was that Werner described the two possibilities in terms of location in the coordination sphere. He claimed that if the ions were to form a chain, this would occur outside of the coordination sphere while the ions that bound directly to the metal would do so within the coordination sphere. In one of his most important discoveries however Werner disproved the majority of the chain theory. Werner discovered the spatial arrangements of the ligands that were involved in the formation of the complex hexacoordinate cobalt. His theory allows one to understand the difference between a coordinated ligand and a charge balancing ion in a compound, for example the chloride ion in the cobaltammine chlorides and to explain many of the previously inexplicable isomers.\nIn 1911, Werner first resolved the coordination complex hexol into optical isomers, overthrowing the theory that only carbon compounds could possess chirality.\nStructures.\nThe ions or molecules surrounding the central atom are called ligands. Ligands are classified as L or X (or a combination thereof), depending on how many electrons they provide for the bond between ligand and central atom. L ligands provide two electrons from a lone electron pair, resulting in a coordinate covalent bond. X ligands provide one electron, with the central atom providing the other electron, thus forming a regular covalent bond. The ligands are said to be coordinated to the atom. For alkenes, the pi bonds can coordinate to metal atoms. An example is ethylene in the complex [PtCl3(C2H4)]\u2212.\nGeometry.\nIn coordination chemistry, a structure is first described by its coordination number, the number of ligands attached to the metal (more specifically, the number of donor atoms). Usually one can count the ligands attached, but sometimes even the counting can become ambiguous. Coordination numbers are normally between two and nine, but large numbers of ligands are not uncommon for the lanthanides and actinides. The number of bonds depends on the size, charge, and electron configuration of the metal ion and the ligands. Metal ions may have more than one coordination number.\nTypically the chemistry of transition metal complexes is dominated by interactions between s and p molecular orbitals of the donor-atoms in the ligands and the d orbitals of the metal ions. The s, p, and d orbitals of the metal can accommodate 18 electrons (see 18-Electron rule). The maximum coordination number for a certain metal is thus related to the electronic configuration of the metal ion (to be more specific, the number of empty orbitals) and to the ratio of the size of the ligands and the metal ion. Large metals and small ligands lead to high coordination numbers, e.g. [Mo(CN)8]4\u2212. Small metals with large ligands lead to low coordination numbers, e.g. Pt[P(CMe3)]2. Due to their large size, lanthanides, actinides, and early transition metals tend to have high coordination numbers.\nMost structures follow the points-on-a-sphere pattern (or, as if the central atom were in the middle of a polyhedron where the corners of that shape are the locations of the ligands), where orbital overlap (between ligand and metal orbitals) and ligand-ligand repulsions tend to lead to certain regular geometries. The most observed geometries are listed below, but there are many cases that deviate from a regular geometry, e.g. due to the use of ligands of diverse types (which results in irregular bond lengths; the coordination atoms do not follow a points-on-a-sphere pattern), due to the size of ligands, or due to electronic effects (see, e.g., Jahn\u2013Teller distortion):\nThe idealized descriptions of 5-, 7-, 8-, and 9- coordination are often indistinct geometrically from alternative structures with slightly differing L-M-L (ligand-metal-ligand) angles, e.g. the difference between square pyramidal and trigonal bipyramidal structures.\nIn systems with low d electron count, due to special electronic effects such as (second-order) Jahn\u2013Teller stabilization, certain geometries (in which the coordination atoms do not follow a points-on-a-sphere pattern) are stabilized relative to the other possibilities, e.g. for some compounds the trigonal prismatic geometry is stabilized relative to octahedral structures for six-coordination.\nIsomerism.\nThe arrangement of the ligands is fixed for a given complex, but in some cases it is mutable by a reaction that forms another stable isomer.\nThere exist many kinds of isomerism in coordination complexes, just as in many other compounds.\nStereoisomerism.\nStereoisomerism occurs with the same bonds in distinct orientations. Stereoisomerism can be further classified into:\nCis\u2013trans isomerism and facial\u2013meridional isomerism.\nCis\u2013trans isomerism occurs in octahedral and square planar complexes (but not tetrahedral). When two ligands are adjacent they are said to be cis, when\nopposite each other, trans. When three identical ligands occupy one face of an octahedron, the isomer is said to be facial, or fac. In a \"fac\" isomer, any two identical ligands are adjacent or \"cis\" to each other. If these three ligands and the metal ion are in one plane, the isomer is said to be meridional, or mer. A \"mer\" isomer can be considered as a combination of a \"trans\" and a \"cis\", since it contains both trans and cis pairs of identical ligands.\nOptical isomerism.\nOptical isomerism occurs when a complex is not superimposable with its mirror image. It is so called because the two isomers are each optically active, that is, they rotate the plane of polarized light in opposite directions. In the first molecule shown, the symbol \u039b (\"lambda\") is used as a prefix to describe the left-handed propeller twist formed by three bidentate ligands. The second molecule is the mirror image of the first, with the symbol \u0394 (\"delta\") as a prefix for the right-handed propeller twist. The third and fourth molecules are a similar pair of \u039b and \u0394 isomers, in this case with two bidentate ligands and two identical monodentate ligands.\nStructural isomerism.\nStructural isomerism occurs when the bonds are themselves different. Four types of structural isomerism are recognized: ionisation isomerism, solvate or hydrate isomerism, linkage isomerism and coordination isomerism.\nElectronic properties.\nMany of the properties of transition metal complexes are dictated by their electronic structures. The electronic structure can be described by a relatively ionic model that ascribes formal charges to the metals and ligands. This approach is the essence of crystal field theory (CFT). Crystal field theory, introduced by Hans Bethe in 1929, gives a quantum mechanically based attempt at understanding complexes. But crystal field theory treats all interactions in a complex as ionic and assumes that the ligands can be approximated by negative point charges.\nMore sophisticated models embrace covalency, and this approach is described by ligand field theory (LFT) and Molecular orbital theory (MO). Ligand field theory, introduced in 1935 and built from molecular orbital theory, can handle a broader range of complexes and can explain complexes in which the interactions are covalent. The chemical applications of group theory can aid in the understanding of crystal or ligand field theory, by allowing simple, symmetry based solutions to the formal equations.\nChemists tend to employ the simplest model required to predict the properties of interest; for this reason, CFT has been a favorite for the discussions when possible. MO and LF theories are more complicated, but provide a more realistic perspective.\nThe electronic configuration of the complexes gives them some important properties:\nColor of transition metal complexes.\nTransition metal complexes often have spectacular colors caused by electronic transitions by the absorption of light. For this reason they are often applied as pigments. Most transitions that are related to colored metal complexes are either d\u2013d transitions or charge transfer bands. In a d\u2013d transition, an electron in a d\u00a0orbital on the metal is excited by a photon to another d orbital of higher energy, therefore d\u2013d transitions occur only for partially-filled d-orbital complexes (d1\u20139). For complexes having d0 or d10 configuration, charge transfer is still possible even though d\u2013d transitions are not. A charge transfer band entails promotion of an electron from a metal-based orbital into an empty ligand-based orbital (metal-to-ligand charge transfer or MLCT). The converse also occurs: excitation of an electron in a ligand-based orbital into an empty metal-based orbital (ligand-to-metal charge transfer or LMCT). These phenomena can be observed with the aid of electronic spectroscopy; also known as UV-Vis. For simple compounds with high symmetry, the d\u2013d transitions can be assigned using Tanabe\u2013Sugano diagrams. These assignments are gaining increased support with computational chemistry.\nColors of lanthanide complexes.\nSuperficially lanthanide complexes are similar to those of the transition metals in that some are colored. However, for the common Ln3+ ions (Ln = lanthanide) the colors are all pale, and hardly influenced by the nature of the ligand. The colors are due to 4f electron transitions. As the 4f orbitals in lanthanides are \"buried\" in the xenon core and shielded from the ligand by the 5s and 5p orbitals they are therefore not influenced by the ligands to any great extent leading to a much smaller crystal field splitting than in the transition metals. The absorption spectra of an Ln3+ ion approximates to that of the free ion where the electronic states are described by spin-orbit coupling. This contrasts to the transition metals where the ground state is split by the crystal field. Absorptions for Ln3+ are weak as electric dipole transitions are parity forbidden (Laporte forbidden) but can gain intensity due to the effect of a low-symmetry ligand field or mixing with higher electronic states (\"e.g.\" d orbitals). f-f absorption bands are extremely sharp which contrasts with those observed for transition metals which generally have broad bands. This can lead to extremely unusual effects, such as significant color changes under different forms of lighting.\nMagnetism.\nMetal complexes that have unpaired electrons are magnetic. Considering only monometallic complexes, unpaired electrons arise because the complex has an odd number of electrons or because electron pairing is destabilized. Thus, monomeric Ti(III) species have one \"d-electron\" and must be (para)magnetic, regardless of the geometry or the nature of the ligands. Ti(II), with two d-electrons, forms some complexes that have two unpaired electrons and others with none. This effect is illustrated by the compounds TiX2[(CH3)2PCH2CH2P(CH3)2]2: when X\u00a0=\u00a0Cl, the complex is paramagnetic (high-spin configuration), whereas when X\u00a0=\u00a0CH3, it is diamagnetic (low-spin configuration). It is important to realize that ligands provide an important means of adjusting the ground state properties.\nIn bi- and polymetallic complexes, in which the individual centres have an odd number of electrons or that are high-spin, the situation is more complicated. If there is interaction (either direct or through ligand) between the two (or more) metal centres, the electrons may couple (antiferromagnetic coupling, resulting in a diamagnetic compound), or they may enhance each other (ferromagnetic coupling). When there is no interaction, the two (or more) individual metal centers behave as if in two separate molecules.\nReactivity.\nComplexes show a variety of possible reactivities:\nIf the ligands around the metal are carefully chosen, the metal can aid in (stoichiometric or catalytic) transformations of molecules or be used as a sensor.\nClassification.\nMetal complexes, also known as coordination compounds, include virtually all metal compounds. The study of \"coordination chemistry\" is the study of \"inorganic chemistry\" of all alkali and alkaline earth metals, transition metals, lanthanides, actinides, and metalloids. Thus, coordination chemistry is the chemistry of the majority of the periodic table. Metals and metal ions exist, in the condensed phases at least, only surrounded by ligands.\nThe areas of coordination chemistry can be classified according to the nature of the ligands, in broad terms:\nMineralogy, materials science, and solid state chemistry\u00a0\u2013 as they apply to metal ions\u00a0\u2013 are subsets of coordination chemistry in the sense that the metals are surrounded by ligands. In many cases these ligands are oxides or sulfides, but the metals are coordinated nonetheless, and the principles and guidelines discussed below apply. In hydrates, at least some of the ligands are water molecules. It is true that the focus of mineralogy, materials science, and solid state chemistry differs from the usual focus of coordination or inorganic chemistry. The former are concerned primarily with polymeric structures, properties arising from a collective effects of many highly interconnected metals. In contrast, coordination chemistry focuses on reactivity and properties of complexes containing individual metal atoms or small ensembles of metal atoms.\nNomenclature of coordination complexes.\nThe basic procedure for naming a complex is:\nExamples:\nThe coordination number of ligands attached to more than one metal (bridging ligands) is indicated by a subscript to the Greek symbol \u03bc placed before the ligand name. Thus the dimer of aluminium trichloride is described by Al2Cl4(\u03bc2-Cl)2.\nAny anionic group can be electronically stabilized by any cation. An anionic complex can be stabilised by a hydrogen cation, becoming an acidic complex which can dissociate to release the cationic hydrogen. This kind of complex compound has a name with \"ic\" added after the central metal. For example, H2[Pt(CN)4] has the name tetracyanoplatinic (II) acid.\nStability constant.\nThe affinity of metal ions for ligands is described by a stability constant, also called the formation constant, and is represented by the symbol Kf. It is the equilibrium constant for its assembly from the constituent metal and ligands, and can be calculated accordingly, as in the following example for a simple case:\nwhere X, Y, and Z are the stoichiometric coefficients of each species. Formation constants vary widely. Large values indicate that the metal has high affinity for the ligand, provided the system is at equilibrium.\nSometimes the stability constant will be in a different form known as the constant of destability. This constant is expressed as the inverse of the constant of formation and is denoted as Kd = 1/Kf . This constant represents the reverse reaction for the decomposition of a complex ion into its individual metal and ligand components. When comparing the values for Kd, the larger the value, the more unstable the complex ion is.\nAs a result of these complex ions forming in solutions they also can play a key role in solubility of other compounds. When a complex ion is formed it can alter the concentrations of its components in the solution. For example:\nIf these reactions both occurred in the same reaction vessel, the solubility of the silver chloride would be increased by the presence of NH4OH because formation of the Diammine argentum(I) complex consumes a significant portion of the free silver ions from the solution. By Le Chatelier's principle, this causes the equilibrium reaction for the dissolving of the silver chloride, which has silver ion as a product, to shift to the right.\nThis new solubility can be calculated given the values of Kf and Ksp for the original reactions. The solubility is found essentially by combining the two separate equilibria into one combined equilibrium reaction and this combined reaction is the one that determines the new solubility. So Kc, the new solubility constant, is denoted by:\nApplication of coordination compounds.\nMetals only exist in solution as coordination complexes, it follows then that this class of compounds is useful in a wide variety of ways.\nBioinorganic chemistry.\nIn bioinorganic chemistry and bioorganometallic chemistry, coordination complexes serve either structural or catalytic functions. An estimated 30% of proteins contain metal ions. Examples include the intensely colored vitamin B12, the heme group in hemoglobin, the cytochromes, the chlorin group in chlorophyll, and carboxypeptidase, a hydrolytic enzyme important in digestion. Another complex ion enzyme is catalase, which decomposes the cell's waste hydrogen peroxide. Synthetic coordination compounds are also used to bind to proteins and especially nucleic acids (e.g. anticancer drug cisplatin).\nIndustry.\nHomogeneous catalysis is a major application of coordination compounds for the production of organic substances. Processes include hydrogenation, hydroformylation, oxidation. In one example, a combination of titanium trichloride and triethylaluminium gives rise to Ziegler\u2013Natta catalysts, used for the polymerization of ethylene and propylene to give polymers of great commercial importance as fibers, films, and plastics.\nNickel, cobalt, and copper can be extracted using hydrometallurgical processes involving complex ions. They are extracted from their ores as ammine complexes. Metals can also be separated using the selective precipitation and solubility of complex ions. Cyanide is used chiefly for extraction of gold and silver from their ores.\nPhthalocyanine complexes are an important class of pigments.\nAnalysis.\nAt one time, coordination compounds were used to identify the presence of metals in a sample. Qualitative inorganic analysis has largely been superseded by instrumental methods of analysis such as atomic absorption spectroscopy (AAS), inductively coupled plasma atomic emission spectroscopy (ICP-AES) and inductively coupled plasma mass spectrometry (ICP-MS)."}
{"id": "7305", "revid": "1008123189", "url": "https://en.wikipedia.org/wiki?curid=7305", "title": "Coleco", "text": "Coleco Industries, Inc. was an American company founded in 1932 by Maurice Greenberg as The Connecticut Leather Company. It became a highly successful toy company in the 1980s, known for its mass-produced version of Cabbage Patch Kids dolls and its video game consoles, the Coleco Telstar dedicated consoles and ColecoVision. While the company disappeared in 1988 as a result of bankruptcy, the Coleco brand was revived in 2005, and remains active to this day.\nOverview.\nColeco Industries, Inc. began in 1932 as The Connecticut Leather Company. The business supplied leather and \"shoe findings\" to shoe repairers. Shoe findings are the supplies and paraphernalia of a shoe repair shop. The company later (1938) branched out to selling rubber footwear. With the advent of World War II the demand for the company's supplies increased. By the end of the war the company was larger and had branched out into new and used shoe machinery, hat cleaning equipment and even marble shoeshine stands.\nBy the early 1950s, and thanks to Maurice Greenberg's son, Leonard Greenberg, the company had diversified further and was making leather lacing and leathercraft kits. In 1954, at the New York Toy Fair, the leather moccasin kit was selected as a Child Guidance Prestige Toy, and Connecticut Leather Company decided to go wholeheartedly into the toy business. In 1956, Leonard read of an emerging technology, the vacuum forming of plastic, which led the company to become very successful, producing an enormous array of different plastic toys and wading pools.\nIn 1961 the leather and shoe findings portion of the business was sold, and Connecticut Leather Company became Coleco Industries, Inc. On January 9, 1962 Coleco went public, offering stock at $5.00 a share.\nIn 1963 the company acquired the Kestral Corporation of Springfield, Massachusetts, a manufacturer of inflatable vinyl pools and toys. This led to Coleco becoming the largest manufacturer of above-ground swimming pools in the world.\nBy 1966, the company had grown so Leonard persuaded his brother Arnold Greenberg to join the company. Further acquisitions added to the company's growth, namely Playtime Products (1966) and Eagle Toys of Canada (1968). By the end of the 1960s, Coleco ran ten manufacturing facilities and had a new corporate headquarters in Hartford, Connecticut.\nThe 1970s were difficult for Coleco. Despite this sales exceeded $100 million. When Coleco became listed on the New York Stock Exchange in 1971 sales had grown to $48.6 million. In 1972 Coleco entered the snowmobile market through acquisition. Poor snowfall and market conditions led to disappointing sales and profits.\nUnder CEO Arnold Greenberg, the company entered the video game console business with the Telstar in 1976. Dozens of companies were introducing game systems that year after Atari's successful Pong console. Nearly all of these new games were based on General Instrument's \"Pong-on-a-chip\". General Instrument had underestimated demand, and there were severe shortages. Coleco had been one of the first to place an order, and was one of the few companies to receive an order in full. Though dedicated game consoles did not last long on the market, their early order enabled Coleco to break even.\nColeco continued to do well in electronics. The company transitioned into handheld electronic games, a market popularized by Mattel. An early success was Electronic Quarterback. Coleco produced two popular lines of games, the \"head to head\" series of two player sports games, (Football, Baseball, Basketball, Soccer, Hockey) and the Mini-Arcade series of licensed video arcade titles such as \"Donkey Kong\" and \"Ms. Pac-Man\". A third line of educational handhelds was also produced and included the Electronic Learning Machine, Lil Genius, Digits, and a trivia game called Quiz Wiz. Launched in 1982, their first four tabletop Mini-Arcades, for \"Pac-Man\", \"Galaxian\", \"Donkey Kong\", and \"Frogger\", sold approximately three million units within a year. Among these, 1.5 million units were sold for \"Pac-Man\" alone. In 1983, it released three more Mini-Arcades: for \"Ms. Pac-Man\", \"Donkey Kong Junior\", and \"Zaxxon\".\nColeco returned to the video game console market in 1982 with the launch of the ColecoVision. While the system was quite popular, selling 500,000 units over two years, Coleco hedged its bet on video games by introducing a line of ROM cartridges for the Atari 2600 and Intellivision. It also introduced the Coleco Gemini, a clone of the popular Atari 2600.\nWhen the video game business began to implode in 1983, it seemed clear that video game consoles were being supplanted by home computers. Bob Greenberg, son of Leonard Greenberg and nephew of Arnold Greenberg, left Microsoft where he had been working as a program developer at the time to assist in Coleco's entry into this market. Coleco's strategy was to introduce the Coleco Adam home computer, both as a stand-alone system and as an expansion module to the ColecoVision. The effort failed, in part because Adams were often unreliable due to being released with fatal bugs, and in part because the computer's release coincided with the home computer industry crashing. Coleco withdrew from electronics early in 1985.\nIn 1983, Coleco released the Cabbage Patch Kids series of dolls which were wildly successful. Flush with success, Coleco purchased Leisure Dynamics (manufacturer of Aggravation and Perfection) and beleaguered Selchow and Righter, manufacturers of Scrabble, Parcheesi, and Trivial Pursuit, in 1986. Sales of Selchow &amp; Righter games had plummeted, leaving them with warehouses full of unsold games. The purchase price for Selchow &amp; Righter was $75 million. That same year, Coleco introduced an ALF plush-based on the furry alien character who had his own television series at the time, as well as a talking version and a cassette-playing \"Storytelling ALF\" doll. The combination of the purchase of Selchow &amp; Righter, the disastrous Adam computer, and the public's waning infatuation with Cabbage Patch dolls all contributed to Coleco's financial decline. In 1988, the company filed for Chapter 11 bankruptcy.\nThe reorganized Coleco sold off all of its North American assets and outsourced thousands of jobs to foreign countries, closing plants in Amsterdam, New York and other cities. In 1988, Canada based SLM Action Sports Inc. purchased Coleco's swimming pool and snow goods divisions. In 1989, Hasbro purchased most of Coleco's remaining product lines.\nBrand.\nColeco as a brand name has been owned by several entities since it was created in 1961 by Coleco Industries, Inc.\nIn 2005, River West Brands, now Dormitus Brands, a Chicago-based brand revitalization company, re-introduced the Coleco brand to the marketplace. In late 2006, the company introduced the Coleco Sonic, a handheld system containing twenty Sega Master System and Sega Game Gear games. In 2014, River West Brands established the subsidiary Coleco Holdings for their Coleco-branded projects.\nIn December 2015, Coleco Holdings announced the development of the Coleco Chameleon, a new cartridge-based video game system; in actuality, a re-branding of the controversial Retro VGS console, whose Indiegogo campaign failed to secure funding when it ended in early November 2015, with only $63,546 raised of its $1.95 million goal. In the press release, it was established that the system would be able to play new and classic games in the 8, 16, and 32-bit styles. The release for the system was announced to be sometime in early 2016, with a demonstration at Toy Fair New York in February. However, some critics suggested that the prototype fell short of its developmental goals and was nothing more than the motherboard of a Super NES model SNS-101 inside an Atari Jaguar case. Later mock images of a prototype posted by AtariAge showed the device utilizing a CCTV capture card in place of a motherboard. After Retro VGS failed to produce a fully working prototype, Coleco Holdings pulled out of involvement with Retro VGS, terminating the project."}
{"id": "7306", "revid": "22297739", "url": "https://en.wikipedia.org/wiki?curid=7306", "title": "ColecoVision", "text": "The ColecoVision is Coleco Industries' second-generation home video-game console that was released in August 1982. The ColecoVision offered a closer experience to more powerful arcade game systems, compared to competitors such as the Atari 2600 and Atari 5200, along with the means to expand the system's basic hardware.\nThe initial catalog of twelve games included Nintendo's \"Donkey Kong\" as the pack-in cartridge, Sega's \"Zaxxon\", and some lesser known arcade titles that found a larger audience on the console, such as \"Lady Bug\", \"Cosmic Avenger\", and \"Venture\". Approximately 145 titles in total were published as ROM cartridges for the system between 1982 and 1984. Coleco released a series of hardware add-ons and special controllers to expand the capabilities of the console.\nThe ColecoVision was discontinued in 1985 when Coleco withdrew from the video game market.\nHistory.\nBy Christmas 1982, Coleco had sold more than 500,000 units, in part on the strength of \"Donkey Kong\" as the bundled game. ColecoVision's main competitor was the less commercially successful Atari 5200. Sales quickly passed 1 million in early 1983.\nThe ColecoVision was distributed by CBS Electronics outside of North America, and was branded the CBS ColecoVision. In Europe the console was released in July 1983, nearly one year after the North American release.\nBy the beginning of 1984, quarterly sales of the ColecoVision had dramatically decreased.\nIn January 1985, Coleco discontinued the Adam, which was a home computer expansion for ColecoVision. By mid-1985, Coleco planned to withdraw from the video game market, and the ColecoVision was officially discontinued by October. Total sales are uncertain but were ultimately in excess of 2 million consoles, with the console continuing to sell modestly up until its discontinuation. The video game crash of 1983 has been cited as the main cause of the system being on the market for less than 30 months.\nIn 1983, Spectravideo announced the SV-603 ColecoVision Video Game Adapter for its SV-318 computer. The company stated that the $70 product allowed users to \"enjoy the entire library of exciting ColecoVision video-game cartridges\".\nHardware.\nColecoVision is based around the Zilog Z80 CPU and a variant of the Texas Instruments TMS9918 video chip that was introduced in 1979.\nOn NTSC ColecoVision consoles, all first-party cartridges and most third-party software titles feature a 12.7 second pause before presenting the game select screen. CBS Electronics reduced this pause in the BIOS to 3.3 seconds for their PAL and SECAM ColecoVision consoles.\nExpansion Modules and accessories.\nFrom its introduction, Coleco touted the ColecoVision's hardware expandability by highlighting the \"Expansion Module Interface\" on the front of the unit. These hardware expansion modules and accessories were sold separately.\nAtari 2600 expansion.\n\"Expansion Module #1\" makes the ColecoVision compatible with Atari 2600 cartridges and controllers. It leveraged the fact that the 2600 used largely off-the-shelf components and was effectively a complete set of 2600 electronics, including a reverse-engineered equivalent of the 2600's sole custom chip the TIA. The ColecoVision console did not do any translation or processing of the game code on the 2600 cartridges; it only provided power and clock input to and audio/video output from the expansion module, which was otherwise entirely self-contained and could be thought of as the first Atari 2600 clone console. Functionally, this gave the ColecoVision the largest software library of any console of its day. The expansion module prompted legal action from Atari. Coleco and Atari settled out of court with Coleco becoming licensed under Atari's patents. The royalty based license also applied to Coleco's Gemini game system, a stand-alone clone of the 2600.\nDriving controller.\n\"Expansion Module #2\" is a driving controller (steering wheel / gas pedal) that came packaged with the cartridge \"Turbo\". The gas pedal is merely a simple on/off switch. Although Coleco called the driving controller an expansion module, it actually plugs into the controller port, not the \"Expansion Module Interface\". The driving controller is also compatible with the cartridges \"Destructor\", \"Bump 'n' Jump\", \"Pitstop\", and \"The Dukes of Hazzard\".\nAdam computer expansion.\n\"Expansion Module #3\" converts the ColecoVision into the Adam computer, complete with keyboard, digital data pack (DDP) cassette drive, 64 kB RAM, and printer.\nRoller Controller.\nThe \"Roller Controller\" is a trackball that came packaged with the cartridge \"Slither\", a conversion of the arcade game. The roller controller uses a special power connector that is not compatible with Expansion Module #3 (the Adam computer). Coleco mailed an adapter to owners of both units who complained. The other cartridge programmed to use the roller controller is \"Victory\". A joystick mode switch on the roller controller allows it to be used with all cartridges including WarGames, \"Omega Race\", and Atarisoft's Centipede.\nSuper Action Controller.\nThe \"Super Action Controller Set\" is a set of two handheld joystick controllers that came packaged with the cartridge \"Super Action Baseball\". Each controller has a ball-top joystick, four finger triggered action buttons, a 12-button numeric keypad, and a \"speed roller\". The cartridges \"Super Action Football\", \"Rocky\" \"Super Action Boxing\", and a conversion of the arcade game \"Front Line\" are also designed to be used with the \"Super Action Controller\".\nUnreleased.\nExpansion Module #3 was originally the Super Game Module. It was advertised for an August 1983 release but was ultimately cancelled and replaced with the Adam computer expansion. The Super Game Module added a tape drive known as the Exatron Stringy Floppy with 128KB capacity, and the additional RAM, said to be 30KB, to load and execute programs from tape. Games could be distributed on tiny tapes, called \"wafers\", and be much larger than the 16KB or 32KB ROM cartridges of the day. \"Super Donkey Kong\", with all screens and animations, \"Super Donkey Kong Jr\", and \"Super Smurf Rescue\" were demonstrated with the Super Game Module. The Adam computer expansion with its 256KB tape drive and 64KB RAM fulfilled the specifications promised by the Super Game Module.\nLegacy.\nMasayuki Uemura, head of Famicom development, stated that the ColecoVision set the bar that influenced how he approached the creation of the Famicom. During the creation of the Nintendo Entertainment System, Takao Sawano, chief manager of the project, brought a ColecoVision home to his family, who were impressed by the system's capability to produce smooth graphics, which contrasted with the flickering commonly seen on Atari 2600 games.\nIn 1986, Bit Corporation produced a ColecoVision clone called the Dina, which was sold in the United States by Telegames as the Telegames Personal Arcade.\nIGN named the ColecoVision their 12th-best video-game console out of their list of 25, citing \"its incredible accuracy in bringing current-generation arcade hits home.\"\nIn 1996, the first homebrew ColecoVision game was released: a \"Tetris\" clone titled \"Kevtris\".\nIn 1997, Telegames released \"Personal Arcade Vol. 1\", a collection of ColecoVision games for Microsoft Windows, and a 1998 follow-up, \"Colecovision Hits Volume One\".\nIn 2012, Opcode Games released their own Super Game Module expansion, which increases RAM from 16KB to 32KB and adds four additional sound channels. This expansion brings the ColecoVision close to the MSX architecture standard, allowing MSX software to be more easily ported.\nIn 2014, AtGames began producing the ColecoVision Flashback console that includes 60 games, but not the original pack-in game, \"Donkey Kong\"."}
{"id": "7309", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=7309", "title": "Coleco Telstar series", "text": "The Coleco Telstar brand is a series of dedicated first-generation home video game consoles produced, released and marketed by Coleco from 1976 to 1978. Starting with Coleco Telstar \"Pong\" clone based video game console on General Instrument's AY-3-8500 chip in 1976, there were 14 consoles released in the Coleco Telstar series. About one million units of the first model called Coleco Telstar were sold.\nThe large product lineup and the impending fading out of the \"Pong\" machines led Coleco to face near-bankruptcy in 1980."}
{"id": "7310", "revid": "41043346", "url": "https://en.wikipedia.org/wiki?curid=7310", "title": "Conventional warfare", "text": "Conventional warfare is a form of warfare conducted by using conventional weapons and battlefield tactics between two or more states in open confrontation. The forces on each side are well-defined, and fight using weapons that primarily target the opponent's military. It is normally fought using conventional weapons, and not with chemical, biological, or nuclear weapons.\nThe general purpose of conventional warfare is to weaken or destroy the opponent's military, thereby negating its ability to engage in conventional warfare. In forcing capitulation, however, one or both sides may eventually resort to unconventional warfare tactics.\nFormation of the state.\nThe state was first advocated by Plato, then found more acceptance in the consolidation of power under the Roman Catholic Church. European monarchs then gained power as the Catholic Church was stripped of temporal power and was replaced by the divine right of kings. In 1648, the powers of Europe signed the Treaty of Westphalia which ended the religious violence for purely political governance and outlook, signifying the birth of the modern 'state'.\nWithin this statist paradigm, only the state and its appointed representatives were allowed to bear arms and enter into war. In fact, war was only understood as a conflict between sovereign states. Kings strengthened this idea and gave it the force of law. Whereas previously any noble could start a war, the monarchs of Europe of necessity consolidated military power in response to the Napoleonic war.\nThe Clausewitzian paradigm.\nPrussia was one country attempting to amass military power. Carl von Clausewitz, one of Prussia's officers, wrote \"On War\", a work rooted solely in the world of the state. All other forms of intrastate conflict, such as rebellion, are not accounted for because in theoretical terms, Clausewitz could not account for warfare before the state. However, near the end of his life, Clausewitz grew increasingly aware of the importance of non-state military actors. This is revealed in his conceptions of \"the people in arms\" which he noted arose from the same social and political sources as traditional inter-state warfare.\nPractices such as raiding or blood feuds were then labeled criminal activities and stripped of legitimacy. This war paradigm reflected the view of most of the modernized world at the beginning of the 21st century, as verified by examination of the conventional armies of the time: large, high maintenance, technologically advanced armies designed to compete against similarly designed forces.\nClausewitz also forwarded the issue of casus belli. While previous wars were fought for social, religious, or even cultural reasons, Clausewitz taught that war is merely \"a continuation of politics by other means.\" It is a rational calculation in which states fight for their interests (whether they are economic, security-related, or otherwise) once normal discourse has broken down.\nPrevalence.\nThe majority of modern wars have been conducted using the means of conventional means. Confirmed use of biological warfare by a nation state has not occurred since 1945, and chemical warfare has been used only a few times (the latest known confrontation in which it was utilized being the Syrian Civil War). Nuclear warfare has only occurred once with the United States bombing the Japanese cities of Hiroshima and Nagasaki in August 1945.\nDecline.\nThe state and Clausewitzian principles peaked in the World Wars of the 20th century, but also laid the groundwork for their dilapidation due to nuclear proliferation and the manifestation of culturally aligned conflict. The nuclear bomb was the result of the state perfecting its quest to overthrow its competitive duplicates. This development seems to have pushed conventional conflict waged by the state to the sidelines. Were two conventional armies to fight, the loser would have redress in its nuclear arsenal.\nThus, no two nuclear powers have yet fought a conventional war directly, with the exception of two brief skirmishes between, China and Russia in the 1969 Sino-Soviet conflict and between India and Pakistan in the 1999 Kargil War.\nReplacement.\nWith the invention of nuclear weapons, the concept of full-scale war carries the prospect of global annihilation, and as such conflicts since WWII have by definition been \"low intensity\" conflicts, typically in the form of proxy wars fought within local regional confines, using what are now referred to as \"conventional weapons\", typically combined with the use of asymmetric warfare tactics and applied use of intelligence.\nSamuel Huntington has posited that the world in the early 21st century exists as a system of nine distinct \"civilizations\", instead of many sovereign states. These civilizations are delineated along cultural lines (for example, Western, Islamic, Sinic, Hindu, Buddhist, and so on). In this way, cultures that have long been dominated by the West are reasserting themselves and looking to challenge the \"status quo\". Thus, culture has replaced the state as the locus of war. This kind of civilizational war, in our time as in times long past, occurs where these cultures buffet up against one another. Some high-profile examples are the Pakistan/India conflict or the battles in the Sudan. This sort of war has defined the field since World War II.\nThese cultural forces will not contend with state-based armies in the traditional way. When faced with battalions of tanks, jets, and missiles, the cultural opponent dissolves away into the population. They benefit from the territorially constrained states, being able to move freely from one country to the next, while states must negotiate with other sovereign states. The state's spy networks are also severely limited by this mobility not constrained by state borders.\nSee also.\nContrast:"}
{"id": "7312", "revid": "38132428", "url": "https://en.wikipedia.org/wiki?curid=7312", "title": "Chauvinism", "text": "Chauvinism is the irrational belief in the superiority or dominance of one's own group or people, who are seen as strong and virtuous, while others are considered weak or unworthy. It can be described as a form of extreme patriotism and nationalism, a fervent faith in national excellence and glory.\nAccording to legend, French soldier Nicolas Chauvin was badly wounded in the Napoleonic Wars and received a meager pension for his injuries. After Napoleon abdicated, Chauvin maintained his fanatical Bonapartist belief in the messianic mission of Imperial France, despite the unpopularity of this view under the Bourbon Restoration. His single-minded blind devotion to his cause, despite neglect by his faction and harassment by its enemies, started the use of the term.\n\"Chauvinism\" has extended from its original use to include fanatical devotion and undue partiality to any group or cause to which one belongs, especially when such partisanship includes prejudice against or hostility toward outsiders or rival groups and persists even in the face of overwhelming opposition. This French quality finds its parallel in the English-language term \"jingoism\", which has retained the meaning of \"chauvinism\" strictly in its original sense; that is, an attitude of belligerent nationalism.\nIn English, the word has come to be used in some quarters as shorthand for male chauvinism, a trend reflected in \"Merriam-Webster's Dictionary\", which, as of 2018, begins its first example of use of the term \"chauvinism\" with \"an attitude of superiority toward members of the opposite sex\".\nAs nationalism.\nIn 1945, political theorist Hannah Arendt described the concept thus:\nMale chauvinism.\nMale chauvinism is the belief that men are superior to women. The first documented use of the phrase \"male chauvinism\" is in the 1935 Clifford Odets play \"Till the Day I Die\".\nIn the workplace.\nThe balance of the workforce changed during World War II. As men left their positions to enlist in the military and fight in the war, women started replacing them. After the war ended, men returned home to find jobs in the workplace now occupied by women, which \"threatened the self-esteem many men derive from their dominance over women in the family, the economy, and society at large.\" Consequently, male chauvinism was on the rise, according to Cynthia B. Lloyd.\nLloyd and Michael Korda have argued that as they integrated back into the workforce, men returned to predominate, holding positions of power while women worked as their secretaries, usually typing dictations and answering telephone calls. This division of labor was understood and expected, and women typically felt unable to challenge their position or male superiors, argue Korda and Lloyd.\nCauses.\nChauvinist assumptions are seen by some as a bias in the TAT psychological personality test. Through cross-examinations, the TAT exhibits a tendency toward chauvinistic stimuli for its questions and has the \"potential for unfavorable clinical evaluation\" for women.\nAn often cited study done in 1976 by Sherwyn Woods, Some Dynamics of Male Chauvinism, attempts to find the underlying causes of male chauvinism.\nAdam Jukes argues that a reason for male chauvinism is masculinity itself:For the vast majority of people all over the world, the mother is a primary carer...There\u2019s an asymmetry in the development of boys and girls. Infant boys have to learn how to be masculine. Girls don\u2019t. Masculinity is not in a state of crisis. Masculinity is a crisis. I don\u2019t believe misogyny is innate, but I believe it\u2019s inescapable because of the development of masculinity.\nFemale chauvinism.\nFemale chauvinism is the belief that women are morally superior to men, and is considered anti-feminist.\nThe term has been adopted by critics of some types or aspects of feminism; second-wave feminist Betty Friedan is a notable example. Ariel Levy used the term in similar, but opposite sense in her book, \"Female Chauvinist Pigs\", in which she argues that many young women in the United States and beyond are replicating male chauvinism and older misogynist stereotypes.\nKaren Salmansohn described what female chauvinists believe in Psychology Today when she wrote, \"female chauvinists believe that men can't be emotionally evolved enough to want to grow, communicate from the heart, empathize and validate [their] female partners,\" and then labeling this description of men the same as calling men \"emotional bimbos.\""}
{"id": "7314", "revid": "23645052", "url": "https://en.wikipedia.org/wiki?curid=7314", "title": "Colonized", "text": ""}
{"id": "7315", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=7315", "title": "Colonies", "text": ""}
{"id": "7316", "revid": "10274643", "url": "https://en.wikipedia.org/wiki?curid=7316", "title": "Hypothetical types of biochemistry", "text": "Hypothetical types of biochemistry are forms of biochemistry speculated to be scientifically viable but not proven to exist at this time. The kinds of living organisms currently known on Earth all use carbon compounds for basic structural and metabolic functions, water as a solvent, and DNA or RNA to define and control their form. If life exists on other planets or moons it may be chemically similar though it is also possible that there are organisms with quite different chemistries\u2014for instance, involving other classes of carbon compounds, compounds of another element, or another solvent in place of water.\nThe possibility of life-forms being based on \"alternative\" biochemistries is the topic of an ongoing scientific discussion, informed by what is known about extraterrestrial environments and about the chemical behaviour of various elements and compounds. It is of interest in synthetic biology and is also a common subject in science fiction.\nThe element silicon has been much discussed as a hypothetical alternative to carbon. Silicon is in the same group as carbon on the periodic table and, like carbon, it is tetravalent. Hypothetical alternatives to water include ammonia, which, like water, is a polar molecule, and cosmically abundant; and non-polar hydrocarbon solvents such as methane and ethane, which are known to exist in liquid form on the surface of Titan.\nShadow biosphere.\nA shadow biosphere is a hypothetical microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. Although life on Earth is relatively well-studied, the shadow biosphere may still remain unnoticed because the exploration of the microbial world targets primarily the biochemistry of the macro-organisms.\nAlternative-chirality biomolecules.\nPerhaps the least unusual alternative biochemistry would be one with differing chirality of its biomolecules. In known Earth-based life, amino acids are almost universally of the form and sugars are of the form. Molecules using amino acids or sugars may be possible; molecules of such a chirality, however, would be incompatible with organisms using the opposing chirality molecules. Amino acids whose chirality is opposite to the norm are found on Earth, and these substances are generally thought to result from decay of organisms of normal chirality. However, physicist Paul Davies speculates that some of them might be products of \"anti-chiral\" life.\nIt is questionable, however, whether such a biochemistry would be truly alien. Although it would certainly be an alternative stereochemistry, molecules that are overwhelmingly found in one enantiomer throughout the vast majority of organisms can nonetheless often be found in another enantiomer in different (often basal) organisms such as in comparisons between members of Archaea and other domains, making it an open topic whether an alternative stereochemistry is truly novel.\nNon-carbon-based biochemistries.\nOn Earth, all known living things have a carbon-based structure and system. Scientists have speculated about the pros and cons of using atoms other than carbon to form the molecular structures necessary for life, but no one has proposed a theory employing such atoms to form all the necessary structures. However, as Carl Sagan argued, it is very difficult to be certain whether a statement that applies to all life on Earth will turn out to apply to all life throughout the universe. Sagan used the term \"carbon chauvinism\" for such an assumption. He regarded silicon and germanium as conceivable alternatives to carbon (other plausible elements include but are not limited to palladium and titanium); but, on the other hand, he noted that carbon does seem more chemically versatile and is more abundant in the cosmos. Norman Horowitz devised the experiments to determine whether life might exist on Mars that were carried out by the Viking Lander of 1976, the first U.S. mission to successfully land an unmanned probe on the surface of Mars. Horowitz argued that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival on other planets. He considered that there was only a remote possibility that non-carbon life forms could exist with genetic information systems capable of self-replication and the ability to evolve and adapt.\nSilicon biochemistry.\nThe silicon atom has been much discussed as the basis for an alternative biochemical system, because silicon has many chemical properties similar to those of carbon and is in the same group of the periodic table, the carbon group. Like carbon, silicon can create molecules that are sufficiently large to carry biological information.\nHowever, silicon has several drawbacks as an alternative to carbon. Silicon, unlike carbon, lacks the ability to form chemical bonds with diverse types of atoms as is necessary for the chemical versatility required for metabolism, and yet this precise inability is what makes silicon less susceptible to bond with all sorts of impurities from which carbon, in comparison, is not shielded. Elements creating organic functional groups with carbon include hydrogen, oxygen, nitrogen, phosphorus, sulfur, and metals such as iron, magnesium, and zinc. Silicon, on the other hand, interacts with very few other types of atoms. Moreover, where it does interact with other atoms, silicon creates molecules that have been described as \"monotonous compared with the combinatorial universe of organic macromolecules\". This is because silicon atoms are much bigger, having a larger mass and atomic radius, and so have difficulty forming double bonds (the double-bonded carbon is part of the carbonyl group, a fundamental motif of carbon-based bio-organic chemistry).\nSilanes, which are chemical compounds of hydrogen and silicon that are analogous to the alkane hydrocarbons, are highly reactive with water, and long-chain silanes spontaneously decompose. Molecules incorporating polymers of alternating silicon and oxygen atoms instead of direct bonds between silicon, known collectively as silicones, are much more stable. It has been suggested that silicone-based chemicals would be more stable than equivalent hydrocarbons in a sulfuric-acid-rich environment, as is found in some extraterrestrial locations.\nOf the varieties of molecules identified in the interstellar medium , 84 are based on carbon, while only 8 are based on silicon. Moreover, of those 8 compounds, 4 also include carbon within them. The cosmic abundance of carbon to silicon is roughly 10 to 1. This may suggest a greater variety of complex carbon compounds throughout the cosmos, providing less of a foundation on which to build silicon-based biologies, at least under the conditions prevalent on the surface of planets. Also, even though Earth and other terrestrial planets are exceptionally silicon-rich and carbon-poor (the relative abundance of silicon to carbon in Earth's crust is roughly 925:1), terrestrial life is carbon-based. The fact that carbon is used instead of silicon may be evidence that silicon is poorly suited for biochemistry on Earth-like planets. Reasons for which this may be that silicon is less versatile than carbon in forming compounds, that the compounds formed by silicon are unstable, and that it blocks the flow of heat.\nEven so, biogenic silica is used by some Earth life, such as the silicate skeletal structure of diatoms. According to the clay hypothesis of A. G. Cairns-Smith, silicate minerals in water played a crucial role in abiogenesis: they replicated their crystal structures, interacted with carbon compounds, and were the precursors of carbon-based life.\nAlthough not observed in nature, carbon\u2013silicon bonds have been added to biochemistry by using directed evolution (artificial selection). A heme containing cytochrome \"c\" protein from \"Rhodothermus marinus\" has been engineered using directed evolution to catalyze the formation of new carbon\u2013silicon bonds between hydrosilanes and diazo compounds.\nSilicon compounds may possibly be biologically useful under temperatures or pressures different from the surface of a terrestrial planet, either in conjunction with or in a role less directly analogous to carbon. Polysilanols, the silicon compounds corresponding to sugars, are soluble in liquid nitrogen, suggesting that they could play a role in very-low-temperature biochemistry.\nIn cinematic and literary science fiction, at a moment when man-made machines cross from nonliving to living, it is often posited, this new form would be the first example of non-carbon-based life. Since the advent of the microprocessor in the late 1960s, these machines are often classed as computers (or computer-guided robots) and filed under \"silicon-based life\", even though the silicon backing matrix of these processors is not nearly as fundamental to their operation as carbon is for \"wet life\".\nArsenic as an alternative to phosphorus.\nArsenic, which is chemically similar to phosphorus, while poisonous for most life forms on Earth, is incorporated into the biochemistry of some organisms. Some marine algae incorporate arsenic into complex organic molecules such as arsenosugars and arsenobetaines. Fungi and bacteria can produce volatile methylated arsenic compounds. Arsenate reduction and arsenite oxidation have been observed in microbes (\"Chrysiogenes arsenatis\"). Additionally, some prokaryotes can use arsenate as a terminal electron acceptor during anaerobic growth and some can utilize arsenite as an electron donor to generate energy.\nIt has been speculated that the earliest life forms on Earth may have used arsenic biochemistry in place of phosphorus in the structure of their DNA. A common objection to this scenario is that arsenate esters are so much less stable to hydrolysis than corresponding phosphate esters that arsenic is poorly suited for this function.\nThe authors of a 2010 geomicrobiology study, supported in part by NASA, have postulated that a bacterium, named GFAJ-1, collected in the sediments of Mono Lake in eastern California, can employ such 'arsenic DNA' when cultured without phosphorus. They proposed that the bacterium may employ high levels of poly-\u03b2-hydroxybutyrate or other means to reduce the effective concentration of water and stabilize its arsenate esters. This claim was heavily criticized almost immediately after publication for the perceived lack of appropriate controls. Science writer Carl Zimmer contacted several scientists for an assessment: \"I reached out to a dozen experts ... Almost unanimously, they think the NASA scientists have failed to make their case\".\nOther authors were unable to reproduce their results and showed that the study had issues with phosphate contamination, suggesting that the low amounts present could sustain extremophile lifeforms.\nAlternatively, it was suggested that GFAJ-1 cells grow by recycling phosphate from degraded ribosomes, rather than by replacing it with arsenate.\nNon-water solvents.\nIn addition to carbon compounds, all currently known terrestrial life also requires water as a solvent. This has led to discussions about whether water is the only liquid capable of filling that role. The idea that an extraterrestrial life-form might be based on a solvent other than water has been taken seriously in recent scientific literature by the biochemist Steven Benner, and by the astrobiological committee chaired by John A. Baross. Solvents discussed by the Baross committee include ammonia, sulfuric acid, formamide, hydrocarbons, and (at temperatures much lower than Earth's) liquid nitrogen, or hydrogen in the form of a supercritical fluid.\nCarl Sagan once described himself as both a carbon chauvinist and a water chauvinist; however, on another occasion he said that he was a carbon chauvinist but \"not that much of a water chauvinist\". \nHe speculated on hydrocarbons, hydrofluoric acid, and ammonia as possible alternatives to water.\nSome of the properties of water that are important for life processes include: \nWater as a compound is cosmically abundant, although much of it is in the form of vapour or ice. Subsurface liquid water is considered likely or possible on several of the outer moons: Enceladus (where geysers have been observed), Europa, Titan, and Ganymede. Earth and Titan are the only worlds currently known to have stable bodies of liquid on their surfaces.\nNot all properties of water are necessarily advantageous for life, however. For instance, water ice has a high albedo, meaning that it reflects a significant quantity of light and heat from the Sun. During ice ages, as reflective ice builds up over the surface of the water, the effects of global cooling are increased.\nThere are some properties that make certain compounds and elements much more favorable than others as solvents in a successful biosphere. The solvent must be able to exist in liquid equilibrium over a range of temperatures the planetary object would normally encounter. Because boiling points vary with the pressure, the question tends not to be \"does\" the prospective solvent remain liquid, but \"at what pressure\". For example, hydrogen cyanide has a narrow liquid-phase temperature range at 1\u00a0atmosphere, but in an atmosphere with the pressure of Venus, with of pressure, it can indeed exist in liquid form over a wide temperature range.\nAmmonia.\nThe ammonia molecule (NH3), like the water molecule, is abundant in the universe, being a compound of hydrogen (the simplest and most common element) with another very common element, nitrogen. The possible role of liquid ammonia as an alternative solvent for life is an idea that goes back at least to 1954, when J.\u00a0B.\u00a0S. Haldane raised the topic at a symposium about life's origin.\nNumerous chemical reactions are possible in an ammonia solution, and liquid ammonia has chemical similarities with water. Ammonia can dissolve most organic molecules at least as well as water does and, in addition, it is capable of dissolving many elemental metals. Haldane made the point that various common water-related organic compounds have ammonia-related analogs; for instance the ammonia-related amine group (\u2212NH2) is analogous to the water-related hydroxyl group (\u2212OH).\nAmmonia, like water, can either accept or donate an H+ ion. When ammonia accepts an H+, it forms the ammonium cation (NH4+), analogous to hydronium (H3O+). When it donates an H+ ion, it forms the amide anion (NH2\u2212), analogous to the hydroxide anion (OH\u2212). Compared to water, however, ammonia is more inclined to accept an H+ ion, and less inclined to donate one; it is a stronger nucleophile. Ammonia added to water functions as Arrhenius base: it increases the concentration of the anion hydroxide. Conversely, using a solvent system definition of acidity and basicity, water added to liquid ammonia functions as an acid, because it increases the concentration of the cation ammonium. The carbonyl group (C=O), which is much used in terrestrial biochemistry, would not be stable in ammonia solution, but the analogous imine group (C=NH) could be used instead.\nHowever, ammonia has some problems as a basis for life. The hydrogen bonds between ammonia molecules are weaker than those in water, causing ammonia's heat of vaporization to be half that of water, its surface tension to be a third, and reducing its ability to concentrate non-polar molecules through a hydrophobic effect. Gerald Feinberg and Robert Shapiro have questioned whether ammonia could hold prebiotic molecules together well enough to allow the emergence of a self-reproducing system. Ammonia is also flammable in oxygen and could not exist sustainably in an environment suitable for aerobic metabolism.\nA biosphere based on ammonia would likely exist at temperatures or air pressures that are extremely unusual in relation to life on Earth. Life on Earth usually exists within the melting point and boiling point of water at normal pressure, between 0\u00a0\u00b0C (273\u00a0K) and 100\u00a0\u00b0C (373\u00a0K); at normal pressure ammonia's melting and boiling points are between \u221278\u00a0\u00b0C (195\u00a0K) and \u221233\u00a0\u00b0C (240\u00a0K). Chemical reactions generally proceed more slowly at a lower temperature. Therefore, ammonia-based life, if it exists, might metabolize more slowly and evolve more slowly than life on Earth. On the other hand, lower temperatures could also enable living systems to use chemical species that would be too unstable at Earth temperatures to be useful.\nAmmonia could be a liquid at Earth-like temperatures, but at much higher pressures; for example, at 60\u00a0atm, ammonia melts at \u221277\u00a0\u00b0C (196\u00a0K) and boils at 98\u00a0\u00b0C (371\u00a0K).\nAmmonia and ammonia\u2013water mixtures remain liquid at temperatures far below the freezing point of pure water, so such biochemistries might be well suited to planets and moons orbiting outside the water-based habitability zone. Such conditions could exist, for example, under the surface of Saturn's largest moon Titan.\nMethane and other hydrocarbons.\nMethane (CH4) is a simple hydrocarbon: that is, a compound of two of the most common elements in the cosmos: hydrogen and carbon. It has a cosmic abundance comparable with ammonia. Hydrocarbons could act as a solvent over a wide range of temperatures, but would lack polarity. Isaac Asimov, the biochemist and science fiction writer, suggested in 1981 that poly-lipids could form a substitute for proteins in a non-polar solvent such as methane. Lakes composed of a mixture of hydrocarbons, including methane and ethane, have been detected on the surface of Titan by the \"Cassini\" spacecraft.\nThere is debate about the effectiveness of methane and other hydrocarbons as a solvent for life compared to water or ammonia. Water is a stronger solvent than the hydrocarbons, enabling easier transport of substances in a cell. However, water is also more chemically reactive and can break down large organic molecules through hydrolysis. A life-form whose solvent was a hydrocarbon would not face the threat of its biomolecules being destroyed in this way. Also, the water molecule's tendency to form strong hydrogen bonds can interfere with internal hydrogen bonding in complex organic molecules. Life with a hydrocarbon solvent could make more use of hydrogen bonds within its biomolecules. Moreover, the strength of hydrogen bonds within biomolecules would be appropriate to a low-temperature biochemistry.\nAstrobiologist Chris McKay has argued, on thermodynamic grounds, that if life does exist on Titan's surface, using hydrocarbons as a solvent, it is likely also to use the more complex hydrocarbons as an energy source by reacting them with hydrogen, reducing ethane and acetylene to methane. Possible evidence for this form of life on Titan was identified in 2010 by Darrell Strobel of Johns Hopkins University; a greater abundance of molecular hydrogen in the upper atmospheric layers of Titan compared to the lower layers, arguing for a downward diffusion at a rate of roughly 1025 molecules per second and disappearance of hydrogen near Titan's surface. As Strobel noted, his findings were in line with the effects Chris McKay had predicted if methanogenic life-forms were present. The same year, another study showed low levels of acetylene on Titan's surface, which were interpreted by Chris McKay as consistent with the hypothesis of organisms reducing acetylene to methane. While restating the biological hypothesis, McKay cautioned that other explanations for the hydrogen and acetylene findings are to be considered more likely: the possibilities of yet unidentified physical or chemical processes (e.g. a non-living surface catalyst enabling acetylene to react with hydrogen), or flaws in the current models of material flow. He noted that even a non-biological catalyst effective at 95\u00a0K would in itself be a startling discovery.\nAzotosome.\nA hypothetical cell membrane termed an \"azotosome\" capable of functioning in liquid methane in Titan conditions was computer-modeled in an article published in February 2015. Composed of acrylonitrile, a small molecule containing carbon, hydrogen, and nitrogen, it is predicted to have stability and flexibility in liquid methane comparable to that of a phospholipid bilayer (the type of cell membrane possessed by all life on Earth) in liquid water. An analysis of data obtained using the Atacama Large Millimeter / submillimeter Array (ALMA), completed in 2017, confirmed substantial amounts of acrylonitrile in Titan's atmosphere.\nHydrogen fluoride.\nHydrogen fluoride (HF), like water, is a polar molecule, and due to its polarity it can dissolve many ionic compounds. Its melting point is \u221284\u00a0\u00b0C, and its boiling point is 19.54\u00a0\u00b0C (at atmospheric pressure); the difference between the two is a little more than 100\u00a0K. HF also makes hydrogen bonds with its neighbor molecules, as do water and ammonia. It has been considered as a possible solvent for life by scientists such as Peter Sneath and Carl Sagan.\nHF is dangerous to the systems of molecules that Earth-life is made of, but certain other organic compounds, such as paraffin waxes, are stable with it. Like water and ammonia, liquid hydrogen fluoride supports an acid-base chemistry. Using a solvent system definition of acidity and basicity, nitric acid functions as a base when it is added to liquid HF.\nHowever, hydrogen fluoride is cosmically rare, unlike water, ammonia, and methane.\nHydrogen sulfide.\nHydrogen sulfide is the closest chemical analog to water, but is less polar and a weaker inorganic solvent. Hydrogen sulfide is quite plentiful on Jupiter's moon Io and may be in liquid form a short distance below the surface; astrobiologist Dirk Schulze-Makuch has suggested it as a possible solvent for life there. On a planet with hydrogen-sulfide oceans the source of the hydrogen sulfide could come from volcanos, in which case it could be mixed in with a bit of hydrogen fluoride, which could help dissolve minerals. Hydrogen-sulfide life might use a mixture of carbon monoxide and carbon dioxide as their carbon source. They might produce and live on sulfur monoxide, which is analogous to oxygen (O2). Hydrogen sulfide, like hydrogen cyanide and ammonia, suffers from the small temperature range where it is liquid, though that, like that of hydrogen cyanide and ammonia, increases with increasing pressure.\nSilicon dioxide and silicates.\nSilicon dioxide, also known as silica and quartz, is very abundant in the universe and has a large temperature range where it is liquid. However, its melting point is , so it would be impossible to make organic compounds in that temperature, because all of them would decompose. Silicates are similar to silicon dioxide and some have lower melting points than silica. Gerald Feinberg and Robert Shapiro have suggested that molten silicate rock could serve as a liquid medium for organisms with a chemistry based on silicon, oxygen, and other elements such as aluminium.\nOther solvents or cosolvents.\nOther solvents sometimes proposed:\nSulfuric acid in liquid form is strongly polar. It remains liquid at higher temperatures than water, its liquid range being 10\u00a0\u00b0C to 337\u00a0\u00b0C at a pressure of 1\u00a0atm, although above 300\u00a0\u00b0C it slowly decomposes. Sulfuric acid is known to be abundant in the clouds of Venus, in the form of aerosol droplets. In a biochemistry that used sulfuric acid as a solvent, the alkene group (C=C), with two carbon atoms joined by a double bond, could function analogously to the carbonyl group (C=O) in water-based biochemistry.\nA proposal has been made that life on Mars may exist and be using a mixture of water and hydrogen peroxide as its solvent. \nA 61.2% (by mass) mix of water and hydrogen peroxide has a freezing point of \u221256.5\u00a0\u00b0C and tends to super-cool rather than crystallize. It is also hygroscopic, an advantage in a water-scarce environment.\nSupercritical carbon dioxide has been proposed as a candidate for alternative biochemistry due to its ability to selectively dissolve organic compounds and assist the functioning of enzymes and because \"super-Earth\"- or \"super-Venus\"-type planets with dense high-pressure atmospheres may be common.\nOther speculations.\nNon-green photosynthesizers.\nPhysicists have noted that, although photosynthesis on Earth generally involves green plants, a variety of other-colored plants could also support photosynthesis, essential for most life on Earth, and that other colors might be preferred in places that receive a different mix of stellar radiation than Earth. \nThese studies indicate that blue plants would be unlikely; however yellow or red plants may be relatively common.\nVariable environments.\nMany Earth plants and animals undergo major biochemical changes during their life cycles as a response to changing environmental conditions, for example, by having a spore or hibernation state that can be sustained for years or even millennia between more active life stages. Thus, it would be biochemically possible to sustain life in environments that are only periodically consistent with life as we know it.\nFor example, frogs in cold climates can survive for extended periods of time with most of their body water in a frozen state, whereas desert frogs in Australia can become inactive and dehydrate in dry periods, losing up to 75% of their fluids, yet return to life by rapidly rehydrating in wet periods. Either type of frog would appear biochemically inactive (i.e. not living) during dormant periods to anyone lacking a sensitive means of detecting low levels of metabolism.\nAlanine world and hypothetical alternatives.\nThe genetic code evolved during the transition from the RNA world to a protein world. The Alanine World Hypothesis postulates that the evolution of the genetic code (the so-called GC phase ) started with only four basic amino acids: alanine, glycine, proline and ornithine (now arginine). The evolution of the genetic code ended with 20 proteinogenic amino acids. From a chemical point of view, most of them are Alanine-derivatives particularly suitable for the construction of \u03b1-helices and \u03b2-sheets - basic secondary structural elements of modern proteins. Direct evidence of this is an experimental procedure in molecular biology known as alanine scanning.\nThe hypothetical \"Proline World\" would create a possible alternative life with the genetic code based on the proline chemical scaffold as the protein backbone. Similarly, \"Glycine\" and \"Ornithine\" worlds are also conceivable, but nature has chosen none of them. Evolution of life with Glycine, Proline or Ornithine as the basic structure for protein-like polymers (foldamers) would lead to parallel biological worlds. They would have morphologically radically different body plans and genetics from the living organisms of the known biosphere.\nNonplanetary life.\nDust and plasma-based.\nIn 2007, Vadim N. Tsytovich and colleagues proposed that lifelike behaviors could be exhibited by dust particles suspended in a plasma, under conditions that might exist in space. Computer models showed that, when the dust became charged, the particles could self-organize into microscopic helical structures, and the authors offer \"a rough sketch of a possible model of...helical grain structure reproduction\".\nScientists who have published on this topic.\nScientists who have considered possible alternatives to carbon-water biochemistry include:"}
{"id": "7322", "revid": "12023796", "url": "https://en.wikipedia.org/wiki?curid=7322", "title": "Creation myth", "text": "A creation myth (or cosmogonic myth) is a symbolic narrative of how the world began and how people first came to inhabit it. While in popular usage the term \"myth\" often refers to false or fanciful stories, members of cultures often ascribe varying degrees of truth to their creation myths. In the society in which it is told, a creation myth is usually regarded as conveying profound truthsmetaphorically, symbolically, historically, or literally. They are commonly, although not always, considered cosmogonical mythsthat is, they describe the ordering of the cosmos from a state of chaos or amorphousness.\nCreation myths often share a number of features. They often are considered sacred accounts and can be found in nearly all known religious traditions. They are all stories with a plot and characters who are either deities, human-like figures, or animals, who often speak and transform easily. They are often set in a dim and nonspecific past that historian of religion Mircea Eliade termed \"in illo tempore\" ('at that time'). Creation myths address questions deeply meaningful to the society that shares them, revealing their central worldview and the framework for the self-identity of the culture and individual in a universal context.\nCreation myths develop in oral traditions and therefore typically have multiple versions; found throughout human culture, they are the most common form of myth.\nDefinitions.\nCreation myth definitions from modern references:\nReligion professor Mircea Eliade defined the word \"myth\" in terms of creation: \nMyth narrates a sacred history; it relates an event that took place in primordial Time, the fabled time of the \"beginnings.\" In other words, myth tells how, through the deeds of Supernatural Beings, a reality came into existence, be it the whole of reality, the Cosmos, or only a fragment of reality \u2013 an island, a species of plant, a particular kind of human behavior, an institution.\nMeaning and function.\nAll creation myths are in one sense etiological because they attempt to explain how the world was formed and where humanity came from. Myths attempt to explain the unknown and sometimes teach a lesson.\nEthnologists and anthropologists who study these myths say that in the modern context theologians try to discern humanity's meaning from revealed truths and scientists investigate cosmology with the tools of empiricism and rationality, but creation myths define human reality in very different terms. In the past historians of religion and other students of myth thought of them as forms of primitive or early-stage science or religion and analyzed them in a literal or logical sense. Today, however, they are seen as symbolic narratives which must be understood in terms of their own cultural context. Charles Long writes, \"The beings referred to in the myth \u2013 gods, animals, plants \u2013 are forms of power grasped existentially. The myths should not be understood as attempts to work out a rational explanation of deity.\"\nWhile creation myths are not literal explications they do serve to define an orientation of humanity in the world in terms of a birth story. They are the basis of a worldview that reaffirms and guides how people relate to the natural world, to any assumed spiritual world, and to each other. The creation myth acts as a cornerstone for distinguishing primary reality from relative reality, the origin and nature of being from non-being. In this sense they serve as a philosophy of life but one expressed and conveyed through symbol rather than systematic reason. And in this sense they go beyond etiological myths which mean to explain specific features in religious rites, natural phenomena or cultural life. Creation myths also help to orient human beings in the world, giving them a sense of their place in the world and the regard that they must have for humans and nature.\nHistorian David Christian has summarised issues common to multiple creation myths:\nEach beginning seems to presuppose an earlier beginning. ... Instead of meeting a single starting point, we encounter an infinity of them, each of which poses the same problem. ... There are no entirely satisfactory solutions to this dilemma. What we have to find is not a solution but some way of dealing with the mystery ... And we have to do so using words. The words we reach for, from \"God\" to \"gravity\", are inadequate to the task. So we have to use language poetically or symbolically; and such language, whether used by a scientist, a poet, or a shaman, can easily be misunderstood.\nClassification.\nMythologists have applied various schemes to classify creation myths found throughout human cultures. Eliade and his colleague Charles Long developed a classification based on some common motifs that reappear in stories the world over. The classification identifies five basic types:\nMarta Weigle further developed and refined this typology to highlight nine themes, adding elements such as \"deus faber\", a creation crafted by a deity, creation from the work of two creators working together or against each other, creation from sacrifice and creation from division/conjugation, accretion/conjunction, or secretion.\nAn alternative system based on six recurring narrative themes was designed by Raymond Van Over:\n\"Ex nihilo\".\nThe myth that God created the world out of nothing \u2013 \"ex nihilo\" \u2013 is central today to Judaism, Christianity and Islam, and the medieval Jewish philosopher Maimonides felt it was the only concept that the three religions shared. Nonetheless, the concept is not found in the entire Hebrew Bible. The authors of Genesis 1 were concerned not with the origins of matter (the material which God formed into the habitable cosmos), but with assigning roles so that the Cosmos should function. In the early 2nd century CE, early Christian scholars were beginning to see a tension between the idea of world-formation and the omnipotence of God, and by the beginning of the 3rd century creation \"ex nihilo\" had become a fundamental tenet of Christian theology.\n\"Ex nihilo\" creation is found in creation stories from ancient Egypt, the Rig Veda, and many animistic cultures in Africa, Asia, Oceania and North America. In most of these stories the world is brought into being by the speech, dream, breath, or pure thought of a creator but creation ex nihilo may also take place through a creator's bodily secretions.\nThe literal translation of the phrase \"ex nihilo\" is \"from nothing\" but in many creation myths the line is blurred whether the creative act would be better classified as a creation \"ex nihilo\" or creation from chaos. In \"ex nihilo\" creation myths the potential and the substance of creation springs from within the creator. Such a creator may or may not be existing in physical surroundings such as darkness or water, but does not create the world from them, whereas in creation from chaos the substance used for creation is pre-existing within the unformed void.\nCreation from chaos.\nIn creation from chaos myth, initially there is nothing but a formless, shapeless expanse. In these stories the word \"chaos\" means \"disorder\", and this formless expanse, which is also sometimes called a void or an abyss, contains the material with which the created world will be made. Chaos may be described as having the consistency of vapor or water, dimensionless, and sometimes salty or muddy. These myths associate chaos with evil and oblivion, in contrast to \"order\" (\"cosmos\") which is the good. The act of creation is the bringing of order from disorder, and in many of these cultures it is believed that at some point the forces preserving order and form will weaken and the world will once again be engulfed into the abyss. One example is the Genesis creation narrative from the first chapter of the Book of Genesis.\nWorld parent.\nThere are two types of world parent myths, both describing a separation or splitting of a primeval entity, the world parent or parents. One form describes the primeval state as an eternal union of two parents, and the creation takes place when the two are pulled apart. The two parents are commonly identified as Sky (usually male) and Earth (usually female) who in the primeval state were so tightly bound to each other that no offspring could emerge. These myths often depict creation as the result of a sexual union, and serve as genealogical record of the deities born from it.\nIn the second form of world parent myth, creation itself springs from dismembered parts of the body of the primeval being. Often in these stories the limbs, hair, blood, bones or organs of the primeval being are somehow severed or sacrificed to transform into sky, earth, animal or plant life, and other worldly features. These myths tend to emphasize creative forces as animistic in nature rather than sexual, and depict the sacred as the elemental and integral component of the natural world. One example of this is the Norse creation myth described in \"Gylfaginning\" exactly in the poem V\u00f6lusp\u00e1.\nEmergence.\nIn emergence myths humanity emerges from another world into the one they currently inhabit. The previous world is often considered the womb of the earth mother, and the process of emergence is likened to the act of giving birth. The role of midwife is usually played by a female deity, like the spider woman of several mythologies of Indigenous peoples in the Americas. Male characters rarely figure into these stories, and scholars often consider them in counterpoint to male-oriented creation myths, like those of the \"ex nihilo\" variety.\nEmergence myths commonly describe the creation of people and/or supernatural beings as a staged ascent or metamorphosis from nascent forms through a series of subterranean worlds to arrive at their current place and form. Often the passage from one world or stage to the next is impelled by inner forces, a process of germination or gestation from earlier, embryonic forms. The genre is most commonly found in Native American cultures where the myths frequently link the final emergence of people from a hole opening to the underworld to stories about their subsequent migrations and eventual settlement in their current homelands.\nEarth-diver.\nThe earth-diver is a common character in various traditional creation myths. In these stories a supreme being usually sends an animal into the primal waters to find bits of sand or mud with which to build habitable land. Some scholars interpret these myths psychologically while others interpret them cosmogonically. In both cases emphasis is placed on beginnings emanating from the depths. Earth-diver myths are common in Native American folklore but can be found among the Chukchi and Yukaghir, the Tatars and many Finno-Ugrian traditions. The pattern of distribution of these stories suggest they have a common origin in the eastern Asiatic coastal region, spreading as peoples migrated west into Siberia and east to the North American continent. However, there are examples of this mytheme found well outside of this boreal distribution pattern, for example the West African Yoruba creation myth of Obatala and Oduduwa.\nCharacteristic of many Native American myths, earth-diver creation stories begin as beings and potential forms linger asleep or suspended in the primordial realm. The earth-diver is among the first of them to awaken and lay the necessary groundwork by building suitable lands where the coming creation will be able to live. In many cases, these stories will describe a series of failed attempts to make land before the solution is found.\nFurther reading.\nOn the Earth-diver motif:"}
{"id": "7324", "revid": "3729738", "url": "https://en.wikipedia.org/wiki?curid=7324", "title": "Crucifix", "text": "A crucifix (from Latin \"cruci fixus\" meaning \"(one) fixed to a cross\") is an image of Jesus on the cross, as distinct from a bare cross. The representation of Jesus himself on the cross is referred to in English as the \"corpus\" (Latin for \"body\").\nThe crucifix is a principal symbol for many groups of Christians, and one of the most common forms of the Crucifixion in the arts. It is especially important in the Latin Rite of the Roman Catholic Church, but is also used in the Eastern Orthodox Church, most Oriental Orthodox Churches (except the Armenian &amp; Syriac Church), and the Eastern Catholic Churches, as well as by the Lutheran, Moravian and Anglican Churches. The symbol is less common in churches of other Protestant denominations, and in the Assyrian Church of the East and Armenian Apostolic Church, which prefer to use a cross without the figure of Jesus (the \"corpus\"). The crucifix emphasizes Jesus' sacrifice\u2014his death by crucifixion, which Christians believe brought about the redemption of mankind. Most crucifixes portray Jesus on a Latin cross, rather than any other shape, such as a Tau cross or a Coptic cross.\nWestern crucifixes usually have a three-dimensional \"corpus\", but in Eastern Orthodoxy Jesus' body is normally painted on the cross, or in low relief. Strictly speaking, to be a crucifix, the cross must be three-dimensional, but this distinction is not always observed. An entire painting of the Crucifixion of Jesus including a landscape background and other figures is not a crucifix either.\nLarge crucifixes high across the central axis of a church are known by the Old English term rood. By the late Middle Ages these were a near-universal feature of Western churches, but are now very rare. Modern Roman Catholic churches and many Lutheran churches often have a crucifix above the altar on the wall; for the celebration of Mass, the Roman Rite of the Catholic Church requires that \"on or close to the altar there is to be a cross with a figure of Christ crucified\".\nDescription.\nThe standard, four-pointed Latin crucifix consists of an upright post or \"stipes\" and a single crosspiece to which the sufferer's arms were nailed. There may also be a short projecting nameplate, showing the letters INRI (Greek: INBI). The Russian Orthodox crucifix usually has an additional third crossbar, to which the feet are nailed, and which is angled upward toward the penitent thief Saint Dismas (to the viewer's left) and downward toward the impenitent thief Gestas (to the viewer's right). The corpus of Eastern crucifixes is normally a two-dimensional or low relief icon that shows Jesus as already dead, his face peaceful and somber. They are rarely three-dimensional figures as in the Western tradition, although these may be found where Western influences are strong, but are more typically icons painted on a piece of wood shaped to include the double-barred cross and perhaps the edge of Christ's hips and halo, and no background. More sculptural small crucifixes in metal relief are also used in Orthodoxy (see gallery examples), including as pectoral crosses and blessing crosses.\nWestern crucifixes may show Christ dead or alive, the presence of the spear wound in his ribs traditionally indicating that he is dead. In either case his face very often shows his suffering. In Orthodoxy he has normally been shown as dead since around the end of the period of Byzantine Iconoclasm. Eastern crucifixes have Jesus' two feet nailed side by side, rather than crossed one above the other, as Western crucifixes have shown them since around the 13th century. The crown of thorns is also generally absent in Eastern crucifixes, since the emphasis is not on Christ's suffering, but on his triumph over sin and death. The \"S\"-shaped position of Jesus' body on the cross is a Byzantine innovation of the late 10th century, though also found in the German Gero Cross of the same date. Probably more from Byzantine influence, it spread elsewhere in the West, especially to Italy, by the Romanesque period, though it was more usual in painting than sculpted crucifixes. It's in Italy that the emphasis was put on Jesus' suffering and realistic details, during a process of general humanization of Christ favored by the Franciscan order. During the 13th century the suffering Italian model (\"Christus patiens\") triumphed over the traditional Byzantine one (\"Christus gloriosus\") anywhere in Europe also due to the works of artists such as Giunta Pisano and Cimabue. Since the Renaissance the \"S\"-shape is generally much less pronounced. Eastern Christian blessing crosses will often have the Crucifixion depicted on one side, and the Resurrection on the other, illustrating the understanding of Orthodox theology that the Crucifixion and Resurrection are two intimately related aspects of the same act of salvation.\nAnother, symbolic, depiction shows a triumphant Christ (), clothed in robes, rather than stripped as for His execution, with arms raised, appearing to rise up from the cross, sometimes accompanied by \"rays of light\", or an aureole encircling His Body. He may be robed as a prophet, crowned as a king, and vested in a stole as Great High Priest.\nOn some crucifixes a skull and crossbones are shown below the corpus, referring to Golgotha (Calvary), the site at which Jesus was crucified, which the Gospels say means in Hebrew \"the place of the skull.\" Medieval tradition held that it was the burial-place of Adam and Eve, and that the cross of Christ was raised directly over Adam's skull, so many crucifixes manufactured in Catholic countries still show the skull and crossbones below the corpus.\nVery large crucifixes have been built, the largest being the Cross in the Woods in Michigan, with a high statue.\nUsage.\nIn the early Church, many Christians hung a cross on the eastern wall of their house in order to indicate the eastward direction of prayer. Prayer in front of a crucifix, which is seen as a sacramental, is often part of devotion for Christians, especially those worshipping in a church, also privately. The person may sit, stand, or kneel in front of the crucifix, sometimes looking at it in contemplation, or merely in front of it with head bowed or eyes closed. During the Middle Ages small crucifixes, generally hung on a wall, became normal in the personal cells or living quarters first of monks, then all clergy, followed by the homes of the laity, spreading down from the top of society as these became cheap enough for the average person to afford. Most towns had a large crucifix erected as a monument, or some other shrine at the crossroads of the town. Building on the ancient custom, many Catholics, Lutherans and Anglicans hang a crucifix inside their homes and also use the crucifix as a focal point of a home altar. The wealthy erected proprietary chapels as they could afford to do this.\nCatholic (both Eastern and Western), Eastern Orthodox, Oriental Orthodox, Moravian, Anglican and Lutheran Christians generally use the crucifix in public religious services. They believe use of the crucifix is in keeping with the statement by Saint Paul in Scripture, \"we preach Christ crucified, a stumbling block to Jews and folly to Gentiles, but to those who are called, both Jews and Greeks, Christ the power of God and the wisdom of God\".\nIn the West altar crosses and processional crosses began to be crucifixes in the 11th century, which became general around the 14th century, as they became cheaper. The Roman Rite requires that \"either on the altar or near it, there is to be a cross, with the figure of Christ crucified upon it, a cross clearly visible to the assembled people. It is desirable that such a cross should remain near the altar even outside of liturgical celebrations, so as to call to mind for the faithful the saving Passion of the Lord.\" The requirement of the altar cross was also mentioned in pre-1970 editions of the Roman Missal, though not in the original 1570 Roman Missal of Pope Pius V. The Rite of Funerals says that the Gospel Book, the Bible, or a cross (which will generally be in crucifix form) may be placed on the coffin for a Requiem Mass, but a second standing cross is not to be placed near the coffin if the altar cross can be easily seen from the body of the church.\nEastern Christian liturgical processions called crucessions include a cross or crucifix at their head. In the Eastern Orthodox Church, the crucifix is often placed above the iconostasis in the church. In the Russian Orthodox Church a large crucifix (\"Golgotha\") is placed behind the Holy Table (altar). During Matins of Good Friday, a large crucifix is taken in procession to the centre of the church, where it is venerated by the faithful. Sometimes the \"soma\" (corpus) is removable and is taken off the crucifix at Vespers that evening during the Gospel lesson describing the Descent from the Cross. The empty cross may then remain in the centre of the church until the Paschal vigil (local practices vary). The blessing cross which the priest uses to bless the faithful at the dismissal will often have the crucifix on one side and an icon of the Resurrection of Jesus on the other, the side with the Resurrection being used on Sundays and during Paschaltide, and the crucifix on other days.\nExorcist Gabriele Amorth has stated that the crucifix is one of the most effective means of averting or opposing demons. In folklore, it is believed to ward off vampires, incubi, succubi, and other evils.\nModern iconoclasts have used an inverted (upside-down) crucifix when showing disdain for Jesus Christ or the Catholic Church which believes in his divinity. According to Christian tradition, Saint Peter was martyred by being crucified upside-down.\nControversies.\nProtestant Reformation.\nIn the Moravian Church, Nicolaus Zinzendorf had an experience in which he believed he encountered Jesus. Seeing a painting of a crucifix, Zinzendorf fell on his knees vowing to glorify Jesus after contemplating on the wounds of Christ and an inscription that stated \"This is what I have done for you, what will you do for me?\u201d\nThe Lutheran Churches retained the use of the crucifix, \"justifying \"their continued use of medieval crucifixes with the same arguments employed since the Middle Ages, as is evident from the example of the altar of the Holy Cross in the Cistercian church of Doberan.\" Martin Luther did not object to them, and this was among his differences with Andreas Karlstadt as early as 1525. At the time of the Reformation, Luther retained the crucifix in the Lutheran Church and they remain the center of worship in Lutheran parishes across Europe. In the United States, however, Lutheranism came under the influence of Calvinism, and the plain cross came to be used in many churches. In contrast to the practice of the Moravian Church and Lutheran Churches, the early Reformed Churches rejected the use of the crucifix, and indeed the unadorned cross, along with other traditional religious imagery, as idolatrous. Calvin, considered to be the father of the Reformed Church, was violently opposed to both cross and crucifix. In England, the Royal Chapels of Elizabeth I were most unusual among local churches in retaining crucifixes, following the Queen's conservative tastes. These disappeared under her successor, James I, and their brief re-appearance in the early 1620s when James' heir was seeking a Spanish marriage was the subject of rumour and close observation by both Catholics and Protestants; when the match fell through they disappeared.\nModern.\nIn 2005, a mother accused her daughter's school in Derby, England, of discriminating against Christians after the teenager was suspended for refusing to take off a crucifix necklace.\nA British prison ordered a multi-faith chapel to remove all crucifixes \"in case it offends Muslims.\"\nIn 2008 in Spain, a local judge ordered crucifixes removed from public schools to settle a decades-old dispute over whether crucifixes should be displayed in public buildings in a non-confessional state.\nOn 18 March 2011, the European Court of Human Rights ruled in the \"Lautsi v. Italy\" case, that the requirement in Italian law that crucifixes be displayed in classrooms of state schools does not violate the European Convention on Human Rights. Crucifixes are common in most other Italian official buildings, including courts of law.\nOn 24 March 2011, the Constitutional Court of Peru ruled that the presence of crucifixes in courts of law does not violate the secular nature of the state."}
{"id": "7325", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7325", "title": "COFDM", "text": ""}
{"id": "7326", "revid": "12070", "url": "https://en.wikipedia.org/wiki?curid=7326", "title": "Coded orthogonal frequency division modulation", "text": ""}
{"id": "7327", "revid": "40413310", "url": "https://en.wikipedia.org/wiki?curid=7327", "title": "Copernican principle", "text": "In physical cosmology, the Copernican principle states that humans, on the Earth or in the Solar System, are not privileged observers of the universe.\nNamed for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus's argument of a moving Earth. In some sense, it is equivalent to the mediocrity principle.\nOrigin and implications.\nHermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets could be explained by reference to an assumption that the Sun is centrally located and stationary in contrast to the then currently upheld belief that the Earth was central. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as \"demoting\" Earth from its central role it had in the Ptolemaic geocentric model, it was successors to Copernicus, notably the 16th century Giordano Bruno, who adopted this new perspective. The Earth's central position had been interpreted as being in the \"lowest and filthiest parts\". Instead, as Galileo said, the Earth is part of the \"dance of the stars\" rather than the \"sump where the universe's filth and ephemera collect\". In the late 20th Century, Carl Sagan asked, \"Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.\"\nIn cosmology, if one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from the vantage point of Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle. In practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. It becomes more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 200 million parsecs. However, on scales comparable to the radius of the observable universe, we see systematic changes with distance from Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. While this might suggest that Earth is at the center of the universe, the Copernican principle requires us to interpret it as evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach Earth and shows the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.\nModern mathematical cosmology is based on the assumption that the Cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations.\nMichael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: \"It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe.\"\nBondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.\nSince the 1990s the term has been used (interchangeably with \"the Copernicus method\") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.\nTests of the principle.\nThe Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\nHistorical.\nBefore the term Copernican principle was even coined, Earth was repeatedly shown not to have any special location in the universe. The Copernican Revolution dethroned Earth to just one of many planets orbiting the Sun. Proper motion was mentioned by Halley. William Herschel found that the Solar System is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that the Milky Way galaxy is just one of many galaxies in the universe. Examination of the galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.\nModern tests.\nRecent and planned tests relevant to the cosmological and Copernican principles include:\nPhysics without the principle.\nThe standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle. The Lambda-CDM model's observations are largely consistent, but there remain unsolved problems. Some cosmologists and theoretical physicists have created models without the Cosmological or Copernican principles to constrain the values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.\nA prominent example in this context is the observed accelerating universe and cosmological constant. Instead of using the current accepted idea of dark energy, this model proposes the universe is much more inhomogeneous than currently assumed, and instead, we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle."}
{"id": "7329", "revid": "1956573", "url": "https://en.wikipedia.org/wiki?curid=7329", "title": "Cyprinidae", "text": "The Cyprinidae are the family of freshwater fish, collectively called cyprinids, that includes the carps, the true minnows, and their relatives (for example, the barbs and barbels). Also commonly called the \"carp family\", or \"minnow family\", Cyprinidae is the largest and most diverse fish family and the largest vertebrate animal family in general, with about 3,000 species of which only 1,270 remain extant, divided into about 370 genera. They range from about 12\u00a0mm to the 3-m \"Catlocarpio siamensis.\" The family belongs to the ostariophysian order Cypriniformes, of whose genera and species the cyprinids make up more than two-thirds. The family name is derived from the Ancient Greek \"kypr\u00eenos\" (\u03ba\u03c5\u03c0\u03c1\u1fd6\u03bd\u03bf\u03c2, \"carp\").\nBiology and ecology.\nCyprinids are stomachless fish with toothless jaws. Even so, food can be effectively chewed by the gill rakers of the specialized last gill bow. These pharyngeal teeth allow the fish to make chewing motions against a chewing plate formed by a bony process of the skull. The pharyngeal teeth are unique to each species and are used by scientists to identify species. Strong pharyngeal teeth allow fish such as the common carp and ide to eat hard baits such as snails and bivalves.\nHearing is a well-developed sense in the cyprinids since they have the Weberian organ, three specialized vertebral processes that transfer motion of the gas bladder to the inner ear. The vertebral processes of the Weberian organ also permit a cyprinid to detect changes in motion of the gas bladder due to atmospheric conditions or depth changes. The cyprinids are considered physostomes because the pneumatic duct is retained in adult stages and the fish are able to gulp air to fill the gas bladder, or they can dispose of excess gas to the gut.\nCyprinids are native to North America, Africa, and Eurasia. The largest known cyprinid is the giant barb (\"Catlocarpio siamensis\"), which may grow up to in length and in weight. Other very large species that can surpass are the golden mahseer (\"Tor putitora\") and mangar (\"Luciobarbus esocinus\"). The largest North American species is the Colorado pikeminnow (\"Ptychocheilus lucius\"), which can reach up to in length. Conversely, many species are smaller than . The smallest known fish is \"Paedocypris progenetica\", reaching at the longest.\nAll fish in this family are egg-layers and most do not guard their eggs; however, a few species build nests and/or guard the eggs. The bitterlings of subfamily Acheilognathinae are notable for depositing their eggs in bivalve molluscs, where the young develop until able to fend for themselves.\nMost cyprinids feed mainly on invertebrates and vegetation, probably due to the lack of teeth and stomach; however, some species, like the asp, are predators that specialize in fish. Many species, such as the ide and the common rudd, prey on small fish when individuals become large enough. Even small species, such as the moderlieschen, are opportunistic predators that will eat larvae of the common frog in artificial circumstances.\nSome cyprinids, such as the grass carp, are specialized herbivores; others, such as the common nase, eat algae and biofilms, while others, such as the black carp, specialize in snails, and some, such as the silver carp, are specialized filter feeders. For this reason, cyprinids are often introduced as a management tool to control various factors in the aquatic environment, such as aquatic vegetation and diseases transmitted by snails.\nUnlike most fish species, cyprinids generally increase in abundance in eutrophic lakes. Here, they contribute towards positive feedback as they are efficient at eating the zooplankton that would otherwise graze on the algae, reducing its abundance.\nRelationship with humans.\nCyprinids are highly important food fish; they are fished and farmed across Eurasia. In land-locked countries in particular, cyprinids are often the major species of fish eaten because they make the largest part of biomass in most water types except for fast-flowing rivers. In Eastern Europe, they are often prepared with traditional methods such as drying and salting. The prevalence of inexpensive frozen fish products made this less important now than it was in earlier times. Nonetheless, in certain places, they remain popular for food, as well as recreational fishing, and have been deliberately stocked in ponds and lakes for centuries for this reason.\nCyprinids are popular for angling especially for match fishing (due to their dominance in biomass and numbers) and fishing for common carp because of its size and strength.\nSeveral cyprinids have been introduced to waters outside their natural ranges to provide food, sport, or biological control for some pest species. The common carp (\"Cyprinus carpio\") and the grass carp (\"Ctenopharyngodon idella\") are the most important of these, for example in Florida. In some cases, such as the Asian carp in the Mississippi Basin, they have become invasive species that compete with native fishes or disrupt the environment. Carp in particular can stir up sediment, reducing the clarity of the water and making plant growth difficult.\nNumerous cyprinids have become important in the aquarium and fishpond hobbies, most famously the goldfish, which was bred in China from the Prussian carp (\"Carassius (auratus) gibelio\"). First imported into Europe around 1728, it was much fancied by Chinese nobility as early as 1150AD and after it arrived there in 1502, also in Japan. In the latter country, from the 18th century onwards, the common carp was bred into the ornamental variety known as koi \u2013 or more accurately , as simply means \"common carp\" in Japanese.\nOther popular aquarium cyprinids include danionins, rasborines, and true barbs. Larger species are bred by the thousands in outdoor ponds, particularly in Southeast Asia, and trade in these aquarium fishes is of considerable commercial importance. The small rasborines and danionines are perhaps only rivalled by characids and poecilid livebearers in their popularity for community aquaria.\nOne particular species of these small and undemanding danionines is the zebrafish (\"Danio rerio\"). It has become the standard model species for studying developmental genetics of vertebrates, in particular fish.\nHabitat destruction and other causes have reduced the wild stocks of several cyprinids to dangerously low levels; some are already entirely extinct. In particular, the cyprinids of the subfamily Leuciscinae from southwestern North America have been hit hard by pollution and unsustainable water use in the early to mid-20th century; most globally extinct cypriniform species are in fact leuciscinid cyprinids from the southwestern United States and northern Mexico.\nSystematics.\nThe massive diversity of cyprinids has so far made it difficult to resolve their phylogeny in sufficient detail to make assignment to subfamilies more than tentative in many cases. Some distinct lineages obviously exist \u2013 for example, the Cultrinae and Leuciscinae, regardless of their exact delimitation, are rather close relatives and stand apart from Cyprininaebut the overall systematics and taxonomy of the Cyprinidae remain a subject of considerable debate. A large number of genera are \"incertae sedis\", too equivocal in their traits and/or too little-studied to permit assignment to a particular subfamily with any certainty.\nPart of the solution seems that the delicate rasborines are the core group, consisting of minor lineages that have not shifted far from their evolutionary niche, or have coevolved for millions of years. These are among the most basal lineages of living cyprinids. Other \"rasborines\" are apparently distributed across the diverse lineages of the family.\nThe validity and circumscription of proposed subfamilies like the Labeoninae or Squaliobarbinae also remain doubtful, although the latter do appear to correspond to a distinct lineage. The sometimes-seen grouping of the large-headed carps (Hypophthalmichthyinae) with \"Xenocypris\", though, seems quite in error. More likely, the latter are part of the Cultrinae.\nThe entirely paraphyletic \"Barbinae\" and the disputed Labeoninae might be better treated as part of the Cyprininae, forming a close-knit group whose internal relationships are still little known. The small African \"barbs\" do not belong in \"Barbus\" \"sensu stricto\" \u2013 indeed, they are as distant from the typical barbels and the typical carps (\"Cyprinus\") as these are from \"Garra\" (which is placed in the Labeoninae by most who accept the latter as distinct) and thus might form another as yet unnamed subfamily. However, as noted above, how various minor lineages tie into this has not yet been resolved; therefore, such a radical move, though reasonable, is probably premature.\nThe tench (\"Tinca tinca\"), a significant food species farmed in western Eurasia in large numbers, is unusual. It is most often grouped with the Leuciscinae, but even when these were rather loosely circumscribed, it always stood apart. A cladistic analysis of DNA sequence data of the S7 ribosomal protein intron1 supports the view that it is distinct enough to constitute a monotypic subfamily. It also suggests it may be closer to the small East Asian \"Aphyocypris\", \"Hemigrammocypris\", and \"Yaoshanicus\". They would have diverged roughly at the same time from cyprinids of east-central Asia, perhaps as a result of the Alpide orogeny that vastly changed the topography of that region in the late Paleogene, when their divergence presumably occurred.\nA DNA-based analysis of these fish places the Rasborinae as the basal lineage with the Cyprininae as a sister clade to the Leuciscinae. The subfamilies Acheilognathinae, Gobioninae, and Leuciscinae are monophyletic.\nSubfamilies and genera.\nThe 5th Edition of Fishes of the World sets out the following subfamilies:\n\"Incertae sedis\".\nWith such a large and diverse family the taxonomy and phylogenies are always being worked on so alternative classifications are being created as new information is discovered, foe example:\nPhylogeny.\nSubfamily Probarbinae\nSubfamily Labeoninae\nSubfamily Torinae\nSubfamily Smiliogastrinae\nSubfamily Cyprininae [incl. Barbinae]\nSubfamily Danioninae\nSubfamily Leptobarbinae\nSubfamily Xenocypridinae [incl. Cultrinae &amp; Squaliobarbinae]\nSubfamily Tincinae\nSubfamily Acheilognathinae (bitterlings)\nSubfamily Gobioninae\nSubfamily Tanichthyinae\nSubfamily Leuciscinae [incl. Alburninae]"}
{"id": "7330", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=7330", "title": "Complementary DNA", "text": "In genetics, complementary DNA (cDNA) is DNA synthesized from a single-stranded RNA (e.g., messenger RNA (mRNA) or microRNA (miRNA)) template in a reaction catalyzed by the enzyme reverse transcriptase. cDNA is often used to clone eukaryotic genes in prokaryotes. When scientists want to express a specific protein in a cell that does not normally express that protein (i.e., heterologous expression), they will transfer the cDNA that codes for the protein to the recipient cell. In molecular biology, cDNA is also generated to analyze transcriptomic profiles in bulk tissue, single cells, or single nuclei in assays such as microarrays and RNA-seq.\ncDNA is also produced naturally by retroviruses (such as HIV-1, HIV-2, simian immunodeficiency virus, etc.) and then integrated into the host's genome, where it creates a provirus.\nThe term \"cDNA\" is also used, typically in a bioinformatics context, to refer to an mRNA transcript's sequence, expressed as DNA bases (deoxy-GCAT) rather than RNA bases (GCAU).\nSynthesis.\nRNA serves as a template for cDNA synthesis. In cellular life, cDNA is generated by viruses and retrotransposons for integration of RNA into target genomic DNA. In molecular biology, RNA is purified from source material after genomic DNA, proteins and other cellular components are removed. cDNA is then synthesized through \"in vitro\" reverse transcription.\nRNA Purification.\nRNA is transcribed from genomic DNA in host cells and is extracted by first lysing cells then purifying RNA utilizing widely-used methods such as phenol-chloroform, silica column, and bead-based RNA extraction methods. Extraction methods vary depending on the source material. For example, extracting RNA from plant tissue requires additional reagents, such as polyvinylpyrrolidone (PVP), to remove phenolic compounds, carbohydrates, and other compounds that will otherwise render RNA unusable. To remove DNA and proteins, enzymes such as DNase and Proteinase K are used for degradation. Importantly, RNA integrity is maintained by inactivating RNases with chaotropic agents such as guanidinium isothiocyanate, sodium dodecyl sulphate (SDS), phenol or chloroform. Total RNA is then separated from other cellular components and precipitated with alcohol. Various commercial kits exist for simple and rapid RNA extractions for specific applications. Additional bead-based methods can be used to isolate specific sub-types of RNA (e.g. mRNA and microRNA) based on size or unique RNA regions.\nReverse Transcription.\nFirst-strand synthesis.\nUsing a reverse transcriptase enzyme and purified RNA templates, one strand of cDNA is produced (first-strand cDNA synthesis). The M-MLV reverse transcriptase from the Moloney murine leukemia virus is commonly used due to its reduced RNAse H activity suited for transcription of longer RNAs. The AMV reverse transcriptase from the avian myeloblastosis virus may also be used for RNA templates with strong secondary structures (i.e. high melting temperature). cDNA is commonly generated from mRNA for gene expression analyses such as RT-qPCR and RNA-seq. mRNA is selectively reverse transcribed using oligo-dT primers that are the reverse complement of the poly-adenylated tail on the 3' end of all mRNA. An optimized mixture of oligo-dT and random hexamer primers increases the chance of obtaining full-length cDNA while reducing 5' or 3' bias. Ribosomal RNA may also be depleted to enrich both mRNA and non-poly-adenylated transcripts such as some non-coding RNA.\nSecond-strand synthesis.\nThe result of first-strand syntheses, RNA-DNA hybrids, can be processed through multiple second-strand synthesis methods or processed directly in downstream assays. An early method known as hairpin-primed synthesis relied on hairpin formation on the 3' end of the first-strand cDNA to prime second-strand synthesis. However, priming is random and hairpin hydrolysis leads to loss of information. The Gubler and Hoffman Procedure uses E. Coli RNase H to nick mRNA that is replaced with E. Coli DNA Polymerase I and sealed with E. Coli DNA Ligase. An optimization of this procedure relies on low RNAse H activity of M-MLV to nick mRNA with remaining RNA later removed by adding RNase H after DNA Polymerase translation of the second-strand cDNA. This prevents lost sequence information at the 5' end of the mRNA.\nApplications.\nComplementary DNA is often used in gene cloning or as gene probes or in the creation of a cDNA library. When scientists transfer a gene from one cell into another cell in order to express the new genetic material as a protein in the recipient cell, the cDNA will be added to the recipient (rather than the entire gene), because the DNA for an entire gene may include DNA that does not code for the protein or that interrupts the coding sequence of the protein (e.g., introns). Partial sequences of cDNAs are often obtained as expressed sequence tags.\nWith amplification of DNA sequences via polymerase chain reaction (PCR) now commonplace, one will typically conduct reverse transcription as an initial step, followed by PCR to obtain an exact sequence of cDNA for intra-cellular expression. This is achieved by designing sequence-specific DNA primers that hybridize to the 5' and 3' ends of a cDNA region coding for a protein. Once amplified, the sequence can be cut at each end with nucleases and inserted into one of many small circular DNA sequences known as expression vectors. Such vectors allow for self-replication, inside the cells, and potentially integration in the host DNA. They typically also contain a strong promoter to drive transcription of the target cDNA into mRNA, which is then translated into protein.\nOn 13 June 2013, the United States Supreme Court ruled in the case of \"Association for Molecular Pathology v. Myriad Genetics\" that while naturally occurring human genes cannot be patented, cDNA is patent eligible because it does not occur naturally.\ncDNA is also used to study gene expression via methods such as RNA-seq or RT-qPCR. For sequencing, RNA must be fragmented due to sequencing platform size limitations. Additionally, second-strand synthesized cDNA must be ligated with adapters that allow cDNA fragments to be PCR amplified and bind to sequencing flow cells. Gene-specific analysis methods commonly use microarrays and RT-qPCR to quantify cDNA levels via fluorometric and other methods.\nViruses and retrotransposons.\nSome viruses also use cDNA to turn their viral RNA into mRNA (viral RNA \u2192 cDNA \u2192 mRNA). The mRNA is used to make viral proteins to take over the host cell.\nAn example of this first step from viral DNA to cDNA can be seen in the HIV cycle of infection. Here, the host cell membrane becomes attached to the virus\u2019 lipid envelope which allows the viral capsid with two copies of viral genome RNA to enter the host. The cDNA copy is then made though reverse transcription of the viral RNA, a process facilitated by the chaperone CypA and a viral capsid associated reverse transcriptase.\ncDNA is also generated by retrotransposons in eukaryotic genomes. Retrotransposons are mobile genetic elements that move themselves within, and sometimes between, genomes via RNA intermediates. This mechanism is shared with viruses with the exclusion of the generation of infectious particles."}
{"id": "7331", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7331", "title": "Cellular digital packet data", "text": "Cellular Digital Packet Data (CDPD) was a wide-area mobile data service which used unused bandwidth normally used by AMPS mobile phones between 800 and 900\u00a0MHz to transfer data. Speeds up to 19.2 kbit/s were possible. The service was discontinued in conjunction with the retirement of the parent AMPS service; it has been functionally replaced by faster services such as 1xRTT, EV-DO, and UMTS/HSPA.\nDeveloped in the early 1990s, CDPD was large on the horizon as a future technology. However, it had difficulty competing against existing slower but less expensive Mobitex and DataTac systems, and never quite gained widespread acceptance before newer, faster standards such as GPRS became dominant.\nCDPD had very limited consumer products. AT&amp;T Wireless first sold the technology in the United States under the PocketNet brand. It was one of the first products of wireless web service. Digital Ocean, Inc. an OEM licensee of the Apple Newton, sold the Seahorse product, which integrated the Newton handheld computer, an AMPS/CDPD handset/modem along with a web browser in 1996, winning the CTIA's hardware product of the year award as a smartphone, arguably the world's first. A company named OmniSky provided service for Palm V devices. Omnisky OmniSky then file bankrupt in 2001 then was picked up by EarthLink Wireless the technician that developed the tech support for all of the wireless technology was a man by the name of Myron Feasel he was brought from company to company ending up at Palm. Wireless later sold CDPD under the Wireless Internet brand (not to be confused with Wireless Internet Express, their brand for GPRS/EDGE data). PocketNet was generally considered a failure with competition from 2G services such as Sprint's Wireless Web. AT&amp;T Wireless sold four PocketNet Phone models to the public: the Samsung Duette and the Mitsubishi MobileAccess-120 were AMPS/CDPD PocketNet phones introduced in October 1997; and two IS-136/CDPD Digital PocketNet phones, the Mitsubishi T-250 and the Ericsson R289LX.\nDespite its limited success as a consumer offering, CDPD was adopted in a number of enterprise and government networks. It was particularly popular as a first-generation wireless data solution for telemetry devices (machine to machine communications) and for public safety mobile data terminals.\nIn 2004, major carriers in the United States announced plans to shut down CDPD service. In July 2005, the AT&amp;T Wireless and Cingular Wireless CDPD networks were shut down. Equipment for this service now has little to no residual value.\nCDPD Network and system.\nPrimary elements of a CDPD network are:\n1. End systems: physical &amp; logical end systems that exchange information\n2. Intermediate systems: CDPD infrastructure elements that store, forward &amp; route the information\nThere are 2 kinds of End systems\n1. Mobile end system: subscriber unit to access CDPD network over a wireless interface\n2. Fixed end system: common host/server that is connected to the CDPD backbone and providing access to specific application and data\nThere are 2 kinds of Intermediate systems\n1. Generic intermediate system: simple router with no knowledge of mobility issues\n2. mobile data intermediate system: specialized intermediate system that routes data based on its knowledge of the current location of Mobile end system. It is a set of hardware and software functions that provide switching, accounting, registration, authentication, encryption, and so on.\nThe design of CDPD was based on several design objectives that are often repeated in designing overlay networks or new networks. A lot of emphasis was laid on open architectures and reusing as much of the existing RF infrastructure as possible. The design goal of CDPD included location independence and independence fro, service provider, so that coverage could be maximized ; application transparency and multiprotocol support, interoperability between products from multiple vendors."}
{"id": "7333", "revid": "37408742", "url": "https://en.wikipedia.org/wiki?curid=7333", "title": "Chimera", "text": "Chimera, chimaera, or chimaira may refer to:"}
{"id": "7335", "revid": "86247", "url": "https://en.wikipedia.org/wiki?curid=7335", "title": "Creature of statute", "text": "A creature of statute (also known as creature of the state) is a legal entity, such as a corporation, created by statute. Creatures of statute may include municipalities and other artificial legal entities or relationships. Thus, when a statute in some fashion requires the formation of a corporate body\u2014often for governmental purposes\u2014such bodies when formed are known as \"creatures of statute.\" The same concept is also expressed with the phrase \"creature of the state.\"\nThe term \"creature of statute\" is most common to the United States. In the United Kingdom, these bodies are simply called statutory corporations (or statutory bodies) and generally have some governmental function. The United Kingdom Atomic Energy Authority is an example. In a wider sense, most companies in the UK are created under statute since the Companies Act 1985 specifies how a company may be created by a member of the public, but these companies are not called 'statutory corporations'. Often, in American legal and business documents that speak of governing bodies (\"e.g.\", a board that governs small businesses in China) these bodies are described as \"creatures of statute\" to inform readers of their origins and format although the national governments that created them may not term them as creatures of statute. Australia also uses the term \"creature of statute\" to describe some governmental bodies.\nThe importance of a corporate body, regardless of its exact function, when such a body is a creature of statute is that its active functions can only be within the scope detailed by the statute which created that corporation. Thereby, the creature of statute is the tangible manifestation of the functions or work described by a given statute. The jurisdiction of a body that is a creature of statute is also therefore limited to the functional scope written into the laws that created that body. Unlike most (private) corporate bodies, creatures of statute cannot expand their business interests into other diverse areas."}
{"id": "7336", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7336", "title": "CPGM", "text": ""}
{"id": "7337", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7337", "title": "Convention of the Metre", "text": ""}
{"id": "7339", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=7339", "title": "General Conference on Weights and Measures", "text": "The General Conference on Weights and Measures (French: \"Conf\u00e9rence G\u00e9n\u00e9rale des Poids et Mesures\", abbreviated CGPM and sometimes referred to as the GCWM) is the supreme authority of the International Bureau of Weights and Measures, the inter-governmental organization established in 1875 under the terms of the Metre Convention through which Member States act together on matters related to measurement science and measurement standards. The CGPM is made up of delegates of the governments of the Member States and observers from the Associates of the CGPM. Under its authority, the International Committee for Weights and Measures (ICWM) (French: \"Comit\u00e9 international des poids et mesures\" (CIPM) executes an exclusive direction and supervision of the BIPM.\nThe General Conference receives the report of the CIPM on work accomplished; it discusses and examines the arrangements required to ensure the propagation and improvement of the International System of Units (SI); it endorses the results of new fundamental metrological determinations and various scientific resolutions of international scope; and it decides all major issues concerning the organization and development of the BIPM, including the financial endowment of the BIPM.\nThe CGPM meets in Paris, usually once every four years. The 25th meeting of the CGPM took place from 18 to 20 November 2014, and the 26th meeting of the CGPM took place in Versailles from 13 to 16 November 2018.\nInitially the Metre Convention was only concerned with the kilogram and the metre, but in 1921 the scope of the treaty was extended to accommodate all physical measurements and hence all aspects of the metric system. In 1960 the 11th CGPM approved the International System of Units, usually known as \"SI\".\nEstablishment.\nOn 20 May 1875 an international treaty known as the \"Convention du M\u00e8tre\" (Metre Convention) was signed by 17 states. This treaty established an international organisation, the Bureau international des poids et mesures (BIPM), consisting of:\nThe CGPM acts on behalf of the governments of its members. In so doing, it appoints members to the CIPM, receives reports from the CIPM which it passes on to the governments and national laboratories on member states, examines and where appropriate approves proposals from the CIPM in respect of changes to the International System of Units (SI), approves the budget for the BIPM (over \u20ac13 million in 2018) and it decides all major issues concerning the organization and development of the BIPM.\nThe structure is analogous to that of a stock corporation. The BIPM is the organisation, the CGPM is the general meeting of the shareholders, the CIPM is the board of directors appointed by the CGPM, and the staff at the site in Saint-Cloud perform the day-to-day work.\nMembership criteria.\nThe CGPM recognises two classes of membership \u2013 full membership for those states that wish to participate in the activities of the BIPM and associate membership for those countries or economies that only wish to participate in the CIPM MRA program. Associate members have observer status at the CGPM. Since all formal liaison between the convention organisations and national governments is handled by the member state's ambassador to France, it is implicit that member states must have diplomatic relations with France, though during both world wars, nations that were at war with France retained their membership of the CGPM. CGPM meetings are chaired by the Pr\u00e9sident de l'Acad\u00e9mie des Sciences de Paris.\nOf the twenty countries that attended the Conference of the Metre in 1875, representatives of seventeen signed the convention on 20 May 1875. In April 1884 HJ Chaney, Warden of Standards in London unofficially contacted the BIPM inquiring whether the BIPM would calibrate some metre standards that had been manufactured in the United Kingdom. Broch, director of the BIPM replied that he was not authorised to perform any such calibrations for non-member states. On 17 September 1884, the British Government signed the convention on behalf of the United Kingdom. This number grew to 21 in 1900, 32 in 1950, and 49 in 2001. , there are 63 Member States and 40 Associate States and Economies of the General Conference (with year of partnership in parentheses):\nMember States.\nArgentina (1877)&lt;br&gt;\nAustralia (1947)&lt;br&gt;\nAustria (1875)&lt;br&gt;\nBelarus&lt;br&gt;\nBelgium (1875)&lt;br&gt;\nBrazil (1921)&lt;br&gt;\nBulgaria (1911)&lt;br&gt;\nCanada (1907)&lt;br&gt;\nChile (1908)&lt;br&gt;\nChina (1977)&lt;br&gt;\nColombia (2012)&lt;br&gt; \nCroatia (2008)&lt;br&gt;\nCzech Republic (1922)&lt;br&gt;\nDenmark (1875)&lt;br&gt;\nEcuador&lt;br&gt;\nEgypt (1962)&lt;br&gt;\nEstonia&lt;br&gt;\nFinland (1913)&lt;br&gt;\nFrance (1875)&lt;br&gt;\nGermany (1875)&lt;br&gt;\nGreece (2001)&lt;br&gt;\nHungary (1925)&lt;br&gt;\nIndia (1957)&lt;br&gt;\nIndonesia (1960)&lt;br&gt;\nIran (1975)&lt;br&gt;\nIraq (2013)&lt;br&gt;\nIreland (1925)&lt;br&gt;\nIsrael (1985)&lt;br&gt;\nItaly (1875)&lt;br&gt;\nJapan (1885)&lt;br&gt;\nKazakhstan (2008)&lt;br&gt;\nKenya (2010)&lt;br&gt;\nLithuania (2015)&lt;br&gt;\nMalaysia (2001)&lt;br&gt;\nMexico (1890)&lt;br&gt;\nMontenegro (2018)&lt;br&gt;\nMorocco&lt;br&gt;\nNetherlands (1929)&lt;br&gt;\nNew Zealand (1991)&lt;br&gt;\nNorway (1875)&lt;br&gt;\nPakistan (1973)&lt;br&gt;\nPoland (1925)&lt;br&gt;\nPortugal (1876)&lt;br&gt;\nRomania (1884)&lt;br&gt;\nRussian Federation (1875)&lt;br&gt;\nSaudi Arabia (2011)&lt;br&gt;\nSerbia (2001)&lt;br&gt;\nSingapore (1994)&lt;br&gt;\nSlovakia (1922)&lt;br&gt;\nSlovenia (2016)&lt;br&gt;\nSouth Africa (1964)&lt;br&gt;\nSouth Korea (1959)&lt;br&gt;\nSpain (1875)&lt;br&gt;\nSweden (1875)&lt;br&gt;\nSwitzerland (1875)&lt;br&gt;\nThailand (1912)&lt;br&gt;\nTunisia (2012)&lt;br&gt;\nTurkey (1875)&lt;br&gt;\nUkraine (2018)&lt;br&gt;\nUnited Arab Emirates (2015)&lt;br&gt;\nUnited Kingdom (1884)&lt;br&gt;\nUnited States (1878)&lt;br&gt;\nUruguay (1908)&lt;br&gt;\nAssociates.\nAt the 21st meeting of the CGPM in October 1999, the category of \"associate\" was created for states not yet BIPM members and for economic unions.\nAlbania (2007)&lt;br&gt;\nAzerbaijan (2015)&lt;br&gt;\nBangladesh (2010)&lt;br&gt;\nBolivia (2008)&lt;br&gt;\nBosnia and Herzegovina (2011)&lt;br&gt;\nBotswana (2012)&lt;br&gt;\nCambodia&lt;br&gt;\nCaribbean Community (2005)&lt;br&gt;\nChinese Taipei (2002)&lt;br&gt;\nCosta Rica (2004)&lt;br&gt;\nCuba (2000)&lt;br&gt;\nEthiopia (2018)&lt;br&gt;\nGeorgia (2008)&lt;br&gt;\nGhana (2009)&lt;br&gt;\nHong Kong (2000)&lt;br&gt;\nJamaica (2003)&lt;br&gt;\nKuwait (2018)&lt;br&gt;\nLatvia (2001)&lt;br&gt;\nLuxembourg (2014)&lt;br&gt;\nMalta (2001)&lt;br&gt;\nMauritius (2010)&lt;br&gt;\nMoldova (2007)&lt;br&gt;\nMongolia (2013)&lt;br&gt;\nNamibia (2012)&lt;br&gt;\nNorth Macedonia (2006)&lt;br&gt;\nOman (2012)&lt;br&gt;\nPanama (2003)&lt;br&gt;\nParaguay (2009)&lt;br&gt;\nPeru (2009)&lt;br&gt;\nPhilippines (2002)&lt;br&gt;\nQatar (2016)&lt;br&gt;\nSeychelles (2010)&lt;br&gt;\nSri Lanka (2007)&lt;br&gt;\nSudan (2014)&lt;br&gt;\nSyria (2012)&lt;br&gt;\nTanzania (2018)&lt;br&gt;\nUzbekistan (2018)&lt;br&gt;\nVietnam (2003)&lt;br&gt;\nZambia (2010)&lt;br&gt;\nZimbabwe (2010)&lt;br&gt;\nInternational Committee for Weights and Measures.\nThe International Committee for Weights and Measures consists of eighteen persons, each of a different nationality elected by the General Conference on Weights and Measures (CGPM) whose principal task is to promote worldwide uniformity in units of measurement by taking direct action or by submitting proposals to the CGPM.\nThe CIPM meets every year (since 2011 in two sessions per year) at the Pavillon de Breteuil where, among other matters, it discusses reports presented to it by its Consultative Committees. Reports of the meetings of the CGPM, the CIPM, and all the Consultative Committees, are published by the BIPM.\nMission.\nThe secretariat is based in Saint-Cloud, Hauts-de-Seine, France.\nIn 1999 the CIPM has established the CIPM \"Arrangement de reconnaissance mutuelle\" (Mutual Recognition Arrangement, MRA) which serves as the framework for the mutual acceptance of national measurement standards and for recognition of the validity of calibration and measurement certificates issued by national metrology institutes.\nA recent focus area of the CIPM has been the revision of the SI.\nConsultative committees.\nThe CIPM has set up a number of consultative committees (CC) to assist it in its work. These committees are under the authority of the CIPM. The president of each committee, who is expected to take the chair at CC meetings, is usually a member of the CIPM. Apart from the CCU, membership of a CC is open to National Metrology Institutes (NMIs) of Member States that are recognized internationally as most expert in the field. NMIs from Member States that are active in the field, but lack the expertise to become Members, are able to attend CC meetings as observers.\nThese committees are: \nThe CCU's role is to advise on matters related to the development of the SI and the preparation of the SI brochure. It has liaison with other international bodies such as International Organization for Standardization (ISO), International Astronomical Union (IAU), International Union of Pure and Applied Chemistry (IUPAC) and International Union of Pure and Applied Physics (IUPAP).\nMajor reports.\nOfficial reports of the CIPM include:\nFrom time to time the CIPM has been charged by the CGPM to undertake major investigations related to activities affecting the CGPM or the BIPM. Reports produced include:\nThe Blevin Report.\nThe Blevin Report, published in 1998, examined the state of worldwide metrology. The report originated from a resolution passed at the 20th CGPM (October 1995) which committed the CIPM to \nThe report identified, amongst other things, a need for closer cooperation between the BIPM and other organisations such as International Organization of Legal Metrology (OIML) and International Laboratory Accreditation Cooperation (ILAC) with clearly defined boundaries and interfaces between the organisations. Another major finding was the need for cooperation between accreditation laboratories and the need to involve developing countries in the world of metrology.\nThe Kaarls Report.\nThe Kaarls Report published in 2003 examined the role of the BIPM in the evolving needs for metrology in trade, industry and society.\nSI Brochure.\nThe CIPM has responsibility for commissioning the SI brochure, which is the formal definition of the International system of units. The brochure is produced by the CCU in conjunction with a number of other international organisations. Initially the brochure was only in French \u2013 the official language of the metre convention, but recent versions have been published simultaneously in both English and French, with the French text being the official text. The 6th edition was published in 1991, the 7th edition was published in 1998, and the 8th in 2006.\nThe most recent edition is the 9th edition, published in 2019."}
{"id": "7341", "revid": "17512117", "url": "https://en.wikipedia.org/wiki?curid=7341", "title": "Cowboy Bebop", "text": " is a Japanese science fiction anime television series animated by Sunrise featuring a production team led by director Shinichir\u014d Watanabe, screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical designer Kimitoshi Yamane, and composer Yoko Kanno. The twenty-six episodes (\"sessions\") of the series are set in the year 2071, and follow the lives of a bounty hunter crew traveling in their spaceship called the \"Bebop\". Although it covers a wide range of genres throughout its run, \"Cowboy Bebop\" draws most heavily from science fiction, western and noir films, and its most recurring thematic focal points include adult existential ennui, loneliness and the difficulties of trying to escape one's past.\nThe series premiered in Japan on TV Tokyo from April 3 until June 26, 1998, broadcasting only twelve episodes and a special due to its controversial adult-themed content. The entire twenty-six episodes of the series were later broadcast on Wowow from October 24 until April 24, 1999. The anime was adapted into two manga series which were serialized in Kadokawa Shoten's \"Asuka Fantasy DX\". A was later released to theaters worldwide.\nThe anime series was dubbed in the English language by Animaze and ZRO Limit Productions, and was licensed by Bandai Entertainment in North America and is now licensed by Funimation. In Britain, it was licensed by Beez Entertainment and is currently licensed by Anime Limited. Madman Entertainment has licensed it for releases in Australia and New Zealand. In 2001, \"Cowboy Bebop\" became the first anime title to be broadcast on Adult Swim in the United States.\n\"Cowboy Bebop\" became a critical and commercial success both in Japanese and international markets (most notably in the United States), garnered several major anime and science fiction awards upon its release, and received unanimous praise for its style, characters, story, voice acting, animation, and soundtrack; the English dub was particularly praised and is still considered one of the best English anime dubs. In the years since its release, critics have hailed \"Cowboy Bebop\" as a masterpiece and frequently cite it as one of the greatest anime titles of all time. Credited with helping to introduce anime to a new wave of Western viewers in the early 2000s, \"Cowboy Bebop\" has also been labelled a gateway series for the medium as a whole.\nPlot.\nIn 2071, roughly fifty years after an accident with a hyperspace gateway made the Earth almost uninhabitable, humanity has colonized most of the rocky planets and moons of the Solar System. Amid a rising crime rate, the Inter Solar System Police (ISSP) set up a legalized contract system, in which registered bounty hunters (also referred to as \"Cowboys\") chase criminals and bring them in alive in return for a reward. The series' protagonists are bounty hunters working from the spaceship \"Bebop\". The original crew are Spike Spiegel, an exiled former hitman of the criminal Red Dragon Syndicate, and Jet Black, a former ISSP officer. They are later joined by Faye Valentine, an amnesiac con artist; Edward, an eccentric girl skilled in hacking; and Ein, a genetically-engineered Pembroke Welsh Corgi with human-like intelligence. Over the course of the series, the team get involved in disastrous mishaps leaving them without money, while often confronting faces and events from their past: these include Jet's reasons for leaving the ISSP, and Faye's past as a young woman from Earth injured in an accident and cryogenically frozen to save her life.\nThe main story arc focuses on Spike and his deadly rivalry with Vicious, an ambitious criminal affiliated with the Red Dragon Syndicate. Spike and Vicious were once partners and friends, but when Spike began an affair with Vicious's girlfriend Julia and resolved to leave the Syndicate with her, Vicious sought to eliminate Spike by blackmailing Julia into killing him. Julia goes into hiding to protect herself and Spike fakes his death to escape the Syndicate. In the present, Julia comes out of hiding and reunites with Spike, intending to complete their plan. Vicious, having staged a \"coup d'\u00e9tat\" and taken over the Syndicate, sends hitmen after the pair. Julia is killed, leaving Spike alone. Spike leaves the \"Bebop\" after saying a final goodbye to Faye and Jet. Upon infiltrating the syndicate, he finds Vicious on the top floor of the building and confronts him after dispatching the remaining Red Dragon members. The final battle ends with Spike killing Vicious, only to be seriously wounded himself in the ensuing confrontation. The series concludes as Spike descends the main staircase of the building into the rising sun before eventually falling to the ground.\nGenre and themes.\nWatanabe created a special tagline for the series to promote it during its original presentation, calling it \"a new genre unto itself\". The line was inserted before and after commercial breaks during its Japanese and US broadcasts. Later, Watanabe called the phrase an \"exaggeration\". The show is a hybrid of multiple genres, including westerns and pulp fiction. One reviewer described it as \"space opera meets noir, meets comedy, meets cyberpunk\". It has also been called a \"genre-bursting space western\".\nThe musical style was emphasized in many of the episode titles. Multiple philosophical themes are explored using the characters, including existentialism, existential ennui, loneliness, and the effect of the past on the protagonists. The series also makes specific references to or pastiches multiple films, including the works of John Woo and Bruce Lee, \"Midnight Run\", \"\", and \"Alien\". The series also includes extensive references and elements from science fiction, bearing strong similarities to the cyberpunk fiction of William Gibson. Several planets and space stations in the series are made in Earth's image. The streets of celestial objects such as Ganymede resemble a modern port city, while Mars features shopping malls, theme parks, casinos and cities. \"Cowboy Bebop\"s universe is filled with video players and hyperspace gates, eco-politics and fairgrounds, spaceships and Native American shamans. This setting has been described as \"one part Chinese diaspora and two parts wild west\".\nCharacters.\nThe characters were created by Watanabe and character designer Toshihiro Kawamoto. Watanabe envisioned each character as an extension of his own personality, or as an opposite person to himself. Each character, from the main cast to supporting characters, were designed to be outlaws unable to fit into society. Kawamoto designed the characters so they were easily distinguished from one another. All the main cast are characterized by a deep sense of loneliness or resignation to their fate and past. From the perspective of Brian Camp and Julie Davis, the main characters resemble the main characters of the anime series \"Lupin III\", if only superficially, given their more troubled pasts and more complex personalities.\nThe show focuses on the character of Spike Spiegel, with the overall theme of the series being Spike's past and its karmic effect on him. Spike was portrayed as someone who had lost his expectations for the future, having lost the woman he loved, and so was in a near-constant lethargy. Spike's artificial eye was included as Watanabe wanted his characters to have flaws. He was originally going to give Spike an eye patch, but the producers vetoed it.\nJet is shown as someone who lost confidence in his former life and has become cynical about the state of society. Spike and Jet were designed to be opposites, with Spike being thin and wearing smart attire, while Jet was bulky and wore more casual clothing. The clothing, which was dark in color, also reflected their states of mind. Faye Valentine, Edward Wong, and Ein joined the crew in later episodes. Their designs were intended to contrast against Spike. Faye was described by her voice actress as initially being an \"ugly\" woman, with her defining traits being her liveliness, sensuality and humanity. To emphasize her situation when first introduced, she was compared to Poker Alice, a famous Western figure.\nEdward and Ein were the only main characters to have real-life models. The former had her behavior based on the antics of Yoko Kanno as observed by Watanabe when he first met her. While generally portrayed as carefree and eccentric, Edward is motivated by a sense of loneliness after being abandoned by her father. Kawamoto initially based Ein's design on a friend's pet corgi, later getting one himself to use as a motion model.\nProduction.\n\"Cowboy Bebop\" was developed by animation studio Sunrise and created by Hajime Yatate, the well-known pseudonym for the collective contributions of Sunrise's animation staff. The leader of the series' creative team was director Shinichir\u014d Watanabe, most notable at the time for directing \"Macross Plus\" and \"\". Other leading members of Sunrise's creative team were screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical art designer Kimitoshi Yamane, composer Yoko Kanno, and producers Masahiko Minami and Yoshiyuki Takei. Most of them had previously worked together, in addition to having credits on other popular anime titles. Nobumoto had scripted \"Macross Plus\", Kawamoto had designed the characters for \"Gundam\", and Kanno had composed the music for \"Macross Plus\" and \"The Vision of Escaflowne\". Yamane had not worked with Watanabe yet, but his credits in anime included \"Bubblegum Crisis\" and \"The Vision of Escaflowne\". Minami joined the project as he wanted to do something different from his previous work on mecha anime.\nConcept.\n\"Cowboy Bebop\" was Watanabe's first project as solo director, as he had been co-director in his previous works. His original concept was for a movie, and during production he treated each episode as a miniature movie. His main inspiration for \"Cowboy Bebop\" was \"Lupin III\", a crime anime series focusing on the exploits of the series' titular character. When developing the series' story, Watanabe began by creating the characters first. He explained, \"the first image that occurred to me was one of Spike, and from there I tried to build a story around him, trying to make him cool.\" While the original dialogue of the series was kept clean to avoid any profanities, its level of sophistication was made appropriate to adults in a criminal environment. Watanabe described \"Cowboy Bebop\" as \"80% serious story and 20% humorous touch\". The comical episodes were harder for the team to write than the serious ones, but though several events in them seemed random, they were carefully planned in advance. Watanabe conceived the series' ending early on, and each episode involving Spike and Vicious was meant to foreshadow their final confrontation. Some of the staff were unhappy about this approach as a continuation of the series would be difficult. While he considered altering the ending, he eventually settled with his original idea. The reason for creating the ending was that Watanabe did not want the series to become like \"Star Trek\", with him being tied to doing it for years.\nDevelopment.\nThe project had initially originated with Bandai's toy division as a sponsor, with the goal of selling spacecraft toys. Watanabe recalled his only instruction was \"So long as there's a spaceship in it, you can do whatever you want.\" But upon viewing early footage, it became clear that Watanabe's vision for the series didn't match with that of Bandai's. Believing the series would never sell toy merchandise, Bandai pulled out of the project, leaving it in development hell until sister company Bandai Visual stepped in to sponsor it. Since there was no need to merchandise toys with the property any more, Watanabe had free rein in the development of the series. Watanabe wanted to design not just a space adventure series for adolescent boys but a program that would also appeal to sophisticated adults. During the making of \"Bebop\", Watanabe often attempted to rally the animation staff by telling them that the show would be something memorable up to three decades later. While some of them were doubtful of that at the time, Watanabe many years later expressed his happiness to have been proven right in retrospect. He joked that if Bandai Visual hadn't intervened then \"you might be seeing me working the supermarket checkout counter right now.\"\nThe city locations were generally inspired by the cities of New York and Hong Kong. The atmospheres of the planets and the ethnic groups in \"Cowboy Bebop\" mostly originated from Watanabe's ideas, with some collaboration from set designers Isamu Imakake, Shoji Kawamori, and Dai Sat\u014d. The animation staff established the particular planet atmospheres early in the production of the series before working on the ethnic groups. It was Watanabe who wanted to have several groups of ethnic diversity appear in the series. Mars was the planet most often used in \"Cowboy Bebop\"s storylines, with Satoshi Toba, the cultural and setting producer, explaining that the other planets \"were unexpectedly difficult to use\". He stated that each planet in the series had unique features, and the producers had to take into account the characteristics of each planet in the story. For the final episode, Toba explained that it was not possible for the staff to have the dramatic rooftop scene occur on Venus, so the staff \"ended up normally falling back to Mars\". In creating the backstory, Watanabe envisioned a world that was \"multinational rather than stateless\". In spite of certain American influences in the series, he stipulated that the country had been destroyed decades prior to the story, later saying the notion of the United States as the center of the world repelled him.\nMusic.\nThe music for \"Cowboy Bebop\" was composed by Yoko Kanno. Kanno formed the blues and jazz band Seatbelts to perform the music of the series. According to Kanno, the music was one of the first aspects of the series to begin production, before most of the characters, story or animation had been finalized. The genres she used for its composition were western, opera and jazz. Watanabe noted that Kanno did not score the music exactly the way he told her to. He stated, \"She gets inspired on her own, follows up on her own imagery and comes to me saying 'this is the song we need for \"Cowboy Bebop\",' and composes something completely on her own.\" Kanno herself was sometimes surprised at how pieces of her music were used in scenes, sometimes wishing it had been used elsewhere, though she also felt that none of their uses were \"inappropriate\". She was pleased with the working environment, finding the team very relaxed in comparison with other teams she had worked with.\nWatanabe further explained that he would take inspiration from Kanno's music after listening to it and create new scenes for the story from it. These new scenes in turn would inspire Kanno and give her new ideas for the music and she would come to Watanabe with even more music. Watanabe cited as an example, \"some songs in the second half of the series, we didn't even ask her for those songs, she just made them and brought them to us.\" He commented that while Kanno's method was normally \"unforgivable and unacceptable\", it was ultimately a \"big hit\" with \"Cowboy Bebop\". Watanabe described his collaboration with Kanno as \"a game of catch between the two of us in developing the music and creating the TV series \"Cowboy Bebop\"\". Since the series' broadcast, Kanno and the Seatbelts have released seven original soundtrack albums, two singles and extended plays, and two compilations through label Victor Entertainment.\nDistribution.\nBroadcast.\n\"Cowboy Bebop\" debuted on TV Tokyo, one of the main broadcasters of anime in Japan, airing from April 3 until June 26, 1998. Due to its 6 PM timeslot and depictions of graphic violence, the show's first run only included episodes 2, 3, 7 to 15, 18 and a special. Later that year, the series was shown in its entirety from October 24 until April 24, 1999, on satellite network Wowow. The full series has also been broadcast across Japan by anime television network Animax, which has also aired the series via its respective networks across Southeast Asia, South Asia and East Asia.\nThe first non-Asian country to air Cowboy Bebop was Italy. There, it was first shown on October 21, 1999, on MTV, where it inaugurated the 9.00-10.30 pm \"Anime Night\" programming block.\nIn the United States, \"Cowboy Bebop\" was one of the programs shown the night Cartoon Network's late night block Adult Swim debuted on September 2, 2001, being the first anime shown on the block. It was successful enough to be broadcast repeatedly for four years. It has been run at least once every year since 2007, and HD remasters of the show began broadcasting in 2015. In the United Kingdom it was first broadcast in 2002 as one of the highlights of the \"cartoon network for adults\", CNX. From November 6, 2007, it was repeated on AnimeCentral until the channel's closure in August 2008. In Australia, \"Cowboy Bebop\" was first broadcast on pay-TV in 2002 on Adult Swim in Australia. It was broadcast on Sci Fi Channel on Foxtel. In Australia, \"Cowboy Bebop\" was first broadcast on free-to-air-TV on ABC2 (the national digital public television channel) on January 2, 2007. It has been repeated several times, most recently starting in 2008. \"Cowboy Bebop: The Movie\" also aired again on February 23, 2009, on SBS (a hybrid-funded Australian public broadcasting television network). In Canada, \"Cowboy Bebop\" was first broadcast on December 24, 2006, on Razer.\nIn Latin America, was first broadcast on pay-TV in 2001 on Locomotion. It is retransmitted from January 9, 2016 on I.Sat (Adult Swim block).\nHome media.\n\"Cowboy Bebop\" has been released in four separate editions in North America.\nThe first release was sold in VHS format either as a box set or as seven individual tapes. The tapes were sold through Anime Village, a division of Bandai.\nThe second release was sold in 2000 individually, and featured uncut versions of the original 26 episodes. In 2001, these DVDs were collected in the special edition \"Perfect Sessions\" which included the first 6 DVDs, the first \"Cowboy Bebop\" soundtrack, and a collector's box. At the time of release, the art box from the Perfect Sessions was made available for purchase on The Right Stuff International as a solo item for collectors who already owned the series.\nThe third release, \"The Best Sessions\", was sold in 2002 and featured what Bandai considered to be the best 6 episodes of the series remastered in Dolby Digital 5.1 and DTS surround sound.\nThe fourth release, \"Cowboy Bebop Remix\", was also distributed on 6 discs and included the original 26 uncut episodes, with sound remastered in Dolby Digital 5.1 and video remastered under the supervision of Shinichiro Watanabe. This release also included various extras that were not present in the original release. Cowboy Bebop Remix was itself collected as the Cowboy Bebop Remix DVD Collection in 2008.\nA fourth release in Blu-ray format was released on December 21, 2012 exclusively in Japan.\nIn December 2012, newly founded distributor Anime Limited announced via Facebook and Twitter that they had acquired the home video license for the United Kingdom. Part 1 of the Blu-ray collection was released on July 29, 2013, while Part 2 was released on October 14. The standard DVD Complete Collection was originally meant to be released on September 23, 2013 with Part 2 of the Blu-ray release but due to mastering and manufacturing errors, the Complete Collection was delayed until November 27. Following the closure of Bandai Entertainment in 2012, Funimation and Sunrise had announced that they rescued \"Cowboy Bebop\", along with a handful of other former Bandai Entertainment properties, for home video and digital release. Funimation released the series on Blu-ray and DVD on December 16, 2014. The series was released in four separate editions: standard DVD, standard Blu-ray, an Amazon.com exclusive Blu-ray/DVD combo, and a Funimation.com exclusive Blu-ray/DVD combo.\nRelated media.\nManga.\nTwo \"Cowboy Bebop\" manga series have been released, both published by Kadokawa Shoten and serialized in \"Asuka Fantasy DX\".&lt;ref name=\"DX 10/1997\"&gt;&lt;/ref&gt;&lt;ref name=\"DX 11/1998\"&gt;&lt;/ref&gt; The first manga series, titled \"Cowboy Bebop: Shooting Star\" and illustrated by Cain Kuga, was serialized from October issue 1997, before the anime series' release, to July issue 1998. It was collected into two volumes in 1998, the first one in May and the second one in September. The second manga series, simply titled \"Cowboy Bebop\" and illustrated by , was serialized from November issue 1998 to March issue 2000. It was collected into three volumes, the first two in April and October 1999 and the third one in April 2000. Both manga series were licensed by Tokyopop for release in North America.\nVideo games.\nA \"Cowboy Bebop\" video game, developed and published by Bandai, was released in Japan for the PlayStation on May 14, 1998. A PlayStation 2 video game, \"\", was released in Japan on August 25, 2005, and an English version had been set for release in North America. However, in January 2007, IGN reported that the release had likely been cancelled, speculating that it did not survive Bandai's merger with Namco to Bandai Namco Games.\nFilm.\nAn anime film titled ' (\u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 \u5929\u56fd\u306e\u6249 / \"Kaub\u014di Bibappu: Tengoku no Tobira\"), also known as ', was released in Japan in September 2001 and in the United States in 2003.\nOn July 22, 2008, \"If\" published an article on its website regarding a rumor of a live-action \"Cowboy Bebop\" movie in development by 20th Century Fox. Producer Erwin Stoff said that the film's development was in the early stages, and that they had \"just signed it\". Keanu Reeves was to play the role of Spike Spiegel. \"Variety\" confirmed on January 15, 2009 that production company Sunrise Animation would be \"closely involved with the development of the English-language project\". The site also confirmed Kenji Uchida, Shinichir\u014d Watanabe and series writer Keiko Nobumoto as associate producers, series producer Masahiko Minami as a production consultant, and Peter Craig as screenwriter. This was lauded by various sources as a promising move for the potential quality of the film. At the time it was slated to release in 2011, but problems with the budget delayed its production. The submitted script was sent back for rewrite to reduce the cost and little has been heard about it since an interview with producer Joshua Long on October 15, 2010; the project currently languishes in development hell. On October 25, 2014, series director Watanabe was asked about the live-action film at the MCM London Comicon. He stated: \"I'm afraid I don't know what they're thinking in Hollywood. Apparently the project hasn't come to a stop but I don't know how it's going to progress from here on. I hear that there are a lot of 'Hollywood' problems.\"\nLive-action series.\nOn June 6, 2017, it was announced that an American live-action adaptation of the series was being developed for television by Tomorrow Studios, a partnership between Marty Adelstein and Sunrise Inc., which also produced the original anime. Christopher Yost is poised to write the series. On November 27, 2018 Netflix announced that the live-action series would be heading to its streaming service. On April 4, 2019, Variety reported that John Cho, Mustafa Shakir, Daniella Pineda and Alex Hassell have all been cast in lead roles as Spike Spiegel, Jet Black, Faye Valentine and Vicious in the series. On August 22, 2019, it was announced that Elena Satine is cast as Julia. Production was shut down in October 2019 due to Cho's knee injury, setting production back by more than half a year. On April 17, 2020, further news was provided about the project, that episodes would be one-hour in length allowing for more in-depth storytelling, and second season script has been noted. On May 19, 2020, while doing an interview with SyFy Wire, Adelstein revealed that there are currently three finished episodes and that they shot at least six episodes before Cho's knee injury. During the same interview it was revealed that Shinichir\u014d Watanabe, the director of the anime series, would be involved with the series as a creative consultant. Production resumed on September 30, 2020, after the New Zealand government gave the green light to continue following the nation's COVID-19 shutdown. On November 19, 2020, Deadline Hollywood reported that Geoff Stults, Tamara Tunie, Mason Alexander Park, Rachel House, Ann Truong and Hoa Xuande have been cast as Chalmers, Ana, Gren, Mao, Shin and Lin.\nOther.\nAn official side story titled \"Cowboy Bebop: UT\" tells the story of Ural and Victoria Terpsichore (V.T. from the session \"Heavy Metal Queen\") when they were bounty hunters. The story was available in its own official site, however the website was closed and is currently available at the site mirror.\nReception.\nCritical reception.\n\"Cowboy Bebop\" received unanimous acclaim, beginning at the time of its initial broadcast. Beginning in 1998, Japanese critic Keith Rhee highlighted the series as a standout in an otherwise \"run-of-the-mill\" season, praising its overall production values, and singling out Kanno's soundtrack as \"a much-welcome change from all the sugary J-pop tunes of most anime features\". Rhee also highlighted the show's Japanese \"all-star cast\", which his colleague Mark L. Johnson described as being filled with \"veteran voice talent\", turning in even greater performances than those of their \"above average\" US counterparts.\nAnime News Network's Mike Crandol gave the series an 'A+' rating for the dubbed version, and an 'A' rating for the subbed version. He claimed the series was \"one of the most popular and respected anime titles in history\", before adding that it was \"a unique television show which skillfully transcends all kinds of genres\". Crandol praised its characters as \"some of the most endearing characters to ever grace an anime\", and commended the voice acting, especially the \"flawless English cast\". He also complimented the series' \"movie-quality\" animation, \"sophisticated\" writing, and its \"incredible\" musical score. Crandol hailed \"Cowboy Bebop\" as a \"landmark\" anime \"that will be remembered long after many others have been forgotten\", and went on to call it \"one of the greatest anime titles ever\". Additionally, Michael Toole of Anime News Network named \"Cowboy Bebop\" as one of the most important anime of the 1990s.\nT.H.E.M. Anime Reviews gave the entire series a perfect score of 5 out of 5 stars, with reviewer Christina Carpenter believing \"Cowboy Bebop\" as \"one of the best [anime]\" and touting it as a masterpiece that \"puts most anime...and Hollywood, to shame\". She described it as a \"very stylish, beautifully crafted series that deserves much more attention than it gets\". Carpenter praised the animation as \"a rarity and a marvel to behold\" and that it was \"beyond superb\", and the plot and characterization as having \"a sophistication and subtlety that is practically one-of-a-kind\". She also praised the soundtrack, and hailed the opening theme as one of the best intro pieces she had ever heard. Carpenter went to say that \"Bebop\" was a \"must-have for any serious collector of Japanese animation\".\nIn his article \"Asteroid Blues: The Lasting Legacy of \"Cowboy Bebop\"\", \"The Atlantic\" writer Alex Suskind states, \"On paper, \"Cowboy Bebop\", the legendary cult anime series from Shinichir\u014d Watanabe, reads like something John Wayne, Elmore Leonard, and Philip K. Dick came up with during a wild, all-night whiskey bender.\" He goes on to write, \"The response from critics and fans may have sounded hyperbolic\u2014the word 'masterpiece' was thrown around a great deal\u2014but the praise was justified. First-time solo director Watanabe had created a gorgeous tale of morality, romance, and violence\u2013a dark look at the lives of outlaws that's shot like an independent film.\"\nIn January 2015, television writer Kyle Mills of DVD Talk awarded the series five stars upon review. He stated, \"Regardless of the medium, be it live action television, film, or animation, \"Cowboy Bebop\" is simply one of the finest examples of storytelling ever created.\" In his review, he describes the finale as \"one of the best in television history\", referring to it as a \"widely revered\" ending that \"still sparks fan conversation, resonating with viewers 15 years on\". He closes by writing, \"\"Cowboy Bebop\" ends with a bang.\"\nIn his 2018 review of the series, \"Paste\" critic John Maher wrote, \"It feels like a \"magnum opus\" produced at the pinnacle of a long career despite being, almost unbelievably, Watanabe's first series as a director. It is a masterwork that should justly rank among the best works of television of all time.\" It was also placed at #1 on the publication's list of the \"50 Best Anime Series of All Time\".\nOn review aggregator Rotten Tomatoes, the series has an approval rating of 100% based on 22 reviews, with an average rating of 8/10. The website's critical consensus reads, \"Blending a head-spinning array of genres and references, \"Cowboy Bebop\" is an anime television classic that must be experienced.\"\nIn an April 2019 interview with Diego Molano, creator of \"Victor &amp; Valentino\", he said that Cowboy Bebop was the first anime he \"obsessed over\", as he spent time tracking down VHS tapes of the show in high school. He also argued that this series showed him \"how cinematic and emotional animation can be\".\nAccolades.\nIn the 1999 Anime Grand Prix awards for the anime of 1998, \"Cowboy Bebop\" won two 1st place awards: Spike Spiegel was awarded the best male character; and Megumi Hayashibara was awarded the best voice actor for her role as Faye Valentine. \"Cowboy Bebop\" also received rankings in other categories: the series itself was awarded the 2nd best anime series; Faye Valentine and Ed were ranked the 5th and 9th best female characters respectively; \"Tank!\" and \"The Real Folk Blues\" were ranked the 3rd and 15th best songs respectively; and \"Ballad of Fallen Angels\", \"Speak Like a Child\", \"Jamming with Edward\" and \"Mish-Mash Blues\" were ranked the 2nd, 8th, 18th and 20th best episodes respectively.\nIn the 2000 Anime Grand Prix awards for the anime of 1999, \"Cowboy Bebop\" won the same two 1st place awards again: best male character for Spike Spiegel; and best voice actor for Megumi Hayashibara. Other rankings the series received are: 2nd best anime series; 6th best female character for Faye Valentine; 7th and 12th best song for \"Tank!\" and \"Blue\" respectively; and 3rd and 17th best episode for \"The Real Folk Blues (Part 2)\" and \"Hard Luck Woman\" respectively. In the 2000 Seiun Awards, Cowboy Bebop was awarded for Best Media of the Year.\nA 2004 poll in \"Newtype USA\", the US edition of the Japanese magazine \"Newtype\", asked its readers to vote the \"Top 25 Anime Titles of All Time\"; \"Cowboy Bebop\" ranked 2nd on the list (after \"Neon Genesis Evangelion\"), placing it as one of the most socially relevant and influential anime series ever created. During that same year, \"Cinefantastique\" listed the anime as one of the \"10 Essential Animations\", citing the series' \"gleeful mix of noir-style, culture-hopping inclusiveness and music\". In 2007, the American Anime magazine \"Anime Insider\" listed the \"50 Best Anime Ever\" by compiling lists of industry regulars and magazine staff, and ranked \"Cowboy Bebop\" as the #1 anime of all time. In 2012, Madman Entertainment compiled the votes of fans online for \"The Top 20 Madman Anime Titles\" and ranked \"Cowboy Bebop\" at #7.\n\"Cowboy Bebop\" has been featured in several lists published by IGN. In the 2009 \"Top 100 Animated TV Series\" list, \"Cowboy Bebop\", labelled as \"a very original\u00a0\u2013 and arguably one of the best\u00a0\u2013 anime\", was placed 14th, making it the second highest ranking anime on the list (after \"Evangelion\") and one of the most influential series of the 1990s. In 2011, \"Bebop\" was ranked 29th in the \"Top 50 Sci-Fi TV Shows\" list, once again being the second highest ranking anime on the list (after \"Evangelion\"). In 2006, \"Cowboy Bebop\"s soundtrack was ranked #1 in \"Top Ten Anime Themes and Soundtracks of All-Time\" list, with the series being commented as \"one of the best anime ever and certainly is tops when it comes to music.\" Spike Spiegel was ranked 4th place in the \"Top 25 Anime Characters of All Time\" article. IGN Movies also placed \"Cowboy Bebop\" in their list of \"10 Cartoon Adaptations We'd Like to See\".\nAnalysis.\nThe series has been subject to study and analysis since its debut, with the main focus being on its style and mixture of genres. Miguel Douglas, describing the series style in a review, said that \"the series distinctly establishes itself outside the realm of conventional Japanese animation and instead chooses to forge its own path. With a setting within the realm of science fiction, the series wisely offers a world that seems entirely realistic considering our present time. Free from many of the elements that accompany science fiction in general\u2014whether that be space aliens, giant robots, or laser guns\u2014the series delegates itself towards presenting a world that is quite similar to our own albeit showcasing some technological advances. Certainly not as pristine a future we would see in other series or films, \"Cowboy Bebop\" decides to deliver a future that closely reflects that of our own time. This aspect of familiarity does wonders in terms of relating to the viewer, and it presents a world that certainly resembles our very own.\" Daryl Surat of \"Otaku USA\", commenting on the series' appeal, said that it was \"that rare breed of science-fiction: 'accessible'. Unlike many anime titles, viewers weren't expected to have knowledge of Asian culture\u2014character names, signs, and the like were primarily in English to begin with\u2014or have seen any other anime series prior.\" Michelle Onley Pirkle, in her book \"Science Fiction Film, Television, and Adaptation: Across the Screens\", said that \"\"Cowboy Bebop\" is taking a new take on genre, not by creating unique images and sounds, but by playing 'freely' with, 'remixing', or adapting the images and sounds of other familiar genres in a dynamic way.\" Robert Baigent, writing for the \"Graduate Journal of Asia-Pacific Studies\", said that the series' appeal likely stemmed from the trend in anime to emulate Western fiction.\nLegacy.\nIn March 2009, the print and web editions of \"The Onion\"'s \"The A.V. Club\" called \"Cowboy Bebop\" \"rightly a huge hit\", and listed it as a gateway series to understanding the medium of anime as a whole. Suskind said: \"It was unlike anything the genre had seen before. It even approached its music differently. The show kicked off with a wormhole of a theme song, and the soundtrack moves so seamlessly through genres, from rock to country to pop to jazz to funk, it's shocking to learn that one set of musicians is behind it all\". In an interview, producer Sean Akins also states that the series \"created a whole new world\". \"It's hard for me to quantify the impact that I think it has had. It changed anime. I think people began to think about what shows would be cool. I think it redefined cool within animation, not only in Japan but in the States\".\nAmerican film director, screenwriter, and producer Rian Johnson has cited \"Cowboy Bebop\" as a visual influence on his films, most notably \"Brick\". \"Ender's Game\" writer Orson Scott Card also praised the series. He states that the series is \"better than most sci-fi films out there\". He goes on to say that he \"found this series brilliant, but what held me was a combination of strong relationship-based storytelling, a moody visual style that never got old and really smart dialogue\".\nAfter the creation of the series, an interviewer asked Watanabe if he had any plans to create more \"Cowboy Bebop\" material. Watanabe responded by saying that he does not believe that he \"should just keep on making \"Cowboy Bebop\" sequels for the sake of it\". Watanabe added that ending production and \"to quit while we're ahead when people still want more\" is more \"in keeping with the \"Bebop\" spirit\". In a more recent interview from 2006 with \"The Daily Texan\", Watanabe was asked if there would ever be more \"Cowboy Bebop\". Watanabe's answer was \"someday...maybe, someday\".\nIn May 2020, composer Mason Lieberman partnered with Sunrise and Funimation to produce an official \"Cowboy Bebop\" charity track for COVID-19 relief. This track was released on vinyl and featured the return of original series composer Y\u014dko Kanno, original recording band The Seatbelts, and a collection of forty other special musical guests."}
{"id": "7342", "revid": "12824384", "url": "https://en.wikipedia.org/wiki?curid=7342", "title": "Clement of Alexandria", "text": "Titus Flavius Clemens, also known as Clement of Alexandria (; c. 150 \u2013 c. 215 AD), was a Christian theologian and philosopher who taught at the Catechetical School of Alexandria. Among his pupils were Origen and Alexander of Jerusalem. A convert to Christianity, he was an educated man who was familiar with classical Greek philosophy and literature. As his three major works demonstrate, Clement was influenced by Hellenistic philosophy to a greater extent than any other Christian thinker of his time, and in particular, by Plato and the Stoics. His secret works, which exist only in fragments, suggest that he was familiar with pre-Christian Jewish esotericism and Gnosticism as well. In one of his works he argued that Greek philosophy had its origin among non-Greeks, claiming that both Plato and Pythagoras were taught by Egyptian scholars. \nClement is usually regarded as a Church Father. He is venerated as a saint in Coptic Christianity, Eastern Catholicism, Ethiopian Christianity, and Anglicanism. He was revered in Western Catholicism until 1586, when his name was removed from the Roman Martyrology by Pope Sixtus V on the advice of Baronius. The Eastern Orthodox Church officially stopped any veneration of Clement of Alexandria in the 10th century.\nBiography.\nNeither Clement's birthdate or birthplace is known with any degree of certainty. It is conjectured that he was born sometime around 150 AD. According to Epiphanius of Salamis, he was born in Athens, but there is also a tradition of an Alexandrian birth.\nHis parents were pagans and Clement was a convert to Christianity. In the \"Protrepticus\" he displays an extensive knowledge of Greek religion and mystery religions, which could only have arisen from the practice of his family's religion.\nHaving rejected paganism as a young man due to its perceived moral corruption, he travelled in Greece, Asia Minor, Palestine, and Egypt. Clement's journeys were primarily a religious undertaking. In Greece, he encountered an Ionian theologian, who has been identified as Athenagoras of Athens; while in the east, he was taught by an Assyrian, sometimes identified with Tatian, and a Jew, possibly Theophilus of Caesarea.\nIn around 180 AD, Clement reached Alexandria, where he met Pantaenus, who taught at the Catechetical School of Alexandria. Eusebius suggests that Pantaenus was the head of the school, but controversy exists about whether the institutions of the school were formalized in this way before the time of Origen. Clement studied under Pantaenus, and was ordained to the priesthood by Pope Julian before 189. Otherwise, virtually nothing is known of Clement's personal life in Alexandria. He may have been married, a conjecture supported by his writings.\nDuring the Severian persecutions of 202\u2013203, Clement left Alexandria. In 211, Alexander of Jerusalem wrote a letter commending him to the Church of Antioch, which may imply that Clement was living in Cappadocia or Jerusalem at that time. The date and location of his death are unknown.\nTheological works.\nTrilogy.\nThree of Clement's major works have survived in full and they are collectively referred to as a trilogy:\n\"Protrepticus\".\nThe \"Protrepticus\" is, as its title suggests, an exhortation to the pagans of Greece to adopt Christianity. Within it, Clement demonstrates his extensive knowledge of pagan mythology and theology. It is chiefly important due to Clement's exposition of religion as an anthropological phenomenon. After a short philosophical discussion, it opens with a history of Greek religion in seven stages. Clement suggests that at first, humans mistakenly believed the Sun, the Moon, and other heavenly bodies to be deities. The next developmental stage was the worship of the products of agriculture, from which he contends the cults of Demeter and Dionysus arose. Humans then paid reverence to revenge and deified human feelings of love and fear, among others. In the following stage, the poets Hesiod and Homer attempt to enumerate the deities; Hesiod's Theogony giving the number of twelve. Finally, humans reached a stage when they proclaimed others, such as Asclepius and Heracles, as deities. Discussing idolatry, Clement contends that the objects of primitive religion were unshaped wood and stone, and idols thus arose when such natural items were carved. Following Plato, Clement is critical of all forms of visual art, suggesting that artworks are but illusions and \"deadly toys\".\nClement criticizes Greek paganism in the \"Protrepticus\" on the basis that its deities are both false and poor moral examples. He attacks the mystery religions for their ritualism and mysticysm. In particular, the worshippers of Dionysus are ridiculed by him for their family-based rituals (such as the use of children's toys in ceremony). He suggests at some points that the pagan deities are based on humans, but at other times he suggests that they are misanthropic demons, and he cites several classical sources in support of this second hypothesis. Clement, like many pre-Nicene church fathers, writes favourably about Euhemerus and other rationalist philosophers, on the grounds that they at least saw the flaws in paganism. However, his greatest praise is reserved for Plato, whose apophatic views of God prefigure Christianity.\nThe figure of Orpheus is prominent throughout the Protrepticus narrative, and Clement contrasts the song of Orpheus, representing pagan superstition, with the divine Logos of Christ. According to Clement, through conversion to Christianity alone can one fully participate in the Logos, which is universal truth.\n\"Paedagogus\".\nThis work's title, translatable as \"tutor\", refers to Christ as the teacher of all humans, and it features an extended metaphor of Christians as children. It is not simply instructional: Clement intends to show how the Christian should respond to the Love of God authentically. Following Plato (Republic 4:441), he divides life into three elements: character, actions, and passions. The first having been dealt with in the \"Protrepticus\", he devotes the \"Paedagogus\" to reflections on Christ's role in teaching humans to act morally and to control their passions. Despite its explicitly Christian nature, Clement's work draws on Stoic philosophy and pagan literature; Homer, alone, is cited more than sixty times in the work.\nAlthough Christ, like a human, is made in the image of God, he alone shares the likeness of God the Father. Christ is both sinless and apathetic, and thus by striving to imitate Christ, one can achieve salvation. To Clement, sin is involuntary, and thus irrational [\u03ac\u03bb\u03bf\u03b3\u03bf\u03bd], removed only through the wisdom of the Logos. God's guidance away from sin is thus a manifestation of God's universal love for mankind. The word play on \u03bb\u03cc\u03b3\u03bf\u03c2 and \u03ac\u03bb\u03bf\u03b3\u03bf\u03bd is characteristic of Clement's writing, and may be rooted in the Epicurean belief that relationships between words are deeply reflective of relationships between the objects they signify.\nClement argues for the equality of sexes, on the grounds that salvation is extended to all humans equally. Unusually, he suggests that Christ is neither female nor male, and that God the Father has both female and male aspects: the eucharist is described as milk from the breast (Christ) of the Father. Clement is supportive of women playing an active role in the leadership of the church and he provides a list of women he considers inspirational, which includes both Biblical and Classical Greek figures. It has been suggested that Clement's progressive views on gender as set out in the \"Paedagogus\" were influenced by Gnosticism, however, later in the work, he argues against the Gnostics that faith, not esoteric knowledge [\u03b3\u03bd\u1ff6\u03c3\u03b9\u03c2], is required for salvation. According to Clement, it is through faith in Christ that one is enlightened and comes to know God.\nIn the second book, Clement provides practical rules on living a Christian life. He argues against overindulgence in food and in favour of good table manners. While prohibiting drunkenness, he promotes the drinking of alcohol in moderation following . Clement argues for a simple way of life in accordance with the innate simplicity of Christian monotheism. He condemns elaborate and expensive furnishings and clothing, and argues against overly passionate music and perfumes, but Clement does not believe in the abandonment of worldly pleasures and argues that the Christian should be able to express joy in God's creation through gaiety and partying. He opposes the wearing of garlands, because the picking of the flowers ultimately kills a beautiful creation of God, and the garland resembles the crown of thorns. Clement treats sex at some length. He argues that both promiscuity and sexual abstinence are unnatural, and that the main goal of human sexuality is procreation. He argues that adultery, coitus with pregnant women, concubinage, homosexuality, and prostitution all should be avoided as they will not contribute toward the generation of legitimate offspring.\nIn his third book, Clement continues along a similar vein, condemning cosmetics on the grounds that it is one's soul, not the body, one should seek to beautify. Clement also opposes the dyeing of men's hair and male depilation as being effeminate. He advises choosing one's company carefully, to avoid being corrupted by immoral people, and while arguing that material wealth is no sin in itself, it is too likely to distract one from the infinitely more important spiritual wealth that is found in Christ. The work finishes with selections of scripture supporting Clement's argument, and following a prayer, the lyrics of a hymn.\n\"Stromata\".\nThe contents of the \"Stromata\", as its title suggests, are miscellaneous. Its place in the trilogy is disputed \u2013 Clement initially intended to write the \"Didasculus\", a work that would complement the practical guidance of the \"Paedagogus\" with a more intellectual schooling in theology. The \"Stromata\" is less systematic and ordered than Clement's other works, and it has been theorized by Andr\u00e9 M\u00e9hat that it was intended for a limited, esoteric readership. Although Eusebius wrote of the eight books of the work, only seven undoubtedly survive. Photius, writing in the 9th century, found various text appended to manuscripts of the seven canonical books, which led Daniel Heinsius to suggest that the original eighth book is lost, and he identified the text purported to be from the eighth book as fragments of the \"Hypotyposes\".\nThe first book starts on the topic of Greek philosophy. Consistent with his other writing, Clement affirms that philosophy had a propaedeutic role for the Greeks, similar to the function of the law for the Jews. He then embarks on a discussion of the origins of Greek culture and technology, arguing that most of the important figures in the Greek world were foreigners, and (erroneously) that Jewish culture was the most significant influence on Greece. In an attempt to demonstrate the primacy of Moses, Clement gives an extended chronology of the world, wherein he dates the birth of Christ to 25 April or May, 4-2 BC, and the creation of the world to 5592 BC. The books ends with a discussion on the origin of languages and the possibility of a Jewish influence on Plato.\nThe second book is largely devoted to the respective roles of faith and philosophical argument. Clement contends that while both are important, the fear of God is foremost, because through faith one receives divine wisdom. To Clement, scripture is an innately true primitive philosophy that is complemented by human reason through the Logos. Faith is voluntary, and the decision to believe is a crucial fundamental step in becoming closer to God. It is never irrational, as it is founded on the knowledge of the truth of the Logos, but all knowledge proceeds from faith, as first principles are unprovable outside a systematic structure.\nThe third book covers asceticism. He discusses marriage, which is treated similarly in the \"Paedagogus\". Clement rejects the Gnostic opposition to marriage, arguing that only men who are uninterested in women should remain celibate, and that sex is a positive good if performed within marriage for the purposes of procreation. He argues that this has not always been so: the Fall occurred because Adam and Eve succumbed to their desire for each other, and copulated before the allotted time. He argues against the idea that Christians should reject their family for an ascetic life, which stems from Luke , contending that Jesus would not have contradicted the precept to \"Honour thy Father and thy Mother\" (Exodus ), one of the Ten Commandments. Clement concludes that asceticism will only be rewarded if the motivation is Christian in nature, and thus the asceticism of non-Christians such as the gymnosophists is pointless.\nClement begins the fourth book with a belated explanation of the disorganized nature of the work, and gives a brief description of his aims for the remaining three or four books. The fourth book focuses on martyrdom. While all good Christians should be unafraid of death, Clement condemns those who actively seek out a martyr's death, arguing that they do not have sufficient respect for God's gift of life. He is ambivalent about whether any believing Christians can become a martyrs by virtue of the manner of their death, or whether martyrdom is reserved for those who have lived exceptional lives. Marcionites cannot become martyrs, because they do not believe in the divinity of God the Father, so their sufferings are in vain. There is then a digression to the subject of theological epistemology. According to Clement, there is no way of empirically testing the existence of God the Father, because the Logos has revelatory, not analysable meaning, although Christ was an object of the senses. God had no beginning, and is the universal first principle.\nThe fifth book returns to the subject of faith. Clement argues that truth, justice, and goodness can be seen only by the mind, not the eye; faith is a way of accessing the unseeable. He stresses that knowledge of God can only be achieved through faith once one's moral faults have been corrected. This parallels Clement's earlier insistence that martyrdom can only be achieved by those who practice their faith in Christ through good deeds, not those who simply profess their faith. God transcends matter entirely, and thus the materialist cannot truly come to know God. Although Christ was God incarnate, it is spiritual, not physical comprehension of him that is important.\nIn the beginning of the sixth book, Clement intends to demonstrate that the works of Greek poets were derived from the prophetic books of the Bible. In order to reinforce his position that the Greeks were inclined toward plagiarism, he cites numerous instances of such inappropriate appropriation by classical Greek writers, reported second-hand from \"On Plagiarism\", an anonymous 3rd-century BC work sometimes ascribed to Aretades. Clement then digresses to the subject of sin and hell, arguing that Adam was not perfect when created, but given the potential to achieve perfection. He espouses broadly universalist doctrine, holding that Christ's promise of salvation is available to all, even those condemned to hell.\nThe final extant book begins with a description of the nature of Christ, and that of the true Christian, who aims to be as similar as possible to both the Father and the Son. Clement then criticizes the simplistic anthropomorphism of most ancient religions, quoting Xenophanes' famous description of African, Thracian, and Egyptian deities. He indicates that the Greek deities may also have had their origins in the personification of material objects: Ares representing iron, and Dionysus wine. Prayer, and the relationship between love and knowledge are then discussed. seems to contradict the characterization of the true Christian as one who knows; but to Clement knowledge vanishes only in that it is subsumed by the universal love expressed by the Christian in reverence for the Creator. Following Socrates, he argues that vice arises from a state of ignorance, not from intention. The Christian is a \"laborer in God's vineyard\", responsible both for one's own path to salvation and that of one's neighbor. The work ends with an extended passage against the contemporary divisions and heresies within the church.\nOther works.\nBesides the great trilogy, Clement's only other extant work is the treatise \"Salvation for the Rich\", also known as \"Who is the Rich Man who is Saved?\" Having begun with a scathing criticism of the corrupting effects of money and misguided servile attitudes toward the wealthy, Clement discusses the implications of . The rich are either unconvinced by the promise of eternal life, or unaware of the conflict between the possession of material and spiritual wealth, and the good Christian has a duty to guide them toward a better life through the Gospel. Jesus' words are not to be taken literally \u2014 the supercelestial [\u1f51\u03c0\u03b5\u03c1\u03bf\u03c5\u03c1\u03ac\u03bd\u03b9\u03bf\u03c2] meanings should be sought in which the true route to salvation is revealed. The holding of material wealth in itself is not a wrong, so long as it is used charitably, but Christians should be careful not to let their wealth dominate their spirit. It is more important to give up sinful passions than external wealth. If the rich are to be saved, all they must do is to follow the two commandments, and while material wealth is of no value to God, it can be used to alleviate the suffering of neighbors.\nOther known works exist in fragments alone, including the four eschatological works in the secret tradition: \"Hypotyposes\", \"Excerpta ex Theodoto\", \"Eclogae Propheticae\", and the \"Adumbraetiones\". These cover Clement's celestial hierarchy, a complex schema in which the universe is headed by the Face of God, below which lie seven \"protoctists\", followed by archangels, angels, and humans. According to Jean Dani\u00e9lou, this schema is inherited from a Judaeo-Christian esotericism, followed by the Apostles, which was only imparted orally to those Christians who could be trusted with such mysteries. The \"proctocists\" are the first beings created by God, and act as priests to the archangels. Clement identifies them both as the \"Eyes of the Lord\" and with the Thrones. Clement characterizes the celestial forms as entirely different from anything earthly, although he argues that members of each order only seem incorporeal to those of lower orders. According to the \"Eclogae Propheticae\", every thousand years every member of each order moves up a degree, and thus humans can become angels. Even the \"protoctists\" can be elevated, although their new position in the hierarchy is not clearly defined. The apparent contradiction between the fact that there can be only seven \"protoctists\" but also a vast number of archangels to be promoted to their order is problematical. \nOne modern solution regards the story as an example of \"interiorized apocalypticism\": imagistic details are not to be taken literally, but as symbolizing interior transformation.\nWe know the titles of several lost works because of a list in Eusebius' \"Ecclesiastical History\", 6.13.1-3. They include the \"Outlines\", in eight books, and \"Against Judaizers\". Others are known only from mentions in Clement's own writings, including \"On Marriage\" and \"On Prophecy\", although few are attested by other writers and it is difficult to separate works that he intended to write from those that were completed.\nThe Mar Saba letter was attributed to Clement by Morton Smith, but there remains much debate today over whether it is an authentic letter from Clement, an ancient pseudepigraph, or a modern forgery. If authentic, its main significance would be in its relating that the Apostle Mark came to Alexandria from Rome and there, wrote a more spiritual Gospel, which he entrusted to the Church in Alexandria on his death; if genuine, the letter pushes back the tradition related by Eusebius connecting Mark with Alexandria by a century.\nLegacy.\nEusebius is the first writer to provide an account of Clement's life and works, in his \"Ecclesiastical History\", 5.11.1-5, 6.6.1 Eusebius provides a list of Clement's works, biographical information, and an extended quotation from the \"Stromata\".\nPhotios I of Constantinople writes against Clement's theology in the \"Bibliotheca\", although he is appreciative of Clement's learning and the literary merits of his work. In particular, he is highly critical of the \"Hypotyposes\", a work of biblical exegesis of which only a few fragments have survived. Photios compared Clement's treatise, which, like his other works, was highly syncretic, featuring ideas of Hellenistic, Jewish, and Gnostic origin, unfavorably against the prevailing orthodoxy of the 9th century. Amongst the particular ideas Photios deemed heretical were:\nAs one of the earliest of the Church fathers whose works have survived, he is the subject of a significant amount of recent academic work, focusing on, among other things, his exegesis of scripture, his Logos-theology and pneumatology, the relationship between his thought and non-Christian philosophy, and his influence on Origen.\nVeneration.\nUp until the 17th century Clement was venerated as a saint in the Roman Catholic Church. His name was to be found in the martyrologies, and his feast fell on the fourth of December, but when the Roman Martyrology was revised by Pope Clement VIII his name was dropped from the calendar on the advice of Cardinal Baronius. Benedict XIV maintained this decision of his predecessor on the grounds that Clement's life was little known, that he had never obtained public cultus in the Church, and that some of his doctrines were, if not erroneous, at least suspect.\nAlthough Clement is not widely venerated in Eastern Christianity, the Prologue of Ohrid repeatedly refers to him as a saint, as do various Orthodox authorities including the Greek Metropolitan Kallinikos of Edessa.\nThe Coptic tradition considers Clement a saint. Saint Clement Coptic Orthodox Christian Academy in Nashville, Tennessee, is specifically named after him. \nClement is commemorated in Anglicanism. Also, the independent Universal Catholic Church's cathedral in Dallas is dedicated to him."}
{"id": "7344", "revid": "6063935", "url": "https://en.wikipedia.org/wiki?curid=7344", "title": "Cogito, ergo sum", "text": " is a philosophical statement that was made in Latin by Ren\u00e9 Descartes, usually translated into English as \"I think, therefore I am\". The phrase originally appeared in French as , in his \"Discourse on the Method\", so as to reach a wider audience than Latin would have allowed. It appeared in Latin in his later \"Principles of Philosophy\". As Descartes explained it, \"we cannot doubt of our existence while we doubt.\" A fuller version, articulated by Antoine L\u00e9onard Thomas, aptly captures Descartes's intent: (\"I doubt, therefore I think, therefore I am\"). The dictum is also sometimes referred to as the cogito.\nDescartes's statement became a fundamental element of Western philosophy, as it purported to provide a certain foundation for knowledge in the face of radical doubt. While other knowledge could be a figment of imagination, deception, or mistake, Descartes asserted that the very act of doubting one's own existence served\u2014at minimum\u2014as proof of the reality of one's own mind; there must be a thinking entity\u2014in this case the self\u2014for there to be a thought.\nOne common critique of the dictum is that it presupposes that there is an \"I\" which must be doing the thinking. According to this line of criticism, the most that Descartes was entitled to say was that \"thinking is occurring\", not that \"I am thinking\".\nIn Descartes's writings.\nDescartes first wrote the phrase in French in his 1637 \"Discourse on the Method\". He referred to it in Latin without explicitly stating the familiar form of the phrase in his 1641 \"Meditations on First Philosophy\". The earliest written record of the phrase in Latin is in his 1644 \"Principles of Philosophy\", where, in a margin note (see below), he provides a clear explanation of his intent: \"[W]e cannot doubt of our existence while we doubt\". Fuller forms of the phrase are attributable to other authors.\n\"Discourse on the Method\".\nThe phrase first appeared (in French) in Descartes' 1637 \"Discourse on the Method\" in the first paragraph of its fourth part:\n\"Meditations on First Philosophy\".\nIn 1641, Descartes published (in Latin) \"Meditations on first philosophy\" in which he referred to the proposition, though not explicitly as \"cogito, ergo sum\" in Meditation II:\n\"Principles of Philosophy\".\nIn 1644, Descartes published (in Latin) his \"Principles of Philosophy\" where the phrase \"ego cogito, ergo sum\" appears in Part 1, article 7:\nDescartes's margin note for the above paragraph is:\n\"The Search for Truth\".\nDescartes, in a lesser-known posthumously published work dated as written ca. 1647 and titled (\"The Search for Truth by Natural Light\"), wrote:\nOther forms.\nThe proposition is sometimes given as . This fuller form was penned by the French literary critic, Antoine L\u00e9onard Thomas, in an award-winning 1765 essay in praise of Descartes, where it appeared as \"\" ('Since I doubt, I think; since I think, I exist'). With rearrangement and compaction, the passage translates to \"I doubt, therefore I think, therefore I am,\" or in Latin, \"\"dubito, ergo cogito, ergo sum\".\" This aptly captures Descartes\u2019s intent as expressed in his posthumously published \"La Recherche de la V\u00e9rit\u00e9 par La Lumiere Naturale\" as noted above: I doubt, therefore I am \u2014 or what is the same \u2014 \"\"'I think, therefore I am\"\u2019\".\nA further expansion, (\"\u2026\u2014a thinking thing\") extends the \"cogito\" with Descartes's statement in the subsequent \"Meditation\", (\"I am a thinking [conscious] thing, that is, a being who doubts, affirms, denies, knows a few objects, and is ignorant of many\u2026\"). This has been referred to as \"the expanded \"cogito\".\"\nTranslation.\n\"I am thinking\" vs. \"I think\".\nNeither nor indicate whether the verb form corresponds to the English simple present or progressive aspect. Translation needs a larger context to determine aspect.\nFollowing John Lyons (1982), Vladimir \u017degarac notes, \"The temptation to use the simple present is said to arise from the lack of progressive forms in Latin and French, and from a misinterpretation of the meaning of \"cogito\" as habitual or generic\" (cf. gnomic aspect). Also following Lyons, Ann Banfield writes, \"In order for the statement on which Descartes's argument depends to represent certain knowledge,\u2026 its tense must be a true present\u2014in English, a progressive,\u2026 not as 'I think' but as 'I am thinking, in conformity with the general translation of the Latin or French present tense in such nongeneric, nonstative contexts.\" Or in the words of Simon Blackburn, \"Descartes\u2019s premise is not \u2018I think\u2019 in the sense of \u2018I ski\u2019, which can be true even if you are not at the moment skiing. It is supposed to be parallel to \u2018I am skiing\u2019.\"\nThe similar translation \u201cI am thinking, therefore I exist\u201d of Descartes's correspondence in French (\u201c, \u201d) appears in \"The Philosophical Writings of Descartes\" by Cottingham et al. (1988).\nThe earliest known translation as \"I am thinking, therefore I am\" is from 1872 by Charles Porterfield Krauth.\nFumitaka Suzuki writes \"Taking consideration of Cartesian theory of continuous creation, which theory was developed especially in the Meditations and in the Principles, we would assure that 'I am thinking, therefore I am/exist' is the most appropriate English translation of 'ego cogito, ergo sum'.\"\n\"I exist\" vs. \"I am\".\nAlexis Deodato S. Itao notes that is \"literally 'I think, therefore I am'.\" Others differ: 1) \"[A] precise English translation will read as 'I am thinking, therefore I exist'.; and 2) \"[S]ince Descartes \u2026 emphasized that existence is such an important 'notion,' a better translation is 'I am thinking, therefore I exist.'\"\nInterpretation.\nAs put succinctly by Krauth (1872), \"That cannot doubt which does not think, and that cannot think which does not exist. I doubt, I think, I exist.\"\nThe phrase \"cogito, ergo sum\" is not used in Descartes's \"Meditations on First Philosophy\" but the term \"the \"cogito\"\" is used to refer to an argument from it. In the \"Meditations\", Descartes phrases the conclusion of the argument as \"that the proposition, \"I am, I exist,\" is necessarily true whenever it is put forward by me or conceived in my mind\" (\"Meditation\" II).\nAt the beginning of the second meditation, having reached what he considers to be the ultimate level of doubt\u2014his argument from the existence of a deceiving god\u2014Descartes examines his beliefs to see if any have survived the doubt. In his belief in his own existence, he finds that it is impossible to doubt that he exists. Even if there were a deceiving god (or an evil demon), one's belief in their own existence would be secure, for there is no way one could be deceived unless one existed in order to be deceived.\nBut I have convinced myself that there is absolutely nothing in the world, no sky, no earth, no minds, no bodies. Does it now follow that I, too, do not exist? No. If I convinced myself of something [or thought anything at all], then I certainly existed. But there is a deceiver of supreme power and cunning who deliberately and constantly deceives me. In that case, I, too, undoubtedly exist, if he deceives me; and let him deceive me as much as he can, he will never bring it about that I am nothing, so long as I think that I am something. So, after considering everything very thoroughly, I must finally conclude that the proposition, \"I am, I exist,\" is necessarily true whenever it is put forward by me or conceived in my mind. (AT VII 25; CSM II 16\u201317)\nThere are three important notes to keep in mind here. First, he claims only the certainty of \"his own\" existence from the first-person point of view \u2014 he has not proved the existence of other minds at this point. This is something that has to be thought through by each of us for ourselves, as we follow the course of the meditations. Second, he does not say that his existence is necessary; he says that \"if he thinks\", then necessarily he exists (see the instantiation principle). Third, this proposition \"I am, I exist\" is held true not based on a deduction (as mentioned above) or on empirical induction but on the clarity and self-evidence of the proposition. Descartes does not use this first certainty, the \"cogito\", as a foundation upon which to build further knowledge; rather, it is the firm ground upon which he can stand as he works to discover further truths. As he puts it:\nArchimedes used to demand just one firm and immovable point in order to shift the entire earth; so I too can hope for great things if I manage to find just one thing, however slight, that is certain and unshakable. (AT VII 24; CSM II 16)\nAccording to many Descartes specialists, including \u00c9tienne Gilson, the goal of Descartes in establishing this first truth is to demonstrate the capacity of his criterion \u2014 the immediate clarity and distinctiveness of self-evident propositions \u2014 to establish true and justified propositions despite having adopted a method of generalized doubt. As a consequence of this demonstration, Descartes considers science and mathematics to be justified to the extent that their proposals are established on a similarly immediate clarity, distinctiveness, and self-evidence that presents itself to the mind. The originality of Descartes's thinking, therefore, is not so much in expressing the \"cogito\"\u2014a feat accomplished by other predecessors, as we shall see\u2014but on using the \"cogito\" as demonstrating the most fundamental epistemological principle, that science and mathematics are justified by relying on clarity, distinctiveness, and self-evidence.\nBaruch Spinoza in \"Principia philosophiae cartesianae\" at its \"Prolegomenon\" identified \"cogito ergo sum\" the \"ego sum cogitans\" (I am a thinking being) as the thinking substance with his ontological interpretation.\nPredecessors.\nAlthough the idea expressed in \"cogito, ergo sum\" is widely attributed to Descartes, he was not the first to mention it. Plato spoke about the \"knowledge of knowledge\" (Greek: \u03bd\u03cc\u03b7\u03c3\u03b9\u03c2 \u03bd\u03bf\u03ae\u03c3\u03b5\u03c9\u03c2, \"n\u00f3esis no\u00e9seos\") and Aristotle explains the idea in full length:\nBut if life itself is good and pleasant\u2026and if one who sees is conscious that he sees, one who hears that he hears, one who walks that he walks and similarly for all the other human activities there is a faculty that is conscious of their exercise, so that whenever we perceive, we are conscious that we perceive, and whenever we think, we are conscious that we think, and to be conscious that we are perceiving or thinking is to be conscious that we exist... (\"Nicomachean Ethics\", 1170a25 ff.) In the late sixth or early fifth century BC, Parmenides is quoted as saying \"For to be aware and to be are the same\". (Fragment B3)\nIn the early fifth century AD, Augustine of Hippo in \"De Civitate Dei\" (book XI, 26) wrote \"If I am mistaken, I am\" (), and anticipated modern refutations of the concept. In 1640, Descartes wrote to thank Andreas Colvius (a friend of Descartes's mentor, Isaac Beeckman) for drawing his attention to Augustine:\nI am obliged to you for drawing my attention to the passage of St Augustine relevant to my \"I am thinking, therefore I exist\". I went today to the library of this town to read it, and I do indeed find that he does use it to prove the certainty of our existence. He goes on to show that there is a certain likeness of the Trinity in us, in that we exist, we know that we exist, and we love the existence and the knowledge we have. I, on the other hand, use the argument to show that this \"I\" which is thinking is an immaterial substance with no bodily element. These are two very different things. In itself it is such a simple and natural thing to infer that one exists from the fact that one is doubting that it could have occurred to any writer. But I am very glad to find myself in agreement with St Augustine, if only to hush the little minds who have tried to find fault with the principle.\nIn the \"Enchiridion\" (ch. 7, sec. 20), Augustine attempts to refute skepticism by stating, \"[B]y not positively affirming that they are alive, the skeptics ward off the appearance of error in themselves, yet they do make errors simply by showing themselves alive; one cannot err who is not alive. That we live is therefore not only true, but it is altogether certain as well.\" \nAnother predecessor was Avicenna's \"Floating Man\" thought experiment on human self-awareness and self-consciousness.\nThe 8th century Hindu philosopher Adi Shankara wrote, in a similar fashion, that no one thinks 'I am not', arguing that one's existence cannot be doubted, as there must be someone there to doubt. The central idea of \"cogito, ergo sum\" is also the topic of \"Mandukya Upanishad\".\nSpanish philosopher G\u00f3mez Pereira in his 1554 work \"De Inmortalitate Animae\", published in 1749, wrote \"nosco me aliquid noscere, &amp; quidquid noscit, est, ergo ego sum\" ('I know that I know something, anyone who knows exists, then I exist').\nCritique.\nUse of \"I\".\nIn \"Descartes, The Project of Pure Enquiry\", Bernard Williams provides a history and full evaluation of this issue. The first to raise the \"I\" problem was Pierre Gassendi. He \"points out that recognition that one has a set of thoughts does not imply that one is a particular thinker or another. Were we to move from the observation that there is thinking occurring to the attribution of this thinking to a particular agent, we would simply assume what we set out to prove, namely, that there exists a particular person endowed with the capacity for thought.\" In other words, \"the only claim that is indubitable here is the agent-independent claim that there is cognitive activity present.\" \nThe objection, as presented by Georg Lichtenberg, is that rather than supposing an entity that is thinking, Descartes should have said: \"thinking is occurring.\" That is, whatever the force of the \"cogito\", Descartes draws too much from it; the existence of a thinking thing, the reference of the \"I,\" is more than the \"cogito\" can justify. Friedrich Nietzsche criticized the phrase in that it presupposes that there is an \"I\", that there is such an activity as \"thinking\", and that \"I\" know what \"thinking\" is. He suggested a more appropriate phrase would be \"it thinks\" wherein the \"it\" could be an impersonal subject as in the sentence \"It is raining.\"\nKierkegaard.\nThe Danish philosopher S\u00f8ren Kierkegaard calls the phrase a tautology in his \"Concluding Unscientific Postscript\". He argues that the \"cogito\" already presupposes the existence of \"I\", and therefore concluding with existence is logically trivial. Kierkegaard's argument can be made clearer if one extracts the premise \"I think\" into the premises \"'x' thinks\" and \"I am that 'x'\", where \"x\" is used as a placeholder in order to disambiguate the \"I\" from the thinking thing.\nHere, the \"cogito\" has already assumed the \"I\"'s existence as that which thinks. For Kierkegaard, Descartes is merely \"developing the content of a concept\", namely that the \"I\", which already exists, thinks. As Kierkegaard argues, the proper logical flow of argument is that existence is already assumed or presupposed in order for thinking to occur, not that existence is concluded from that thinking.\nWilliams.\nBernard Williams claims that what we are dealing with when we talk of thought, or when we say \"I am thinking,\" is something conceivable from a third-person perspective\u2014namely objective \"thought-events\" in the former case, and an objective thinker in the latter. He argues, first, that it is impossible to make sense of \"there is thinking\" without relativizing it to \"something.\" However, this something cannot be Cartesian egos, because it is impossible to differentiate objectively between things just on the basis of the pure content of consciousness. The obvious problem is that, through introspection, or our experience of consciousness, we have no way of moving to conclude the existence of any third-personal fact, to conceive of which would require something above and beyond just the purely subjective contents of the mind.\nHeidegger.\nAs a critic of Cartesian subjectivity, Heidegger sought to ground human subjectivity in death as that certainty which individualizes and authenticates our being. As he wrote in 1925 in \"History of the Concept of Time\":This certainty, that \"I myself am in that I will die,\" is the basic certainty of Dasein itself. It is a genuine statement of Dasein, while \"cogito sum\" is only the semblance of such a statement. If such pointed formulations mean anything at all, then the appropriate statement pertaining to Dasein in its being would have to be \"sum moribundus\" [I am in dying], \"moribundus\" not as someone gravely ill or wounded, but insofar as I am, I am \"moribundus\". The \"MORIBUNDUS\" first gives the \"SUM\" its sense.\nJohn Macmurray.\nThe Scottish philosopher John Macmurray rejects the \"cogito\" outright in order to place action at the center of a philosophical system he entitles the Form of the Personal. \"We must reject this, both as standpoint and as method. If this be philosophy, then philosophy is a bubble floating in an atmosphere of unreality.\" The reliance on thought creates an irreconcilable dualism between thought and action in which the unity of experience is lost, thus dissolving the integrity of our selves, and destroying any connection with reality. In order to formulate a more adequate \"cogito\", Macmurray proposes the substitution of \"I do\" for \"I think,\" ultimately leading to a belief in God as an agent to whom all persons stand in relation."}
