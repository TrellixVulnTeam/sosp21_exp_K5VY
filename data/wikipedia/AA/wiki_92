{"id": "10869", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=10869", "title": "Frequentist probability", "text": "Frequentist probability or frequentism is an interpretation of probability; it defines an event's probability as the limit of its relative frequency in many trials. Probabilities can be found (in principle) by a repeatable objective process (and are thus ideally devoid of opinion). This interpretation supports the statistical needs of many experimental scientists and pollsters. It does not support all needs, however; gamblers typically require estimates of the odds without experiments.\nThe development of the frequentist account was motivated by the problems and paradoxes of the previously dominant viewpoint, the classical interpretation. In the classical interpretation, probability was defined in terms of the principle of indifference, based on the natural symmetry of a problem, so, \"e.g.\" the probabilities of dice games arise from the natural symmetric 6-sidedness of the cube. This classical interpretation stumbled at any statistical problem that has no natural symmetry for reasoning.\nDefinition.\nIn the frequentist interpretation, probabilities are discussed only when dealing with well-defined random experiments (or random samples). The set of all possible outcomes of a random experiment is called the sample space of the experiment. An event is defined as a particular subset of the sample space to be considered. For any given event, only one of two possibilities may hold: it occurs or it does not. The relative frequency of occurrence of an event, observed in a number of repetitions of the experiment, is a measure of the probability of that event. This is the core conception of probability in the frequentist interpretation.\nA claim of the frequentist approach is that, as the number of trials increases, the change in the relative frequency will diminish. Hence, one can view a probability as the \"limiting value\" of the corresponding relative frequencies.\nScope.\nThe frequentist interpretation is a philosophical approach to the definition and use of probabilities; it is one of several such approaches. It does not claim to capture all connotations of the concept 'probable' in colloquial speech of natural languages.\nAs an interpretation, it is not in conflict with the mathematical axiomatization of probability theory; rather, it provides guidance for how to apply mathematical probability theory to real-world situations. It offers distinct guidance in the construction and design of practical experiments, especially when contrasted with the Bayesian interpretation. As to whether this guidance is useful, or is apt to mis-interpretation, has been a source of controversy. Particularly when the frequency interpretation of probability is mistakenly assumed to be the only possible basis for frequentist inference. So, for example, a list of mis-interpretations of the meaning of p-values accompanies the article on p-values; controversies are detailed in the article on statistical hypothesis testing. The Jeffreys\u2013Lindley paradox shows how different interpretations, applied to the same data set, can lead to different conclusions about the 'statistical significance' of a result.\nAs William Feller noted:\nFeller's comment was criticism of Laplace, who published a solution to the sunrise problem using an alternative probability interpretation. Despite Laplace's explicit and immediate disclaimer in the source, based on expertise in astronomy as well as probability, two centuries of criticism have followed.\nHistory.\nThe frequentist view may have been foreshadowed by Aristotle, in \"Rhetoric\", when he wrote:\nPoisson clearly distinguished between objective and subjective probabilities in 1837. Soon thereafter a flurry of nearly simultaneous publications by Mill, Ellis (\"On the Foundations of the Theory of Probabilities\" and \"Remarks on the Fundamental Principles of the Theory of Probabilities\"), Cournot (\"Exposition de la th\u00e9orie des chances et des probabilit\u00e9s\") and Fries introduced the frequentist view. Venn provided a thorough exposition (\"The Logic of Chance: An Essay on the Foundations and Province of the Theory of Probability\" (published editions in 1866, 1876, 1888)) two decades later. These were further supported by the publications of Boole and Bertrand. By the end of the 19th century the frequentist interpretation was well established and perhaps dominant in the sciences. The following generation established the tools of classical inferential statistics (significance testing, hypothesis testing and confidence intervals) all based on frequentist probability.\nAlternatively, Jacob Bernoulli (AKA James or Jacques) understood the concept of frequentist probability and published a critical proof (the weak law of large numbers) posthumously in \"1713\". He is also credited with some appreciation for subjective probability (prior to and without Bayes theorem). Gauss and Laplace used frequentist (and other) probability in derivations of the least squares method a century later, a generation before Poisson. Laplace considered the probabilities of testimonies, tables of mortality, judgments of tribunals, etc. which are unlikely candidates for classical probability. In this view, Poisson's contribution was his sharp criticism of the alternative \"inverse\" (subjective, Bayesian) probability interpretation. Any criticism by Gauss and Laplace was muted and implicit. (Their later derivations did not use inverse probability.)\nMajor contributors to \"classical\" statistics in the early 20th century included Fisher, Neyman and Pearson. Fisher contributed to most of statistics and made significance testing the core of experimental science; Neyman formulated confidence intervals and contributed heavily to sampling theory; Neyman and Pearson paired in the creation of hypothesis testing. All valued objectivity, so the best interpretation of probability available to them was frequentist. All were suspicious of \"inverse probability\" (the available alternative) with prior probabilities chosen by using the principle of indifference. Fisher said, \"...the theory of inverse probability is founded upon an error, [referring to Bayes theorem] and must be wholly rejected.\" (from his Statistical Methods for Research Workers). While Neyman was a pure frequentist, Fisher's views of probability were unique; Both had nuanced view of probability. von Mises offered a combination of mathematical and philosophical support for frequentism in the era.\nEtymology.\nAccording to the \"Oxford English Dictionary\", the term 'frequentist' was first used by M. G. Kendall in 1949, to contrast with Bayesians, whom he called \"non-frequentists\". He observed\n\"The Frequency Theory of Probability\" was used a generation earlier as a chapter title in Keynes (1921).\nThe historical sequence: probability concepts were introduced and much of probability mathematics derived (prior to the 20th century), classical statistical inference methods were developed, the mathematical foundations of probability were solidified and current terminology was introduced (all in the 20th century). The primary historical sources in probability and statistics did not use the current terminology of classical, subjective (Bayesian) and frequentist probability.\nAlternative views.\nProbability theory is a branch of mathematics. While its roots reach centuries into the past, it reached maturity with the axioms of Andrey Kolmogorov in 1933. The theory focuses on the valid operations on probability values rather than on the initial assignment of values; the mathematics is largely independent of any interpretation of probability.\nApplications and interpretations of probability are considered by philosophy, the sciences and statistics. All are interested in the extraction of knowledge from observations\u2014inductive reasoning. There are a variety of competing interpretations; All have problems. The frequentist interpretation does resolve difficulties with the classical interpretation, such as any problem where the natural symmetry of outcomes is not known. It does not address other issues, such as the dutch book."}
{"id": "10870", "revid": "31530", "url": "https://en.wikipedia.org/wiki?curid=10870", "title": "List of French-language poets", "text": "List of poets who have written in the French language:"}
{"id": "10871", "revid": "21006650", "url": "https://en.wikipedia.org/wiki?curid=10871", "title": "FM-2030", "text": "FM-2030 (originally born as Fereidoun M. Esfandiary (); October 15, 1930 \u2013 July 8, 2000) was a Belgian-born Iranian-American author, teacher, transhumanist philosopher, futurist, consultant and athlete.\nHe became notable as a transhumanist with the book \"Are You a Transhuman?: Monitoring and Stimulating Your Personal Rate of Growth in a Rapidly Changing World\", published in 1989. In addition, he wrote a number of works of fiction under his original name F.M. Esfandiary.\nEarly life and education.\nThe son of an Iranian diplomat, he travelled widely as a child, having lived in 17 countries by age 11; then, as a young man, he represented Iran as a basketball player at the 1948 Olympic Games in London. He then started his college education at the University of California, Berkeley, but later transferred to the University of California, Los Angeles, where he graduated in 1952. Afterwards, he served on the United Nations Conciliation Commission for Palestine from 1952 to 1954.\nName change.\nIn the mid-1970s F.M. Esfandiary legally changed his name to FM-2030 for two main reasons. Firstly, to reflect the hope and belief that he would live to celebrate his 100th birthday in 2030; secondly, and more importantly, to break free of the widespread practice of naming conventions that he saw as rooted in a collectivist mentality, and existing only as a relic of humankind's tribalistic past. He viewed traditional names as almost always stamping a label of collective identity \u2013 varying from gender to nationality \u2013 on the individual, thereby existing as prima facie elements of thought processes in the human cultural fabric, that tended to degenerate into stereotyping, factionalism, and discrimination. In his own words, \"Conventional names define a person's past: ancestry, ethnicity, nationality, religion. I am not who I was ten years ago and certainly not who I will be in twenty years. [...] The name 2030 reflects my conviction that the years around 2030 will be a magical time. In 2030 we will be ageless and everyone will have an excellent chance to live forever. 2030 is a dream and a goal.\"\nPersonal life.\nHe was a lifelong vegetarian and said he would not eat anything that had a mother. FM-2030 once said, \"I am a 21st century person who was accidentally launched in the 20th. I have a deep nostalgia for the future.\" He taught at The New School, University of California, Los Angeles, and Florida International University. He worked as a corporate consultant for Lockheed and J. C. Penney. He was also an atheist.\nFM-2030 was, in his own words, a follower of \"upwing\" politics, in which he meant that he endorsed universal progress. He had been in a non-exclusive \"friendship\" (his preferred term for relationship) with Flora Schnall, a lawyer and fellow Harvard Law Class of 1959 graduate with Ruth Bader Ginsburg, from the 1960s until his death.\nDeath.\nOn July 8, 2000, FM-2030 died from pancreatic cancer and was placed in cryonic suspension at the Alcor Life Extension Foundation in Scottsdale, Arizona, where his body remains today. He did not yet have remote standby arrangements, so no Alcor team member was present at his death, but FM-2030 was the first person to be vitrified, rather than simply frozen as previous cryonics patients had been. FM-2030 was survived by four sisters and one brother."}
{"id": "10874", "revid": "39715069", "url": "https://en.wikipedia.org/wiki?curid=10874", "title": "West Flemish", "text": "West Flemish (\"West-Vlams\" or \"West-Vloams\" or \"Vlaemsch\" (in French-Flanders), , ) is a West Germanic language spoken in western Belgium and the neighboring areas of France and Netherlands.\nWest Flemish is spoken by about a million people in the Belgian province of West Flanders, and a further 50,000 in the neighbouring Dutch coastal district of Zeelandic Flanders (200,000 if including the closely related dialects of Zeelandic) and 10-20,000 in the northern part of the French \"d\u00e9partement\" of Nord. Some of the main cities where West Flemish is widely spoken are Bruges, Dunkirk, Kortrijk, Ostend, Roeselare, Ypres and Newport.\nWest Flemish is listed as a \"vulnerable\" language in UNESCO's online Red Book of Endangered Languages. The language has its own dedicated Wikipedia.\nPhonology.\nWest Flemish has a phonology that differs significantly from that of Standard Dutch, being similar to Afrikaans in the case of long E and O, and where Standard Dutch has \"sch\", Flemish, like Afrikaans, has \"sk\". However, the best known traits are the replacement of Standard Dutch (pre-)velar fricatives \"g\" and \"ch\" in Dutch () with glottal \"h\" , and the overall lack of diphthongs. The following differences are listed by their Dutch spelling, as some different letters have merged their sounds in Standard Dutch but remained separate sounds in West Flemish. Pronunciations can also differ slightly from region to region.\nThe absence of and in West Flemish makes pronouncing them very difficult for native speakers. That often causes hypercorrection of the sounds to a or .\nStandard Dutch also has many words with an \"-en\" () suffix (mostly plural forms of verbs and nouns). While Standard Dutch and most dialects do not pronounce the final \"n\", West Flemish typically drops the \"e\" and pronounces the \"n\" inside the base word. For base words already ending with \"n\", the final \"n\" sound is often lengthened to clarify the suffix. That makes many words become similar to those of English: \"beaten\", \"listen\" etc.\nThe short \"o\" () can also be pronounced as a short \"u\" (), a phenomenon also occurring in Russian and some other Slavic languages, called akanye. That happens spontaneously to some words, but other words keep their original short \"o\" sounds. Similarly, the short \"a\" () can turn into a short \"o\" () in some words spontaneously, another phenomenon called okanye in Slavic languages.\nThe diphthong \"ui\" () does not exist in West Flemish and is replaced by a long \"u\" () or a long \"ie\" (). Like for the \"ui\", the long \"o\" () can be replaced by an (\"eu\") for some words but a for others. That often causes similarities to ranchers English.\nHere are some examples showing the sound shifts that are part of the vocabulary:\nGrammar.\nPlural form.\nPlural forms in Standard Dutch most often add \"-en\", but West Flemish usually uses \"-s\", like the Lower Saxon Germanic dialects and even more prominently in English in which \"-en\" has become very rare. Under the influence of Standard Dutch, \"-s\" is being used by fewer people, and younger speakers tend to use \"-en\".\nVerb conjugation.\nThe verbs \"zijn\" (\"to be\") and \"hebben\" (\"to have\") are also conjugated differently.\nDouble subject.\nWest Flemish often has a double subject.\nArticles.\nStandard Dutch has an indefinite article that does not depend on gender, unlike in West Flemish. However, a gender-independent article is increasingly used. Like in English, \"n\" is pronounced only if the next word begins with a vowel sound.\nConjugation of \"yes\" and \"no\".\nAnother feature of West Flemish is the conjugation of \"ja\" and \"nee\" (\"yes\" and \"no\") to the subject of the sentence. That is somewhat related to the double subject, but even when the rest of the sentence is not pronounced, \"ja\" and \"nee\" are generally used with the first part of the double subject. There is also an extra word, \"toet\" (), negates the previous sentence but gives a positive answer. It's an abbreviation of \" 't en doe 't\" - it does it.\n\"Ja\", \"nee\" and \"toet\" can also all be strengthened by adding \"mo-\" or \"ba-\". Both mean \"but\" and are derived from Dutch \"but\" or \"maar\") and can be even used together (\"mobatoet\").\nVocabulary.\nWest Flemish inherited many words from Saxon settlers and later on had English loanwords from the wool and cloth trades. Both categories differ from Standard Dutch and show similarities with English and so is difficult to separate both categories.\nDuring the Industrial Revolution, the increasing trade with France caused many industrial loanwords from French.\nFalse friends.\nEven when words exist in both Dutch and West Flemish, their meaning can be different. That sometimes causes confusion for native speakers who do not realise that words are used differently."}
{"id": "10875", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=10875", "title": "Fritz Leiber", "text": "Fritz Reuter Leiber Jr. (December 24, 1910\u00a0\u2013 September 5, 1992) was an American writer of fantasy, horror, and science fiction. He was also a poet, actor in theater and films, playwright and chess expert. With writers such as Robert E. Howard and Michael Moorcock, Leiber can be regarded as one of the fathers of sword and sorcery fantasy, having coined the term.\nLife.\nFritz Leiber was born December 24, 1910, in Chicago, Illinois, to the actors Fritz Leiber and Virginia Bronson Leiber. For a time, he seemed inclined to follow in his parents' footsteps; the theater and actors are prominently featured in his fiction. He spent 1928 touring with his parents' Shakespeare company (Fritz Leiber &amp; Co.) before entering the University of Chicago, where he was elected to Phi Beta Kappa and received an undergraduate Ph.B. degree in psychology and physiology or biology with honors in 1932. From 1932 to 1933, he worked as a lay reader and studied as a candidate for the ministry at the General Theological Seminary in Chelsea, Manhattan, an affiliate of the Episcopal Church, without taking a degree.\nAfter pursuing graduate studies in philosophy at the University of Chicago from 1933 to 1934 and failing once more to take a degree, he remained based in Chicago while touring intermittently with his parents' company (under the stage name of \"Francis Lathrop\") and pursuing a concurrent literary career (six short stories later included in the 2010 collection \"Strange Wonders: A Collection of Rare Fritz Leiber Works\" carry 1934 and 1935 dates). He also appeared alongside his father in uncredited parts in several films, including George Cukor's \"Camille\" (1936), James Whale's \"The Great Garrick\" (1937) and William Dieterle's \"The Hunchback of Notre Dame\" (1939).\nIn 1936, he initiated a brief yet intense correspondence with H. P. Lovecraft, who \"encouraged and influenced [Leiber's] literary development\" before succumbing to small intestine cancer and malnutrition in March 1937. Leiber introduced Fafhrd and the Gray Mouser in \"Two Sought Adventure\", his first professionally published short story in the August 1939 edition of \"Unknown\", edited by John W. Campbell.\nLeiber married Jonquil Stephens on January 16, 1936; their only child, the philosopher and science fiction writer Justin Leiber, was born in 1938. From 1937 to 1941, he was employed by Consolidated Book Publishing as a staff writer for the \"Standard American Encyclopedia\". In 1941, the family moved to California, where Leiber served as a speech and drama instructor at Occidental College during the 1941\u20131942 academic year.\nUnable to conceal his disdain for academic politics as the United States entered World War II, he decided that the struggle against fascism was more important than his long-held pacifist convictions. He accepted a position with Douglas Aircraft in quality inspection, primarily working on the C-47 Skytrain; throughout the war, he continued to regularly publish fiction in a variety of periodicals.\nThereafter, the family returned to Chicago, where Leiber served as associate editor of \"Science Digest\" from 1945 to 1956. During this decade (forestalled by a fallow interregnum from 1954 to 1956), his output (including the 1947 Arkham House anthology \"Night's Black Agents\") was characterized by Poul Anderson as \"a lot of the best science fiction and fantasy in the business\". In 1958, the Leibers returned to Los Angeles. By this juncture, he was able to relinquish his journalistic career and support his family as a full-time fiction writer.\nJonquil's death in 1969 precipitated Leiber's permanent relocation to San Francisco and exacerbated his longstanding alcoholism after twelve years of fellowship in Alcoholics Anonymous; however, he would gradually regain relative sobriety (an effort impeded by comorbid barbiturate abuse) over the next two decades. In 1977, he returned to his original form with a fantasy novel set in modern-day San Francisco, \"Our Lady of Darkness\", which is about a writer of weird tales who must deal with the death of his wife and his recovery from alcoholism.\nPerhaps as a result of his substance abuse, Leiber seems to have suffered periods of penury in the 1970s; Harlan Ellison wrote of his anger at finding that the much-awarded Leiber had to write his novels on a manual typewriter that was propped up over the sink in his apartment, and Marc Laidlaw wrote that, when visiting Leiber as a fan in 1976, he \"was shocked to find him occupying one small room of a seedy San Francisco residence hotel, its squalor relieved mainly by walls of books\". Other reports suggest that Leiber preferred to live simply in the city, spending his money on dining, movies and travel. In the last years of his life, royalty checks from TSR, Inc. (the makers of \"Dungeons &amp; Dragons\", who had licensed the mythos of the Fafhrd and Gray Mouser series) were enough in themselves to ensure that he lived comfortably.\nIn 1992, the last year of his life, Leiber married his second wife, Margo Skinner, a journalist and poet with whom he had been friends for many years.\nLeiber's death occurred a few weeks after a physical collapse while traveling from a science fiction convention in London, Ontario, with Skinner. The cause of his death was stated by his wife to be stroke.\nHe wrote a 100-page-plus memoir, \"Not Much Disorder and Not So Early Sex\", which can be found in \"The Ghost Light\" (1984).\nLeiber's own literary criticism, including several essays on Lovecraft, was collected in the volume \"Fafhrd and Me\" (1990).\nTheater.\nAs the child of two Shakespearean actors\u2014Fritz Sr. and Virginia (n\u00e9e Bronson)\u2014Leiber was fascinated with the stage, describing itinerant Shakespearean companies in stories like \"No Great Magic\" and \"Four Ghosts in Hamlet\", and creating an actor/producer protagonist for his novel \"A Specter is Haunting Texas\".\nAlthough his \"Change War\" novel, \"The Big Time\", is about a war between two factions, the \"Snakes\" and the \"Spiders\", changing and rechanging history throughout the universe, all the action takes place in a small bubble of isolated space-time about the size of a theatrical stage, with only a handful of characters. Judith Merril (in the July 1969 issue of \"The Magazine of Fantasy &amp; Science Fiction\") remarks on Leiber's acting skills when the writer won a science fiction convention costume ball. Leiber's costume consisted of a cardboard military collar over turned-up jacket lapels, cardboard insignia, an armband, and a spider pencilled large in black on his forehead, thus turning him into an officer of the Spiders, one of the combatants in his Change War stories. \"The only other component,\" Merril writes, \"was the Leiber instinct for theatre.\"\nFilms.\nDue to the similarity of the names of the father and the son, some filmographies incorrectly attribute to Fritz Jr. roles which were in fact played by his father, Fritz Leiber Sr., who was the evil Inquisitor in the Errol Flynn adventure film \"The Sea Hawk\" (1940) and had played in many other movies from 1917 onwards until the late 1950s. It is the elder Leiber, not the younger, who appears in the Vincent Price vehicle \"The Web\" (1947) and in Charlie Chaplin's \"Monsieur Verdoux\" (1947).\nThe younger Leiber can be seen briefly, as Valentin, in the 1936 film version of \"Camille\" starring Greta Garbo, probably his most widely-seen film performance. In the cult horror film \"Equinox\" (1970) directed by Dennis Muren and Jack Woods, Leiber has a cameo appearance as Dr. Watermann, a geologist. In the edited second version of the movie Leiber has no spoken dialogue in the film but features in a few scenes. The original version of the movie has a longer appearance by Leiber recounting the ancient book and a brief speaking role, all of which was cut from the re-release of the film.\nHe also appears in the 1979 Schick Sunn Classics documentary \"The Bermuda Triangle\", based on the book by Charles Berlitz, as Chavez.\nWriting career.\nLeiber was heavily influenced by H. P. Lovecraft, Robert Graves, John Webster among other playwrights along with Shakespeare in the first two decades of his career. Beginning in the late 1950s, he was increasingly influenced by the works of Carl Jung, particularly by the concepts of the anima and the shadow. From the mid-1960s onwards, he began incorporating elements of Joseph Campbell's \"The Hero with a Thousand Faces\". These concepts are often openly mentioned in his stories, especially the anima, which becomes a method of exploring his fascination with, but estrangement from, the female.\nLeiber liked cats, which feature prominently in many of his stories. Tigerishka, for example, is a cat-like alien who is sexually attractive to the human protagonist yet repelled by human customs in the novel \"The Wanderer\". Leiber's \"Gummitch\" stories feature a kitten with an I.Q. of 160, just waiting for his ritual cup of coffee so that he can become human, too.\nHis first stories in the 1930s and 40s were inspired by Lovecraft's Cthulhu Mythos. A notable critic and historian of the wider Mythos, S. T. Joshi, has singled out Leiber's \"The Sunken Land\" (\"Unknown Worlds\", February 1942) as perhaps the most accomplished of the early stories based on Lovecraft's Mythos. Leiber also later wrote several essays on Lovecraft the man, such as \"A Literary Copernicus\" (1949), the publication of which formed a key moment in the emergence of a serious critical appreciation of Lovecraft's life and work.\nLeiber's first professional sale was \"Two Sought Adventure\" (\"Unknown\", August 1939), which introduced his most famous characters, Fafhrd and the Gray Mouser. In 1943, his first two novels were serialized in \"Unknown\" (the supernatural horror-oriented \"Conjure Wife\", partially inspired by his deleterious experiences on the faculty of Occidental College) and \"Astounding Science Fiction\" (\"Gather, Darkness\").\n1947 marked the publication of his first book, \"Night's Black Agents\", a short story collection containing seven stories grouped as 'Modern Horrors', one as a 'Transition', and two grouped as 'Ancient Adventures': \"The Sunken Land\" and \"Adept's Gambit\", which are both stories of Fafhrd and the Gray Mouser.\nBook publication of the science fiction novel \"Gather, Darkness\" followed in 1950. It deals with a futuristic world that follows the Second Atomic Age which is ruled by scientists, until in the throes of a new Dark Age, the witches revolt.\nIn 1951, Leiber was Guest of Honor at the World Science Fiction Convention in New Orleans. Further novels followed during the 1950s, and in 1958 \"The Big Time\" won the Hugo Award for Best Novel.\nLeiber published further books in the 1960s. His novel \"The Wanderer\" (1964) also received the Hugo for Best Novel. In the novel, an artificial planet, quickly nicknamed the Wanderer, materializes from hyperspace within earth's orbit. The Wanderer's gravitational field captures the moon and shatters it into something like one of Saturn's rings. On Earth, the Wanderer's gravity well triggers massive earthquakes, tsunamis, and tidal phenomena. The multi-threaded plot follows the exploits of a large ensemble cast as they struggle to survive the global disaster.\nLeiber received the Hugo Award for Best Novella in 1970 and 1971 for \"Ship of Shadows\" (1969) and \"Ill Met in Lankhmar\" (1970). \"Gonna Roll the Bones\" (1967), his contribution to Harlan Ellison's \"Dangerous Visions\" anthology, received the Hugo Award for Best Novelette and the Nebula Award for Best Novelette in 1968.\n\"Our Lady of Darkness\" (1977)\u2014originally serialized in short form in \"The Magazine of Fantasy &amp; Science Fiction\" under the title \"The Pale Brown Thing\" (1977)\u2014featured cities as the breeding grounds for new types of elementals called paramentals, summonable by the dark art of megapolisomancy, with such activities centering on the Transamerica Pyramid. Its main characters include Franz Westen, Jaime Donaldus Byers, and the magician Thibault de Castries. \"Our Lady of Darkness\" won the World Fantasy Award\u2014Novel.\nLeiber also did the 1966 novelization of the Clair Huffaker screenplay of \"Tarzan and the Valley of Gold\".\nMany of Leiber's most-acclaimed works are short stories, especially in the horror genre. Owing to such stories as \"The Smoke Ghost\", \"The Girl With the Hungry Eyes\" and \"You're All Alone\" (later expanded as \"The Sinful Ones\"), he is widely regarded as one of the forerunners of the modern urban horror story. Leiber also challenged the conventions of science fiction through reflexive narratives such as \"A Bad Day For Sales\" (first published in \"Galaxy Science Fiction\", July 1953), in which the protagonist, Robie, \"America\u2019s only genuine mobile salesrobot\", references the title character of Isaac Asimov's idealistic robot story, \"Robbie\". Questioning Isaac Asimov's Three Laws of Robotics, Leiber imagines the futility of automatons in a post-apocalyptic New York City. In his later years, Leiber returned to short story horror in such works as \"Horrible Imaginings\", \"Black Has Its Charms\" and the award-winning \"The Button Moulder\".\nThe short parallel worlds story \"Catch That Zeppelin!\" (1975) received the Hugo Award for Best Short Story and the Nebula Award for Best Short Story in 1976. This story shows a plausible alternate reality that is much better than our own, whereas the typical parallel universe story depicts a world that is much worse than our own. \"Belsen Express\" (1975) won the World Fantasy Award\u2014Short Fiction. Both stories reflect Leiber's uneasy fascination with Nazism, an uneasiness compounded by his mixed feelings about his German ancestry and his philosophical pacifism during World War II.\nLeiber was named the second Gandalf Grand Master of Fantasy by participants in the 1975 World Science Fiction Convention (Worldcon), after the posthumous inaugural award to J. R. R. Tolkien. Next year he won the World Fantasy Award for Life Achievement. He was Guest of Honor at the 1979 Worldcon in Brighton, England (1979). The Science Fiction Writers of America made him its fifth SFWA Grand Master in 1981; the Horror Writers Association made him an inaugural winner of the Bram Stoker Award for Lifetime Achievement in 1988 (named in 1987); and the Science Fiction and Fantasy Hall of Fame inducted him in 2001, its sixth class of two deceased and two living writers.\nLeiber was a founding member of the Swordsmen and Sorcerers' Guild of America (SAGA), a loose-knit group of Heroic fantasy authors founded in the 1960s, led by Lin Carter, with entry by fantasy credentials alone. Some works by SAGA members were published in Lin Carter's \"Flashing Swords!\" anthologies. Leiber himself is credited with inventing the term sword and sorcery for the particular subgenre of epic fantasy exemplified by his Fafhrd and Grey Mouser stories.\nIn an appreciation in the July 1969 \"Special Fritz Leiber Issue\" of \"The Magazine of Fantasy &amp; Science Fiction\", Judith Merril writes of Leiber's connection with his readers: \"That this kind of \"personal\" response...is shared by thousands of other readers, has been made clear on several occasions.\" The November 1959 issue of \"Fantastic\", for instance: Leiber had just come out of one of his recurrent dry spells, and editor Cele Lalli bought up all his new material until there was enough [five stories] to fill an issue; the magazine came out with a big black headline across its cover \u2014 \"Leiber Is Back!\"\nFafhrd and the Gray Mouser.\nHis legacy appears to have been consolidated by the most famous of his creations, the \"Fafhrd and the Gray Mouser\" stories, written over a span of 50 years. The first of them, \"Two Sought Adventure\", appeared in \"Unknown\", August 1939. They are concerned with an unlikely pair of heroes found in and around the city of Lankhmar. Fafhrd was based on Leiber himself and the Mouser on his friend Harry Otto Fischer, and the two characters were created in a series of letters exchanged by the two in the mid-1930s. These stories were among the progenitors of many of the tropes of the sword and sorcery genre. They are also notable among sword and sorcery stories in that, over the course of the stories, his two heroes mature, take on more responsibilities, and eventually settle down into marriage.\nSome Fafhrd and Mouser stories were recognized by annual genre awards: \"Scylla's Daughter\" (1961) was \"Short Story\" Hugo finalist and \"Ill Met in Lankhmar\" (1970) won the \"Best Novella\" Hugo and Nebula Awards. Leiber's last major work, \"The Knight and Knave of Swords\" (1991), brought the series to a close while leaving room for possible sequels. In the last year of his life, Leiber was considering allowing the series to be continued by other writers, but his sudden death made this more difficult. One new Fafhrd and the Mouser novel, \"Swords Against the Shadowland\", by Robin Wayne Bailey, did appear in 1998.\nThe stories were influential in shaping the genre and were influential on other works. Joanna Russ' stories about thief-assassin Alyx (collected in 1976 in \"The Adventures of Alyx\") were in part inspired by Fafhrd and the Gray Mouser, and Alyx in fact made guest appearances in two of Leiber's stories. Numerous writers have paid homage to the stories. For instance, Terry Pratchett's city of Ankh-Morpork bears something more than a passing resemblance to Lankhmar (acknowledged by Pratchett by the placing of the swordsman-thief \"The Weasel\" and his giant barbarian comrade \"Bravd\" in the opening scenes of the first Discworld novel). More recently, playing off the visit of Fafhrd and the Grey Mouser to our world in \"Adept's Gambit\" (set in second century B.C. Tyre), Steven Saylor's short story \"Ill Seen in Tyre\" takes his Roma Sub Rosa series hero Gordianus to the city of Tyre a hundred years later, where the two visitors from Nehwon are remembered as local legends.\nFischer and Leiber contributed to the original game design of the wargame \"Lankhmar\"\u2014published in 1976 by TSR.\nSelected works.\nScreen adaptations.\n\"Conjure Wife\" has been made into feature films four times under other titles:\n\"The Girl with the Hungry Eyes\" was filmed under that title by Kastenbaum Films in 1995. (This film is not to be confused with the 1967 William Rotsler film \"The Girl with the Hungry Eyes\" which is entirely unrelated to Leiber's story).\nTwo Leiber stories were filmed for TV for Rod Serling's \"Night Gallery\". These were \"The Girl with the Hungry Eyes\" (1970) (adapted by Robert M. Young and directed by John Badham); and \"The Dead Man\" (adapted and directed by Douglas Heyes)."}
{"id": "10878", "revid": "12374079", "url": "https://en.wikipedia.org/wiki?curid=10878", "title": "Flanders", "text": "Flanders (, ; Dutch: \"Vlaanderen\" ; French: \"Flandre\" ; German: \"Flandern\" ) is the Dutch-speaking northern portion of Belgium and one of the communities, regions and language areas of Belgium. However, there are several overlapping definitions, including ones related to culture, language, politics and history, and sometimes involving neighbouring countries. The demonym associated with Flanders is Fleming, while the corresponding adjective is Flemish. The official capital of Flanders is the City of Brussels, although the Brussels-Capital Region has an independent regional government, and the government of Flanders only oversees the community aspects of Flanders life in Brussels such as Flemish culture and education.\nFlanders, despite not being the biggest part of Belgium by area, is the area with the largest population (68.2%) if Brussels is included. 7,797,611 out of 11,431,406 Belgian inhabitants live in Flanders or the bilingual Brussels region. Only about 8% of Brussels inhabitants identify as Flemish, while the rest identify as French-speaking. Not including Brussels, there are five present-day Flemish provinces.\nIn medieval contexts, the original \"County of Flanders\" stretched around AD 900 from the Strait of Dover to the Scheldt estuary and expanded from there. This county also still corresponds roughly with the modern-day Belgian provinces of West Flanders and East Flanders, along with neighbouring parts of France and the Netherlands. Although this original meaning is still relevant, in modern times the term \"Flanders\" came to refer to a larger area, and is used to refer to the entire Dutch-speaking part of Belgium, stretching all the way to the Meuse, as well as cultural movements such as Flemish art. In accordance with late 20th century Belgian state reforms the Belgian part of this area was made into two political entities: the \"Flemish Community\" () and the \"Flemish Region\" (). These entities were merged, although geographically the Flemish Community, which has a broader cultural mandate, covers Brussels, whereas the Flemish Region does not.\nThe area of today's Flanders, by every definition, has figured prominently in European history since the Middle Ages. In this period, cities such as Ghent and Bruges of the historic County of Flanders, and later Antwerp of the Duchy of Brabant made it one of the richest and most urbanised parts of Europe, trading, and weaving the wool of neighbouring lands into cloth for both domestic use and export. As a consequence, a very sophisticated culture developed, with impressive achievements in the arts and architecture, rivaling those of northern Italy. Belgium was one of the centres of the 19th-century industrial revolution but Flanders was at first overtaken by French-speaking Wallonia. In the second half of the 20th century, and due to massive national investments in port infrastructure, Flanders' economy modernised rapidly, and today Flanders and Brussels are much wealthier than Wallonia and are among the wealthiest regions in Europe and the world.\nGeographically, Flanders is mainly flat, and has a small section of coast on the North Sea. Much of Flanders is agriculturally fertile and densely populated, with a density of . It touches the French department of Nord to the southwest near the coast, and borders the Dutch provinces of Zeeland, North Brabant and Limburg to the north and east, and the Walloon provinces of Hainaut, Walloon Brabant and Li\u00e8ge to the south. The Brussels Capital Region is an officially bilingual enclave within the Flemish Region. Flanders has exclaves of its own: Voeren in the east is between Wallonia and the Netherlands and Baarle-Hertog in the north consists of 22 exclaves surrounded by the Netherlands.\nTerminology.\nIn Belgium.\nThe term \"Flanders\" has several main modern meanings:\nDutch-speaking part of Belgium.\nThe significance of the County of Flanders and its counts eroded through time, but the designation survived with a broader cultural meaning which could be applied also to neighbouring areas. In the Early modern period, the term Flanders was associated with the southern part of the Low Countries: the Southern Netherlands. During the 19th and 20th centuries, it became increasingly common to refer to the whole Dutch-speaking part of Belgium as \"Flanders\". The linguistic limit between French and Dutch was recorded in the early '60s, from Kortrijk to Maastricht. Now, Flanders extends over the northern part of Belgium, including Belgian Limburg (corresponding closely to the medieval County of Loon), and the Dutch-speaking Belgian parts of the medieval Duchy of Brabant.\nThe ambiguity between this wider cultural area and that of the County or Province still remains in discussions about the region. In most present-day contexts, however, the term Flanders is taken to refer to either the political, social, cultural, and linguistic community (and the corresponding official institution, the Flemish Community), or the geographical area, one of the three institutional regions in Belgium, namely the Flemish Region.\nIn the history of art and other fields, the adjectives Flemish and Netherlandish are commonly used to designate all the artistic production in this area before about 1580, after which it refers specifically to the southern Netherlands. For example, the term \"Flemish Primitives\", now outdated in English but used in French, Dutch and other languages, is a synonym for \"Early Netherlandish painting\", and it is not uncommon to see Mosan art categorized as Flemish art. In music the \"Franco-Flemish School\" is also known as the \"Dutch School\".\nWithin this Dutch-speaking part of Belgium, French has never ceased to be spoken by some citizens and Jewish groups have been speaking Yiddish in Antwerp for centuries. Today, Flanders' minority residents include 170 nationalities\u2014the largest groups speaking French, English, Berber, Turkish, Arabic, Spanish, Italian and Polish.\nHistory.\nEarly history.\nWhen Julius Caesar conquered the area he described it as the less economically developed and more warlike part of \"Gallia Belgica\". His informants told him that especially in the east, the tribes claimed ancestral connections and kinship with the \"Germanic\" peoples then east of the Rhine. Under the Roman empire the whole of \"Gallia Belgica\" became an administrative province. The future counties of Flanders and Brabant remained part of this province connected to what is now France, but in the east modern Limburg became part of the Rhine frontier province of \"Germania Inferior\" connected to what is now the Netherlands and Germany. \"Gallia Belgica\" and \"Germania Inferior\" were the two most northerly continental provinces of the Roman empire.\nIn the future county of Flanders, the main Belgic tribe in early Roman times was the Menapii, but also on the coast were the Marsacii and Morini. In the central part of modern Belgium were the Nervii and in the east were the Tungri. The Tungri especially were understood to have links to Germanic tribes east of the Rhine. Another notable Germanic group were the Toxandrians who appear to have lived in the Kempen region, in the northern parts of both the Nervian and Tungrian provinces, probably stretching into the modern Netherlands. The Roman administrative districts (\"civitates\") of the Menapii, Nervii and Tungri therefore corresponded roughly with the medieval counties of Flanders, Brabant and Loon, and the modern Flemish provinces of East and West Flanders (Menapii), Brabant and Antwerp (the northern Nervii), and Belgian Limburg (Tungri). Brabant appears to have been home to relatively unpopulated forest area, the Silva Carbonaria, forming a natural boundary between northeast and southwest Belgium.\nLinguistically, the tribes in this area were under Celtic influence in the south, and Germanic influence in the east, but there is disagreement about what languages were spoken locally (apart from Vulgar Latin), and there may even have been an intermediate \"Nordwestblock\" language related to both. By the first century BC Germanic languages had become prevalent.\nAs Roman influence waned, Frankish populations settled east of the Silva Carbonaria, and eventually pushed through it under Chlodio. They had kings in each city (\"civitas\"). In the meantime, the Franks contributed to the Roman military. The first Merovingian king Childeric I was king of the Franks in the military, who became leader of the administration of \"Belgica Secunda\", which included the \"civitas\" of the Menapii (the future county of Flanders). From there, his son Clovis I managed to conquer both the Roman populations of northern France and the Frankish populations beyond the forest areas.\nHistorical Flanders.\nThe County of Flanders was a feudal fief in West Francia. The first certain Count in the comital family, Baldwin I of Flanders, is first reported in a document of 862 when he eloped with a daughter of his king Charles the Bald. The region developed as a medieval economic power with a large degree of political autonomy. While its trading cities remained strong, it was weakened and divided when districts fell under direct French royal rule in the late 12th century. The remaining parts of Flanders came under the rule of the counts of neighbouring imperial Hainaut under Baldwin V of Hainaut in 1191.\nDuring the late Middle Ages Flanders' trading towns (notably Ghent, Bruges and Ypres) made it one of the richest and most urbanized parts of Europe, weaving the wool of neighbouring lands into cloth for both domestic use and export. As a consequence, a sophisticated culture developed, with impressive art and architecture, rivaling those of northern Italy. Ghent, Bruges, Ypres and the Franc of Bruges formed the Four Members, a form of parliament that exercised considerable power in Flanders.\nIncreasingly powerful from the 12th century, the territory's autonomous urban communes were instrumental in defeating a French attempt at annexation (1300\u20131302), finally defeating the French in the Battle of the Golden Spurs (11 July 1302), near Kortrijk. Two years later, the uprising was defeated and Flanders indirectly remained part of the French Crown. Flemish prosperity waned in the following century, due to widespread European population decline following the Black Death of 1348, the disruption of trade during the Anglo-French Hundred Years' War (1337\u20131453), and increased English cloth production. Flemish weavers had gone over to Worstead and North Walsham in Norfolk in the 12th century and established the woolen industry.\nThe County of Flanders started to take control of the neighbouring County of Brabant during the life of Louis II, Count of Flanders (1330-1384), who fought his sister-in-law Joanna, Duchess of Brabant for control of it.\nThe entire area, straddling the ancient boundary of France and the Holy Roman Empire, later passed to Philip the Bold in 1384, the Duke of Burgundy, with his capital in Brussels. The titles were eventually more clearly united under his grandson Philip the Good (1396 \u2013 1467). This large Duchy passed in to the Habsburg dynasty, and in 1556 to the kings of Spain. Western and southern districts of Flanders were confirmed under French rule under successive treaties of 1659 (Artois), 1668, and 1678.\nThe County of Loon, approximately the modern Flemish province of Limburg, remained independent of France, forming a part of the Prince-Bishopric of Li\u00e8ge until the French Revolution, but surrounded by the Burgundians, and under their influence.\nLow Countries.\nBeeldenstorm.\nIn 1500, Charles V was born in Ghent. He inherited the Seventeen Provinces (1506), Spain (1516) with its colonies and in 1519 was elected Holy Roman Emperor. Charles V issued the Pragmatic Sanction of 1549, which established the Low Countries as the Seventeen Provinces (or Spanish Netherlands in its broad sense) as an entity separate from the Holy Roman Empire and from France. In 1556 Charles V abdicated due to ill health (he suffered from crippling gout). Spain and the Seventeen Provinces went to his son, Philip II of Spain.\nOver the first half of the 16th century Antwerp grew to become the second-largest European city north of the Alps by 1560. Antwerp was the richest city in Europe at this time. According to Luc-Normand Tellier \"It is estimated that the port of Antwerp was earning the Spanish crown seven times more revenues than the Americas.\"\nMeanwhile, Protestantism had reached the Low Countries. Among the wealthy traders of Antwerp, the Lutheran beliefs of the German Hanseatic traders found appeal, perhaps partly for economic reasons. The spread of Protestantism in this city was aided by the presence of an Augustinian cloister (founded 1514) in the St. Andries quarter. Luther, an Augustinian himself, had taught some of the monks, and his works were in print by 1518. The first Lutheran martyrs came from Antwerp. The Reformation resulted in consecutive but overlapping waves of reform: a Lutheran, followed by a militant Anabaptist, then a Mennonite, and finally a Calvinistic movement. These movements existed independently of each other.\nPhilip II, a devout Catholic and self-proclaimed protector of the Counter-Reformation, suppressed Calvinism in Flanders, Brabant and Holland (what is now approximately Belgian Limburg was part of the Prince-Bishopric of Li\u00e8ge and was Catholic \"de facto\"). In 1566, the wave of iconoclasm known as the \"Beeldenstorm\" was a prelude to religious war between Catholics and Protestants, especially the Anabaptists. The \"Beeldenstorm\" started in what is now French Flanders, with open-air sermons () that spread through the Low Countries, first to Antwerp and Ghent, and from there further east and north.\nThe Eighty Years' War and its consequences.\nSubsequently, Philip II of Spain sent the Duke of Alba to the Provinces to repress the revolt. Alba recaptured the southern part of the Provinces, who signed the Union of Atrecht, which meant that they would accept the Spanish government on condition of more freedom. But the northern part of the provinces signed the Union of Utrecht and settled in 1581 the Republic of the Seven United Netherlands. Spanish troops quickly started fighting the rebels, and the Spanish armies conquered the important trading cities of Bruges and Ghent. Antwerp, which was then the most important port in the world, also had to be conquered. But before the revolt was defeated, a war between Spain and England broke out, forcing Spanish troops to halt their advance. On 17 August 1585, Antwerp fell. This ended the Eighty Years' War for the (from now on) Southern Netherlands. The United Provinces (the Northern Netherlands) fought on until 1648 \u2013 the Peace of Westphalia.\nDuring the war with England, the rebels from the north, strengthened by refugees from the south, started a campaign to reclaim areas lost to Philip II's Spanish troops. They conquered a considerable part of Brabant (the later Noord-Brabant of the Netherlands), and the south bank of the Scheldt estuary (Zeelandic Flanders), before being stopped by Spanish troops. The front at the end of this war stabilized and became the border between present-day Belgium and the Netherlands. The Dutch (as they later became known) had managed to reclaim enough of Spanish-controlled Flanders to close off the river Scheldt, effectively cutting Antwerp off from its trade routes.\nThe fall of Antwerp to the Spanish and the closing of the Scheldt caused considerable emigration. Many Calvinist merchants of Antwerp and other Flemish cities left Flanders and migrated north. Many of them settled in Amsterdam, which was a smaller port, important only in the Baltic trade. The Flemish exiles helped to rapidly transform Amsterdam into one of the world's most important ports. This is why the exodus is sometimes described as \"creating a new Antwerp\"\".\nFlanders and Brabant, went into a period of relative decline from the time of the Thirty Years War. In the Northern Netherlands, the mass emigration from Flanders and Brabant became an important driving force behind the Dutch Golden Age.\nSouthern Netherlands (1581\u20131795).\nAlthough arts remained relatively impressive for another century with Peter Paul Rubens (1577\u20131640) and Anthony van Dyck, Flanders lost its former economic and intellectual power under Spanish, Austrian, and French rule. Heavy taxation and rigid imperial political control compounded the effects of industrial stagnation and Spanish-Dutch and Franco-Austrian conflict. The Southern Netherlands suffered severely under the War of the Spanish Succession. But under the reign of Empress Maria-Theresia, these lands again flourished economically. Influenced by the Enlightenment, the Austrian Emperor Joseph II was the first sovereign who had been in the Southern Netherlands since King Philip II of Spain left them in 1559.\nFrench Revolution and Napoleonic France (1795\u20131815).\nIn 1794, the French Republican Army started using Antwerp as the northernmost naval port of France. The following year, France officially annexed Flanders as the \"d\u00e9partements\" of Lys, Escaut, Deux-N\u00e8thes, Meuse-Inf\u00e9rieure and Dyle. Obligatory (French) army service for all men aged 16\u201325 years was a main reason for the uprising against the French in 1798, known as the \"Boerenkrijg\" (\"Peasants' War\"), with the heaviest fighting in the Campine area.\nUnited Kingdom of the Netherlands (1815\u20131830).\nAfter the defeat of Napoleon Bonaparte at the 1815 Battle of Waterloo in Brabant, the Congress of Vienna (1815) gave sovereignty over the Austrian Netherlands \u2013 Belgium minus the East Cantons and Luxembourg \u2013 to the United Netherlands (Dutch: \"Verenigde Nederlanden\") under Prince William I of Orange Nassau, making him William I of the United Kingdom of the Netherlands. William I started rapid industrialisation of the southern parts of the Kingdom. But the political system failed to forge a true union between the north and south. Most of the southern bourgeoisie was Roman Catholic and French-speaking, while the north was mainly Protestant and Dutch-speaking.\nIn 1815, the Dutch Senate was reinstated (Dutch: \"Eerste Kamer der Staaten Generaal\"). The nobility, mainly coming from the south, became more and more estranged from their northern colleagues. Resentment grew between the Roman Catholics from the south and the Protestants from the north, and also between the powerful liberal bourgeoisie from the south and their more moderate colleagues from the north. On 25 August 1830 (after the showing of the opera 'La Muette de Portici' of Daniel Auber in Brussels) the Belgian Revolution sparked. On 4 October 1830, the Provisional Government (Dutch: \"Voorlopig Bewind\") proclaimed its independence, which was later confirmed by the National Congress that issued a new Liberal Constitution and declared the new state a Constitutional Monarchy, under the House of Saxe-Coburg. Flanders now became part of the Kingdom of Belgium, which was recognized by the major European Powers on 20 January 1831. The cessation was recognized by the United Kingdom of the Netherlands on 19 April 1839.\nKingdom of Belgium.\nIn 1830, the Belgian Revolution led to the splitting up of the two countries. Belgium was confirmed as an independent state by the Treaty of London of 1839, but deprived of the eastern half of Limburg (now Dutch Limburg), and the Eastern half of Luxembourg (now the Grand-Duchy of Luxembourg). Sovereignty over Zeelandic Flanders, south of the Westerscheldt river delta, was left with the Kingdom of the Netherlands, which was allowed to levy a toll on all traffic to Antwerp harbour until 1863.\nRise of the Flemish Movement.\nThe Belgian Revolution was not supported in Flanders and even on 4 October 1830, when the Belgian independence was declared, Flemish authorities refused to take orders from the new Belgian government in Brussels. But a large French military force led by the Count de Pont\u00e9coulant helped to subdue Flanders one month later, and Flanders become a true part of Belgium.\nThe French-speaking bourgeoisie showed little respect for the Dutch-speaking part of the population. French became the only official language in Belgium and all secondary and higher education in Dutch was abolished. \nIn 1834, anyone suspected of being \"Flemish minded\" or calling for the reunification of the Netherlands were prosecuted and their houses looted and burnt. Flanders, until then a very prosperous region in Europe, was not considered worthwhile for investment and scholarship. A study in 1918 showed that in the first 88 years of its existence, 80% of the Belgian GNP was invested in Wallonia. This led to a widespread poverty in Flanders, forcing roughly 300,000 Flemish to emigrate to Wallonia to work in the heavy industry there.\nThese events led to a silent uprising in Flanders against French-speaking domination. It was not until 1878 that Dutch was allowed to be used for official purposes in Flanders (see language legislation in Belgium), although French remained the only official language in Belgium.\nIn 1873, Dutch became the official language in public secondary schools. In 1898, Dutch and French were declared equal languages in laws and Royal orders. In 1930 the first Flemish university was opened.\nThe first official translation of the Belgian constitution in Dutch was not published until 1967.\nWorld War I and its consequences.\nFlanders (and Belgium as a whole) saw some of the greatest loss of life on the Western Front of the First World War, in particular from the three battles of Ypres.\nThe war strengthened Flemish identity and consciousness. The occupying German authorities took several Flemish-friendly measures. More importantly, French-speaking officers on the front catalysed Flemish desire for emancipation. The officers often gave orders in French only, followed by \"et pour les Flamands, la m\u00eame chose!\", meaning \"and for the Flemish, the same thing!\", which was not understood by the Flemish conscripts, who were mostly uneducated. The resulting suffering is remembered by Flemish organizations during the yearly Yser pilgrimage in Diksmuide at the monument of the Yser Tower.\nRight-Wing Nationalism in the interbellum and World War II.\nDuring the interbellum and World War II, several right-wing fascist and/or national-socialistic parties emerged in Belgium. Since these parties were promised more rights for the Flemings by the German government during World War II, many of them collaborated with the Nazi regime. After the war, collaborators (or people who were \"Zwart\", \"Black\" during the war) were prosecuted and punished, among them many Flemish Nationalists whose main political goal had been the emancipation of Flanders. As a result, until today Flemish Nationalism is often associated with right-wing and sometimes fascist ideologies.\nFlemish autonomy.\nAfter World War II, the differences between Dutch-speaking and French-speaking Belgians became clear in a number of conflicts, such as the Royal Question, the question whether King Leopold III should return (which most Flemings supported but Walloons did not) and the use of Dutch in the Catholic University of Leuven. As a result, several state reforms took place in the second half of the 20th century, which transformed the unitary Belgium into a federal state with communities, regions and language areas. This resulted also in the establishment of a Flemish Parliament and Government. During the 1970s, all major political parties split into a Dutch and French-speaking party.\nSeveral Flemish parties still advocate for more Flemish autonomy, some even for Flemish independence (see Partition of Belgium), whereas the French-speakers would like to keep the current state as it is. Recent governments (such as Verhofstadt I Government) have transferred certain federal competences to the regional governments.\nOn 13 December 2006, a spoof news broadcast by the Belgian Francophone public broadcasting station RTBF announced that Flanders had decided to declare independence from Belgium.\nThe 2007 federal elections showed more support for Flemish autonomy, marking the start of the 2007\u20132011 Belgian political crisis. All the political parties that advocated a significant increase of Flemish autonomy gained votes as well as seats in the Belgian federal parliament. This was especially the case for Christian Democratic and Flemish and New Flemish Alliance (N-VA) (who had participated on a shared electoral list). The trend continued during the 2009 regional elections, where CD&amp;V and N-VA were the clear winners in Flanders, and N-VA became even the largest party in Flanders and Belgium during the 2010 federal elections, followed by the longest-ever government formation after which the Di Rupo I Government was formed excluding N-VA. Eight parties agreed on a sixth state reform which aim to solve the disputes between Flemings and French-speakers. However, the 2012 provincial and municipal elections continued the trend of N-VA becoming the biggest party in Flanders.\nHowever, sociological studies show no parallel between the rise of nationalist parties and popular support for their agenda. Instead, a recent study revealed a majority in favour of returning regional competences to the federal level.\nGovernment and politics.\nBoth the Flemish Community and the Flemish Region are constitutional institutions of the Kingdom of Belgium, exercising certain powers within their jurisdiction, granted following a series of state reforms. In practice, the Flemish Community and Region together form a single body, with its own parliament and government, as the Community legally absorbed the competences of the Region. The parliament is a directly elected legislative body composed of 124 representatives. The government consists of up to 11 members and is presided by a Minister-President, currently Geert Bourgeois (New Flemish Alliance) leading a coalition of his party (N-VA) with Christen-Democratisch en Vlaams (CD&amp;V) and Open Vlaamse Liberalen en Democraten (Open VLD).\nThe area of the Flemish Community is represented on the maps above, including the area of the Brussels-Capital Region (hatched on the relevant map). Roughly, the Flemish Community exercises competences originally oriented towards the individuals of the Community's language: culture (including audiovisual media), education, and the use of the language. Extensions to personal matters less directly associated with language comprise sports, health policy (curative and preventive medicine), and assistance to individuals (protection of youth, social welfare, aid to families, immigrant assistance services, etc.)\nThe area of the Flemish Region is represented on the maps above. It has a population of more than 6 million (excluding the Dutch-speaking community in the Brussels Region, grey on the map for it is not a part of the Flemish Region). Roughly, the Flemish Region is responsible for territorial issues in a broad sense, including economy, employment, agriculture, water policy, housing, public works, energy, transport, the environment, town and country planning, nature conservation, credit, and foreign trade. It supervises the provinces, municipalities, and intercommunal utility companies.\nThe number of Dutch-speaking Flemish people in the Capital Region is estimated to be between 11% and 15% (official figures do not exist as there is no language census and no official subnationality). According to a survey conducted by the University of Louvain (UCLouvain) in Louvain-la-Neuve and published in June 2006, 51% of respondents from Brussels claimed to be bilingual, even if they do not have Dutch as their first language. They are governed by the Brussels Region for economics affairs and by the Flemish Community for educational and cultural issues.\nAs mentioned above, Flemish institutions such as the Flemish Parliament and Government, represent the Flemish Community and the Flemish Region. The region and the community thus \"de facto\" share the same parliament and the same government. All these institutions are based in Brussels. Nevertheless, both types of subdivisions (the Community and the Region) still exist legally and the distinction between both is important for the people living in Brussels. Members of the Flemish Parliament who were elected in the Brussels Region cannot vote on affairs belonging to the competences of the Flemish Region.\nThe official language for all Flemish institutions is Dutch. French enjoys a limited official recognition in a dozen municipalities along the borders with French-speaking Wallonia, and a large recognition in the bilingual Brussels Region. French is widely known in Flanders, with 59% claiming to know French according to a survey conducted by UCLouvain in Louvain-la-Neuve and published in June 2006.\nPolitics.\nHistorically, the political parties reflected the pillarisation (\"verzuiling\") in Flemish society. The traditional political parties of the three pillars are Christian-Democratic and Flemish (CD&amp;V), the Open Flemish Liberals and Democrats (Open Vld) and the Socialist Party \u2013 Differently (sp.a).\nHowever, during the last half century, many new political parties were founded in Flanders. One of the first was the nationalist People's Union, of which the right nationalist Flemish Block (now Flemish Interest) split off, and which later dissolved into the now-defunct Spirit or Social Liberal Party, moderate nationalism rather left of the spectrum, on the one hand, and the New Flemish Alliance (N-VA), more conservative but independentist, on the other hand. Other parties are the leftist alternative/ecological Green party; the short-lived anarchistic libertarian spark ROSSEM and more recently the conservative-right liberal List Dedecker, founded by Jean-Marie Dedecker, and the socialist Workers' Party.\nParticularly the Flemish Block/Flemish Interest has seen electoral success roughly around the turn of the century, and the New Flemish Alliance during the last few elections, even becoming the largest party in the 2010 federal elections.\nFlemish independence.\nFor some inhabitants, Flanders is more than just a geographical area or the federal institutions (Flemish Community and Region). Supporters of the Flemish Movement even call it a nation and pursue Flemish independence, but most people (approximately 75%) living in Flanders say they are proud to be Belgian and opposed to the dissolution of Belgium. 20% is even \"very proud\", while some 25% are not proud and 8% is \"very not proud\". Mostly students claim to be proud of their nationality, with 90% of them saying so. Of the people older than 55, 31% claim to be proud of being a Belgian. Particular opposition to secession comes from women, people employed in services, the highest social classes and people from big families. Strongest of all opposing the notion are housekeepers\u2014both housewives and house husbands.\nIn 2012, the Flemish government drafted a \"Charter for Flanders\" (\"Handvest voor Vlaanderen\") of which the first article says \"Vlaanderen is een deelstaat van de federale Staat Belgi\u00eb en maakt deel uit van de Europese Unie.\" (\"Flanders is a component state of the federal State of Belgium and is part of the European Union\"). Although interpreted by many Flemish nationalists as a statement, this phrase is merely a quotation from the Belgian constitution and has no further legal value whatsoever.\nGeography.\nFlanders shares its borders with Wallonia in the south, Brussels being an enclave within the Flemish Region. The rest of the border is shared with the Netherlands (Zeelandic Flanders in Zeeland, North Brabant and Limburg) in the north and east, and with France (French Flanders in Hauts-de-France) and the North Sea in the west. Voeren is an exclave of Flanders between Wallonia and the Netherlands, while Baarle-Hertog in Flanders forms a complicated series of enclaves and exclaves with Baarle-Nassau in the Netherlands. Germany, although bordering Wallonia and close to Voeren in Limburg, does not share a border with Flanders. The German-speaking Community of Belgium, also close to Voeren, does not border Flanders either. (The commune of Plombi\u00e8res, majority French speaking, lies between them.)\nFlanders is a highly urbanised area, lying completely within the Blue Banana. Antwerp, Ghent, Bruges and Leuven are the largest cities of the Flemish Region. Antwerp has a population of more than 500,000 citizens and is the largest city, Ghent has a population of 250,000 citizens, followed by Bruges with 120,000 citizens and Leuven counts almost 100,000 citizens.\nBrussels is a part of Flanders as far as community matters are concerned, but does not belong to the Flemish Region.\nFlanders has two main geographical regions: the coastal Yser basin plain in the north-west and a central plain. The first consists mainly of sand dunes and clayey alluvial soils in the polders. Polders are areas of land, close to or below sea level that have been reclaimed from the sea, from which they are protected by dikes or, a little further inland, by fields that have been drained with canals. With similar soils along the lowermost Scheldt basin starts the central plain, a smooth, slowly rising fertile area irrigated by many waterways that reaches an average height of about five metres (16.4\u00a0ft) above sea level with wide valleys of its rivers upstream as well as the Campine region to the east having sandy soils at altitudes around thirty metres. Near its southern edges close to Wallonia one can find slightly rougher land richer of calcium with low hills reaching up to and small valleys, and at the eastern border with the Netherlands, in the Meuse basin, there are marl caves (\"mergelgrotten\"). Its exclave around Voeren between the Dutch border and Wallonia's Li\u00e8ge Province attains a maximum altitude of above sea level.\nAdministrative divisions.\nThe present-day Flemish Region covers and is divided into five provinces, 22 arrondissements and 308 cities or municipalities.\nThe province of Flemish Brabant is the most recent one, being formed in 1995 after the splitting of the province of Brabant.\nMost municipalities are made up of several former municipalities, now called \"deelgemeenten\". The largest municipality (both in terms of population and area) is Antwerp, having more than half a million inhabitants. Its nine \"deelgemeenten\" have a special status and are called districts, which have an elected council and a college. While any municipality with more than 100,000 inhabitants can establish districts, only Antwerp did this so far. The smallest municipality (also both in terms of population and area) is Herstappe (Limburg).\nThe Flemish Community covers both the Flemish Region and, together with the French Community, the Brussels-Capital Region. Brussels, an enclave within the province of Flemish Brabant, is not divided into any province nor is it part of any. It coincides with the Arrondissement of Brussels-Capital and includes 19 municipalities.\nThe Flemish Government has its own local institutions in the Brussels-Capital Region, being the \"Vlaamse Gemeenschapscommissie\" (VGC), and its municipal antennae (\"Gemeenschapscentra\", community centres for the Flemish community in Brussels). These institutions are independent from the educational, cultural and social institutions that depend directly on the Flemish Government. They exert, among others, all those cultural competences that outside Brussels fall under the provinces.\nClimate.\nThe climate is maritime temperate, with significant precipitation in all seasons (K\u00f6ppen climate classification: \"Cfb\"; the average temperature is in January, and in July; the average precipitation is 65\u00a0millimetres (2.6\u00a0in) in January, and 78\u00a0millimetres (3.1\u00a0in) in July).\nEconomy.\nTotal GDP of the Flemish Region in 2018 was \u20ac270 billion (Eurostat figures). Per capita GDP at purchasing power parity was 20% above the EU average. Flemish productivity per capita is about 13% higher than that in Wallonia, and wages are about 7% higher than in Wallonia.\nFlanders was one of the first continental European areas to undergo the Industrial Revolution, in the 19th century. Initially, the modernization relied heavily on food processing and textile. However, by the 1840s the textile industry of Flanders was in severe crisis and there was famine in Flanders (1846\u201350). After World War II, Antwerp and Ghent experienced a fast expansion of the chemical and petroleum industries. Flanders also attracted a large majority of foreign investments in Belgium. The 1973 and 1979 oil crises sent the economy into a recession. The steel industry remained in relatively good shape. In the 1980s and 90s, the economic centre of Belgium continued to shift further to Flanders and is now concentrated in the populous Flemish Diamond area. Nowadays, the Flemish economy is mainly service-oriented.\nBelgium is a founding member of the European Coal and Steel Community in 1951, which evolved into the present-day European Union. In 1999, the euro, the single European currency, was introduced in Flanders. It replaced the Belgian franc in 2002.\nThe Flemish economy is strongly export-oriented, in particular of high value-added goods. The main imports are food products, machinery, rough diamonds, petroleum and petroleum products, chemicals, clothing and accessories, and textiles. The main exports are automobiles, food and food products, iron and steel, finished diamonds, textiles, plastics, petroleum products, and non-ferrous metals. Since 1922, Belgium and Luxembourg have been a single trade market within a customs and currency union\u2014the Belgium\u2013Luxembourg Economic Union. Its main trading partners are Germany, the Netherlands, France, the United Kingdom, Italy, the United States, and Spain.\nAntwerp is the number one diamond market in the world, diamond exports account for roughly 1/10 of Belgian exports. The Antwerp-based BASF plant is the largest BASF-base outside Germany, and accounts on its own for about 2% of Belgian exports. Other industrial and service activities in Antwerp include car manufacturing, telecommunications, photographic products.\nFlanders is home to several science and technology institutes, such as IMEC, VITO, Flanders DC and Flanders Make.\nInfrastructure.\nFlanders has developed an extensive transportation infrastructure of ports, canals, railways and highways. The Port of Antwerp is the second-largest in Europe, after Rotterdam. Other ports are Bruges-Zeebrugge, Ghent and Ostend, of which Zeebrugge and Ostend are located at the .\nWhereas railways are managed by the federal National Railway Company of Belgium, other public transport (De Lijn) and roads are managed by the Flemish region.\nThe main airport is Brussels Airport, the only other civilian airport with scheduled services in Flanders is Antwerp International Airport, but there are two other ones with cargo or charter flights: Ostend-Bruges International Airport and Kortrijk-Wevelgem International Airport, both in West Flanders.\nDemographics.\nThe highest population density is found in the area circumscribed by the Brussels-Antwerp-Ghent-Leuven agglomerations that surround Mechelen and is known as the Flemish Diamond, in other important urban centres as Bruges, Roeselare and Kortrijk to the west, and notable centres Turnhout and Hasselt to the east. On 1 January 2015, the Flemish Region had a population of 6,444,127 and about 15% of the 1,175,173 people in the Brussels Region are also considered Flemish.\nReligion.\nThe Belgian constitution provides for freedom of religion, and the various governments in general respect this right in practice. Since independence, Catholicism, counterbalanced by strong freethought movements, has had an important role in Belgium's politics, since the 20th century in Flanders mainly via the Christian trade union ACV and the Christian Democratic and Flemish party (CD&amp;V). According to the \"2001 Survey and Study of Religion\", about 47 percent of the Belgian population identify themselves as belonging to the Catholic Church, while Islam is the second-largest religion at 3.5 percent. A 2006 inquiry in Flanders, considered more religious than Wallonia, showed that 55% considered themselves religious, and 36% believed that God created the world.\nJews have been present in Flanders for a long time, in particular in Antwerp. More recently, Muslims have immigrated to Flanders, now forming the largest minority religion with about 3.9% in the Flemish Region and 25% in Brussels. The largest Muslim group is Moroccan in origin, while the second largest is Turkish in origin.\nEducation.\nEducation is compulsory from the ages of six to 18, but most Flemings continue to study until around 23. Among the Organisation for Economic Co-operation and Development countries in 1999, Flanders had the third-highest proportion of 18- to 21-year-olds enrolled in postsecondary education. Flanders also scores very high in international comparative studies on education. Its secondary school students consistently rank among the top three for mathematics and science. However, the success is not evenly spread: ethnic minority youth score consistently lower, and the difference is larger than in most comparable countries.\nMirroring the historical political conflicts between the secular and Catholic segments of the population, the Flemish educational system is split into a secular branch controlled by the communities, the provinces, or the municipalities, and a subsidised religious\u2014mostly Catholic\u2014branch. For the subsidised schools, the main costs such as the teacher's wages and building maintenance completely borne by the Flemish government. Subsidised schools are also free to determine their own teaching and examination methods, but in exchange, they must be able to prove that certain minimal terms are achieved by keeping records of the given lessons and exams. It should however be noted that\u2014at least for the Catholic schools\u2014the religious authorities have very limited power over these schools, neither do the schools have a lot of power on their own. Instead, the Catholic schools are a member of the Catholic umbrella organisation . The VSKO determines most practicalities for schools, like the advised schedules per study field. However, there's freedom of education in Flanders, which doesn't only mean that every pupil can choose his/her preferred school, but also that every organisation can found a school, and even be subsidised when abiding the different rules. This resulted also in some smaller school systems follow 'methodical pedagogies' (e.g. Steiner, Montessori, or Freinet) or serve the Jewish and Protestant minorities.\nDuring the school year 2003\u20132004, 68.30% of the total population of children between the ages of six and 18 went to subsidized private schools (both religious schools or 'methodical pedagogies' schools).\nThe big freedom given to schools results in a constant competition to be the \"best\" school. The schools get certain reputations amongst parents and employers. So it's important for schools to be the best school since the subsidies depend on the number of pupils. This competition has been pinpointed as one of the main reasons for the high overall quality of the Flemish education. However, the importance of a school's reputation also makes schools more eager to expel pupils that don't perform well. Resulting in the ethnic differences and the well-known waterfall system: pupils start high in the perceived hierarchy, and then drop towards more professional oriented directions or \"easier\" schools when they can't handle the pressure any longer.\nHealthcare.\nHealthcare is a federal matter, but the Flemish Government is responsible for care, health education and preventive care.\nThe 10 largest groups of foreign residents in 2018 are :\nCulture.\nAt first sight, \"Flemish culture\" is defined by the Dutch language and its gourmandic mentality, as compared to the more Calvinistic Dutch culture. Dutch and Flemish paintings enjoyed more equal international admiration.\nLanguage and literature.\nThe standard language in Flanders is Dutch; spelling and grammar are regulated by a single authority, the Dutch Language Union (\"Nederlandse Taalunie\"), comprising a committee of ministers of the Flemish and Dutch governments, their advisory council of appointed experts, a controlling commission of 22 parliamentarians, and a secretariate. The term Flemish can be applied to the Dutch spoken in Flanders; it shows many regional and local variations.\nThe biggest difference between Belgian Dutch and Dutch used in the Netherlands is in the pronunciation of words. The Dutch spoken in the north of the Netherlands is typically described as being \"sharper\", while Belgian Dutch is \"softer\". In Belgian Dutch, there are also fewer vowels pronounced as diphthongs. When it comes to spelling, Belgian Dutch language purists historically avoided writing words using a French spelling, or searched for specific translations of words derived from French, while the Dutch prefer to stick with French spelling, as it differentiates Dutch more from the neighbouring German. For example, the Dutch word \"punaise\" (English: \"Drawing pin\") is derived directly from the French language. Belgian Dutch language purists have lobbied to accept the word \"duimspijker\" (literally: \"thumb spike\") as official Dutch, though the Dutch Language Union never accepted it as standard Dutch. Other proposals by purists were sometimes accepted, and sometimes reverted again in later spelling revisions. As language purists were quite often professionally involved in language (e.g. as a teacher), these unofficial purist translations are found more often in Belgian Dutch texts.\nThe earliest example of literature in non-standardized dialects in the current area of Flanders is Hendrik van Veldeke's \"Eneas Romance\", the first courtly romance in a Germanic language (12th century). With a writer of Hendrik Conscience's stature, Flemish literature rose ahead of French literature in Belgium's early history. Guido Gezelle not only explicitly referred to his writings as Flemish but used it in many of his poems, and strongly defended it:\nOriginal \n&lt;poem&gt;\n\"Gij zegt dat 't vlaamsch te niet zal gaan:\"\n\"'t en zal!\"\n\"dat 't waalsch gezwets zal boven slaan:\"\n\"'t en zal!\"\n\"Dat hopen, dat begeren wij:\"\n\"dat zeggen en dat zweren wij:\"\n\"zoo lange als wij ons weren, wij:\"\n\"'t en zal, 't en zal,\"\n\"'t en zal!\"\n&lt;/poem&gt;\nTranslation . For explanations, continue along\n\"It shan't!\"\n\"This we hope, for this we hanker:\"\n\"this we say and this we vow:\"\n\"as long as we fight back, we:\"\n\"It shan't, It shan't,\"\n\"It shan't!\"\n&lt;/poem&gt;\nThe distinction between Dutch and Flemish literature, often perceived politically, is also made on intrinsic grounds by some experts such as Kris Humbeeck, professor of Literature at the University of Antwerp. Nevertheless, most Dutch-language literature read (and appreciated to varying degrees) in Flanders is the same as that in the Netherlands.\nInfluential Flemish writers include Ernest Claes, Stijn Streuvels and Felix Timmermans. Their novels mostly describe rural life in Flanders in the 19th century and at beginning of the 20th. Widely read by the older generations, they are considered somewhat old-fashioned by present-day critics. Some famous Flemish writers of the early 20th century wrote in French, including Nobel Prize winners (1911) Maurice Maeterlinck and Emile Verhaeren. They were followed by a younger generation, including Paul van Ostaijen and Gaston Burssens, who \"activated\" the Flemish Movement. Still widely read and translated into other languages (including English) are the novels of authors such as Willem Elsschot, Louis Paul Boon and Hugo Claus. The recent crop of writers includes the novelists Tom Lanoye and Herman Brusselmans, and poets such as the married couple Herman de Coninck and Kristien Hemmerechts.\nLanguages.\nAt the creation of the Belgian state, French was the only official language. Historically Flanders was a Dutch-speaking region. For a long period, French was used as a second language and, like elsewhere in Europe, commonly spoken among the aristocracy. There is still a French-speaking minority in Flanders, especially in the municipalities with language facilities, along the language border and the Brussels periphery (Vlaamse Rand), though many of them are French-speakers that migrated to Flanders in recent decades.\nIn French Flanders, French is the only official language and now the native language of the majority of the population, but there is still a minority of Dutch-speakers living there. French is also the primary language in the officially bilingual Brussels Capital Region, (see Francization of Brussels).\nMany Flemings are also able to speak French, children in Flanders generally get their first French lessons in the 5th primary year (normally around 10 years). But the current lack of French outside the educational context makes it hard to maintain a decent level of French. As such, the proficiency of French is declining. Flemish pupils are also obligated to follow English lessons as their third language. Normally from the second secondary year (around 14 years old), but the ubiquity of English in movies, music, IT and even advertisements makes it easier to learn and maintain the English language.\nMedia.\nThe public radio and television broadcaster in Flanders is VRT, which operates the TV channels \u00e9\u00e9n, Canvas, Ketnet, OP12 and (together with the Netherlands) BVN. Flemish provinces each have up to two TV channels as well. Commercial television broadcasters include vtm and Vier (VT4). Popular TV series are for example \"Thuis\" and \"F.C. De Kampioenen\".\nThe five most successful Flemish films were \"Loft\" (2008; 1,186,071 visitors), \"Koko Flanel\" (1990; 1,082,000 tickets sold), \"Hector\" (1987; 933,000 tickets sold), \"Daens\" (1993; 848,000 tickets sold) and \"De Zaak Alzheimer\" (2003; 750,000 tickets sold). The first and last ones were directed by Erik Van Looy, and an American remake is being made of both of them, respectively \"The Loft\" (2012) and \"The Memory of a Killer\". The other three ones were directed by Stijn Coninx.\nNewspapers are grouped under three main publishers: De Persgroep with \"Het Laatste Nieuws\", the most popular newspaper in Flanders, \"De Morgen\" and \"De Tijd\". Then Corelio with \"\", the oldest extant Flemish newspaper, \"Het Nieuwsblad\" and \"De Standaard\". Lastly, Concentra publishes \"Gazet van Antwerpen\" and \"Het Belang van Limburg\".\nMagazines include \"Knack\" and \"HUMO\".\nSports.\nAssociation football (soccer) is one of the most popular sports in both parts of Belgium, together with cycling, tennis, swimming and judo.\nIn cycling, the Tour of Flanders is considered one of the five \"Monuments\". Other \"Flanders Classics\" races include \"Dwars door Vlaanderen\" and Gent\u2013Wevelgem. Eddy Merckx is widely regarded as the greatest cyclist of all time, with five victories in the Tour de France and numerous other cycling records. His hour speed record (set in 1972) stood for 12 years.\nJean-Marie Pfaff, a former Belgian goalkeeper, is considered one of the greatest in the history of football (soccer).\nKim Clijsters (as well as the French-speaking Belgian Justine Henin) was Player of the Year twice in the Women's Tennis Association as she was ranked the number one female tennis player.\nKim Gevaert and Tia Hellebaut are notable track and field stars from Flanders.\nThe 1920 Summer Olympics were held in Antwerp. Jacques Rogge has been president of the International Olympic Committee since 2001.\nThe Flemish government agency for sports is Bloso.\nMusic.\nFlanders is known for its music festivals, like the annual Rock Werchter, Tomorrowland and Pukkelpop. The Gentse Feesten is another very large yearly event.\nThe best-selling Flemish group or artist is the (Flemish-Dutch) group 2 Unlimited, followed by (Italian-born) Rocco Granata, Technotronic, Helmut Lotti and Vaya Con Dios.\nThe weekly charts of best-selling singles is the Ultratop 50. \"Kvraagetaan\" by the Fixkes holds the current record for longest time at No. 1 on the chart."}
{"id": "10879", "revid": "998606936", "url": "https://en.wikipedia.org/wiki?curid=10879", "title": "Freud (disambiguation)", "text": "Sigmund Freud (1856\u20131939) was the inventor of psychoanalysis, psychosexual stages, and the personality theory of Ego, Superego, and Id.\nFreud may also refer to:\nPeople with the surname.\nThe Freud family:\nOther people:"}
{"id": "10880", "revid": "21386896", "url": "https://en.wikipedia.org/wiki?curid=10880", "title": "Plurality voting", "text": "Plurality voting is an electoral system in which each voter is allowed to vote for only one candidate, and the candidate who polls more than any other counterpart (a plurality) is elected. In a system based on single-member districts, it may be called first-past-the-post (FPTP), single-choice voting, simple plurality or relative/simple majority. In a system based on multi-member districts, it may be referred to as winner-takes-all or bloc voting.\nThe system is still used to elect members of a legislative assembly or executive officers in only a handful of countries in the world. It is used in most elections in the United States, the lower house (Lok Sabha) in India, elections to the British House of Commons and English local elections in the United Kingdom, France (run-off election) and federal and provincial elections in Canada.\nPlurality voting is distinguished from a majoritarian electoral system in which a winning candidate must receive an absolute majority of votes: more votes than all other candidates combined. Under plurality voting, the leading candidate, whether he or she has majority of votes, is elected.\nBoth plurality and majoritarian systems may use single-member or multi-member constituencies. In the latter case, it may be referred to as an exhaustive counting system, and one member is elected at a time and the process repeated until the number of vacancies is filled.\nIn some, including France and some of the United States including Louisiana and Georgia, a \"two-ballot\" or \"runoff-election\" plurality system is used, which may require two rounds of voting. If, during the first round, no candidate receives over 50% of the votes, a second round takes place with only the top two candidates in the first round. That ensures that the winner gains a majority of votes in the second round.\nAlternatively, all candidates above a certain threshold in the first round may compete in the second round. If there are more than two candidates standing, a plurality vote may decide the result.\nIn political science, the use of plurality voting with multiple, single-winner constituencies to elect a multi-member body is often referred to as single-member district plurality or SMDP. The combination is also variously referred to as \"winner-take-all\" to contrast it with proportional representation systems.\nThe term \"winner-take-all\" is sometimes also used to refer to elections for multiple winners in a particular constituency using bloc voting, or MMDP. This system at the state-level is used for election of most of the electoral college in US presidential elections.\nVoting.\nPlurality voting is used for local and/or national elections in 43 of the 193 countries that are members of the United Nations. It is particularly prevalent in the United Kingdom, the United States, Canada and India.\nIn single-winner plurality voting, each voter is allowed to vote for only one candidate, and the winner of the election is the candidate who represents a plurality of voters or, in other words, received the largest number of votes. That makes plurality voting among the simplest of all electoral systems for voters and vote counting officials. (However, the drawing of district boundary lines can be very contentious in the plurality system.)\nIn an election for a legislative body with single-member seats, each voter in a geographically-defined electoral district may vote for one candidate from a list of the candidates who are competing to represent that district. Under the plurality system, the winner of the election then becomes the representative of the entire electoral district and serves with representatives of other electoral districts.\nIn an election for a single seat, such as for president in a presidential system, the same style of ballot is used, and the winner is whichever candidate receives the largest number of votes.\nIn the two-round system, usually the top two candidates in the first ballot progress to the second round, also called the runoff.\nIn a multiple-member plurality election with \"n\" seats available, the winners are the \"n\" candidates with the highest numbers of votes. The rules may allow the voter to vote for one candidate, up to \"n\" candidates, or some other number.\nBallot types.\nGenerally, plurality ballots can be categorized into two forms. The simplest form is a blank ballot in which the name of a candidate(s) is written in by hand. A more structured ballot will list all the candidates and allow a mark to be made next to the name of a single candidate (or more than one, in some cases); however, a structured ballot can also include space for a write-in candidate.\nExamples of plurality voting.\nGeneral elections in the United Kingdom.\nThe United Kingdom, like the United States and Canada, uses single-member districts as the base for national elections. Each electoral district (constituency) chooses one member of parliament, the candidate who gets the most votes, whether or not they get at least 50% of the votes cast (\"first past the post\"). In 1992, for example, a Liberal Democrat in Scotland won a seat with just 26% of the votes. The system of single-member districts with plurality winners tends to produce two large political parties. In countries with proportional representation there is not such a great incentive to vote for a large party, which contributes to multi-party systems.\nScotland, Wales and Northern Ireland use the first-past-the-post system for UK general elections but versions of proportional representation for elections to their own assemblies and parliaments. All of the UK has used a form of proportional representation for European Parliament elections.\nThe countries that inherited the British majoritarian system tend toward two large parties: one left and the other right, such as the U.S. Democrats and Republicans. Canada is an exception, with three major political parties consisting of the New Democratic Party, which is to the left; the Conservative Party, which is to the right; and the Liberal Party, which is slightly off-centre but to the left. A fourth party that no longer has major party status is the separatist Bloc Qu\u00e9b\u00e9cois party, which is territorial and runs only in Quebec. New Zealand once used the British system, which yielded two large parties as well. It also left many New Zealanders unhappy because other viewpoints were ignored, which made the New Zealand Parliament in 1993 adopted a new electoral law modelled on Germany's system of proportional representation (PR) with a partial selection by constituencies. New Zealand soon developed a more complex party system.\nAfter the 2015 Elections in the United Kingdom, there were calls from UKIP for a change to proportional representation after it had received 3,881,129 votes but only 1 MP. The Green Party was similarly underrepresented, which contrasted greatly with the SNP, a Scottish separatist party that received only 1,454,436 votes but won 56 seats because of more-concentrated support.\nExample.\nThis is a general example, using population percentages taken from one U.S. state for illustrative purposes.\nIf each voter in each city naively selects one city on the ballot (Memphis voters select Memphis, Nashville voters select Nashville, and so on), Memphis will be selected, as it has the most votes (42%). Note that the system does not require that the winner have a majority, only a plurality. Memphis wins because it has the most votes even though 58% of the voters in the example preferred Memphis least. That problem does not arise with the two-round system in which Nashville would have won. (In practice, with FPTP, many voters in Chattanooga and Knoxville are likely to vote tactically for Nashville: see below.)\nDisadvantages.\nTactical voting.\nTo a much greater extent than many other electoral methods, plurality electoral systems encourage tactical voting techniques like \"compromising\". Voters are under pressure to vote for one of the two candidates most likely to win even if their true preference is neither of them because a vote for any other candidate is unlikely to lead to the preferred candidate being elected, but it will instead reduce support for one of the two major candidates whom the voter might prefer to the other. The minority party will then simply take votes away from one of the major parties, which could change the outcome and gain nothing for the voters. Any other party will typically need to build up its votes and credibility over a series of elections before it is seen as electable.\nIn the Tennessee example, if all the voters for Chattanooga and Knoxville had instead voted for Nashville, Nashville would have won (with 58% of the vote). That would have only been the third choice for those voters, but voting for their respective first choices (their own cities) actually results in their fourth choice (Memphis) being elected.\nThe difficulty is sometimes summed up in an extreme form, as \"All votes for anyone other than the second place are votes for the winner\". That is because by voting for other candidates, voters have denied those votes to the second-place candidate, who could have won had they received them. It is often claimed by United States Democrats that Democrat Al Gore lost the 2000 Presidential Election to Republican George W. Bush because some voters on the left voted for Ralph Nader of the Green Party, who, exit polls indicated, would have preferred Gore at 45% to Bush at 27%, with the rest not voting in Nader's absence.\nThat thinking is illustrated by elections in Puerto Rico and its three principal voter groups: the Independentistas (pro-independence), the Populares (pro-commonwealth), and the Estadistas (pro-statehood). Historically, there has been a tendency for Independentista voters to elect Popular candidates and policies. The phenomenon is responsible for some Popular victories even though the Estadistas have the most voters on the island. It is so widely recognised that the Puerto Ricans sometimes call the Independentistas who vote for the Populares \"melons\" in reference to the party colours because the fruit is green on the outside but red on the inside.\nBecause voters have to predict who the top two candidates will be, that can cause significant perturbation to the system:\nProponents of other single-winner electoral systems argue that their proposals would reduce the need for tactical voting and reduce the spoiler effect. Examples include the commonly used two-round system of runoffs and instant-runoff voting, along with less-tested systems such as approval voting, score voting and Condorcet methods.\nFewer political parties.\nDuverger's law is a theory that constituencies that use first-past-the-post systems will have a two-party system after enough time.\nPlurality voting tends to reduce the number of political parties to a greater extent than most other methods do, making it more likely that a single party will hold a majority of legislative seats. (In the United Kingdom, 21 out of 24 general elections since 1922 have produced a single-party majority government.)\nPlurality voting's tendency toward fewer parties and more-frequent majorities of one party can also produce government that may not consider as wide a range of perspectives and concerns. It is entirely possible that a voter finds all major parties to have similar views on issues and that a voter does not have a meaningful way of expressing a dissenting opinion through their vote.\nAs fewer choices are offered to voters, voters may vote for a candidate although they disagree with them because they disagree even more with their opponents. That will make candidates less closely reflect the viewpoints of those who vote for them.\nFurthermore, one-party rule is more likely to lead to radical changes in government policy even though the changes are favoured only by a plurality or a bare majority of the voters, but a multi-party system usually requires more consensus to make dramatic changes in policy.\nWasted votes.\nWasted votes are those cast for candidates who are virtually sure to lose in a safe seat, and votes cast for winning candidates in excess of the number required for victory. For example, in the UK general election of 2005, 52% of votes were cast for losing candidates and 18% were excess votes, a total of 70% wasted votes. That is perhaps the most fundamental criticism of FPTP since a large majority of votes may play no part in determining the outcome. Alternative electoral systems attempt to ensure that almost all of the votes are effective in influencing the result, which minimises vote wastage.\nGerrymandering.\nBecause FPTP permits a high level of wasted votes, an election under FPTP is easily gerrymandered unless safeguards are in place. In gerrymandering, a party in power deliberately manipulates constituency boundaries to increase the number of seats that it wins unfairly.\nIn brief, if a governing party G wishes to reduce the seats that will be won by opposition party O in the next election, it can create a number of constituencies in each of which O has an overwhelming majority of votes. O will win these seats, but many of its voters will waste their votes. Then, the rest of the constituencies are designed to have small majorities for G. Few G votes are wasted, and G will win many seats by small margins. As a result of the gerrymander, O's seats have cost it more votes than G's seats.\nManipulation charges.\nThe presence of spoilers often gives rise to suspicions that manipulation of the slate has taken place. The spoiler may have received incentives to run. A spoiler may also drop out at the last moment, which induces charges that such an act was intended from the beginning.\nSpoiler effect.\nThe spoiler effect is the effect of vote splitting between candidates or ballot questions with similar ideologies. One spoiler candidate's presence in the election draws votes from a major candidate with similar politics, which causes a strong opponent of both or several to win. Smaller parties can disproportionately change the outcome of an FPTP election by swinging what is called the 50-50% balance of two party systems by creating a faction within one or both ends of the political spectrum, which shifts the winner of the election from an absolute majority outcome to a plurality outcome, which favours the party that had been less favoured. In comparison, electoral systems that use proportional representation have small groups win only their proportional share of representation.\nIssues specific to particular countries.\nSolomon Islands.\nIn August 2008, Sir Peter Kenilorea commented on what he perceived as the flaws of a first-past-the-post electoral system in the Solomon Islands:\nInternational examples.\nThe United Kingdom continues to use the first-past-the-post electoral system for general elections, and for local government elections in England and Wales. Changes to the UK system have been proposed, and alternatives were examined by the Jenkins Commission in the late 1990s. After the formation of a new coalition government in 2010, it was announced as part of the coalition agreement that a referendum would be held on switching to the alternative vote system. However the alternative vote system was rejected 2-1 by British voters in a referendum held on 5 May 2011.\nCanada also uses FPTP for national and provincial elections. In May 2005 the Canadian province of British Columbia had a referendum on abolishing single-member district plurality in favour of multi-member districts with the Single Transferable Vote system after the Citizens' Assembly on Electoral Reform made a recommendation for the reform. The referendum obtained 57% of the vote, but failed to meet the 60% requirement for passing. A second referendum was held in May 2009, this time the province's voters defeated the change with 39% voting in favour.\nAn October 2007 referendum in the Canadian province of Ontario on adopting a Mixed Member Proportional system, also requiring 60% approval, failed with only 36.9% voting in favour. British Columbia again called a referendum on the issue in 2018 which was defeated by 62% voting to keep current system.\nNorthern Ireland, Scotland, Wales, the Republic of Ireland, Australia and New Zealand are notable examples of countries within the UK, or with previous links to it, that use non-FPTP electoral systems (Northern Ireland, Scotland and Wales use FPTP in United Kingdom general elections, however).\nNations which have undergone democratic reforms since 1990 but have not adopted the FPTP system include South Africa, almost all of the former Eastern bloc nations, Russia, and Afghanistan.\nList of countries.\nCountries that use plurality voting to elect the lower or only house of their legislature include:\nReferences.\nThe fatal flaws of Plurality (first-past-the-post) electoral systems - Proportional Representation Society of Australia"}
{"id": "10881", "revid": "410906", "url": "https://en.wikipedia.org/wiki?curid=10881", "title": "Fetish", "text": "Fetish may refer to:"}
{"id": "10882", "revid": "1011160788", "url": "https://en.wikipedia.org/wiki?curid=10882", "title": "February 14", "text": ""}
{"id": "10883", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=10883", "title": "Free-trade area", "text": "A free-trade area is the region encompassing a trade bloc whose member countries have signed a free trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers, import quotas and tariffs, and to increase trade of goods and services with each other. If natural persons are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. It can be considered the second stage of economic integration.\nIt is important to note the difference between customs unions and free-trade areas. Both types of trading blocs are related to internal arrangements which parties conclude in order to liberalize and facilitate trade among themselves. The crucial difference between customs unions and free-trade areas is their approach to third parties. While a customs union requires all parties to establish and maintain identical external tariffs with regard to trade with non-parties, parties to a free-trade area are not subject to such requirement. Instead, they may establish and maintain whatever tariff regime applying to imports from non-parties as deemed necessary. In a free-trade area without harmonized external tariffs, to eliminate the risk of trade deflection, parties will adopt a system of preferential rules of origin.\nRegarding the term \"free-trade area\", it is originally meant by the General Agreement on Tariffs and Trade (GATT 1994) to include only trade in goods. An agreement with a similar purpose, i.e., to enhance liberalization of trade in services, is named under Article V of the General Agreement on Trade in Service (GATS) as an \"economic integration agreement\". However, in practice, the term is now widely used to refer to agreements covering not only goods but also services and even investment.\nLegal aspects of free-trade areas.\nThe formation of free-trade areas is considered an exception to the most favored nation (MFN) principle in the World Trade Organization (WTO) because the preferences that parties to a free-trade area exclusively grant each other go beyond their accession commitments. Although Article XXIV of the GATT allows WTO members to establish free-trade areas or to adopt interim agreements necessary for the establishment thereof, there are several conditions with respect to free-trade areas, or interim agreements leading to the formation of free-trade areas.\nFirstly, duties and other regulations maintained in each of the signatory parties to a free-trade area, which are applicable at the time such free-trade area is formed, to the trade with non-parties to such free-trade area shall not be higher or more restrictive than the corresponding duties and other regulations existing in the same signatory parties prior to the formation of the free-trade area. In other words, the establishment of a free-trade area to grant preferential treatment among its member is legitimate under WTO law, but the parties to a free-trade area are not permitted to treat non-parties less favorably than before the area is established. A second requirement stipulated by Article XXIV is that tariffs and other barriers to trade must be eliminated to substantially all the trade within the free-trade area.\nFree trade agreements forming free-trade areas generally lie outside the realm of the multilateral trading system. However, WTO members must notify to the Secretariat when they conclude new free trade agreements and in principle the texts of free trade agreements are subject to review under the Committee on Regional Trade Agreements. Although a dispute arising within free-trade areas are not subject to litigation at the WTO's Dispute Settlement Body, \"there is no guarantee that WTO panels will abide by them and decline to exercise jurisdiction in a given case\".\nEconomic aspects of free-trade areas.\nTrade diversion and trade creation\nIn general, \"trade diversion\" means that a free-trade area would divert trade away from more efficient suppliers outside the area towards less efficient ones within the areas. Whereas, \"trade creation\" implies that a free-trade area creates trade which may not have otherwise existed. In all cases trade creation will raise a country's national welfare.\nBoth trade creation and trade diversion are crucial effects found upon the establishment of a free-trade area. Trade creation will cause consumption to shift from a high-cost producer to a low-cost one, and trade will thus expand. In contrast, trade diversion will lead to trade shifting from a lower-cost producer outside the area to a higher-cost one inside the area. Such a shift will not benefit consumers within the free-trade area as they are deprived the opportunity to purchase cheaper imported goods. However, economists find that trade diversion does not always harm aggregate national welfare: it can even improve aggregate national welfare if the volume of diverted trade is small.\nFree-trade areas as public goods\nEconomist have made attempts to evaluate the extent to which free-trade areas can be considered public goods. They firstly address one key element of free-trade areas, which is the system of embedded tribunals which act as arbitrators in international trade disputes. This system as a force of clarification for existing statutes and international economic policies as affirmed in the trade treaties.\nThe second way in which free-trade areas are considered public goods is tied to the evolving trend of them becoming \u201cdeeper\u201d. The depth of a free-trade area refers to the added types of structural policies that it covers. While older trade deals are deemed \u201cshallower\u201d as they cover fewer areas (such as tariffs and quotas), more recently concluded agreements address a number of other fields, from services to e-commerce and data localization. Since transactions among parties to a free-trade area are relatively cheaper as compared to those with non-parties, free-trade areas are conventionally found to be excludable. Now that deep trade deals will enhance regulatory harmonization and increase trade flows with non-parties, thus reduce the excludability of FTA benefits, new generation free-trade areas are obtaining essential characteristics of public goods.\nQualifying for preferences under a free-trade area.\nUnlike a customs union, parties to a free-trade area do not maintain common external tariffs, which means they apply different customs duties, as well as other policies with respect to non-members. This feature creates the possibility of non-parties may free-riding preferences under a free-trade area by penetrating the market with the lowest external tariffs. Such risk necessitates the introduction of rules to determine originating goods eligible for preferences under a free-trade area, a need that does not arise upon the formation of a customs union. Basically, there is a requirement for a minimum extent of processing that results in \"substantial transformation\" to the goods so that they can be considered originating. By defining which goods are originating in the PTA, preferential rules of origin distinguish between originating and non-originating goods: only the former will be entitled to preferential tariffs scheduled by the free-trade area, the latter must pay MFN import duties.\nIt is noted that in qualifying for origin criteria, there is a differential treatment between inputs originating within and outside a free-trade area. Normally inputs originating in one FTA party will be considered as originating in the other party if they are incorporated in the manufacturing process in that other party. Sometimes, production costs arising in one party is also considered as that arising in another party. In preferential rules of origin, such differential treatment is normally provided for in the cumulation or accumulation provision. Such clause further explains the trade creation and trade diversion effects of a free-trade area mentioned above, because a party to a free-trade area has the incentive to use inputs originating in another party so that their products may qualify for originating status.\nDatabases on free-trade areas.\nSince there are hundreds of free-trade areas currently in force and being negotiated (about 800 according to ITC's Rules of Origin Facilitator, counting also non-reciprocal trade arrangements), it is important for businesses and policy-makers to keep track of their status. There are a number of depositories of free trade agreements available either at national, regional or international levels. Some significant ones include the database on Latin American free trade agreements constructed by the Latin American Integration Association (ALADI), the database maintained by the Asian Regional Integration Center (ARIC) providing information agreements of Asian countries, and the portal on the European Union's free trade negotiations and agreements.\nAt the international level, there are two important free-access databases developed by international organizations for policy-makers and businesses:\nWTO's Regional Trade Agreements Information System\nAs WTO members are obliged to notify to the Secretariat their free trade agreements, this database is constructed based on the most official source of information on free trade agreements (referred to as regional trade agreements in the WTO language). The database allows users to seek information on trade agreements notified to the WTO by country or by topic (goods, services or goods and services). This database provides users with an updated list of all agreements in force, however, those not notified to the WTO may be missing. It also displays reports, tables and graphs containing statistics on these agreements, and particularly preferential tariff analysis.\nITC's Market Access Map\nThe Market Access Map was developed by the International Trade Centre (ITC) with the objectives to facilitate businesses, governments and researchers in market access issues. The database, visible via the online tool Market Access Map, includes information on tariff and non-tariff barriers in all active trade agreements, not limited to those officially notified to the WTO. It also documents data on non-preferential trade agreements (for instance, Generalized System of Preferences schemes). Up until 2019, Market Access Map has provided downloadable links to texts agreements and their rules of origin. The new version of Market Access Map forthcoming this year will provide direct web links to relevant agreement pages and connect itself to other ITC's tools, particularly the Rules of Origin Facilitator. It is expected to become a versatile tool which assists enterprises in understanding free trade agreements and qualifying for origin requirements under these agreements."}
{"id": "10885", "revid": "49920", "url": "https://en.wikipedia.org/wiki?curid=10885", "title": "French fries", "text": "French fries, or simply fries (North American English), chips (British and Commonwealth English, Hiberno-English), finger chips (Indian English), or French-fried potatoes, are \"batonnet\" or \"allumette\"-cut deep-fried potatoes.\nFrench fries are served hot, either soft or crispy, and are generally eaten as part of lunch or dinner or by themselves as a snack, and they commonly appear on the menus of diners, fast food restaurants, pubs, and bars. They are usually salted and, depending on the country, may be served with ketchup, vinegar, mayonnaise, tomato sauce, or other local specialties. Fries can be topped more heavily, as in the dishes of poutine or chili cheese fries. Chips can be made from sweet potatoes instead of potatoes. A baked variant, oven chips, uses less oil or no oil.\nPreparation.\nFrench fries are prepared by first cutting the potato (peeled or unpeeled) into even strips, which are then wiped off or soaked in cold water to remove the surface starch, and thoroughly dried. They may then be fried in one or two stages. Chefs generally agree that the \"two-bath\" technique produces better results. Potatoes fresh out of the ground can have too high a water content\u2014resulting in soggy fries\u2014so preference is for those that have been stored for a while.\nIn the two-stage or two-bath method, the first bath, sometimes called blanching, is in hot fat (around 160\u00a0\u00b0C/320\u00a0\u00b0F) to cook them through. This step can be done in advance. Then they are more briefly fried in very hot fat (190\u00a0\u00b0C/375\u00a0\u00b0F) to crisp the exterior. They are then placed in a colander or on a cloth to drain, salted, and served. The exact times of the two baths depend on the size of the potatoes. For example, for 2\u20133\u00a0mm strips, the first bath takes about 3 minutes, and the second bath takes only seconds. One can cook french fries using several techniques. Deep frying submerges food in hot fat, most commonly oil. Vacuum fryers are suitable to process low-quality potatoes with higher sugar levels than normal, as they frequently have to be processed in spring and early summer before the potatoes from the new harvest become available. In the UK, a chip pan is a deep-sided cooking pan used for deep-frying. Chip pans are named for their traditional use in frying chips.\nMost french fries are produced from frozen potatoes which have been blanched or at least air-dried industrially. Most chains that sell fresh cut fries use the Idaho Russet Burbank variety of potatoes. It has been the standard for french fries in the United States. The usual fat for making french fries is vegetable oil. In the past, beef suet was recommended as superior, with vegetable shortening as an alternative. In fact, McDonald's used a mixture of 93% beef tallow and 7% cottonseed oil until 1990, when they switched to vegetable oil with beef flavoring. Starting in the 1960s, more fast food restaurants have been using frozen french fries.\nChemical and physical changes.\nFrench fries are fried in a two step process: the first time is to cook the starch throughout the entire cut at a low heat, and the second time is to create the golden crispy exterior of the fry at a higher temperature. This is necessary because if the potato cuts are only fried once, the temperature would either be too hot, causing only the exterior to be cooked and not the inside, or not hot enough where the entire fry is cooked, but its crispy exterior will not develop. Although the potato cuts may be baked or steamed as a preparation method, this section will only focus on french fries made using frying oil. During the initial frying process (approximately 150\u00a0\u00b0C), water on the surface of the cuts evaporates off the surface and the water inside the cuts gets absorbed by the starch granules, causing them to swell and produce the fluffy interior of the fry. The starch granules are able to retain the water and expand due to gelatinization. The water and heat break the glycosidic linkages between amylopectin and amylose strands, allowing a new gel matrix to form via hydrogen bonds which aid in water retention. The moisture that gets trapped in between the gel matrix is responsible for the fluffy interior of the fry. The gelatinized starch molecules move towards the surface of the fries \"forming a thick layer of gelatinised starch\" and this layer of pre-gelatinized starch will turn into the crispy exterior after the potato cuts are fried for a second time. During the second frying process (approximately 180\u00a0\u00b0C), the remaining water on the surface of the cuts will evaporate and the gelatinized starch molecules that collected towards the potato surface are cooked again, forming the crispy exterior. The golden-brown color of the fry will develop when the amino acids and glucose on the exterior participate in a Maillard browning reaction.\nEtymology.\nIn the United States and most of Canada, the term \"french fries\", sometimes capitalized as \"French fries\", or shortened to \"fries\", refers to all dishes of fried elongated pieces of potatoes. Variations in shape and size may have names such as \"curly fries\", \"shoestring fries\", etc. (see ). In the United Kingdom, Australia, South Africa, Ireland and New Zealand, the term \"chips\" is generally used instead, though thinly cut fried potatoes are usually called \"french fries\", \"skinny fries\", or \"pommes frites\" (from French), to distinguish them from \"chips\", which are cut thicker. A person from the US or Canada might instead refer to these more thickly-cut \"chips\" as \"steak fries\", depending on the shape, as the word \"chips\" is more often used to refer to \"potato chips\", known in the UK and Ireland as \"crisps\".\nThomas Jefferson had \"potatoes served in the French manner\" at a White House dinner in 1802. The expression \"french fried potatoes\" first occurred in print in English in the 1856 work \"Cookery for Maids of All Work\" by E. Warren: \"French Fried Potatoes. \u2013 Cut new potatoes in thin slices, put them in boiling fat, and a little salt; fry both sides of a light golden brown colour; drain.\" This account referred to thin, shallow-fried slices of potato (French cut) \u2013 it is not clear where or when the now familiar deep-fried batons or fingers of potato were first prepared. In the early 20th century, the term \"french fried\" was being used in the sense of \"deep-fried\" for foods like onion rings or chicken.\nBy country or region.\nLatin America.\nFries are a common side dish in Latin American cuisine or part of larger preparations like the salchipapas in Peru or chorrillana in Chile.\nBelgium and the Netherlands.\nThe French and Belgians have an ongoing dispute about where fries were invented, with both countries claiming ownership. From the Belgian standpoint the popularity of the term \"french fries\" is explained as a \"French gastronomic hegemony\" into which the cuisine of Belgium was assimilated because of a lack of understanding coupled with a shared language and geographic proximity of the countries.\nBelgian journalist Jo G\u00e9rard claims that a 1781 family manuscript recounts that potatoes were deep-fried prior to 1680 in the Meuse valley, in what was then the Spanish Netherlands (present-day Belgium): \"The inhabitants of Namur, Andenne, and Dinant had the custom of fishing in the Meuse for small fish and frying, especially among the poor, but when the river was frozen and fishing became hazardous, they cut potatoes in the form of small fish and put them in a fryer like those here.\" G\u00e9rard has not produced the manuscript that supports this claim due to the fact that it is unrelated to the later history of the French fry, as the potato did not arrive in the region until around 1735. Also, given 18th century economic conditions: \"It is absolutely unthinkable that a peasant could have dedicated large quantities of fat for cooking potatoes. At most they were saut\u00e9ed in a pan\u00a0...\".\nAt least one source says that \"french fries\" for deep-fried potato batons was also introduced when American, Canadian, and British soldiers arrived in Belgium during World War I. The Belgians had previously been catering to the British soldiers' love of chips and continued to serve them to the other troops when they took over the western end of the front. The Belgians served them, and since French was the language of the Belgian Army, the name \"French\" was associated with the food. However, other sources disagree. Since a Frenchman (Parmentier) first made the potato popular, it is not surprising that the first reference to fried potatoes appears to come from France: in 1775, investigators there found fried potatoes in a dish. Through the nineteenth century, fried potatoes became common enough that songs and engravings took the \"Fried Potato Vendor\" as a popular subject. But at first, these were cut in rounds. In 1865, Gogu\u00e9, in France, wrote to cut them either round or \"in long and squared pieces\"; in 1870, Cauderlier, in Belgium, also offered both options. Given the lag between practice and print, one cannot say that the five years\u2019 difference is significant; the most likely hypothesis is that the practice grew up spontaneously and spread across both countries. Some other sources refer to the old-English verb \"to french\", meaning \"to cut lengthwise\", as the origin of the name. At that time, the term \"french fries\" was growing in popularity \u2013 the term was already used in the United States as early as 1899 \u2013 although it is not clear whether this referred to batons (chips) or slices of potato e.g. in an item in \"Good Housekeeping\" which specifically references \"Kitchen Economy in France\": \"The perfection of french fries is due chiefly to the fact that plenty of fat is used\".\n\"\"Pommes frites\" or just \"frites\" (French), \"frieten\" (a word used in Flanders and the southern provinces of the Netherlands) or \"patat\"\" (used in the north and central parts of the Netherlands) became the national snack and a substantial part of several national dishes, such as Moules-frites or Steak-frites. Fries are very popular in Belgium, where they are known as \"frieten\" (in Dutch) or \"frites\" (in French), and the Netherlands, where among the working classes they are known as \"patat\" in the north and, in the south, \"friet(en)\". In Belgium, fries are sold in shops called \"friteries\" (French), \"frietkot\"/\"frituur\" (Belgian Dutch), \"snackbar\" (Dutch in The Netherlands) or \"Frit\u00fcre\"/\"Fritt\u00fcre\" (German). They are served with a large variety of Belgian sauces and eaten either on their own or with other snacks. Traditionally fries are served in a \"cornet de frites\" (French), \"patatzak\" /\"frietzak\"/\"fritzak\" (Dutch/Flemish), or \"Frittent\u00fcte\" (German), a white cardboard cone, then wrapped in paper, with a spoonful of sauce (often mayonnaise) on top.\n\"Friteries\" and other fast food establishments tend to offer a number of different sauces for the fries and meats. In addition to ketchup and mayonnaise, popular options include: aioli, sauce andalouse, sauce Americaine, joppiesaus, \"Bicky\" Dressing (Gele Bicky-sauce), curry mayonnaise, mammoet-sauce, peanut sauce, samurai-sauce, sauce \"Pickles\", pepper-sauce, tartar sauce, zigeuner sauce, and \u00e0 la zingara.\nSpain.\nIn Spain, fried potatoes are called \"patatas fritas\" or \"papas fritas\". Another common form, involving larger irregular cuts, is \"patatas bravas\". The potatoes are cut into big chunks, partially boiled and then fried. They are usually seasoned with a spicy tomato sauce, and the dish is one of the most preferred tapas by Spaniards. Fries may have been invented in Spain, the first European country in which the potato appeared from the New World colonies, and assume fries' first appearance to have been as an accompaniment to fish dishes in Galicia, from which it spread to the rest of the country and then further away, to the \"Spanish Netherlands\", which became Belgium more than a century later. Professor Paul Ilegems, curator of the Frietmuseum in Bruges, Belgium, believes that Saint Teresa of \u00c1vila of Spain cooked the first french fries, and refers also to the tradition of frying in Mediterranean cuisine as evidence.\nFrance.\nIn France and other French-speaking countries, fried potatoes are formally \"pommes de terre frites\", but more commonly \"pommes frites\", \"patates frites\", or simply \"frites\". The words \"aiguillettes\" (\"needle-ettes\") or \"allumettes\" (\"matchsticks\") are used when the french fries are very small and thin. One enduring origin story holds that french fries were invented by street vendors on the Pont Neuf bridge in Paris in 1789, just before the outbreak of the French Revolution. However, a reference exists in France from 1775 to \"a few pieces of fried potato\" and to \"fried potatoes\".\nEating potatoes for sustenance was promoted in France by Antoine-Augustin Parmentier, but he did not mention \"fried\" potatoes in particular. Many Americans attribute the dish to France and offer as evidence a notation by U.S. President Thomas Jefferson: \"Pommes de terre frites \u00e0 cru, en petites tranches\" (\"Potatoes deep-fried while raw, in small slices\") in a manuscript in Thomas Jefferson's hand (circa 1801\u20131809) and the recipe almost certainly comes from his French chef, Honor\u00e9 Julien. In addition, from 1813 on, recipes for what can be described as \"french fries\" occur in popular American cookbooks. By the late 1850s, a cookbook was published that used the term \"French Fried Potatoes\". The thick-cut fries are called \"Pommes Pont-Neuf\" or simply \"pommes frites (\"about 10\u00a0mm); thinner variants are \"pommes allumettes\" (matchstick potatoes; about 7\u00a0mm), and \"pommes paille\" (potato straws; 3\u20134\u00a0mm). (Roughly 0.4, 0.3 and 0.15 inch respectively.) \"Pommes gaufrettes\" are waffle fries. A popular dish in France is steak frites, which is steak accompanied by thin french fries.\nCanada.\nThe town of Florenceville-Bristol, New Brunswick, headquarters of McCain Foods, calls itself \"the French fry capital of the world\" and also hosts a museum about potatoes called \"Potato World\". It is also one of the world's largest manufacturers of frozen french fries and other potato specialties.\nFrench fries are the main ingredient in the Canadian/Qu\u00e9b\u00e9cois dish known (in both Canadian English and Canadian French) as \"poutine\", a dish consisting of fried potatoes covered with cheese curds and brown gravy. Poutine has a growing number of variations, but it is generally considered to have been developed in rural Qu\u00e9bec sometime in the 1950s, although precisely where in the province it first appeared is a matter of contention. Canada is also responsible for providing 22% of China's french fries.\nGermany and Austria.\nFrench fries migrated to the German-speaking countries during the 19th century. In Germany, they are usually known by the French words , or only or (derived from the French words, but pronounced as German words). Often served with ketchup or mayonnaise, they are popular as a side dish in restaurants, or as a street-food snack purchased at an (snack stand). Since the 1950s, \"currywurst\" has become a widely-popular dish that is commonly offered with fries. Currywurst is a sausage (often bratwurst or bockwurst) in a spiced ketchup-based sauce, dusted with curry powder.\nSouth Africa.\nWhilst eating 'regular' crispy french fries is common in South Africa, a regional favorite, particularly in Cape Town, is a soft soggy version doused in white vinegar called \"slap-chips\" (pronounced \"\"slup-chips\" in English or \"slaptjips\"\" in Afrikaans). These chips are typically thicker and fried at a lower temperature for a longer period of time than regular french fries. Slap-chips are an important component of a Gatsby sandwich, also a common Cape Town delicacy. Slap-chips are also commonly served with deep fried fish which are also served with the same white vinegar.\nUnited Kingdom and Ireland.\nThe standard deep-fried cut potatoes in the United Kingdom are called chips, and are cut into pieces between wide. They are occasionally made from unpeeled potatoes (skins showing). British \"chips\" are not the same thing as potato chips (an American term); those are called \"crisps\" in Britain. In the UK, chips are part of the popular, and now international, fast food dish fish and chips. In the UK, chips are considered a separate item to french fries. Chips are a thicker cut than french fries, they are generally cooked only once and at a lower temperature.\nThe first commercially available chips in the UK were sold by Mrs. 'Granny' Duce in one of the West Riding towns in 1854. A blue plaque in Oldham marks the origin of the fish-and-chip shop, and thus the start of the fast food industry in Britain. In Scotland, chips were first sold in Dundee: \"in the 1870s, that glory of British gastronomy \u2013 the chip \u2013 was first sold by Belgian immigrant Edward De Gernier in the city's Greenmarket\". In Ireland the first chip shop was \"opened by Giuseppe Cervi\", an Italian immigrant, \"who arrived there in the 1880s\". It is estimated that in the UK, 80% of households buy frozen chips each year.\nUnited States.\nAlthough french fries were a popular dish in most British Commonwealth countries, the \"thin style\" french fries have been popularized worldwide in large part by the large American fast food chains such as McDonald's, Burger King, and Wendy's. In the United States, the J. R. Simplot Company is credited with successfully commercializing french fries in frozen form during the 1940s. Subsequently, in 1967, Ray Kroc of McDonald's contracted the Simplot company to supply them with frozen fries, replacing fresh-cut potatoes. In 2004, 29% of the United States' potato crop was used to make frozen fries \u2013 90% consumed by the food services sector and 10% by retail. The United States is also known for supplying China with most of their french fries as 70% of China's french fries are imported.\nPre-made french fries have been available for home cooking since the 1960s, having been pre-fried (or sometimes baked), frozen and placed in a sealed plastic bag. Some varieties of french fries that appeared later have been battered and breaded, and many fast food chains in the U.S. dust the potatoes with kashi, dextrin, and other flavor coatings for crispier fries with particular tastes. French fries are one of the most popular dishes in the United States, commonly being served as a side dish to entrees and being seen in fast food restaurants. The average American eats around 30 pounds of french fries a year.\nVariants.\nFrench fries come in multiple variations. A partial list, in alphabetical order:\nAccompaniments.\nFries tend to be served with a variety of accompaniments, such as salt and vinegar (malt, balsamic or white), pepper, Cajun seasoning, grated cheese, melted cheese, mushy peas, heated curry sauce, curry ketchup (mildly spiced mix of the former), hot sauce, relish, mustard, mayonnaise, bearnaise sauce, tartar sauce, chili, tzatziki, feta cheese, garlic sauce, fry sauce, butter, sour cream, ranch dressing, barbecue sauce, gravy, honey, aioli, brown sauce, ketchup, lemon juice, piccalilli, pickled cucumber, pickled gherkins, pickled onions or pickled eggs.\nHealth aspects.\nFrench fries primarily contain carbohydrates (mostly in the form of starch) and protein from the potato, and fat absorbed during the deep-frying process. Salt, which contains sodium, is almost always applied as a surface seasoning. For example, a large serving of french fries at McDonald's in the United States is 154 grams. The 510 calories come from 66 g of carbohydrates, 24 g of fat, 7 g of protein and 350\u00a0mg of sodium.\nExperts have criticized french fries for being very unhealthy. According to Jonathan Bonnet, MD, in a \"TIME\" magazine article, \"fries are nutritionally unrecognizable from a spud\" because they \"involve frying, salting, and removing one of the healthiest parts of the potato: the skin, where many of the nutrients and fiber are found.\" Kristin Kirkpatrick, RD, calls french fries \"... an extremely starchy vegetable dipped in a fryer that then loads on the unhealthy fat, and what you have left is a food that has no nutritional redeeming value in it at all.\" David Katz, MD states that \"French fries are often the super-fatty side dish to a burger\u2014and both are often used as vehicles for things like sugar-laced ketchup and fatty mayo.\"\nFrying french fries in beef tallow, lard, or other animal fats adds saturated fat to the diet. Replacing animal fats with tropical vegetable oils, such as palm oil, simply substitutes one saturated fat for another. For many years partially hydrogenated vegetable oils were used as a means of avoiding cholesterol and reducing saturated fatty acid content, but in time the trans fat content of these oils was perceived as contributing to cardiovascular disease. Starting in 2008, many restaurant chains and manufacturers of pre-cooked frozen french fries for home reheating phased out trans fat containing vegetable oils.\nFrench fries contain some of the highest levels of acrylamides of any foodstuff, and experts have raised concerns about the effects of acrylamides on human health. According to the American Cancer Society, it is not clear whether acrylamide consumption affects people's risk of getting cancer. A meta-analysis indicated that dietary acrylamide is not related to the risk of most common cancers, but could not exclude a modest association for kidney, endometrial or ovarian cancers. A lower-fat method for producing a French fry-like product is to coat \"Frenched\" or wedge potatoes in oil and spices/flavoring before baking them. The temperature will be lower compared to deep frying, which also reduces acrylamide formation.\nLegal issues.\nIn June 2004, the United States Department of Agriculture (USDA), with the advisement of a federal district judge from Beaumont, Texas, classified batter-coated french fries as a vegetable under the \"Perishable Agricultural Commodities Act\". This was primarily for trade reasons; french fries do not meet the standard to be listed as a processed food. This classification, referred to as the \"French fry rule\", was upheld in the United States Court of Appeals for the Fifth Circuit case \"Fleming Companies, Inc. v. USDA\".\nIn the United States, in 2002, the McDonald's Corporation agreed to donate to Hindus and other groups to settle lawsuits filed against the chain for mislabeling french fries and hash browns as vegetarian even though beef extract flavoring was added in their production."}
{"id": "10886", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10886", "title": "Field hockey", "text": "Field hockey is a widely played team sport of the hockey family. The game can be played on grass, watered turf, artificial turf or synthetic field, as well as an indoor boarded surface. Each team plays with ten field players and a goalkeeper. Players use sticks made of wood, carbon fibre, fibre glass, or a combination of carbon fibre and fibre glass in different quantities, to hit a round, hard, plastic hockey ball. The length of the hockey stick is based on the player's individual height: the top of the stick usually comes to the players hip, and taller players typically have longer sticks. The sticks have a round side and a flat side, and only the flat face of the stick is allowed to be used. Use of the other side results in a foul. Goalies often have a different design of stick, although they can also use an ordinary field hockey stick. The specific goal-keeping sticks have another curve at the end of the stick, which is to give it more surface area to block the ball. The uniform consists of shin guards, shoes, shorts or a skirt, a mouthguard and a jersey.\nThe game is played globally, particularly in parts of Western Europe, South Asia, Southern Africa, Australia, New Zealand, Argentina, and parts of the United States, primarily New England and the Mid-Atlantic states.\nKnown simply as \"hockey\" in most territories, the term \"field hockey\" is used primarily in Canada and the United States where ice hockey is more popular. In Sweden, the term \"landhockey\" is used, and to some degree in Norway, where the game is governed by the Norges Bandyforbund.\nDuring play, goal keepers are the only players allowed to touch the ball with any part of their body, while field players can only play the ball with the flat side of their stick. A player's hand is considered part of the stick if holding the stick. If the ball is touched with the rounded part of the stick, it will result in a penalty. Goal keepers also cannot play the ball with the back of their stick.\nThe team that scores the most goals by the end of the match wins. If the score is tied at the end of the game, either a draw is declared or the game goes into extra time, or there is a penalty shoot-out, depending on the format of the competition. There are many variations to overtime play that depend on the league or tournament rules. In American college play, a seven-aside overtime period consists of a 10-minute golden goal period with seven players for each team. If a tie still remains, the game enters a one-on-one competition where each team chooses five players to dribble from the line down to the circle against the opposing goalie. The player has eight seconds to score against the goalie while keeping the ball in bounds. The game ends after a goal is scored, the ball goes out of bounds, a foul is committed (ending in either a penalty stroke or flick or the end of the one-on-one) or time expires. If the tie still persists, more rounds are played until one team has scored.\nThe governing body of field hockey is the International Hockey Federation (FIH), called the \"F\u00e9d\u00e9ration Internationale de Hockey\" in French, with men and women being represented internationally in competitions including the Olympic Games, World Cup, World League, Champions Trophy and Junior World Cup, with many countries running extensive junior, senior, and masters club competitions. The FIH is also responsible for organizing the Hockey Rules Board and developing the rules of the game.\nA popular variant of field hockey is indoor field hockey, which differs in a number of respects while embodying the primary principles of hockey. Indoor hockey is a 5-a-side variant, using a field which is reduced to approximately . Although many of the rules remain the same, including obstruction and feet, there are several key variations: players may not raise the ball unless shooting at goal, players may not hit the ball, instead using pushes to transfer it, and the sidelines are replaced with solid barriers, from which the ball will rebound and remain in play. In addition, the regulation guidelines for the indoor field hockey stick require a slightly thinner, lighter stick than an outdoor one.\nHistory.\nThere is a depiction of a field hockey-like game in Ancient Greece, dating to c.\u00a0510\u00a0BC, when the game may have been called (\"ker\u0113t\u00edzein\") because it was played with a horn (, \"k\u00e9ras\", in Ancient Greek) and a ball. Researchers disagree over how to interpret this image. It could have been a team or one-on-one activity (the depiction shows two active players, and other figures who may be teammates awaiting a face-off, or non-players waiting for their turn at play). Billiards historians Stein and Rubino believe it was among the games ancestral to lawn-and-field games like hockey and ground billiards, and near-identical depictions (but with only two figures) appear both in the Beni Hasan tomb of Ancient Egyptian administrator Khety of the 11th Dynasty (c.\u00a02000\u00a0BCE), and in European illuminated manuscripts and other works of the 14th through 17th centuries, showing contemporary courtly and clerical life. In East Asia, a similar game was entertained, using a carved wooden stick and ball prior, to 300 BC. In Inner Mongolia, China, the Daur people have for about 1,000 years been playing \"beikou\", a game with some similarities to field hockey. A similar field hockey or ground billiards variant, called \"suigan\", was played in China during the Ming dynasty (1368\u20131644, post-dating the Mongol-led Yuan dynasty). A game similar to field hockey was played in the 17th century in Punjab state in India under name \"khido khundi\" (\"khido\" refers to the woolen ball, and \"khundi\" to the stick).\nIn South America, most specifically in Chile, the local natives of the 16th century used to play a game called chueca, which also shares common elements with hockey.\nIn Northern Europe, the games of hurling (Ireland) and ' (Iceland), both team ball games involving sticks to drive a ball to the opponents' goal, date at least as far back as the Early Middle Ages. By the 12th century, a team ball game called ' or \"\", akin to a chaotic and sometimes long-distance version of hockey or rugby football (depending on whether sticks were used in a particular local variant), was regularly played in France and southern Britain between villages or parishes. Throughout the Middle Ages to the Early Modern era, such games often involved the local clergy or secular aristocracy, and in some periods were limited to them by various anti-gaming edicts, or even banned altogether. Stein and Rubino, among others, ultimately trace aspects of these games both to rituals in antiquity involving orbs and sceptres (on the aristocratic and clerical side), and to ancient military training exercises (on the popular side); polo (essentially hockey on horseback) was devised by the Ancient Persians for cavalry training, based on the local proto-hockey foot game of the region.\nThe word \"hockey\" itself has no clear origin. One belief is that it was recorded in 1363 when Edward III of England issued the proclamation: \"Moreover we ordain that you prohibit under penalty of imprisonment all and sundry from such stone, wood and iron throwing; handball, football, or hockey; coursing and cock-fighting, or other such idle games.\" The belief is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games \"Pilam Manualem, Pedivam, &amp; Bacularem: &amp; ad Canibucam &amp; Gallorum Pugnam\". It may be recalled at this point that \"baculum\" is the Latin for 'stick', so the reference would appear to be to a game played with sticks. The English historian and biographer John Strype did not use the word \"hockey\" when he translated the proclamation in 1720, and the word 'hockey' remains of unknown origin.\nThe modern game grew from English public schools in the early 19th century. The first club was in 1849 at Blackheath in south-east London, but the modern rules grew out of a version played by Middlesex cricket clubs for winter game. Teddington Hockey Club formed the modern game by introducing the striking circle and changing the ball to a sphere from a rubber cube. The Hockey Association was founded in 1886. The first international competition took place in 1895 (Ireland 3, Wales 0), and the International Rules Board was founded in 1900.\nField hockey was played at the Summer Olympics in 1908 and 1920. It was dropped in 1924, leading to the foundation of the F\u00e9d\u00e9ration Internationale de Hockey sur Gazon (FIH) as an international governing body by seven continental European nations; and hockey was reinstated as an Olympic game in 1928. Men's hockey united under the FIH in 1970.\nThe two oldest trophies are the Irish Senior Cup, which dates back to 1894, and the Irish Junior Cup, a second XI-only competition instituted in 1895.\nIn India, the Beighton Cup and the Aga Khan tournament commenced within ten years. Entering the Olympics in 1928, India won all five games without conceding a goal, and won from 1932 until 1956 and then in 1964 and 1980. Pakistan won in 1960, 1968 and 1984.\nIn the early 1970s, artificial turf began to be used. Synthetic pitches changed most aspects of field hockey, gaining speed. New tactics and techniques such as the Indian dribble developed, followed by new rules to take account. The switch to synthetic surfaces ended Indian and Pakistani domination because artificial turf was too expensive in developing countries. Since the 1970s, Australia, the Netherlands, and Germany have dominated at the Olympics and World Cup stages.\nWomen's field hockey was first played at British universities and schools. The first club, the Molesey Ladies, was founded in 1887. The first national association was the Irish Ladies Hockey Union in 1894, and though rebuffed by the Hockey Association, women's field hockey grew rapidly around the world. This led to the International Federation of Women's Hockey Association (IFWHA) in 1927, though this did not include many continental European countries where women played as sections of men's associations and were affiliated to the FIH. The IFWHA held conferences every three years, and tournaments associated with these were the primary IFWHA competitions. These tournaments were non-competitive until 1975.\nBy the early 1970s, there were 22 associations with women's sections in the FIH and 36 associations in the IFWHA. Discussions started about a common rule book. The FIH introduced competitive tournaments in 1974, forcing the acceptance of the principle of competitive field hockey by the IFWHA in 1973. It took until 1982 for the two bodies to merge, but this allowed the introduction of women's field hockey to the Olympic games from 1980 where, as in the men's game, The Netherlands, Germany, and Australia have been consistently strong. Argentina has emerged as a team to be reckoned with since 2000, winning the world championship in 2002 and 2010 and medals at the last three Olympics.\nIn the United States field hockey is played predominantly by females. However, outside North America, participation is now fairly evenly balanced between men and women. For example, in England, England Hockey reports that as of the 2008\u201309 season there were 2488 registered men's teams, 1969 women's teams, 1042 boys' teams, 966 girls' teams and 274 mixed teams. In 2006 the Irish Hockey Association reported that the gender split among its players was approximately 65% female and 35% male. In its 2008 census, Hockey Australia reported 40,534 male club players and 41,542 female. However, in the United States of America, there are few field hockey clubs, most play taking place between high school or college sides, consisting almost entirely of women. The strength of college field hockey reflects the impact of Title IX which mandated that colleges should fund men's and women's games programmes comparably.\nThe game's roots in the English public girls' school mean that the game is associated in the UK with active or overachieving middle class and upper class women. For example, in \"Nineteen Eighty-Four\", George Orwell's novel set in a totalitarian London, main character Winston Smith initially dislikes Julia, the woman he comes to love, because of \"the atmosphere of hockey-fields and cold baths and community hikes and general clean-mindedness which she managed to carry about with her.\"\nThe game of field hockey is also very present in the United States. Many high schools and colleges in the U.S. offer the sport and in some areas, it is even offered for youth athletes. It has been predominantly played on the East Coast, specifically the Mid-Atlantic in states such as New Jersey, New York, Pennsylvania, Maryland, and Virginia. It recent years however it has become increasingly present on the West Coast and in the Midwest.\nField of play.\nMost hockey field dimensions were originally fixed using whole numbers of imperial measures. Nevertheless, metric measurements are now the official dimensions as laid down by the International Hockey Federation (FIH) in the \"Rules of Hockey\". The pitch is a rectangular field. At each end is a goal high and wide, as well as lines across the field from each end-line (generally referred to as the 23-metre lines or the 25-yard lines) and in the center of the field. A spot in diameter, called the penalty spot or stroke mark, is placed with its centre from the centre of each goal. The shooting circle is from the base line.\nField hockey goals are made of two upright posts, joined at the top by a horizontal crossbar, with a net positioned to catch the ball when it passes through the goalposts. The goalposts and crossbar must be white and rectangular in shape, and should be wide and deep.\nField hockey goals also include sideboards and a backboard, which stand from the ground. The backboard runs the full width of the goal, while the sideboards are deep.\nPlaying surface.\nHistorically the game developed on natural grass turf. In the early 1970s, \"synthetic grass\" fields began to be used for hockey, with the first Olympic Games on this surface being held at Montreal in 1976. Canadian Organizer, Peter Buckland, from Vancouver, is credited with convincing the International Hockey F\u00e9d\u00e9ration(FIH) to accept Artificial Turf at the Montreal Games. Synthetic pitches are now mandatory for all international tournaments and for most national competitions. While hockey is still played on traditional grass fields at some local levels and lesser national divisions, it has been replaced by synthetic surfaces almost everywhere in the western world. There are three main types of artificial hockey surface:\nSince the 1970s, sand-based pitches have been favoured as they dramatically speed up the game. However, in recent years there has been a massive increase in the number of \"water-based\" artificial turfs. Water-based synthetic turfs enable the ball to be transferred more quickly than on sand-based surfaces. It is this characteristic that has made them the surface of choice for international and national league competitions. Water-based surfaces are also less abrasive than sand-based surfaces and reduce the level of injury to players when they come into contact with the surface. The FIH are now proposing that new surfaces being laid should be of a hybrid variety which require less watering. This is due to the negative ecological effects of the high water requirements of water-based synthetic fields. It has also been stated that the decision to make artificial surfaces mandatory greatly favoured more affluent countries who could afford these new pitches.\nRules and play.\nThe game is played between two teams of eleven, 10 field players and one goal keeper, are permitted to be on the pitch at any one time. The remaining players may be substituted in any combination. There is an unlimited number of times a team can sub in and out. Substitutions are permitted at any point in the game, apart from between the award and end of a penalty corner; two exceptions to this rule is for injury or suspension of the defending goalkeeper, which is not allowed when playing with a field keep, or a player can exit the field, but you must wait until after the inserter touches the ball to put somebody back in.\nPlayers are permitted to play the ball with the flat of the 'face side' and with the edges of the head and handle of the field hockey stick with the exception that, for reasons of safety, the ball may not be struck 'hard' with a forehand edge stroke, because of the difficulty of controlling the height and direction of the ball from that stroke.\nThe flat side is always on the \"natural\" side for a right-handed person swinging the stick at the ball from right to left. Left-handed sticks are rare, but available; however they are pointless as the rules forbid their use in a game. To make a strike at the ball with a left-to-right swing the player must present the flat of the 'face' of the stick to the ball by 'reversing' the stick head, i.e. by turning the handle through approximately 180\u00b0 (while a reverse edge hit would turn the stick head through approximately 90\u00b0 from the position of an upright forehand stroke with the 'face' of the stick head).\nEdge hitting of the ball underwent a two-year \"experimental period\", twice the usual length of an \"experimental trial\" and is still a matter of some controversy within the game. Ric Charlesworth, the former Australian coach, has been a strong critic of the unrestricted use of the reverse edge hit. The 'hard' forehand edge hit was banned after similar concerns were expressed about the ability of players to direct the ball accurately, but the reverse edge hit does appear to be more predictable and controllable than its counterpart. This type of hit is now more commonly referred to as the \"forehand sweep\" where the ball is hit with the flat side or \"natural\" side of the stick and not the rounded edge.\nOther rules include; no foot-to-ball contact, no use of hands, no obstructing other players, no high back swing, no hacking, and no third party. If a player is dribbling the ball and either loses control and kicks the ball or another player interferes that player is not permitted to gain control and continue dribbling. The rules do not allow the person who kicked the ball to gain advantage from the kick, so the ball will automatically be passed on to the opposing team. Conversely, if no advantage is gained from kicking the ball, play should continue. Players may not obstruct another's chance of hitting the ball in any way. No shoving/using your body/stick to prevent advancement in the other team. Penalty for this is the opposing team receives the ball and if the problem continues, the player can be carded. While a player is taking a free hit or starting a corner the back swing of their hit cannot be too high for this is considered dangerous. Finally there may not be three players touching the ball at one time. Two players from opposing teams can battle for the ball, however if another player interferes it is considered third party and the ball automatically goes to the team who only had one player involved in the third party.\nThe game.\nA match ordinarily consists of two periods of 35 minutes and a halftime interval of 5 minutes. Other periods and interval may be agreed by\nboth teams except as specified in Regulations for particular competitions. Since 2014, some International games have four 15-minute quarters with 2 minutes break between each quarter and 15 minutes break between quarter two and three. At the 2018 Commonwealth Games Held on the Gold Coast in Brisbane, Australia the hockey games for both men and women had four 15-minute quarters.\nIn December 2018 the FIH announced rule changes that would make 15-minute quarters universal from January 2019. England Hockey confirmed that while no changes would be made to the domestic game mid-season, the new rules would be implemented at the start of the 2019\u201320 season. However, in July 2019 England Hockey announced that 17.5-minute quarters would only be implemented in elite domestic club games.\nThe game begins with a pass back from the centre-forward usually to the centre-half back from the halfway line, the opposing team can not try to tackle this play until the ball has been pushed back. The team consists of eleven players, the players are usually set up as follows: Goalkeeper, Left Fullback, Right Fullback, 3 half-backs and 4 forwards consisting of Left Wing, Left Inner, Right Inner and Right Wing. These positions can change and adapt throughout the course of the game depending on the attacking and defensive style of the opposition.\nPositions.\nWhen hockey positions are discussed, notions of fluidity are very common. Each team can be fielded with a maximum of 11 players and will typically arrange themselves into forwards, midfielders, and defensive players (fullbacks) with players frequently moving between these lines with the flow of play. Each team may also play with:\n\"* a goalkeeper who wears a different color shirt and full protective equipment comprising at least headgear, leg guards and kickers; this player is referred to in the rules as a goalkeeper; or\"\n\"* Only field players; no player has goalkeeping privileges or wears a different color shirt; no player may wear protective headgear except a face mask when defending a penalty corner or stroke.\"\nFormations.\nAs hockey has a very dynamic style of play, it is difficult to simplify positions to the static formations which are common in association football. Although positions will typically be categorized as either fullback, halfback, midfield/inner or striker, it is important for players to have an understanding of every position on the field. For example, it is not uncommon to see a halfback overlap and end up in either attacking position, with the midfield and strikers being responsible for re-adjusting to fill the space they left. Movement between lines like this is particularly common across all positions.\nThis fluid Australian culture of hockey has been responsible for developing an international trend towards players occupying spaces on the field, not having assigned positions. Although they may have particular spaces on the field which they are more comfortable and effective as players, they are responsible for occupying the space nearest them. This fluid approach to hockey and player movement has made it easy for teams to transition between formations such as; \"3 at the back\", \"5 midfields\", \"\"2 at the front\",\" and more.\nGoalkeepers.\nWhen the ball is inside the circle they are defending and they have their stick in their hand, goalkeepers wearing full protective equipment are permitted to use their stick, feet, kickers or leg guards to propel the ball and to use their stick, feet, kickers, leg guards or any other part of their body to stop the ball or deflect it in any direction including over the back line. Similarly, field players are permitted to use their stick. They are not allowed to use their feet and legs to propel the ball, stop the ball or deflect it in any direction including over the back line. However, neither goalkeepers, or players with goalkeeping privileges are permitted to conduct themselves in a manner which is dangerous to other players by taking advantage of the protective equipment they wear.\nNeither goalkeepers or players with goalkeeping privileges may lie on the ball, however, they are permitted to use arms, hands and any other part of their body to push the ball away. Lying on the ball deliberately will result in a penalty stroke, whereas if an umpire deems a goalkeeper has lain on the ball accidentally (e.g. it gets stuck in their protective equipment), a penalty corner is awarded.\n\"* The action above is permitted only as part of a goal saving action or to move the ball away from the possibility of a goal scoring action by opponents. It does not permit a goalkeeper or player with goalkeeping privileges to propel the ball forcefully with arms, hands or body so that it travels a long distance\"\nWhen the ball is outside the circle they are defending, goalkeepers or players with goalkeeping privileges are only permitted to play the ball with their stick. Further, a goalkeeper, or player with goalkeeping privileges who is wearing a helmet, must not take part in the match outside the 23m area they are defending, except when taking a penalty stroke. A goalkeeper must wear protective headgear at all times, except when taking a penalty stroke.\nGeneral play.\nFor the purposes of the rules, all players on the team in possession of the ball are attackers, and those on the team without the ball are defenders, yet throughout the game being played you are always \"defending\" your goal and \"attacking\" the opposite goal.\nThe match is officiated by two field umpires. Traditionally each umpire generally controls half of the field, divided roughly diagonally. These umpires are often assisted by a technical bench including a timekeeper and record keeper.\nPrior to the start of the game, a coin is tossed and the winning captain can choose a starting end or whether to start with the ball. Since 2017 the game consists of four periods of 15 minutes with a 2-minute break after every period, and a 15-minute intermission at half time before changing ends. At the start of each period, as well as after goals are scored, play is started with a pass from the centre of the field. All players must start in their defensive half (apart from the player making the pass), but the ball may be played in any direction along the floor. Each team starts with the ball in one half, and the team that conceded the goal has possession for the restart. Teams trade sides at halftime.\nField players may only play the ball with the face of the stick. If the back side of the stick is used, it is a penalty and the other team will get the ball back. Tackling is permitted as long as the tackler does not make contact with the attacker or the other person's stick before playing the ball (contact after the tackle may also be penalized if the tackle was made from a position where contact was inevitable). Further, the player with the ball may not deliberately use his body to push a defender out of the way.\nField players may not play the ball with their feet, but if the ball accidentally hits the feet, and the player gains no benefit from the contact, then the contact is not penalized. Although there has been a change in the wording of this rule from 1 January 2007, the current FIH umpires' briefing instructs umpires not to change the way they interpret this rule.\nObstruction typically occurs in three circumstances\u00a0\u2013 when a defender comes between the player with possession and the ball in order to prevent them tackling; when a defender's stick comes between the attacker's stick and the ball or makes contact with the attacker's stick or body; and also when blocking the opposition's attempt to tackle a teammate with the ball (called \"third party obstruction\").\nWhen the ball passes completely over the sidelines (on the sideline is still in), it is returned to play with a sideline hit, taken by a member of the team whose players were not the last to touch the ball before crossing the sideline. The ball must be placed on the sideline, with the hit taken from as near the place the ball went out of play as possible. If it crosses the back line after last touched by an attacker, a hit is awarded. A 15\u00a0m hit is also awarded for offences committed by the attacking side within 15\u00a0m of the end of the pitch they are attacking.\nSet plays.\nSet plays are often utilized for specific situations such as a penalty corner or free hit. For instance, many teams have penalty corner variations that they can use to beat the defensive team. The coach may have plays that sends the ball between two defenders and lets the player attack the opposing team's goal. There are no set plays unless your team has them.\nFree hits.\nFree hits are awarded when offences are committed outside the scoring circles (the term 'free hit' is standard usage but the ball need not be hit). The ball may be hit, pushed or lifted in any direction by the team offended against. The ball can be lifted from a free hit but not by hitting, you must flick or scoop to lift from a free hit. (In previous versions of the rules, hits in the area outside the circle in open play have been permitted but lifting one direction from a free hit was prohibited). Opponents must move from the ball when a free hit is awarded. A free hit must be taken from within playing distance of the place of the offence for which it was awarded and the ball must be stationary when the free hit is taken.\nAs mentioned above, a 15\u00a0m hit is awarded if an attacking player commits a foul forward of that line, or if the ball passes over the back line off an attacker. These free hits are taken in-line with where the foul was committed (taking a line parallel with the sideline between where the offence was committed, or the ball went out of play). When an attacking free hit is awarded within 5\u00a0m of the circle everyone including the person taking the penalty must be five metres from the circle and everyone apart from the person taking the free hit must be five metres away from the ball. When taking an attacking free hit, the ball may not be hit straight into the circle if you are within your attacking 23 meter area (25-yard area). It must travel 5 meters before going in.\n2009 experimental changes.\nIn February 2009 the FIH introduced, as a \"Mandatory Experiment\" for international competition, an updated version of the free-hit rule. The changes allows a player taking a free hit to pass the ball to themselves. Importantly, this is not a \"play on\" situation, but to the untrained eye it may appear to be. The player must play the ball any distance in two separate motions, before continuing as if it were a play-on situation. They may raise an aerial or overhead immediately as the second action, or any other stroke permitted by the rules of field hockey. At high-school level, this is called a self pass and was adopted in Pennsylvania in 2010 as a legal technique for putting the ball in play.\nAlso, all players (from both teams) must be at least 5\u00a0m from any free hit awarded to the attack within the 23\u00a0m area. The ball may not travel directly into the circle from a free hit to the attack within the 23 m area without first being touched by another player or being dribbled at least 5 m by a player making a \"self-pass\". These experimental rules apply to all free-hit situations, including sideline and corner hits. National associations may also choose to introduce these rules for their domestic competitions.\nLong Corner.\nA free hit from the 23-metre line \u2013 called a long corner \u2013 is awarded to the attacking team if the ball goes over the back-line after last being touched by a defender, provided they do not play it over the back-line deliberately, in which case a penalty corner is awarded. This free hit is played by the attacking team from a spot on the 23-metre line, in line with where the ball went out of play. All the parameters of an attacking free hit within the attacking quarter of the playing surface apply.\nPenalty corner.\nThe short or penalty corner is awarded: \nShort corners begin with five defenders (usually including the keeper) positioned behind the back line and the ball placed at least 10 yards from the nearest goal post. All other players in the defending team must be beyond the centre line, that is not in their 'own' half of the pitch, until the ball is in play. Attacking players begin the play standing outside the scoring circle, except for one attacker who starts the corner by playing the ball from a mark 10\u00a0m either side of the goal (the circle has a 14.63\u00a0m radius). This player puts the ball into play by pushing or hitting the ball to the other attackers outside the circle; the ball must pass outside the circle and then put back into the circle before the attackers may make a shot at the goal from which a goal can be scored. FIH rules do not forbid a shot at goal before the ball leaves the circle after being 'inserted', nor is a shot at the goal from outside the circle prohibited, but a goal cannot be scored at all if the ball has not gone out of the circle and cannot be scored from a shot from outside the circle if it is not again played by an attacking player before it enters the goal.\nFor safety reasons, the first shot of a penalty corner must not exceed 460\u00a0mm high (the height of the \"backboard\" of the goal) at the point it crosses the goal line if it is hit. However, if the ball is deemed to be below backboard height, the ball can be subsequently deflected above this height by another player (defender or attacker), providing that this deflection does not lead to danger. Note that the \"Slap\" stroke (a sweeping motion towards the ball, where the stick is kept on or close to the ground when striking the ball) is classed as a hit, and so the first shot at goal must be below backboard height for this type of shot also.\nIf the first shot at goal in a short corner situation is a push, flick or scoop, in particular the \"drag flick\" (which has become popular at international and national league standards), the shot is permitted to rise above the height of the backboard, as long as the shot is not deemed dangerous to any opponent. This form of shooting was developed because it is not height restricted in the same way as the first hit shot at the goal and players with good technique are able to drag-flick with as much power as many others can hit a ball.\nPenalty stroke.\nA penalty stroke is awarded when a defender commits a foul in the circle (accidental or otherwise) that prevents a probable goal or commits a deliberate foul in the circle or if defenders repeatedly run from the back line too early at a penalty corner. The penalty stroke is taken by a single attacker in the circle, against the goalkeeper, from a spot 6.4\u00a0m from goal. The ball is played only once at goal by the attacker using a push, flick or scoop stroke. If the shot is saved, play is restarted with a 15 m hit to the defenders. When a goal is scored, play is restarted in the normal way.\nDangerous play and raised balls.\nAccording to the Rules of Hockey 2015 issued by the FIH there are only two criteria for a dangerously played ball. The first is legitimate evasive action by an opponent (what constitutes legitimate evasive action is an umpiring judgment). The second is specific to the rule concerning a shot at goal at a penalty corner but is generally, if somewhat inconsistently, applied throughout the game and in all parts of the pitch: it is that a ball lifted above knee height and at an opponent who is within 5m of the ball is certainly dangerous.\nThe velocity of the ball is not mentioned in the rules concerning a dangerously played ball. A ball that hits a player above the knee may on some occasions not be penalized, this is at the umpire's discretion. A jab tackle, for example, might accidentally lift the ball above knee height into an opponent from close range but at such low velocity as not to be, in the opinion of the umpire, dangerous play. In the same way a high-velocity hit at very close range into an opponent, but below knee height, could be considered to be dangerous or reckless play in the view of the umpire, especially when safer alternatives are open to the striker of the ball.\nA ball that has been lifted high so that it will fall among close opponents may be deemed to be potentially dangerous and play may be stopped for that reason. A lifted ball that is falling to a player in clear space may be made potentially dangerous by the actions of an opponent closing to within 5m of the receiver before the ball has been controlled to ground\u00a0\u2013 a rule which is often only loosely applied; the distance allowed is often only what might be described as playing distance, 2\u20133\u00a0m, and opponents tend to be permitted to close on the ball as soon as the receiver plays it: these unofficial variations are often based on the umpire's perception of the skill of the players i.e. on the level of the game, in order to maintain game flow, which umpires are in general in both Rules and Briefing instructed to do, by not penalising when it is unnecessary to do so; this is also a matter at the umpire's discretion.\nThe term \"falling ball\" is important in what may be termed encroaching offences. It is generally only considered an offence to encroach on an opponent receiving a lifted ball that has been lifted to above head height (although the height is not specified in rule) and is falling. So, for example, a lifted shot at the goal which is still rising as it crosses the goal line (or would have been rising as it crossed the goal line) can be legitimately followed up by any of the attacking team looking for a rebound.\nIn general even potentially dangerous play is not penalised if an opponent is not disadvantaged by it or, obviously, not injured by it so that he cannot continue. A personal penalty, that is a caution or a suspension, rather than a team penalty, such as a free ball or a penalty corner, may be (many would say should be or even must be, but again this is at the umpire's discretion) issued to the guilty party after an advantage allowed by the umpire has been played out in any situation where an offence has occurred, including dangerous play (but once advantage has been allowed the umpire cannot then call play back and award a team penalty).\nIt is not an offence to lift the ball over an opponent's stick (or body on the ground), provided that it is done with consideration for the safety of the opponent and not dangerously. For example, a skillful attacker may lift the ball over a defenders stick or prone body and run past them, however if the attacker lifts the ball into or at the defender's body, this would almost certainly be regarded as dangerous.\nIt is not against the rules to bounce the ball on the stick and even to run with it while doing so, as long as that does not lead to a potentially dangerous conflict with an opponent who is attempting to make a tackle. For example, two players trying to play at the ball in the air at the same time, would probably be considered a dangerous situation and it is likely that the player who first put the ball up or who was so 'carrying' it would be penalised.\nDangerous play rules also apply to the usage of the stick when approaching the ball, making a stroke at it (replacing what was at one time referred to as the \"sticks\" rule, which once forbade the raising of any part of the stick above the shoulder during any play. This last restriction has been removed but the stick should still not be used in a way that endangers an opponent) or attempting to tackle, (fouls relating to tripping, impeding and obstruction). The use of the stick to strike an opponent will usually be much more severely dealt with by the umpires than offences such as barging, impeding and obstruction with the body, although these are also dealt with firmly, especially when these fouls are intentional: field hockey is a non-contact game.\nPlayers may not play or attempt to play at the ball above their shoulders unless trying to save a shot that could go into the goal, in which case they are permitted to stop the ball or deflect it safely away. A swing, as in a hit, at a high shot at the goal (or even wide of the goal) will probably be considered dangerous play if at opponents within 5\u00a0m and such a stroke would be contrary to rule in these circumstances anyway.\nWithin the English National League it is now a legal action to take a ball above shoulder height if completed using a controlled action.\nWarnings and suspensions.\nHockey uses a three-tier penalty card system of warnings and suspensions:\nIf a coach is sent off, depending on local rules, a player may have to leave the field for the remaining length of the match.\nIn addition to their colours, field hockey penalty cards are often shaped differently, so they can be recognized easily. Green cards are normally triangular, yellow cards rectangular and red cards circular.\nUnlike football, a player may receive more than one green or yellow card. However, they cannot receive the same card for the same offence (for example two yellows for dangerous play), and the second must always be a more serious card. In the case of a second yellow card for a different breach of the rules (for example a yellow for deliberate foot, and a second later in the game for dangerous play) the temporary suspension would be expected to be of considerably longer duration than the first. However, local playing conditions may mandate that cards are awarded only progressively, and not allow any second awards.\nUmpires, if the free hit would have been in the attacking 23\u00a0m area, may upgrade the free hit to a penalty corner for dissent or other misconduct after the free hit has been awarded.\nScoring.\nThe teams' object is to play the ball into their attacking circle and, from there, hit, push or flick the ball into the goal, scoring a goal. The team with more goals after 60 minutes wins the game. The playing time may be shortened, particularly when younger players are involved, or for some tournament play. If the game is played in a countdown clock, like ice hockey, a goal can only count if the ball completely crosses the goal line and into the goal \"before\" time expires, not when the ball leaves the stick in the act of shooting.\nIn many competitions (such as regular club competition, or in pool games in FIH international tournaments such as the Olympics or the World Cup), a tied result stands and the overall competition standings are adjusted accordingly. Since March 2013, when tie breaking is required, the official FIH Tournament Regulations mandate to no longer have extra time and go directly into a penalty shoot-out when a classification match ends in a tie. However, many associations follow the previous procedure consisting of two periods of 7.5 minutes of \"golden goal\" extra time during which the game ends as soon as one team scores.\nRule change procedure.\nThe FIH implemented a two-year rules cycle with the 2007\u201308 edition of the rules, with the intention that the rules be reviewed on a biennial basis. The 2009 rulebook was officially released in early March 2009 (effective 1 May 2009), however the FIH published the major changes in February. The current rule book is effective from 1 January 2019.\nThe FIH has adopted a policy of including major changes to the rules as \"Mandatory Experiments\", showing that they must be played at international level, but are treated as experimental and will be reviewed before the next rulebook is published and either changed, approved as permanent rules, or deleted.\nLocal rules.\nThere are sometimes minor variations in rules from competition to competition; for instance, the duration of matches is often varied for junior competitions or for carnivals. Different national associations also have slightly differing rules on player equipment.\nThe new Euro Hockey League and the Olympics has made major alterations to the rules to aid television viewers, such as splitting the game into four-quarters, and to try to improve player behavior, such as a two-minute suspension for green cards\u2014the latter was also used in the 2010 World Cup and 2016 Olympics. In the United States, the NCAA has its own rules for inter-collegiate competitions; high school associations similarly play to different rules, usually using the rules published by the National Federation of State High School Associations (NFHS). This article assumes FIH rules unless otherwise stated. USA Field Hockey produces an annual summary of the differences.\nIn the United States, the games at the junior high level consist of four 12-minute periods, while the high-school level consists of two 30-minute periods. Many private American schools play 12-minute quarters, and some have adopted FIH rules rather than NFHS rules.\nPlayers are required to wear mouth guards and shin guards in order to play the game. Also, there is a newer rule requiring certain types of sticks be used. In recent years, the NFHS rules have moved closer to FIH, but in 2011 a new rule requiring protective eyewear was introduced for the 2011 Fall season. Further clarification of NFHS's rule requiring protective eyewear states, \"effective 1 January 2019, all eye protection shall be permanently labeled with the current ASTM 2713 standard for field hockey.\" Metal 'cage style' goggles favored by US high school lacrosse and permitted in high school field hockey is prohibited under FIH rules.\nEquipment.\nField hockey stick.\nEach player carries a \"stick\" that normally measures between 80 and 95\u00a0cm (31\u201338\"); shorter or longer sticks are available. Sticks were traditionally made of wood, but are now often made also with fibreglass, kevlar or carbon fibre composites. Metal is forbidden from use in field hockey sticks, due to the risk of injury from sharp edges if the stick were to break. The stick has a rounded handle, has a J-shaped hook at the bottom, and is flattened on the left side (when looking down the handle with the hook facing upwards). All sticks must be right-handed; left-handed ones are prohibited.\nThere was traditionally a slight curve (called the bow, or rake) from the top to bottom of the face side of the stick and another on the 'heel' edge to the top of the handle (usually made according to the angle at which the handle part was inserted into the splice of the head part of the stick), which assisted in the positioning of the stick head in relation to the ball and made striking the ball easier and more accurate.\nThe hook at the bottom of the stick was only recently the tight curve (Indian style) that we have nowadays. The older 'English' sticks had a longer bend, making it very hard to use the stick on the reverse. For this reason players now use the tight curved sticks.\nThe handle makes up about the top third of the stick. It is wrapped in a grip similar to that used on tennis racket. The grip may be made of a variety of materials, including chamois leather, which improves grip in the wet and gives the stick a softer touch and different weighting it wrapped over a preexisting grip.\nIt was recently discovered that increasing the depth of the face bow made it easier to get high speeds from the dragflick and made the stroke easier to execute. At first, after this feature was introduced, the Hockey Rules Board placed a limit of 50\u00a0mm on the maximum depth of bow over the length of the stick but experience quickly demonstrated this to be excessive. New rules now limit this curve to under 25\u00a0mm so as to limit the power with which the ball can be flicked.\nField hockey ball.\nStandard field hockey balls are hard spherical balls, made of solid plastic (sometimes over a cork core), and are usually white, although they can be any colour as long as they contrast with the playing surface. The balls have a diameter of and a mass of . The ball is often covered with indentations to reduce aquaplaning that can cause an inconsistent ball speed on wet surfaces.\nGoalkeeping equipment.\nThe 2007 rulebook saw major changes regarding goalkeepers. A fully equipped goalkeeper must wear a helmet, leg guards and kickers, and like all players, they must carry a stick. Goalkeepers may use either a field player's stick or a specialised goalkeeping stick provided always the stick is of legal dimensions. Usually field hockey goalkeepers also wear extensive additional protective equipment including chest guards, padded shorts, heavily padded hand protectors, groin protectors, neck protectors and arm guards. A goalie may not cross the 23\u00a0m line, the sole exception to this being if the goalkeeper is to take a penalty stroke at the other end of the field, when the clock is stopped. The goalkeeper can also remove their helmet for this action. While goalkeepers are allowed to use their feet and hands to clear the ball, like field players they may only use the one side of their stick. Slide tackling is permitted as long as it is with the intention of clearing the ball, not aimed at a player.\nIt is now also even possible for teams to have a full eleven outfield players and no goalkeeper at all. No player may wear a helmet or other goalkeeping equipment, neither will any player be able to play the ball with any other part of the body than with their stick. This may be used to offer a tactical advantage, for example, if a team is trailing with only a short time to play, or to allow for play to commence if no goalkeeper or kit is available.\nTactics.\nThe basic tactic in field hockey, as in association football and many other team games, is to outnumber the opponent in a particular area of the field at a moment in time. When in possession of the ball this temporary numerical superiority can be used to pass the ball around opponents so that they cannot effect a tackle because they cannot get within playing reach of the ball and to further use this numerical advantage to gain time and create clear space for making scoring shots on the opponent's goal. When not in possession of the ball numerical superiority is used to isolate and channel an opponent in possession and 'mark out' any passing options so that an interception or a tackle may be made to gain possession. Highly skillful players can sometimes get the better of more than one opponent and retain the ball and successfully pass or shoot but this tends to use more energy than quick early passing.\nEvery player has a role depending on their relationship to the ball if the team communicates throughout the play of the game. There will be players on the ball (offensively \u2013 ball carriers; defensively \u2013 pressure, support players, and movement players.\nThe main methods by which the ball is moved around the field by players are a) passing b) pushing the ball and running with it controlled to the front or right of the body and c) \"dribbling\"; where the player controls the ball with the stick and moves in various directions with it to elude opponents. To make a pass the ball may be propelled with a pushing stroke, where the player uses their wrists to push the stick head through the ball while the stick head is in contact with it; the \"flick\" or \"scoop\", similar to the push but with an additional arm and leg and rotational actions to lift the ball off the ground; and the \"hit\", where a swing at ball is taken and contact with it is often made very forcefully, causing the ball to be propelled at velocities in excess of . In order to produce a powerful hit, usually for travel over long distances or shooting at the goal, the stick is raised higher and swung with maximum power at the ball, a stroke sometimes known as a \"drive\".\nTackles are made by placing the stick into the path of the ball or playing the stick head or shaft directly at the ball. To increase the effectiveness of the tackle, players will often place the entire stick close to the ground horizontally, thus representing a wider barrier. To avoid the tackle, the ball carrier will either pass the ball to a teammate using any of the push, flick, or hit strokes, or attempt to maneuver or \"drag\" the ball around the tackle, trying to deceive the tackler.\nIn recent years, the penalty corner has gained importance as a goal scoring opportunity. Particularly with the technical development of the drag flick. Tactics at penalty corners to set up time for a shot with a drag flick or a hit shot at the goal involve various complex plays, including multiple passes before deflections towards the goal is made but the most common method of shooting is the direct flick or hit at the goal.\nAt the highest level, field hockey is a fast moving, highly skilled game, with players using fast moves with the stick, quick accurate passing, and hard hits, in attempts to keep possession and move the ball towards the goal. Tackling with physical contact and otherwise physically obstructing players is not permitted. Some of the tactics used resemble football (soccer), but with greater ball speed.\nWith the 2009 changes to the rules regarding free hits in the attacking 23m area, the common tactic of hitting the ball hard into the circle was forbidden. Although at higher levels this was considered tactically risky and low-percentage at creating scoring opportunities, it was used with some effect to 'win' penalty corners by forcing the ball onto a defender's foot or to deflect high (and dangerously) off a defender's stick. The FIH felt it was a dangerous practice that could easily lead to raised deflections and injuries in the circle, which is often crowded at a free-hit situation, and outlawed it.\nInternational competition.\nThe biggest two field hockey tournaments are the Olympic Games tournament, and the Hockey World Cup, which is also held every 4 years. Apart from this, there is the Champions Trophy held each year for the six top-ranked teams. Field hockey has also been played at the Commonwealth Games since 1998. Amongst the men, Pakistan have won maximum Hockey World Cups 4 times. India lead in Olympic competition, having won 8 golds (6 successive in row). Amongst the women, Australia and Netherlands have 3 Olympic golds while Netherlands has clinched the World Cup 6 times. The Sultan Azlan Shah Hockey Tournament and Sultan Ibrahim Ismail Hockey Tournament for the junior team, both tournaments held annually in Malaysia, are becoming prominent field hockey tournaments where teams from around the world participate to win the cup.\nIndia and Pakistan dominated men's hockey until the early 1980s, winning eight Olympic golds and three of the first five world cups, respectively, but have become less prominent with the ascendancy of Belgium, the Netherlands, Germany, New Zealand, Australia, and Spain since the late 1980s, as grass playing surfaces were replaced with artificial turf (which conferred increased importance on athleticism). Other notable men's nations include Argentina, England (who combine with other British \"Home Nations\" to form the Great Britain side at Olympic events) and South Korea. Despite their recent drop in international rankings, Pakistan still holds the record of four World Cup wins.\nNetherlands, Australia and Argentina are the most successful national teams among women. The Netherlands was the predominant women's team before field hockey was added to Olympic events. In the early 1990s, Australia emerged as the strongest women's country although retirement of a number of players weakened the team. Argentina improved its play on the 2000s, heading IFH rankings in 2003, 2010 and 2013. Other prominent women's teams are China, South Korea, Germany and India.\n Argentina's men's team and the Netherlands' women's teams lead the FIH world rankings.\nFor a couple of years, Belgium has emerged as a leading nation, with a World Champions title (2018), a European Champions title (2019), a silver medal at the Olympics (2016) and a lead on the FIH men's team world ranking.\nThis is a list of the major International field hockey tournaments, in chronological order. Tournaments included are:\nAlthough invitational or not open to all countries, the following are also considered international tournaments:\nVariants.\nHockey5s.\nAs the name suggests, Hockey5s is a hockey variant which features five players on each team (which must include a goalkeeper). The field of play is 55\u00a0m long and 41.70\u00a0m wide\u2014 this is approximately half the size of a regular pitch. Few additional markings are needed as there is no penalty circle nor penalty corners; shots can be taken from anywhere on the pitch. Penalty strokes are replaced by a \"challenge\" which is like the one-on-one method used in a penalty shoot-out. The duration of the match is three 12-minute periods with an interval of two minutes between periods; golden goal periods are multiple 5-minute periods. The rules are simpler and it is intended that the game is faster, creating more shots on goal with less play in midfield, and more attractive to spectators.\nAn Asian qualification tournament for two places at the 2014 Youth Olympic Games was the first time an FIH event used the Hockey5s format. Hockey5s was also used for the Youth Olympic hockey tournament, and at the Pacific Games in 2015.\nHockey in popular culture.\nHockey features in F. J. Campbell's 2018 novel \"No Number Nine\", the final chapters of which are set at the Sydney 2000 Olympics. Field hockey has featured prominently in Indian films such as \"Chak De! India\", \"Soorma\" and \"Gold\"."}
{"id": "10887", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=10887", "title": "Finagle's law", "text": "Finagle's law of dynamic negatives (also known as Melody's law, Sod's Law or Finagle's corollary to Murphy's law) is usually rendered as \"Anything that can go wrong, will\u2014at the worst possible moment.\"\nThe term \"Finagle's law\" was first used by John W. Campbell Jr., the influential editor of \"Astounding Science Fiction\" (later \"Analog\"). He used it frequently in his editorials for many years in the 1940s to 1960s, but it never came into general usage the way Murphy's law has.\nVariants.\nOne variant (known as O'Toole's corollary of Finagle's law) favored among hackers is a takeoff on the second law of thermodynamics (related to the augmentation of entropy):\nIn the \"Star Trek\" episode \"Amok Time\" (written by Theodore Sturgeon in 1967), Captain Kirk tells Spock, \"As one of Finagle's laws puts it: 'Any home port the ship makes will be somebody else's, not mine.'\"\nThe term \"Finagle's law\" was popularized by science fiction author Larry Niven in several stories (for example, \"Protector\" [Ballantine Books paperback edition, 4th printing, p.\u00a023]), depicting a frontier culture of asteroid miners; this \"Belter\" culture professed a religion or running joke involving the worship of the dread god Finagle and his mad prophet Murphy.\n\"Finagle's law\" can also be the related belief \"Inanimate objects are out to get us\", also known as Resistentialism.\nSimilar to Finagle's law is the verbless phrase of the German novelist Friedrich Theodor Vischer: \"die T\u00fccke des Objekts\" (the perfidy of inanimate objects).\nA related concept, the \"Finagle factor\", is an \"ad hoc\" multiplicative or additive term in an equation, which can only be justified by the fact that it gives more correct results. Also known as Finagle's variable constant, it is sometimes defined as the correct answer divided by your answer.\nThe first written record of \"Finagle factor\" is probably a December 1962 article in The Michigan Technic, credited to Cambell, but bylined \"I Finaglin\" "}
{"id": "10888", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10888", "title": "Finagles law", "text": ""}
{"id": "10890", "revid": "40925441", "url": "https://en.wikipedia.org/wiki?curid=10890", "title": "Fundamental interaction", "text": "In physics, the fundamental interactions, also known as fundamental forces, are the interactions that do not appear to be reducible to more basic interactions. There are four fundamental interactions known to exist: the gravitational and electromagnetic interactions, which produce significant long-range forces whose effects can be seen directly in everyday life, and the strong and weak interactions, which produce forces at minuscule, subatomic distances and govern nuclear interactions. Some scientists hypothesize that a fifth force might exist, but these hypotheses remain speculative.\nEach of the known fundamental interactions can be described mathematically as a \"field\". The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.\nWithin the Standard Model, the strong interaction is carried by a particle called the gluon, and is responsible for quarks binding together to form hadrons, such as protons and neutrons. As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay. The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for the attraction between orbital electrons and atomic nuclei which holds atoms together, as well as chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than gravity, it tends to cancel itself out within large objects, so over large (astronomical) distances gravity tends to be the dominant force, and is responsible for holding together the large scale structures in the universe, such as planets, stars, and galaxies.\nMany theoretical physicists believe these fundamental forces to be related and to become unified into a single force at very high energies on a minuscule scale, the Planck scale, but particle accelerators cannot produce the enormous energies required to experimentally probe this. Devising a common theoretical framework that would explain the relation between the forces in a single theory is perhaps the greatest goal of today's theoretical physicists. The weak and electromagnetic forces have already been unified with the electroweak theory of Sheldon Glashow, Abdus Salam, and Steven Weinberg for which they received the 1979 Nobel Prize in physics. Progress is currently being made in uniting the electroweak and strong fields within what is called a Grand Unified Theory (GUT). A bigger challenge is to find a way to quantize the gravitational field, resulting in a theory of quantum gravity (QG) which would unite gravity in a common theoretical framework with the other three forces. Some theories, notably string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).\nHistory.\nClassical theory.\nIn his 1687 theory, Isaac Newton postulated space as an infinite and unalterable physical structure existing before, within, and around all objects while their states and relations unfold at a constant pace everywhere, thus absolute space and time. Inferring that all objects bearing mass approach at a constant rate, but collide by impact proportional to their masses, Newton inferred that matter exhibits an attractive force. His law of universal gravitation mathematically stated it to span the entire universe instantly (despite absolute time), or, if not actually a force, to be instant interaction among all objects (despite absolute space). As conventionally interpreted, Newton's theory of motion modelled a \"central force\" without a communicating medium. Thus Newton's theory violated the first principle of mechanical philosophy, as stated by Descartes, \"No action at a distance\". Conversely, during the 1820s, when explaining magnetism, Michael Faraday inferred a \"field\" filling space and transmitting that force. Faraday conjectured that ultimately, all forces unified into one.\nIn 1873, James Clerk Maxwell unified electricity and magnetism as effects of an electromagnetic field whose third consequence was light, travelling at constant speed in a vacuum. The electromagnetic field theory contradicted predictions of Newton's theory of motion, unless physical states of the luminiferous aether\u2014presumed to fill all space whether within matter or in a vacuum and to manifest the electromagnetic field\u2014aligned all phenomena and thereby held valid the Newtonian principle relativity or invariance.\nThe Standard Model.\nThe Standard Model of particle physics was developed throughout the latter half of the 20th century. In the Standard Model, the electromagnetic, strong, and weak interactions associate with elementary particles, whose behaviours are modelled in quantum mechanics (QM). For predictive success with QM's probabilistic outcomes, particle physics conventionally models QM events across a field set to special relativity, altogether relativistic quantum field theory (QFT). Force particles, called gauge bosons\u2014\"force carriers\" or \"messenger particles\" of underlying fields\u2014interact with matter particles, called fermions. Everyday matter is atoms, composed of three fermion types: up-quarks and down-quarks constituting, as well as electrons orbiting, the atom's nucleus. Atoms interact, form molecules, and manifest further properties through electromagnetic interactions among their electrons absorbing and emitting photons, the electromagnetic field's force carrier, which if unimpeded traverse potentially infinite distance. Electromagnetism's QFT is quantum electrodynamics (QED).\nThe electromagnetic interaction was modelled with the weak interaction, whose force carriers are W and Z bosons, traversing the minuscule distance, in electroweak theory (EWT). Electroweak interaction would operate at such high temperatures as soon after the presumed Big Bang, but, as the early universe cooled, split into electromagnetic and weak interactions. The strong interaction, whose force carrier is the gluon, traversing minuscule distance among quarks, is modeled in quantum chromodynamics (QCD). EWT, QCD, and the Higgs mechanism, whereby the Higgs field manifests Higgs bosons that interact with some quantum particles and thereby endow those particles with mass, comprise particle physics' Standard Model (SM). Predictions are usually made using calculational approximation methods, although such perturbation theory is inadequate to model some experimental observations (for instance bound states and solitons). Still, physicists widely accept the Standard Model as science's most experimentally confirmed theory.\nBeyond the Standard Model, some theorists work to unite the electroweak and strong interactions within a Grand Unified Theory (GUT). Some attempts at GUTs hypothesize \"shadow\" particles, such that every known matter particle associates with an undiscovered force particle, and vice versa, altogether supersymmetry (SUSY). Other theorists seek to quantize the gravitational field by the modelling behaviour of its hypothetical force carrier, the graviton and achieve quantum gravity (QG). One approach to QG is loop quantum gravity (LQG). Still other theorists seek both QG and GUT within one framework, reducing all four fundamental interactions to a Theory of Everything (ToE). The most prevalent aim at a ToE is string theory, although to model matter particles, it added SUSY to force particles\u2014and so, strictly speaking, became superstring theory. Multiple, seemingly disparate superstring theories were unified on a backbone, M-theory. Theories beyond the Standard Model remain highly speculative, lacking great experimental support.\nOverview of the fundamental interactions.\nIn the conceptual model of fundamental interactions, matter consists of fermions, which carry properties called charges and spin \u00b1 (intrinsic angular momentum \u00b1, where \u0127 is the reduced Planck constant). They attract or repel each other by exchanging bosons.\nThe interaction of any pair of fermions in perturbation theory can then be modelled thus:\nThe exchange of bosons always carries energy and momentum between the fermions, thereby changing their speed and direction. The exchange may also transport a charge between the fermions, changing the charges of the fermions in the process (e.g., turn them from one type of fermion to another). Since bosons carry one unit of angular momentum, the fermion's spin direction will flip from + to \u2212 (or vice versa) during such an exchange (in units of the reduced Planck's constant).\nBecause an interaction results in fermions attracting and repelling each other, an older term for \"interaction\" is force.\nAccording to the present understanding, there are four fundamental interactions or forces: gravitation, electromagnetism, the weak interaction, and the strong interaction. Their magnitude and behaviour vary greatly, as described in the table below. Modern physics attempts to explain every observed physical phenomenon by these fundamental interactions. Moreover, reducing the number of different interaction types is seen as desirable. Two cases in point are the unification of:\nBoth magnitude (\"relative strength\") and \"range\", as given in the table, are meaningful only within a rather complex theoretical framework. The table below lists properties of a conceptual scheme that is still the subject of ongoing research.\nThe modern (perturbative) quantum mechanical view of the fundamental forces other than gravity is that particles of matter (fermions) do not directly interact with each other, but rather carry a charge, and exchange virtual particles (gauge bosons), which are the interaction carriers or force mediators. For example, photons mediate the interaction of electric charges, and gluons mediate the interaction of color charges.\nThe interactions.\nGravity.\n\"Gravitation\" is by far the weakest of the four interactions at the atomic scale, where electromagnetic interactions dominate. But the idea that the weakness of gravity can easily be demonstrated by suspending a pin using a simple magnet (such as a refrigerator magnet) is fundamentally flawed. The only reason the magnet is able to hold the pin against the gravitational pull of the entire Earth is due to its relative proximity. There is clearly a short distance of separation between magnet and pin where a breaking point is reached, and due to the large mass of Earth this distance is quite small.\nThus gravitation is very important for macroscopic objects and over macroscopic distances for the following reasons. Gravitation:\nEven though electromagnetism is far stronger than gravitation, electrostatic attraction is not relevant for large celestial bodies, such as planets, stars, and galaxies, simply because such bodies contain equal numbers of protons and electrons and so have a net electric charge of zero. Nothing \"cancels\" gravity, since it is only attractive, unlike electric forces which can be attractive or repulsive. On the other hand, all objects having mass are subject to the gravitational force, which only attracts. Therefore, only gravitation matters on the large-scale structure of the universe.\nThe long range of gravitation makes it responsible for such large-scale phenomena as the structure of galaxies and black holes and it retards the expansion of the universe. Gravitation also explains astronomical phenomena on more modest scales, such as planetary orbits, as well as everyday experience: objects fall; heavy objects act as if they were glued to the ground, and animals can only jump so high.\nGravitation was the first interaction to be described mathematically. In ancient times, Aristotle hypothesized that objects of different masses fall at different rates. During the Scientific Revolution, Galileo Galilei experimentally determined that this hypothesis was wrong under certain circumstances \u2014 neglecting the friction due to air resistance, and buoyancy forces if an atmosphere is present (e.g. the case of a dropped air-filled balloon vs a water-filled balloon) all objects accelerate toward the Earth at the same rate. Isaac Newton's law of Universal Gravitation (1687) was a good approximation of the behaviour of gravitation. Our present-day understanding of gravitation stems from Einstein's General Theory of Relativity of 1915, a more accurate (especially for cosmological masses and distances) description of gravitation in terms of the geometry of spacetime.\nMerging general relativity and quantum mechanics (or quantum field theory) into a more general theory of quantum gravity is an area of active research. It is hypothesized that gravitation is mediated by a massless spin-2 particle called the graviton.\nAlthough general relativity has been experimentally confirmed (at least for weak fields ) on all but the smallest scales, there are rival theories of gravitation. Those taken seriously by the physics community all reduce to general relativity in some limit, and the focus of observational work is to establish limitations on what deviations from general relativity are possible.\nProposed extra dimensions could explain why the gravity force is so weak.\nElectroweak interaction.\nElectromagnetism and weak interaction appear to be very different at everyday low energies. They can be modeled using two different theories. However, above unification energy, on the order of 100 GeV, they would merge into a single electroweak force.\nThe electroweak theory is very important for modern cosmology, particularly on how the universe evolved. This is because shortly after the Big Bang, when the temperature was still above approximately 1015\u00a0K, the electromagnetic force and the weak force were still merged as a combined electroweak force.\nFor contributions to the unification of the weak and electromagnetic interaction between elementary particles, Abdus Salam, Sheldon Glashow and Steven Weinberg were awarded the Nobel Prize in Physics in 1979.\nElectromagnetism.\nElectromagnetism is the force that acts between electrically charged particles. This phenomenon includes the electrostatic force acting between charged particles at rest, and the combined effect of electric and magnetic forces acting between charged particles moving relative to each other.\nElectromagnetism has an infinite range like gravity, but is vastly stronger than it, and therefore describes a number of macroscopic phenomena of everyday experience such as friction, rainbows, lightning, and all human-made devices using electric current, such as television, lasers, and computers. Electromagnetism fundamentally determines all macroscopic, and many atomic levels, properties of the chemical elements, including all chemical bonding.\nIn a four kilogram (~1 gallon) jug of water there is\nformula_1\nof total electron charge. Thus, if we place two such jugs a meter apart, the electrons in one of the jugs repel those in the other jug with a force of\nformula_2\nThis force is many times larger than the weight of the planet Earth. The atomic nuclei in one jug also repel those in the other with the same force. However, these repulsive forces are canceled by the attraction of the electrons in jug A with the nuclei in jug B and the attraction of the nuclei in jug A with the electrons in jug B, resulting in no net force. Electromagnetic forces are tremendously stronger than gravity but cancel out so that for large bodies gravity dominates.\nElectrical and magnetic phenomena have been observed since ancient times, but it was only in the 19th century James Clerk Maxwell discovered that electricity and magnetism are two aspects of the same fundamental interaction. By 1864, Maxwell's equations had rigorously quantified this unified interaction. Maxwell's theory, restated using vector calculus, is the classical theory of electromagnetism, suitable for most technological purposes.\nThe constant speed of light in a vacuum (customarily described with a lowercase letter \"c\") can be derived from Maxwell's equations, which are consistent with the theory of special relativity. Albert Einstein's 1905 theory of special relativity, however, which follows from the observation that the speed of light is constant no matter how fast the observer is moving, showed that the theoretical result implied by Maxwell's equations has profound implications far beyond electromagnetism on the very nature of time and space.\nIn another work that departed from classical electro-magnetism, Einstein also explained the photoelectric effect by utilizing Max Planck's discovery that light was transmitted in 'quanta' of specific energy content based on the frequency, which we now call photons. Starting around 1927, Paul Dirac combined quantum mechanics with the relativistic theory of electromagnetism. Further work in the 1940s, by Richard Feynman, Freeman Dyson, Julian Schwinger, and Sin-Itiro Tomonaga, completed this theory, which is now called quantum electrodynamics, the revised theory of electromagnetism. Quantum electrodynamics and quantum mechanics provide a theoretical basis for electromagnetic behavior such as quantum tunneling, in which a certain percentage of electrically charged particles move in ways that would be impossible under the classical electromagnetic theory, that is necessary for everyday electronic devices such as transistors to function.\nWeak interaction.\nThe \"weak interaction\" or \"weak nuclear force\" is responsible for some nuclear phenomena such as beta decay. Electromagnetism and the weak force are now understood to be two aspects of a unified electroweak interaction \u2014 this discovery was the first step toward the unified theory known as the Standard Model. In the theory of the electroweak interaction, the carriers of the weak force are the massive gauge bosons called the W and Z bosons. The weak interaction is the only known interaction that does not conserve parity; it is left-right asymmetric. The weak interaction even violates CP symmetry but does conserve CPT.\nStrong interaction.\nThe \"strong interaction\", or \"strong nuclear force\", is the most complicated interaction, mainly because of the way it varies with distance. At distances greater than 10 femtometers, the strong force is practically unobservable. Moreover, it holds only inside the atomic nucleus.\nAfter the nucleus was discovered in 1908, it was clear that a new force, today known as the nuclear force, was needed to overcome the electrostatic repulsion, a manifestation of electromagnetism, of the positively charged protons. Otherwise, the nucleus could not exist. Moreover, the force had to be strong enough to squeeze the protons into a volume whose diameter is about 10\u221215 m, much smaller than that of the entire atom. From the short range of this force, Hideki Yukawa predicted that it was associated with a massive particle, whose mass is approximately 100 MeV.\nThe 1947 discovery of the pion ushered in the modern era of particle physics. Hundreds of hadrons were discovered from the 1940s to 1960s, and an extremely complicated theory of hadrons as strongly interacting particles was developed. Most notably:\nWhile each of these approaches offered deep insights, no approach led directly to a fundamental theory.\nMurray Gell-Mann along with George Zweig first proposed fractionally charged quarks in 1961. Throughout the 1960s, different authors considered theories similar to the modern fundamental theory of quantum chromodynamics (QCD) as simple models for the interactions of quarks. The first to hypothesize the gluons of QCD were Moo-Young Han and Yoichiro Nambu, who introduced the quark color charge and hypothesized that it might be associated with a force-carrying field. At that time, however, it was difficult to see how such a model could permanently confine quarks. Han and Nambu also assigned each quark color an integer electrical charge, so that the quarks were fractionally charged only on average, and they did not expect the quarks in their model to be permanently confined.\nIn 1971, Murray Gell-Mann and Harald Fritzsch proposed that the Han/Nambu color gauge field was the correct theory of the short-distance interactions of fractionally charged quarks. A little later, David Gross, Frank Wilczek, and David Politzer discovered that this theory had the property of asymptotic freedom, allowing them to make contact with experimental evidence. They concluded that QCD was the complete theory of the strong interactions, correct at all distance scales. The discovery of asymptotic freedom led most physicists to accept QCD since it became clear that even the long-distance properties of the strong interactions could be consistent with experiment if the quarks are permanently confined.\nAssuming that quarks are confined, Mikhail Shifman, Arkady Vainshtein and Valentine Zakharov were able to compute the properties of many low-lying hadrons directly from QCD, with only a few extra parameters to describe the vacuum. In 1980, Kenneth G. Wilson published computer calculations based on the first principles of QCD, establishing, to a level of confidence tantamount to certainty, that QCD will confine quarks. Since then, QCD has been the established theory of the strong interactions.\nQCD is a theory of fractionally charged quarks interacting by means of 8 bosonic particles called gluons. The gluons interact with each other, not just with the quarks, and at long distances the lines of force collimate into strings. In this way, the mathematical theory of QCD not only explains how quarks interact over short distances but also the string-like behavior, discovered by Chew and Frautschi, which they manifest over longer distances.\nHiggs interaction.\nAlthough not a gauge interaction nor generated by any diffeomorphism symmetry, the Higgs field's cubic Yukawa coupling produces a weakly attractive fifth interaction. After spontaneous symmetry breaking via the Higgs mechanism, Yukawa terms remain of the form\nwith Yukawa coupling formula_4, particle mass formula_5 (in eV), and Higgs vacuum expectation value . Hence coupled particles can exchange a virtual Higgs boson, yielding classical potentials of the form\nwith Higgs mass . Because the reduced Compton wavelength of the Higgs boson is so small (, comparable to the W and Z bosons), this potential has an effective range of a few attometers. Between two electrons, it begins roughly 1011 times weaker than the weak interaction, and grows exponentially weaker at non-zero distances.\nBeyond the Standard Model.\nNumerous theoretical efforts have been made to systematize the existing four fundamental interactions on the model of electroweak unification.\nGrand Unified Theories (GUTs) are proposals to show that the three fundamental interactions described by the Standard Model are all different manifestations of a single interaction with symmetries that break down and create separate interactions below some extremely high level of energy. GUTs are also expected to predict some of the relationships between constants of nature that the Standard Model treats as unrelated, as well as predicting gauge coupling unification for the relative strengths of the electromagnetic, weak, and strong forces (this was, for example, verified at the Large Electron\u2013Positron Collider in 1991 for supersymmetric theories).\nTheories of everything, which integrate GUTs with a quantum gravity theory face a greater barrier, because no quantum gravity theories, which include string theory, loop quantum gravity, and twistor theory, have secured wide acceptance. Some theories look for a graviton to complete the Standard Model list of force-carrying particles, while others, like loop quantum gravity, emphasize the possibility that time-space itself may have a quantum aspect to it.\nSome theories beyond the Standard Model include a hypothetical fifth force, and the search for such a force is an ongoing line of experimental research in physics. In supersymmetric theories, there are particles that acquire their masses only through supersymmetry breaking effects and these particles, known as moduli can mediate new forces. Another reason to look for new forces is the discovery that the expansion of the universe is accelerating (also known as dark energy), giving rise to a need to explain a nonzero cosmological constant, and possibly to other modifications of general relativity. Fifth forces have also been suggested to explain phenomena such as CP violations, dark matter, and dark flow."}
{"id": "10891", "revid": "43393", "url": "https://en.wikipedia.org/wiki?curid=10891", "title": "Floppy disk", "text": "A floppy disk or floppy diskette (sometimes casually referred to as a floppy or diskette) is a type of disk storage composed of a thin and flexible disk of a magnetic storage medium in a square or nearly square plastic enclosure lined with a fabric that removes dust particles from the spinning disk. Floppy disks are read from and written to by a floppy disk drive (FDD).\nThe first floppy disks, invented and made by IBM, had a disk diameter of . Subsequently and then became a ubiquitous form of data storage and transfer into the first years of the 21st century. By 2006, however, computers were rarely manufactured with installed floppy disk drives; -inch floppy disks can still be used with an external USB floppy disk drive. USB drives for -inch, 8-inch, and non-standard floppy disks are rare to non-existent. Some individuals and organizations continue to use older equipment to read or transfer data from floppy disks.\nFloppy disks were so common in late 20th-century culture that many electronic and software programs continue to use save icons that look like floppy disks well into the 21st century. While floppy disk drives still have some limited uses, especially with legacy industrial computer equipment, they have been superseded by data storage methods with much greater data storage capacity and data transfer speed, such as USB flash drives, memory cards, optical discs, and storage available through local computer networks and cloud storage.\nHistory.\nThe first commercial floppy disks, developed in the late 1960s, were in diameter; they became commercially available in 1971 as a component of IBM products and then were sold separately starting in 1972 by Memorex and others. These disks and associated drives were produced and improved upon by IBM and other companies such as Memorex, Shugart Associates, and Burroughs Corporation. The term \"floppy disk\" appeared in print as early as 1970, and although IBM announced its first media as the \"Type 1 Diskette\" in 1973, the industry continued to use the terms \"floppy disk\" or \"floppy\".\nIn 1976, Shugart Associates introduced the -inch FDD. By 1978, there were more than 10 manufacturers producing such FDDs. There were competing floppy disk formats, with hard- and soft-sector versions and encoding schemes such as differential Manchester encoding (DM), modified frequency modulation (MFM), M2FM and group coded recording (GCR). The -inch format displaced the 8-inch one for most uses, and the hard-sectored disk format disappeared. The most common capacity of the -inch format in DOS-based PCs was 360\u00a0KB, for the Double-Sided Double-Density (DSDD) format using MFM encoding. In 1984, IBM introduced with its PC-AT model the 1.2\u00a0MB dual-sided -inch floppy disk, but it never became very popular. IBM started using the 720\u00a0KB double density -inch microfloppy disk on its Convertible laptop computer in 1986 and the 1.44\u00a0MB high-density version with the IBM Personal System/2 (PS/2) line in 1987. These disk drives could be added to older PC models. In 1988, IBM introduced a drive for 2.88\u00a0MB Double-Sided Extended-Density (DSED) diskettes in its top-of-the-line PS/2 models, but this was a commercial failure.\nThroughout the early 1980s, limits of the -inch format became clear. Originally designed to be more practical than the 8-inch format, it was becoming considered too large; as the quality of recording media grew, data could be stored in a smaller area. Several solutions were developed, with drives at 2-, -, 3-, -, - and 4-inches (and Sony's disk) offered by various companies. They all shared several advantages over the old format, including a rigid case with a sliding metal (or later, sometimes plastic) shutter over the head slot, which helped protect the delicate magnetic medium from dust and damage, and a sliding write protection tab, which was far more convenient than the adhesive tabs used with earlier disks. The large market share of the well-established -inch format made it difficult for these diverse mutually-incompatible new formats to gain significant market share. A variant on the Sony design, introduced in 1982 by many manufacturers, was then rapidly adopted. By 1988, the -inch was outselling the -inch.\nGenerally, the term floppy disk persisted, even though later style floppy disks have a rigid case around an internal floppy disk.\nBy the end of the 1980s, -inch disks had been superseded by -inch disks. During this time, PCs frequently came equipped with drives of both sizes. By the mid-1990s, -inch drives had virtually disappeared, as the -inch disk became the predominant floppy disk. The advantages of the -inch disk were its higher capacity, its smaller physical size, and its rigid case which provided better protection from dirt and other environmental risks. If a person touches the exposed disk surface of a -inch disk through the drive hole, fingerprints may foul the disk\u2014and later the disk drive head if the disk is subsequently loaded into a drive\u2014and it is also easily possible to damage a disk of this type by folding or creasing it, usually rendering it at least partly unreadable. However, largely due to its simpler construction (with no metal parts) the -inch disk unit price was lower throughout its history, usually in the range of a third to a half that of a -inch disk.\nPrevalence.\nFloppy disks became commonplace during the 1980s and 1990s in their use with personal computers to distribute software, transfer data, and create backups. Before hard disks became affordable to the general population, floppy disks were often used to store a computer's operating system (OS). Most home computers from that time have an elementary OS and BASIC stored in read-only memory (ROM), with the option of loading a more advanced OS from a floppy disk.\nBy the early 1990s, the increasing software size meant large packages like Windows or Adobe Photoshop required a dozen disks or more. In 1996, there were an estimated five billion standard floppy disks in use. Then, distribution of larger packages was gradually replaced by CD-ROMs, DVDs, and online distribution.\nAn attempt to enhance the existing -inch designs was the SuperDisk in the late 1990s, using very narrow data tracks and a high precision head guidance mechanism with a capacity of 120 MB and backward-compatibility with standard -inch floppies; a format war briefly occurred between SuperDisk and other high-density floppy-disk products, although ultimately recordable CDs/DVDs, solid-state flash storage, and eventually online storage would render all these removable disk formats obsolete. External USB-based floppy disk drives are still available, and many modern systems provide firmware support for booting from such drives.\nGradual transition to other formats.\nIn the mid-1990s, mechanically incompatible higher-density floppy disks were introduced, like the Iomega Zip disk. Adoption was limited by the competition between proprietary formats and the need to buy expensive drives for computers where the disks would be used. In some cases, failure in market penetration was exacerbated by the release of higher-capacity versions of the drive and media being not backward-compatible with the original drives, dividing the users between new and old adopters. Consumers were wary of making costly investments into unproven and rapidly changing technologies, so none of the technologies became the established standard.\nApple introduced the iMac G3 in 1998 with a CD-ROM drive but no floppy drive; this made USB-connected floppy drives popular accessories, as the iMac came without any writable removable media device.\nRecordable CDs were touted as an alternative, because of the greater capacity, compatibility with existing CD-ROM drives, and\u2014with the advent of re-writeable CDs and packet writing\u2014a similar reusability as floppy disks.\nHowever, CD-R/RWs remained mostly an archival medium, not a medium for exchanging data or editing files on the medium itself, because there was no common standard for packet writing which allowed for small updates.\nOther formats, such as Magneto-optical discs, had the flexibility of floppy disks combined with greater capacity, but remained niche due to costs.\nHigh-capacity backward compatible floppy technologies became popular for a while and were sold as an option or even included in standard PCs, but in the long run, their use was limited to professionals and enthusiasts.\nFlash-based USB-thumb drives finally were a practical and popular replacement, that supported traditional file systems and all common usage scenarios of floppy disks. As opposed to other solutions, no new drive type or special software was required that impeded adoption, since all that was necessary was an already common USB port.\nUse in the early 21st century.\nBy 2002, most manufacturers still provided floppy disk drives as standard equipment to meet user demand for file-transfer and an emergency boot device, as well as for the general secure feeling of having the familiar device. By this time, the retail cost of a floppy drive had fallen to around $20 (), so there was little financial incentive to omit the device from a system. Subsequently, enabled by the widespread support for USB flash drives and BIOS boot, manufacturers and retailers progressively reduced the availability of floppy disk drives as standard equipment. In February 2003, Dell, a leading computer company at the time, announced that floppy drives would no longer be pre-installed on Dell Dimension home computers, although they were still available as a selectable option and purchasable as an aftermarket OEM add-on. By January 2007, only 2% of computers sold in stores contained built-in floppy disk drives.\nFloppy disks are used for emergency boots in aging systems lacking support for other bootable media and for BIOS updates, since most BIOS and firmware programs can still be executed from bootable floppy disks. If BIOS updates fail or become corrupt, floppy drives can sometimes be used to perform a recovery. The music and theatre industries still use equipment requiring standard floppy disks (e.g. synthesizers, samplers, drum machines, sequencers, and lighting consoles). Industrial automation equipment such as programmable machinery and industrial robots may not have a USB interface; data and programs are then loaded from disks, damageable in industrial environments. This equipment may not be replaced due to cost or requirement for continuous availability; existing software emulation and virtualization do not solve this problem because a customized operating system is used that has no drivers for USB devices. Hardware floppy disk emulators can be made to interface floppy-disk controllers to a USB port that can be used for flash drives.\nIn May 2016, the United States Government Accountability Office released a report that covered the need to upgrade or replace legacy computer systems within federal agencies. According to this document, old IBM Series/1 minicomputers running on 8-inch floppy disks are still used to coordinate \"the operational functions of the United States' nuclear forces\". The government planned to update some of the technology by the end of the 2017 fiscal year.\nExternal USB floppy drives function as a USB mass storage device class. Windows 10 removed the driver for internal floppy drives, which are a different device. External USB floppy drives continue to function.\nThe British Airways Boeing 747-400 fleet, up to its retirement in 2020, used 3.5 inch floppy disks to load avionics software.\nLegacy.\nFor more than two decades, the floppy disk was the primary external writable storage device used. Most computing environments before the 1990s were non-networked, and floppy disks were the primary means to transfer data between computers, a method known informally as sneakernet. Unlike hard disks, floppy disks are handled and seen; even a novice user can identify a floppy disk. Because of these factors, a picture of a -inch floppy disk became an interface metaphor for saving data. The floppy disk symbol is still used by software on user-interface elements related to saving files, such as the release of Microsoft Office 2019, even though the physical floppy disks are largely obsolete.\nDesign.\nStructure.\n8-inch and -inch disks.\nThe 8-inch and -inch floppy disks contain a magnetically coated round plastic medium with a large circular hole in the center for a drive's spindle. The medium is contained in a square plastic cover that has a small oblong opening in both sides to allow the drive's heads to read and write data and a large hole in the center to allow the magnetic medium to spin by rotating it from its middle hole.\nInside the cover are two layers of fabric with the magnetic medium sandwiched in the middle. The fabric is designed to reduce friction between the medium and the outer cover, and catch particles of debris abraded off the disk to keep them from accumulating on the heads. The cover is usually a one-part sheet, double-folded with flaps glued or spot-welded together.\nA small notch on the side of the disk identifies that it is writable, detected by a mechanical switch or phototransistor above it; if it is not present, the disk can be written; in the 8-inch disk the notch is covered to enable writing while in the -inch disk the notch is open to enable writing. Tape may be used over the notch to change the mode of the disk. Punch devices were sold to convert read-only disks to writable ones and enable writing on the unused side of single sided disks; such modified disks became known as flippy disks.\nAnother LED/photo-transistor pair located near the center of the disk detects the \"index hole\" once per rotation in the magnetic disk; it is used to detect the angular start of each track and whether or not the disk is rotating at the correct speed. Early 8\u2011inch and \u2011inch disks had physical holes for each sector and were termed \"hard sectored\" disks. Later \"soft-sectored\" disks have only one index hole, and sector position is determined by the disk controller or low-level software from patterns marking the start of a sector. Generally, the same drives are used to read and write both types of disks, with only the disks and controllers differing. Some operating systems using soft sectors, such as Apple DOS, do not use the index hole, and the drives designed for such systems often lack the corresponding sensor; this was mainly a hardware cost-saving measure.\n-inch disk.\nThe core of the -inch disk is the same as the other two disks, but the front has only a label and a small opening for reading and writing data, protected by the shutter\u2014a spring-loaded metal or plastic cover, pushed to the side on entry into the drive. Rather than having a hole in the center, it has a metal hub which mates to the spindle of the drive. Typical -inch disk magnetic coating materials are:\nTwo holes at the bottom left and right indicate whether the disk is write-protected and whether it is high-density; these holes are spaced as far apart as the holes in punched A4 paper, allowing write-protected high-density floppies to be clipped into standard ring binders. The dimensions of the disk shell are not quite square: its width is slightly less than its depth, so that it is impossible to insert the disk into a drive slot sideways (i.e. rotated 90 degrees from the correct shutter-first orientation). A diagonal notch at top right ensures that the disk is inserted into the drive in the correct orientation\u2014not upside down or label-end first\u2014and an arrow at top left indicates direction of insertion. The drive usually has a button that, when pressed, ejects the disk with varying degrees of force, the discrepancy due to the ejection force provided by the spring of the shutter. In IBM PC compatibles, Commodores, Apple II/IIIs, and other non-Apple-Macintosh machines with standard floppy disk drives, a disk may be ejected manually at any time. The drive has a disk-change switch that detects when a disk is ejected or inserted. Failure of this mechanical switch is a common source of disk corruption if a disk is changed and the drive (and hence the operating system) fails to notice.\nOne of the chief usability problems of the floppy disk is its vulnerability; even inside a closed plastic housing, the disk medium is highly sensitive to dust, condensation and temperature extremes. As with all magnetic storage, it is vulnerable to magnetic fields. Blank disks have been distributed with an extensive set of warnings, cautioning the user not to expose it to dangerous conditions. Rough treatment or removing the disk from the drive while the magnetic media is still spinning is likely to cause damage to the disk, drive head, or stored data. On the other hand, the \u2011inch floppy has been lauded for its mechanical usability by human\u2013computer interaction expert Donald Norman:\nOperation.\nA spindle motor in the drive rotates the magnetic medium at a certain speed, while a stepper motor-operated mechanism moves the magnetic read/write heads radially along the surface of the disk. Both read and write operations require the media to be rotating and the head to contact the disk media, an action originally accomplished by a disk-load solenoid. Later drives held the heads out of contact until a front-panel lever was rotated (-inch) or disk insertion was complete (-inch). To write data, current is sent through a coil in the head as the media rotates. The head's magnetic field aligns the magnetization of the particles directly below the head on the media. When the current is reversed the magnetization aligns in the opposite direction, encoding one bit of data. To read data, the magnetization of the particles in the media induce a tiny voltage in the head coil as they pass under it. This small signal is amplified and sent to the floppy disk controller, which converts the streams of pulses from the media into data, checks it for errors, and sends it to the host computer system.\nFormatting.\nA blank unformatted diskette has a coating of magnetic oxide with no magnetic order to the particles. During formatting, the magnetizations of the particles are aligned forming tracks, each broken up into sectors, enabling the controller to properly read and write data. The tracks are concentric rings around the center, with spaces between tracks where no data is written; gaps with padding bytes are provided between the sectors and at the end of the track to allow for slight speed variations in the disk drive, and to permit better interoperability with disk drives connected to other similar systems.\nEach sector of data has a header that identifies the sector location on the disk. A cyclic redundancy check (CRC) is written into the sector headers and at the end of the user data so that the disk controller can detect potential errors.\nSome errors are soft and can be resolved by automatically re-trying the read operation; other errors are permanent and the disk controller will signal a failure to the operating system if multiple attempts to read the data still fail.\nInsertion and ejection.\nAfter a disk is inserted, a catch or lever at the front of the drive is manually lowered to prevent the disk from accidentally emerging, engage the spindle clamping hub, and in two-sided drives, engage the second read/write head with the media.\nIn some -inch drives, insertion of the disk compresses and locks an ejection spring which partially ejects the disk upon opening the catch or lever. This enables a smaller concave area for the thumb and fingers to grasp the disk during removal.\nNewer -inch drives and all -inch drives automatically engage the spindle and heads when a disk is inserted, doing the opposite with the press of the eject button.\nOn Apple Macintosh computers with built-in floppy drives, the ejection button is replaced by software controlling an ejection motor which only does so when the operating system no longer needs to access the drive. The user could drag the image of the floppy drive to the trash can on the desktop to eject the disk. In the case of a power failure or drive malfunction, a loaded disk can be removed manually by inserting a straightened paper clip into a small hole at the drive's front panel, just as one would do with a CD-ROM drive in a similar situation.\nFinding track zero.\nBefore a disk can be accessed, the drive needs to synchronize its head position with the disk tracks. In some drives, this is accomplished with a Track Zero Sensor, while for others it involves the drive head striking an immobile reference surface.\nIn either case, the head is moved so that it is approaching track zero position of the disk. When a drive with the sensor has reached track zero, the head stops moving immediately and is correctly aligned. For a drive without the sensor, the mechanism attempts to move the head the maximum possible number of positions needed to reach track zero, knowing that once this motion is complete, the head will be positioned over track zero.\nSome drive mechanisms such as the Apple II -inch drive without a track zero sensor, produce characteristic mechanical noises when trying to move the heads past the reference surface. This physical striking is responsible for the -inch drive clicking during the boot of an Apple II, and the loud rattles of its DOS and ProDOS when disk errors occurred and track zero synchronization was attempted.\nFinding sectors.\nAll 8 inch and some -inch drives used a mechanical method to locate sectors, known as either \"hard sectors\" or \"soft sectors\", and is the purpose of the small hole in the jacket, off to the side of the spindle hole. A light beam sensor detects when a punched hole in the disk is visible through the hole in the jacket.\nFor a soft-sectored disk, there is only a single hole, which is used to locate the first sector of each track. Clock timing is then used to find the other sectors behind it, which requires precise speed regulation of the drive motor.\nFor a hard-sectored disk, there are many holes, one for each sector row, plus an additional hole in a half-sector position, that is used to indicate sector zero.\nThe Apple II computer system is notable in that it did not have an index hole sensor and ignored the presence of hard or soft sectoring. Instead, it used special repeating data synchronization patterns written to the disk between each sector, to assist the computer in finding and synchronizing with the data in each track.\nThe later -inch drives of the mid-1980s did not use sector index holes, but instead also used synchronization patterns.\nMost -inch drives used a constant speed drive motor and contain the same number of sectors across all tracks. In order to fit more data onto a disk, some -inch drives instead use variable speed drive motor than spins more slowly as the head moves away from the center of the disk. This allows more consecutive sectors to be written to the longer middle and outer tracks as the track length increases.\nSizes.\nDifferent sizes of floppy disks are mechanically incompatible, and disks can fit only one size of drive. Drive assemblies with both -inch and -inch slots were available during the transition period between the sizes, but they contained two separate drive mechanisms. In addition, there are many subtle, usually software-driven incompatibilities between the two. -inch disks formatted for use with Apple II computers would be unreadable and treated as unformatted on a Commodore. As computer platforms began to form, attempts were made at interchangeability. For example, the \"SuperDrive\" included from the Macintosh SE to the Power Macintosh G3 could read, write and format IBM PC format -inch disks, but few IBM-compatible computers had drives that did the reverse. 8-inch, -inch and -inch drives were manufactured in a variety of sizes, most to fit standardized drive bays. Alongside the common disk sizes were non-classical sizes for specialized systems.\n8-inch floppy disk.\nThe first floppy disk was 8 inches in diameter, was protected by a flexible plastic jacket and was a read-only device used by IBM as a way of loading microcode. Read/write floppy disks and their drives became available in 1972 but it was IBM's 1973 introduction of the 3740 data entry system that began the establishment of floppy disks, called by IBM the \"Diskette 1\", as an industry standard for information interchange. The formatted diskette for this system stored 242,944 bytes. Early microcomputers used for engineering, business, or word processing often used one or more 8-inch disk drives for removable storage; the CP/M operating system was developed for microcomputers with 8-inch drives.\nThe family of 8-inch disks and drives increased over time and later versions could store up to 1.2\u00a0MB; many microcomputer applications did not need that much capacity on one disk, so a smaller size disk with lower-cost media and drives was feasible. The -inch drive succeeded the 8-inch size in many applications, and developed to about the same storage capacity as the original 8-inch size, using higher-density media and recording techniques.\n-inch floppy disk.\nThe head gap of an 80\u2011track high-density (1.2\u00a0MB in the MFM format) \u2011inch drive (a.k.a. Mini diskette, Mini disk, or Minifloppy) is smaller than that of a 40\u2011track double-density (360\u00a0KB if double-sided) drive but can also format, read and write 40\u2011track disks provided the controller supports double stepping or has a switch to do so. -inch 80-track drives were also called hyper drives.\nA blank 40\u2011track disk formatted and written on an 80\u2011track drive can be taken to its native drive without problems, and a disk formatted on a 40\u2011track drive can be used on an 80\u2011track drive. Disks written on a 40\u2011track drive and then updated on an 80 track drive become unreadable on any 40\u2011track drives due to track width incompatibility.\nSingle-sided disks were coated on both sides, despite the availability of more expensive double sided disks. The reason usually given for the higher price was that double sided disks were certified error-free on both sides of the media. Double-sided disks could be used in some drives for single-sided disks, as long as an index signal was not needed. This was done one side at a time, by turning them over (flippy disks); more expensive dual-head drives which could read both sides without turning over were later produced, and eventually became used universally.\n-inch floppy disk.\nIn the early 1980s, many manufacturers introduced smaller floppy drives and media in various formats. A consortium of 21 companies eventually settled on a a.k.a. \"Micro diskette\", \"Micro disk\", or \"Micro floppy\", similar to a Sony design but improved to support both single-sided and double-sided media, with formatted capacities generally of 360\u00a0KB and 720\u00a0KB respectively. Single-sided drives shipped in 1983, and double sided in 1984. What became the most common format, the double-sided, high-density (HD) \"1.44\u00a0MB\" (actually 1440\u00a0KiB) disk drive, first shipped in 1986. The first Macintosh computers use single-sided -inch floppy disks, but with 400\u00a0KB formatted capacity. These were followed in 1986 by double-sided 800\u00a0KB floppies. The higher capacity was achieved at the same recording density by varying the disk rotation speed with head position so that the linear speed of the disk was closer to constant. Later Macs could also read and write \"1.44\u00a0MB\" HD disks in PC format with fixed rotation speed.\nAll -inch disks have a rectangular hole in one corner which, if obstructed, write-enables the disk. A sliding detented piece can be moved to block or reveal the part of the rectangular hole that is sensed by the drive. The HD \"1.44\u00a0MB\" disks have a second, unobstructed hole in the opposite corner which identifies them as being of that capacity.\nIn IBM-compatible PCs, the three densities of -inch floppy disks are backwards-compatible: higher density drives can read, write and format lower density media. It is also possible to format a disk at a lower density than it was intended for, but only if the disk is first thoroughly demagnetized with a bulk eraser, as the high density format is magnetically stronger and will prevent the disk from working in lower density modes.\nWriting at different densities than disks were intended for, sometimes by altering or drilling holes, was possible but not supported by manufacturers. A hole on one side of a \u2011inch disk can be altered as to make some disk drives and operating systems treat the disk as one of higher or lower density, for bidirectional compatibility or economical reasons. Some computers, such as the PS/2 and Acorn Archimedes, ignored these holes altogether.\nOther sizes.\nOther, smaller, floppy sizes were proposed, especially for portable or pocket-sized devices that needed a smaller storage device. 3-inch disks similar in construction to -inch were manufactured and used for a time, particularly by Amstrad computers and word processors. A 2-inch nominal size known as the Video Floppy was introduced by Sony for use with its Mavica still video camera. An incompatible 2-inch floppy was produced by Fujifilm called the LT-1 was used in the Zenith Minisport portable computer. Neither of these sizes achieved much market success.\nSizes, performance and capacity.\nFloppy disk size is often referred to in inches, even in countries using metric and though the size is defined in metric. The ANSI specification of -inch disks is entitled in part \"90 mm (3.5 inch)\" though 90\u00a0mm is closer to 3.54\u00a0inches. Formatted capacities are generally set in terms of kilobytes and megabytes.\nData is generally written to floppy disks in sectors (angular blocks) and tracks (concentric rings at a constant radius). For example, the HD format of -inch floppy disks uses 512 bytes per sector, 18 sectors per track, 80 tracks per side and two sides, for a total of 1,474,560 bytes per disk. Some disk controllers can vary these parameters at the user's request, increasing storage on the disk, although they may not be able to be read on machines with other controllers. For example, Microsoft applications were often distributed on -inch 1.68\u00a0MB DMF disks formatted with 21 sectors instead of 18; they could still be recognized by a standard controller. On the IBM PC, MSX and most other microcomputer platforms, disks were written using a constant angular velocity (CAV) format, with the disk spinning at a constant speed and the sectors holding the same amount of information on each track regardless of radial location.\nBecause the sectors have constant angular size, the 512 bytes in each sector are compressed more near the disk's center. A more space-efficient technique would be to increase the number of sectors per track toward the outer edge of the disk, from 18 to 30 for instance, thereby keeping nearly constant the amount of physical disk space used for storing each sector; an example is zone bit recording. Apple implemented this in early Macintosh computers by spinning the disk more slowly when the head was at the edge, while maintaining the data rate, allowing 400\u00a0KB of storage per side and an extra 80\u00a0KB on a double-sided disk. This higher capacity came with a disadvantage: the format used a unique drive mechanism and control circuitry, meaning that Mac disks could not be read on other computers. Apple eventually reverted to constant angular velocity on HD floppy disks with their later machines, still unique to Apple as they supported the older variable-speed formats.\nDisk formatting is usually done by a utility program supplied by the computer OS manufacturer; generally, it sets up a file storage directory system on the disk, and initializes its sectors and tracks. Areas of the disk unusable for storage due to flaws can be locked (marked as \"bad sectors\") so that the operating system does not attempt to use them. This was time-consuming so many environments had quick formatting which skipped the error checking process. When floppy disks were often used, disks pre-formatted for popular computers were sold. The unformatted capacity of a floppy disk does not include the sector and track headings of a formatted disk; the difference in storage between them depends on the drive's application. Floppy disk drive and media manufacturers specify the unformatted capacity (for example, 2\u00a0MB for a standard -inch HD floppy). It is implied that this should not be exceeded, since doing so will most likely result in performance problems. DMF was introduced permitting 1.68\u00a0MB to fit onto an otherwise standard -inch disk; utilities then appeared allowing disks to be formatted as such.\nMixtures of decimal prefixes and binary sector sizes require care to properly calculate total capacity. Whereas semiconductor memory naturally favors powers of two (size doubles each time an address pin is added to the integrated circuit), the capacity of a disk drive is the product of sector size, sectors per track, tracks per side and sides (which in hard disk drives with multiple platters can be greater than 2). Although other sector sizes have been known in the past, formatted sector sizes are now almost always set to powers of two (256 bytes, 512 bytes, etc.), and, in some cases, disk capacity is calculated as multiples of the sector size rather than only in bytes, leading to a combination of decimal multiples of sectors and binary sector sizes. For example, 1.44\u00a0MB -inch HD disks have the \"M\" prefix peculiar to their context, coming from their capacity of 2,880 512-byte sectors (1,440 KiB), consistent with neither a decimal megabyte nor a binary mebibyte (MiB). Hence, these disks hold 1.47\u00a0MB or 1.41\u00a0MiB. Usable data capacity is a function of the disk format used, which in turn is determined by the FDD controller and its settings. Differences between such formats can result in capacities ranging from approximately 1300 to 1760 KiB (1.80\u00a0MB) on a standard -inch high-density floppy (and up to nearly 2\u00a0MB with utilities such as 2M/2MGUI). The highest capacity techniques require much tighter matching of drive head geometry between drives, something not always possible and unreliable. For example, the LS-240 drive supports a 32\u00a0MB capacity on standard -inch HD disks, but it is, however, a write-once technique, and requires its own drive.\nThe raw maximum transfer rate of -inch ED floppy drives (2.88\u00a0MB) is nominally 1,000\u00a0kilobits/s, or approximately 83% that of single-speed CD\u2011ROM (71% of audio CD). This represents the speed of raw data bits moving under the read head; however, the effective speed is somewhat less due to space used for headers, gaps and other format fields and can be even further reduced by delays to seek between tracks."}
{"id": "10892", "revid": "799598564", "url": "https://en.wikipedia.org/wiki?curid=10892", "title": "Fullerenes", "text": ""}
{"id": "10893", "revid": "32932700", "url": "https://en.wikipedia.org/wiki?curid=10893", "title": "Fencing", "text": "Fencing is a group of three related combat sports. The three disciplines in modern fencing are the foil, the \u00e9p\u00e9e, and the sabre (also \"saber\"); winning points are made through the weapon's contact with an opponent. A fourth discipline, singlestick, appeared in the 1904 Olympics but was dropped after that, and is not a part of modern fencing. Fencing was one of the first sports to be played in the Olympics. Based on the traditional skills of swordsmanship, the modern sport arose at the end of the 19th century, with the Italian school having modified the historical European martial art of classical fencing, and the French school later refining the Italian system. There are three forms of modern fencing, each of which uses a different kind of weapon and has different rules; thus the sport itself is divided into three competitive scenes: foil, \u00e9p\u00e9e, and sabre. Most competitive fencers choose to specialize in one weapon only.\nCompetitive fencing is one of the five activities which have been featured in every modern Olympic Games, the other four being athletics, cycling, swimming, and gymnastics.\nCompetitive fencing.\nGoverning body.\nFencing is governed by F\u00e9d\u00e9ration Internationale d'Escrime (FIE). Today, its head office is in Lausanne, Switzerland. The FIE is composed of 145 national federations, each of which is recognised by its state Olympic Committee as the sole representative of Olympic-style fencing in that country.\nRules.\nThe FIE maintains the current rules used by FIE sanctioned international events, including world cups, world championships and the Olympic Games. The FIE handles proposals to change the rules the first year after an Olympic year in the annual congress. The US Fencing Association has slightly different rules, but usually adheres to FIE standards.\nHistory.\nFencing traces its roots to the development of swordsmanship for duels and self defense. Fencing is believed to have originated in Spain; some of the most significant books on fencing were written by Spanish fencers. \"Treatise on Arms\" was written by Diego de Valera between 1458 and 1471 and is one of the oldest surviving manuals on western fencing (in spite of the title, the book of Diego Valera was on heraldry, not about fencing) shortly before dueling came under official ban by the Catholic Monarchs. In conquest, the Spanish forces carried fencing around the world, particularly to southern Italy, one of the major areas of strife between both nations. Fencing was mentioned in the play \"The Merry Wives of Windsor\" written sometime prior to 1602.\nThe mechanics of modern fencing originated in the 18th century in an Italian school of fencing of the Renaissance, and under their influence, were improved by the French school of fencing. The Spanish school of fencing stagnated and was replaced by the Italian and French schools.\nDevelopment into a sport.\nThe shift towards fencing as a sport rather than as military training happened from the mid-18th century, and was led by Domenico Angelo, who established a fencing academy, Angelo's School of Arms, in Carlisle House, Soho, London in 1763. There, he taught the aristocracy the fashionable art of swordsmanship. His school was run by three generations of his family and dominated the art of European fencing for almost a century.\nHe established the essential rules of posture and footwork that still govern modern sport fencing, although his attacking and parrying methods were still much different from current practice. Although he intended to prepare his students for real combat, he was the first fencing master to emphasize the health and sporting benefits of fencing more than its use as a killing art, particularly in his influential book \"L'\u00c9cole des armes\" (\"The School of Fencing\"), published in 1763.\nBasic conventions were collated and set down during the 1880s by the French fencing master Camille Pr\u00e9vost. It was during this time that many officially recognised fencing associations began to appear in different parts of the world, such as the Amateur Fencers League of America was founded in 1891, the Amateur Fencing Association of Great Britain in 1902, and the F\u00e9d\u00e9ration Nationale des Soci\u00e9t\u00e9s d\u2019Escrime et Salles d\u2019Armes de France in 1906.\nThe first regularized fencing competition was held at the inaugural Grand Military Tournament and Assault at Arms in 1880, held at the Royal Agricultural Hall, in Islington in June. The Tournament featured a series of competitions between army officers and soldiers. Each bout was fought for five hits and the foils were pointed with black to aid the judges. The Amateur Gymnastic &amp; Fencing Association drew up an official set of fencing regulations in 1896.\nFencing was part of the Olympic Games in the summer of 1896. Sabre events have been held at every Summer Olympics; foil events have been held at every Summer Olympics except 1908; \u00e9p\u00e9e events have been held at every Summer Olympics except in the summer of 1896 because of unknown reasons.\nStarting with \u00e9p\u00e9e in 1933, side judges were replaced by the Laurent-Pagan electrical scoring apparatus, with an audible tone and a red or green light indicating when a touch landed. Foil was automated in 1956, sabre in 1988. The scoring box reduced the bias in judging, and permitted more accurate scoring of faster actions, lighter touches, and more touches to the back and flank than before.\nWeapons.\nThere are three weapons in modern fencing: foil, \u00e9p\u00e9e, and sabre. Each weapon has its own rules and strategies. Equipment needed includes at least 2 swords, a lam\u00e9 (not for \u00e9p\u00e9e), a white jacket, underarm protector, two body and mask cords, knee high socks, glove and knickers.\nFoil.\nThe foil is a light thrusting weapon with a maximum weight of 500 grams. The foil targets the torso, but not the arms or legs. The foil has a small circular hand guard that serves to protect the hand from direct stabs. As the hand is not a valid target in foil, this is primarily for safety. Touches are scored only with the tip; hits with the side of the blade do not register on the electronic scoring apparatus (and do not halt the action). Touches that land outside the target area (called an \"off-target touch\" and signaled by a distinct color on the scoring apparatus) stop the action, but are not scored. Only a single touch can be awarded to either fencer at the end of a phrase. If both fencers land touches within a close enough interval of milliseconds to register two lights on the machine, the referee uses the rules of \"right of way\" to determine which fencer is awarded the touch, or if an off-target hit has priority over a valid hit, in which case no touch is awarded. If the referee is unable to determine which fencer has right of way, no touch is awarded.\n\u00c9p\u00e9e.\nThe \u00e9p\u00e9e is a thrusting weapon like the foil, but heavier, with a maximum total weight of 775 grams. In \u00e9p\u00e9e, the entire body is a valid target. The hand guard on the \u00e9p\u00e9e is a large circle that extends towards the pommel, effectively covering the hand, which is a valid target in \u00e9p\u00e9e. Like foil, all hits must be with the tip and not the sides of the blade. Hits with the side of the blade do not register on the electronic scoring apparatus (and do not halt the action). As the entire body is legal target, there is no concept of an off-target touch, except if the fencer accidentally strikes the floor, setting off the light and tone on the scoring apparatus. Unlike foil and sabre, \u00e9p\u00e9e does not use \"right of way\", and awards simultaneous touches to both fencers. However, if the score is tied in a match at the last point and a double touch is scored, the point is null and void.\nSabre.\nThe sabre is a light cutting and thrusting weapon that targets the entire body above the waist, except the weapon hand. Sabre is the newest weapon to be used. Like the foil, the maximum legal weight of a sabre is 500 grams. The hand guard on the sabre extends from hilt to the point at which the blade connects to the pommel. This guard is generally turned outwards during sport to protect the sword arm from touches. Hits with the entire blade or point are valid. As in foil, touches that land outside the target area are not scored. However, unlike foil, these \"off-target\" touches do not stop the action, and the fencing continues. In the case of both fencers landing a scoring touch, the referee determines which fencer receives the point for the action, again through the use of \"right of way\".\nEquipment.\nProtective clothing.\nMost personal protective equipment for fencing is made of tough cotton or nylon. Kevlar was added to top level uniform pieces (jacket, breeches, underarm protector, lam\u00e9, and the bib of the mask) following the death of Vladimir Smirnov at the 1982 World Championships in Rome. However, Kevlar is degraded by both ultraviolet light and chlorine, which can complicate cleaning.\nOther ballistic fabrics, such as Dyneema, have been developed that resist puncture, and which do not degrade the way that Kevlar does. FIE rules state that tournament wear must be made of fabric that resists a force of , and that the mask bib must resist twice that amount.\nThe complete fencing kit includes:\nTraditionally, the fencer's uniform is white, and an instructor's uniform is black. This may be due to the occasional pre-electric practice of covering the point of the weapon in dye, soot, or colored chalk in order to make it easier for the referee to determine the placing of the touches. As this is no longer a factor in the electric era, the FIE rules have been relaxed to allow colored uniforms (save black). The guidelines also limit the permitted size and positioning of sponsorship logos.\nGrips.\nSome pistol grips used by foil and \u00e9p\u00e9e fencers \nElectric equipment.\nA set of electric fencing equipment is required to participate in electric fencing. Electric equipment in fencing varies depending on the weapon with which it is used in accordance. The main component of a set of electric equipment is the body cord. The body cord serves as the connection between a fencer and a reel of wire that is part of a system for electrically detecting that the weapon has touched the opponent. There are two types: one for \u00e9p\u00e9e, and one for foil and sabre.\n\u00c9p\u00e9e body cords consist of two sets of three prongs each connected by a wire. One set plugs into the fencer's weapon, with the other connecting to the reel. Foil and sabre body cords have only two prongs (or a twist-lock bayonet connector) on the weapon side, with the third wire connecting instead to the fencer's lam\u00e9. The need in foil and sabre to distinguish between on and off-target touches requires a wired connection to the valid target area.\nA body cord consists of three wires known as the A, B, and C lines. At the reel connector (and both connectors for \u00c9p\u00e9e cords) The B pin is in the middle, the A pin is 1.5\u00a0cm to one side of B, and the C pin is 2\u00a0cm to the other side of B. This asymmetrical arrangement ensures that the cord cannot be plugged in the wrong way around.\nIn foil, the A line is connected to the lam\u00e9 and the B line runs up a wire to the tip of the weapon. The B line is normally connected to the C line through the tip. When the tip is depressed, the circuit is broken and one of three things can happen:\nIn \u00c9p\u00e9e, the A and B lines run up separate wires to the tip (there is no lam\u00e9). When the tip is depressed, it connects the A and B lines, resulting in a valid touch. However, if the tip is touching the opponents weapon (their C line) or the grounded strip, nothing happens when it is depressed, as the current is redirected to the C line. Grounded strips are particularly important in \u00c9p\u00e9e, as without one, a touch to the floor registers as a valid touch (rather than off-target as in Foil).\nIn Sabre, similarly to Foil, the A line is connected to the lam\u00e9, but both the B and C lines are connected to the body of the weapon. Any contact between the one's B/C line (doesn't matter which, as they are always connected) and the opponent's A line (their lam\u00e9) results in a valid touch. There is no need for grounded strips in Sabre, as hitting something other than the opponent's lame does nothing.\nIn a professional fencing competition, a complete set of electric equipment is needed.\nA complete set of foil electric equipment includes:\nThe electric equipment of sabre is very similar to that of foil. In addition, equipment used in sabre includes:\n\u00c9p\u00e9e fencers lack a lam\u00e9, conductive bib, and head cord due to their target area. Also, their body cords are constructed differently as described above. However, they possess all of the other components of a foil fencer's equipment.\nTechniques.\nTechniques or movements in fencing can be divided into two categories: offensive and defensive. Some techniques can fall into both categories (\"e.g.\" the beat). Certain techniques are used offensively, with the purpose of landing a hit on your opponent while holding the right of way (foil and sabre). Others are used defensively, to protect against a hit or obtain the right of way.\nThe attacks and defences may be performed in countless combinations of feet and hand actions. For example, fencer A attacks the arm of fencer B, drawing a high outside parry; fencer B then follows the parry with a high line riposte. Fencer A, expecting that, then makes his own parry by pivoting his blade under fencer B's weapon (from straight out to more or less straight down), putting fencer B's tip off target and fencer A now scoring against the low line by angulating the hand upwards.\nWhenever a point is scored, the fencers will go back to their starting mark. The fight will start again after the following commands have been given by the referee (in French in international settings): \"En garde\" (On guard), \"\u00cates-vous pr\u00eats ?\" (Are you ready?), \"Allez\" (Fence!).\nUniversities and schools.\nUniversity students compete internationally at the World University Games. The United States holds two national level university tournaments including the NCAA championship and the USACFC National Championships tournaments in the US and the BUCS fencing championships in the United Kingdom. \nNational fencing organisations have set up programmes to encourage more students to fence. Examples include the Regional Youth Circuit program in the US and the Leon Paul Youth Development series in the UK.\nIn recent years, attempts have been made to introduce fencing to a wider and younger audience, by using foam and plastic swords, which require much less protective equipment. This makes it much less expensive to provide classes, and thus easier to take fencing to a wider range of schools than traditionally has been the case. There is even a competition series in Scotland \u2013 the Plastic-and-Foam Fencing FunLeague \u2013 specifically for Primary and early Secondary school-age children using this equipment.\nThe UK hosts two national competitions in which schools compete against each other directly: the Public Schools Fencing Championship, a competition only open to Independent Schools, and the Scottish Secondary Schools Championships, open to all secondary schools in Scotland. It contains both teams and individual events and is highly anticipated. Schools organise matches directly against one another and school age pupils can compete individually in the British Youth Championships.\nMany universities in Ontario, Canada have fencing teams that participate in an annual inter-university competition called the OUA Finals.\nOther variants.\nOther variants include wheelchair fencing for those with disabilities, chair fencing, \"one-hit \u00e9p\u00e9e\" (one of the five events which constitute modern pentathlon) and the various types of non-Olympic competitive fencing. Chair fencing is similar to wheelchair fencing, but for the able bodied. The opponents set up opposing chairs and fence while seated; all the usual rules of fencing are applied. An example of the latter is the American Fencing League (distinct from the United States Fencing Association): the format of competitions is different and the right of way rules are interpreted in a different way. In a number of countries, school and university matches deviate slightly from the FIE format. A variant of the sport using toy lightsabers earned national attention when ESPN2 acquired the rights to a selection of matches and included it as part of its \"ESPN8: The Ocho\" programming block in August 2018.\nIn popular culture.\nOne of the most notable films related to fencing is the 2015 Finnish-Estonian-German film \"The Fencer\", directed by Klaus H\u00e4r\u00f6, which is loosely based on the life of Endel Nelis, an accomplished Estonian fencer and coach. The film was nominated for the 73rd Golden Globe Awards in the Foreign Language Film Category."}
{"id": "10894", "revid": "32317386", "url": "https://en.wikipedia.org/wiki?curid=10894", "title": "The Free Software Definition", "text": "The Free Software Definition written by Richard Stallman and published by Free Software Foundation (FSF), defines free software as being software that ensures that the end users have freedom in using, studying, sharing and modifying that software. The term \"free\" is used in the sense of \"free speech,\" not of \"free of charge.\" The earliest-known publication of the definition was in the February 1986 edition of the now-discontinued GNU's Bulletin publication of FSF. The canonical source for the document is in the philosophy section of the GNU Project website. , it is published there in 39 languages. FSF publishes a list of licences which meet this definition.\nThe Four Essential Freedoms of Free Software.\nThe definition published by FSF in February 1986 had two points:\nIn 1996, when the gnu.org website was launched, \"free software\" was defined referring to \"three levels of freedom\" by adding an explicit mention of the freedom to study the software (which could be read in the two-point definition as being part of the freedom to change the program). Stallman later avoided the word \"levels\", saying that all of the freedoms are needed, so it is misleading to think in terms of levels.\nFinally, another freedom was added, to explicitly say that users should be able to run the program. The existing freedoms were already numbered one to three, but this freedom should come before the others, so it was added as \"freedom zero\".\nThe modern definition defines free software by whether or not the recipient has the following four freedoms:\nFreedoms 1 and 3 require source code to be available because studying and modifying software without its source code is highly impractical.\nLater definitions.\nIn July 1997, Bruce Perens published the Debian Free Software Guidelines. A definition based on the DFSG was also used by the Open Source Initiative (OSI) under the name \"The Open Source Definition\".\nComparison with \"The Open Source Definition\".\nDespite the philosophical differences between the free-software movement and the open-source-software movement, the official definitions of free software by the FSF and of open-source software by the OSI basically refer to the same software licences, with a few minor exceptions. While stressing the philosophical differences, the Free Software Foundation comments:"}
{"id": "10895", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10895", "title": "Fluid Mechanics", "text": ""}
{"id": "10896", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=10896", "title": "Felix Bloch", "text": "Felix Bloch (23 October 1905 \u2013 10 September 1983) was a Swiss-American physicist and Nobel physics laureate who worked mainly in the U.S. He and Edward Mills Purcell were awarded the 1952 Nobel Prize for Physics for \"their development of new ways and methods for nuclear magnetic precision measurements.\" In 1954\u20131955, he served for one year as the first Director-General of CERN. Felix Bloch made fundamental theoretical contributions to the understanding of electron behavior in crystal lattices, ferromagnetism, and nuclear magnetic resonance.\nBiography.\nEarly life, education, and family.\nBloch was born in Z\u00fcrich, Switzerland to Jewish parents Gustav and Agnes Bloch. Gustav Bloch, his father, was financially unable to attend University and worked as a wholesale grain dealer in Z\u00fcrich. Gustav moved to Z\u00fcrich in 1890 to become a Swiss citizen. Their first child was a girl born in 1902 while Felix was born three years later.\nBloch entered public elementary school at the age of six and is said to have been teased, in part because he \"spoke Swiss German with a somewhat different accent than most members of the class\". He received support from his older sister during much of this time, but she died at the age of twelve, devastating Felix, who is said to have lived a \"depressed and isolated life\" in the following years. Bloch learned to play the piano by the age of eight and was drawn to arithmetic for its \"clarity and beauty\". Bloch graduated from elementary school at twelve and enrolled in the Cantonal Gymnasium in Z\u00fcrich for secondary school in 1918. He was placed on a six-year curriculum here to prepare him for University. He continued his curriculum through 1924, even through his study of engineering and physics in other schools, though it was limited to mathematics and languages after the first three years. After these first three years at the Gymnasium, at age fifteen Bloch began to study at the Eidgen\u00f6ssische Technische Hochschule (ETHZ), also in Z\u00fcrich. Although he initially studied engineering he soon changed to physics. During this time he attended lectures and seminars given by Peter Debye and Hermann Weyl at ETH Z\u00fcrich and Erwin Schr\u00f6dinger at the neighboring University of Z\u00fcrich. A fellow student in these seminars was John von Neumann. \nBloch graduated in 1927, and was encouraged by Debye to go to Leipzig to study with Werner Heisenberg. Bloch became Heisenberg's first graduate student, and gained his doctorate in 1928. His doctoral thesis established the quantum theory of solids, using waves to describe electrons in periodic lattices.\nOn March 14, 1940, Bloch married Lore Clara Misch (1911\u20131996), a fellow physicist working on X-ray crystallography, whom he had met at an American Physical Society meeting. They had four children, twins George Jacob Bloch and Daniel Arthur Bloch (born January 15, 1941), son Frank Samuel Bloch (born January 16, 1945), and daughter Ruth Hedy Bloch Alexander (born September 15, 1949).\nCareer.\nBloch remained in European academia, working on superconductivity with Wolfgang Pauli in Z\u00fcrich; with Hans Kramers and Adriaan Fokker in Holland; with Heisenberg on ferromagnetism, where he developed a description of boundaries between magnetic domains, now known as \"Bloch walls\", and theoretically proposed a concept of spin waves, excitations of magnetic structure; with Niels Bohr in Copenhagen, where he worked on a theoretical description of the stopping of charged particles traveling through matter; and with Enrico Fermi in Rome. In 1932, Bloch returned to Leipzig to assume a position as \"Privatdozent\" (lecturer). In 1933, immediately after Hitler came to power, he left Germany because he was Jewish, returning to Z\u00fcrich, before traveling to Paris to lecture at the Institut Henri Poincar\u00e9.\nIn 1934, the chairman of Stanford Physics invited Bloch to join the faculty. Bloch accepted the offer and emigrated to the United States. In the fall of 1938, Bloch began working with the 37 inch cyclotron at the University of California at Berkeley to determine the magnetic moment of the neutron. Bloch went on to become the first professor for theoretical physics at Stanford. In 1939, he became a naturalized citizen of the United States.\nDuring WWII, Bloch briefly worked on the atomic bomb project at Los Alamos. Disliking the military atmosphere of the laboratory and uninterested in the theoretical work there, Bloch left to join the radar project at Harvard University.\nAfter the war, he concentrated on investigations into nuclear induction and nuclear magnetic resonance, which are the underlying principles of MRI. In 1946 he proposed the Bloch equations which determine the time evolution of nuclear magnetization. Along with Edward Purcell, Bloch was awarded the 1952 Nobel Prize in Physics for his work on nuclear magnetic induction.\nWhen CERN was being set up in the early 1950s, its founders were searching for someone of stature and international prestige to head the fledgling international laboratory, and in 1954 Professor Bloch became CERN's first Director-General, at the time when construction was getting under way on the present Meyrin site and plans for the first machines were being drawn up. After leaving CERN, he returned to Stanford University, where he in 1961 was made Max Stein Professor of Physics.\nIn 1964, he was elected a foreign member of the Royal Netherlands Academy of Arts and Sciences.\nBloch died in Z\u00fcrich in 1983."}
{"id": "10897", "revid": "1006017957", "url": "https://en.wikipedia.org/wiki?curid=10897", "title": "Fugue", "text": "In music, a fugue () is a contrapuntal compositional technique in two or more voices, built on a subject (a musical theme) that is introduced at the beginning in imitation (repetition at different pitches) and which recurs frequently in the course of the composition. It is not to be confused with a \"fuguing tune\", which is a style of song popularized by and mostly limited to early American (i.e. shape note or \"Sacred Harp\") music and West Gallery music. A fugue usually has three main sections: an exposition, a development and a final entry that contains the return of the subject in the fugue's tonic key. Some fugues have a recapitulation.\nIn the Middle Ages, the term was widely used to denote any works in canonic style; by the Renaissance, it had come to denote specifically imitative works. Since the 17th century, the term \"fugue\" has described what is commonly regarded as the most fully developed procedure of imitative counterpoint.\nMost fugues open with a short main theme, the subject, which then sounds successively in each voice (after the first voice is finished stating the subject, a second voice repeats the subject at a different pitch, and other voices repeat in the same way); when each voice has completed the subject, the \"exposition\" is complete. This is often followed by a connecting passage, or \"episode\", developed from previously heard material; further \"entries\" of the subject then are heard in related keys. Episodes (if applicable) and entries are usually alternated until the \"final entry\" of the subject, by which point the music has returned to the opening key, or tonic, which is often followed by closing material, the coda. In this sense, a fugue is a style of composition, rather than a fixed structure.\nThe form evolved during the 18th century from several earlier types of contrapuntal compositions, such as imitative ricercars, capriccios, canzonas, and fantasias. The famous fugue composer Johann Sebastian Bach (1685\u20131750) shaped his own works after those of Jan Pieterszoon Sweelinck (1562-1621), Johann Jakob Froberger (1616\u20131667), Johann Pachelbel (1653\u20131706), Girolamo Frescobaldi (1583\u20131643), Dieterich Buxtehude (c. 1637\u20131707) and others. With the decline of sophisticated styles at the end of the baroque period, the fugue's central role waned, eventually giving way as sonata form and the symphony orchestra rose to a dominant position. Nevertheless, composers continued to write and study fugues for various purposes; they appear in the works of Wolfgang Amadeus Mozart (1756\u20131791) and Ludwig van Beethoven (1770\u20131827), as well as modern composers such as Dmitri Shostakovich (1906\u20131975).\nEtymology.\nThe English term \"fugue\" originated in the 16th century and is derived from the French word \"fugue\" or the Italian \"fuga\". This in turn comes from Latin, also \"fuga\", which is itself related to both \"fugere\" (\"to flee\") and \"fugare\" (\"to chase\"). The adjectival form is \"fugal\". Variants include \"fughetta\" (literally, \"a small fugue\") and \"fugato\" (a passage in fugal style within another work that is not a fugue).\nMusical outline.\nA fugue begins with the \"exposition\" and is written according to certain predefined rules; in later portions the composer has more freedom, though a logical key structure is usually followed. Further entries of the subject will occur throughout the fugue, repeating the accompanying material at the same time. The various entries may or may not be separated by \"episodes\".\nWhat follows is a chart displaying a fairly typical fugal outline, and an explanation of the processes involved in creating this structure.\nExposition.\nA fugue begins with the exposition of its subject in one of the voices alone in the tonic key. After the statement of the subject, a second voice enters and states the subject with the subject transposed to another key (usually the dominant or subdominant), which is known as the \"answer\". To make the music run smoothly, it may also have to be altered slightly. When the answer is an exact copy of the subject to the new key, with identical intervals to the first statement, it is classified as a \"real answer\"; if the intervals are altered to maintain the key it is a \"tonal answer\".\nA tonal answer is usually called for when the subject begins with a prominent dominant note, or where there is a prominent dominant note very close to the beginning of the subject. To prevent an undermining of the music's sense of key, this note is transposed up a fourth to the tonic rather than up a fifth to the supertonic. Answers in the subdominant are also employed for the same reason.\nWhile the answer is being stated, the voice in which the subject was previously heard continues with new material. If this new material is reused in later statements of the subject, it is called a \"countersubject\"; if this accompanying material is only heard once, it is simply referred to as \"free counterpoint\". The countersubject is written in invertible counterpoint at the octave or fifteenth. The distinction is made between the use of free counterpoint and regular countersubjects accompanying the fugue subject/answer, because in order for a countersubject to be heard accompanying the subject in more than one instance, it must be capable of sounding correctly above or below the subject, and must be conceived, therefore, in invertible (double) counterpoint.\nIn tonal music, invertible contrapuntal lines must be written according to certain rules because several intervallic combinations, while acceptable in one particular orientation, are no longer permissible when inverted. For example, when the note \"G\" sounds in one voice above the note \"C\" in lower voice, the interval of a fifth is formed, which is considered consonant and entirely acceptable. When this interval is inverted (\"C\" in the upper voice above \"G\" in the lower), it forms a fourth, considered a dissonance in tonal contrapuntal practice, and requires special treatment, or preparation and resolution, if it is to be used. The countersubject, if sounding at the same time as the answer, is transposed to the pitch of the answer. Each voice then responds with its own subject or answer, and further countersubjects or free counterpoint may be heard.\nWhen a tonal answer is used, it is customary for the exposition to alternate subjects (S) with answers (A), however, in some fugues this order is occasionally varied: e.g., see the SAAS arrangement of Fugue No. 1 in C Major, BWV 846, from J.S. Bach's \"Well-Tempered Clavier, Book 1\". A brief codetta is often heard connecting the various statements of the subject and answer. This allows the music to run smoothly. The codetta, just as the other parts of the exposition, can be used throughout the rest of the fugue.\nThe first answer must occur as soon after the initial statement of the subject as possible; therefore the first codetta is often extremely short, or not needed. In the above example, this is the case: the subject finishes on the quarter note (or crotchet) B of the third beat of the second bar which harmonizes the opening G of the answer. The later codettas may be considerably longer, and often serve to (a) develop the material heard so far in the subject/answer and countersubject and possibly introduce ideas heard in the second countersubject or free counterpoint that follows (b) delay, and therefore heighten the impact of the reentry of the subject in another voice as well as modulating back to the tonic.\nThe exposition usually concludes when all voices have given a statement of the subject or answer. In some fugues, the exposition will end with a redundant entry, or an extra presentation of the theme. Furthermore, in some fugues the entry of one of the voices may be reserved until later, for example in the pedals of an organ fugue (see J.S. Bach's Fugue in C major for Organ, BWV 547).\nEpisode.\nFurther entries of the subject follow this initial exposition, either immediately (as for example in Fugue No. 1 in C major, BWV 846 of the \"Well-Tempered Clavier\") or separated by episodes. Episodic material is always modulatory and is usually based upon some element heard in the exposition. Each episode has the primary function of transitioning for the next entry of the subject in a new key, and may also provide release from the strictness of form employed in the exposition, and middle-entries. Andr\u00e9 Gedalge states that the episode of the fugue is generally based on a series of imitations of the subject that have been fragmented.\nDevelopment.\nFurther entries of the subject, or middle entries, occur throughout the fugue. They must state the subject or answer at least once in its entirety, and may also be heard in combination with the countersubject(s) from the exposition, new countersubjects, free counterpoint, or any of these in combination. It is uncommon for the subject to enter alone in a single voice in the middle entries as in the exposition; rather, it is usually heard with at least one of the countersubjects and/or other free contrapuntal accompaniments.\nMiddle entries tend to occur at pitches other than the initial. As shown in the typical structure above, these are often closely related keys such as the relative dominant and subdominant, although the key structure of fugues varies greatly. In the fugues of J.S. Bach, the first middle entry occurs most often in the relative major or minor of the work's overall key, and is followed by an entry in the dominant of the relative major or minor when the fugue's subject requires a tonal answer. In the fugues of earlier composers (notably, Buxtehude and Pachelbel), middle entries in keys other than the tonic and dominant tend to be the exception, and non-modulation the norm. One of the famous examples of such non-modulating fugue occurs in Buxtehude's Praeludium (Fugue and Chaconne) in C, BuxWV 137.\nWhen there is no entrance of the subject and answer material, the composer can develop the subject by altering the subject. This is called an \"episode\", often by \"inversion\", although the term is sometimes used synonymously with middle entry and may also describe the exposition of completely new subjects, as in a double fugue for example (see below). In any of the entries within a fugue, the subject may be altered, by inversion, retrograde (a less common form where the entire subject is heard back-to-front) and diminution (the reduction of the subject's rhythmic values by a certain factor), augmentation (the increase of the subject's rhythmic values by a certain factor) or any combination of them.\nExample and analysis.\nThe excerpt below, bars 7\u201312 of J.S. Bach's Fugue No. 2 in C minor, BWV 847, from the \"Well-Tempered Clavier\", Book 1 illustrates the application of most of the characteristics described above. The fugue is for keyboard and in three voices, with regular countersubjects. This excerpt opens at last entry of the exposition: the subject is sounding in the bass, the first countersubject in the treble, while the middle-voice is stating a second version of the second countersubject, which concludes with the characteristic rhythm of the subject, and is always used together with the first version of the second countersubject. Following this an episode modulates from the tonic to the relative major by means of sequence, in the form of an accompanied canon at the fourth. Arrival in E major is marked by a quasi perfect cadence across the bar line, from the last quarter note beat of the first bar to the first beat of the second bar in the second system, and the first middle entry. Here, Bach has altered the second countersubject to accommodate the change of mode.\nFalse entries.\nAt any point in the fugue there may be \"false entries\" of the subject, which include the start of the subject but are not completed. False entries are often abbreviated to the head of the subject, and anticipate the \"true\" entry of the subject, heightening the impact of the subject proper.\nCounter-exposition.\nThe counter-exposition is a second exposition. However, there are only two entries, and the entries occur in reverse order. The counter-exposition in a fugue is separated from the exposition by an episode and is in the same key as the original exposition.\nStretto.\nSometimes counter-expositions or the middle entries take place in \"stretto,\" whereby one voice responds with the subject/answer before the first voice has completed its entry of the subject/answer, usually increasing the intensity of the music. Only one entry of the subject must be heard in its completion in a \"stretto\". However, a \"stretto\" in which the subject/answer is heard in completion in all voices is known as \"stretto maestrale\" or \"grand stretto\". \"Strettos\" may also occur by inversion, augmentation and diminution. A fugue in which the opening exposition takes place in \"stretto\" form is known as a \"close fugue\" or \"stretto fugue\" (see for example, the \"Gratias agimus tibi\" and \"\" choruses from J.S. Bach's Mass in B Minor).\nFinal entries and coda.\nThe closing section of a fugue often includes one or two counter-expositions, and possibly a stretto, in the tonic; sometimes over a tonic or dominant pedal note. Any material that follows the final entry of the subject is considered to be the final coda and is normally cadential.\nTypes.\nSimple fugue.\nA simple fugue has only one subject, and does not utilize invertible counterpoint.\nDouble (triple, quadruple) fugue.\nA double fugue has two subjects that are often developed simultaneously. Similarly, a triple fugue has three subjects. There are two kinds of double (triple) fugue: (a) a fugue in which the second (third) subject is (are) presented simultaneously with the subject in the exposition (e.g. as in Kyrie Eleison of Mozart's Requiem in D minor or the fugue of Bach's Passacaglia and Fugue in C minor, BWV 582), and (b) a fugue in which all subjects have their own expositions at some point, and they are not combined until later (see for example, the three-subject Fugue No. 14 in F minor from Bach's \"Well-Tempered Clavier\" Book 2, or more famously, Bach's \"St. Anne\" Fugue in E major, BWV 552, a triple fugue for organ.)\nCounter-fugue.\nA counter-fugue is a fugue in which the first answer is presented as the subject in inversion (upside down), and the inverted subject continues to feature prominently throughout the fugue. Examples include \"Contrapunctus V\" through \"Contrapunctus VII\", from Bach's \"The Art of Fugue\".\nPermutation fugue.\nPermutation fugue describes a type of composition (or technique of composition) in which elements of fugue and strict canon are combined. Each voice enters in succession with the subject, each entry alternating between tonic and dominant, and each voice, having stated the initial subject, continues by stating two or more themes (or countersubjects), which must be conceived in correct invertible counterpoint. (In other words, the subject and countersubjects must be capable of being played both above and below all the other themes without creating any unacceptable dissonances.) Each voice takes this pattern and states all the subjects/themes in the same order (and repeats the material when all the themes have been stated, sometimes after a rest).\nThere is usually very little non-structural/thematic material. During the course of a permutation fugue, it is quite uncommon, actually, for every single possible voice-combination (or \"permutation\") of the themes to be heard. This limitation exists in consequence of sheer proportionality: the more voices in a fugue, the greater the number of possible permutations. In consequence, composers exercise editorial judgment as to the most musical of permutations and processes leading thereto. One example of permutation fugue can be seen in the eighth and final chorus of J.S. Bach's cantata, \"Himmelsk\u00f6nig, sei willkommen\", BWV 182.\nPermutation fugues differ from conventional fugue in that there are no connecting episodes, nor statement of the themes in related keys. So for example, the fugue of Bach's Passacaglia and Fugue in C minor, BWV 582 is not purely a permutation fugue, as it does have episodes between permutation expositions. Invertible counterpoint is essential to permutation fugues but is not found in simple fugues.\nFughetta.\nA fughetta is a short fugue that has the same characteristics as a fugue. Often the contrapuntal writing is not strict, and the setting less formal. See for example, variation 24 of Beethoven's \"Diabelli Variations\" Op. 120.\nHistory.\nMiddle Ages and Renaissance.\nThe term \"fuga\" was used as far back as the Middle Ages, but was initially used to refer to any kind of imitative counterpoint, including canons, which are now thought of as distinct from fugues. Prior to the 16th century, fugue was originally a genre. It was not until the 16th century that fugal technique as it is understood today began to be seen in pieces, both instrumental and vocal. Fugal writing is found in works such as fantasias, ricercares and canzonas.\n\"Fugue\" as a theoretical term first occurred in 1330 when Jacobus of Liege wrote about the \"fuga\" in his \"Speculum musicae\". The fugue arose from the technique of \"imitation\", where the same musical material was repeated starting on a different note.\nGioseffo Zarlino, a composer, author, and theorist in the Renaissance, was one of the first to distinguish between the two types of imitative counterpoint: fugues and canons (which he called imitations). Originally, this was to aid improvisation, but by the 1550s, it was considered a technique of composition. The composer Giovanni Pierluigi da Palestrina (1525?\u20131594) wrote masses using modal counterpoint and imitation, and fugal writing became the basis for writing motets as well. Palestrina's imitative motets differed from fugues in that each phrase of the text had a different subject which was introduced and worked out separately, whereas a fugue continued working with the same subject or subjects throughout the entire length of the piece.\nBaroque era.\nIt was in the Baroque period that the writing of fugues became central to composition, in part as a demonstration of compositional expertise. Fugues were incorporated into a variety of musical forms. Jan Pieterszoon Sweelinck, Girolamo Frescobaldi, Johann Jakob Froberger and Dieterich Buxtehude all wrote fugues, and George Frideric Handel included them in many of his oratorios. Keyboard suites from this time often conclude with a fugal gigue. Domenico Scarlatti has only a few fugues among his corpus of over 500 harpsichord sonatas. The French overture featured a quick fugal section after a slow introduction. The second movement of a sonata da chiesa, as written by Arcangelo Corelli and others, was usually fugal.\nThe Baroque period also saw a rise in the importance of music theory. Some fugues during the Baroque period were pieces designed to teach contrapuntal technique to students. The most influential text was Johann Joseph Fux's \"Gradus Ad Parnassum\" (\"Steps to Parnassus\"), which appeared in 1725. This work laid out the terms of \"species\" of counterpoint, and offered a series of exercises to learn fugue writing. Fux's work was largely based on the practice of Palestrina's modal fugues. Mozart studied from this book, and it remained influential into the nineteenth century. Haydn, for example, taught counterpoint from his own summary of Fux and thought of it as the basis for formal structure.\nBach's most famous fugues are those for the harpsichord in \"The Well-Tempered Clavier\", which many composers and theorists look at as the greatest model of fugue. \"The Well-Tempered Clavier\" comprises two volumes written in different times of Bach's life, each comprising 24 prelude and fugue pairs, one for each major and minor key. Bach is also known for his organ fugues, which are usually preceded by a prelude or toccata. \"The Art of Fugue\", BWV 1080, is a collection of fugues (and four canons) on a single theme that is gradually transformed as the cycle progresses. Bach also wrote smaller single fugues and put fugal sections or movements into many of his more general works. J.S. Bach's influence extended forward through his son C.P.E. Bach and through the theorist Friedrich Wilhelm Marpurg (1718\u20131795) whose \"Abhandlung von der Fuge\" (\"Treatise on the fugue\", 1753) was largely based on J.S. Bach's work.\nClassical era.\nDuring the Classical era, the fugue was no longer a central or even fully natural mode of musical composition. Nevertheless, both Haydn and Mozart had periods of their careers in which they in some sense \"rediscovered\" fugal writing and used it frequently in their work.\nHaydn.\nJoseph Haydn was the leader of fugal composition and technique in the Classical era. Haydn's most famous fugues can be found in his \"Sun\" Quartets (op. 20, 1772), of which three have fugal finales. This was a practice that Haydn repeated only once later in his quartet-writing career, with the finale of his String Quartet, Op. 50 No. 4 (1787). Some of the earliest examples of Haydn's use of counterpoint, however, are in three symphonies (No. 3, No. 13, and No. 40) that date from 1762 to 1763. The earliest fugues, in both the symphonies and in the Baryton trios, exhibit the influence of Joseph Fux's treatise on counterpoint, \"Gradus ad Parnassum\" (1725), which Haydn studied carefully.\nHaydn's second fugal period occurred after he heard, and was greatly inspired by, the oratorios of Handel during his visits to London (1791\u20131793, 1794\u20131795). Haydn then studied Handel's techniques and incorporated Handelian fugal writing into the choruses of his mature oratorios \"The Creation\" and \"The Seasons,\" as well as several of his later symphonies, including No. 88, No. 95, and No. 101; and the late string quartets, Opus 71 no. 3 and (especially) Opus 76 no. 6.\nMozart.\nThe young Wolfgang Amadeus Mozart studied counterpoint with Padre Martini in Bologna. Under the employment of Archbishop Colloredo, and the musical influence of his predecessors and colleagues such as Johann Ernst Eberlin, Anton Cajetan Adlgasser, Michael Haydn, and his own father, Leopold Mozart at the Salzburg Cathedral, the young Mozart composed ambitious fugues and contrapuntal passages in Catholic choral works such as Mass in C minor, K. 139 \"Waisenhaus\" (1768), Mass in C major, K. 66 \"Dominicus\" (1769), Mass in C major, K. 167 \"in honorem Sanctissimae Trinitatis\" (1773), Mass in C major, K. 262 \"Missa longa\" (1775), Mass in C major, K. 337 \"Solemnis\" (1780), various litanies, and vespers. Leopold admonished his son openly in 1777 that he not forget to make public demonstration of his abilities in \"fugue, canon, and contrapunctus\" (Konrad).\nLater in life, the major impetus to fugal writing for Mozart was the influence of Baron Gottfried van Swieten in Vienna around 1782. Van Swieten, during diplomatic service in Berlin, had taken the opportunity to collect as many manuscripts by Bach and Handel as he could, and he invited Mozart to study his collection and encouraged him to transcribe various works for other combinations of instruments. Mozart was evidently fascinated by these works and wrote a set of transcriptions for string trio of fugues from Bach's \"Well-Tempered Clavier\", introducing them with preludes of his own. In a letter to his sister Nannerl Mozart, dated in Vienna on 20 April 1782, Mozart recognizes that he had not written anything in this form, but moved by his wife's interest he composed one piece, which is sent with the letter. He begs her not to let anybody see the fugue and manifests the hope to write five more and then present them to Baron van Swieten. Regarding the piece, he said \"I have taken particular care to write \"andante maestoso\" upon it, so that it should not be played fast \u2013 for if a fugue is not played slowly the ear cannot clearly distinguish the new subject as it is introduced and the effect is missed\". Mozart then set to writing fugues on his own, mimicking the Baroque style. These included the fugues for String Quartet, K. 405 (1782) and a Fugue in C Minor K. 426 for two pianos (1783). Later, Mozart incorporated fugal writing into his opera \"Die Zauberfl\u00f6te\" and the finale of his Symphony No. 41.\nThe parts of the Requiem he completed also contain several fugues (most notably the Kyrie, and the three fugues in the Domine Jesu; he also left behind a sketch for an Amen fugue which, some believe, would have come at the end of the Sequentia).\nBeethoven.\nLudwig van Beethoven was familiar with fugal writing from childhood, as an important part of his training was playing from \"The Well-Tempered Clavier\". During his early career in Vienna, Beethoven attracted notice for his performance of these fugues. There are fugal sections in Beethoven's early piano sonatas, and fugal writing is to be found in the second and fourth movements of the \"Eroica Symphony\" (1805). Beethoven incorporated fugues in his sonatas, and reshaped the episode's purpose and compositional technique for later generations of composers.\nNevertheless, fugues did not take on a truly central role in Beethoven's work until his late period. The finale of Beethoven's \"Hammerklavier\" Sonata contains a fugue, which was practically unperformed until the late 19th century, due to its tremendous technical difficulty and length. The last movement of his Cello Sonata, Op. 102 No. 2 is a fugue, and there are fugal passages in the last movements of his Piano Sonatas in A major, Op. 101 and A major Op. 110. According to Charles Rosen (1971, p.\u00a0503) \"With the finale of 110, Beethoven re-conceived the significance of the most traditional elements of fugue writing.\"\nFugal passages are also found in the \"Missa Solemnis\" and all movements of the Ninth Symphony, except the third. A massive, dissonant fugue forms the finale of his String Quartet, Op. 130 (1825); the latter was later published separately as Op. 133, the \"Gro\u00dfe Fuge\" (\"Great Fugue\"). However, it is the fugue that opens Beethoven's String Quartet in C minor, Op. 131 that several commentators regard as one of the composer's greatest achievements. Joseph Kerman (1966, p.\u00a0330) calls it \"this most moving of all fugues\". J.W.N. Sullivan (1927, p.\u00a0235) hears it as \"the most superhuman piece of music that Beethoven has ever written.\" Philip Radcliffe (1965, p.\u00a0149) says \"[a] bare description of its formal outline can give but little idea of the extraordinary profundity of this fugue .\"\nRomantic era.\nBy the beginning of the Romantic era, fugue writing had become specifically attached to the norms and styles of the Baroque. Felix Mendelssohn wrote many fugues inspired by his study of the music of J.S. Bach. Franz Liszt's Piano Sonata in B minor (1853) contains a powerful fugue, demanding incisive virtuosity from its player: Richard Wagner included several fugues in his opera Die Meistersinger von N\u00fcrnberg. Giuseppe Verdi included a whimsical example at the end of his opera\" Falstaff\" and his setting of the Requiem Mass contained two (originally three) choral fugues. Anton Bruckner and Gustav Mahler also included them in their respective symphonies. The exposition of the finale of Bruckner's Fifth Symphony begins with a fugal exposition. The exposition ends with a chorale, the melody of which is then used as a second fugal exposition at the beginning of the development. The recapitulation features both fugal subjects concurrently. The finale of Mahler's Symphony No. 5 features a \"fugue-like\" passage early in the movement, though this is not actually an example of a fugue.\n20th century.\nTwentieth-century composers brought fugue back to its position of prominence, realizing its uses in full instrumental works, its importance in development and introductory sections, and the developmental capabilities of fugal composition.\nThe second movement of Maurice Ravel's piano suite \"Le Tombeau de Couperin\" (1917) is a fugue that Roy Howat (200, p.\u00a088) describes as having \"a subtle glint of jazz\". B\u00e9la Bart\u00f3k's \"Music for Strings, Percussion and Celesta\" (1936) opens with a slow fugue that Pierre Boulez (1986, pp.\u00a0346\u201347) regards as \"certainly the finest and most characteristic example of Bart\u00f3k's subtle style... probably the most \"timeless\" of all Bart\u00f3k's works \u2013 a fugue that unfolds like a fan to a point of maximum intensity and then closes, returning to the mysterious atmosphere of the opening.\"\nIgor Stravinsky also incorporated fugues into his works, including the Symphony of Psalms and the Dumbarton Oaks concerto. Stravinsky recognized the compositional techniques of Bach, and in the second movement of his Symphony of Psalms (1930), he lays out a fugue that is much like that of the Baroque era. It employs a double fugue with two distinct subjects, the first beginning in C and the second in E. Techniques such as stretto, sequencing, and the use of subject incipits are frequently heard in the movement. Dimitri Shostakovich's 24 Preludes and Fugues is the composer's homage to Bach's two volumes of The Well-Tempered Clavier. In the first movement of his Fourth Symphony, starting at rehearsal mark 63, is a gigantic fugue in which the 20-bar subject (and tonal answer) consist entirely of semiquavers, played at the speed of quaver=168. \nOlivier Messiaen, writing about his \"Vingt regards sur l'enfant-J\u00e9sus\" (1944) wrote of the sixth piece of that collection, \"Par Lui tout a \u00e9t\u00e9 fait\" (\"By Him were all things made\"): \nGy\u00f6rgy Ligeti wrote a five-part double fugue for his \"Requiem\"'s second movement, the Kyrie, in which each part (SMATB) is subdivided in four-voice \"bundles\" that make a canon. The melodic material in this fugue is totally chromatic, with melismatic (running) parts overlaid onto skipping intervals, and use of polyrhythm (multiple simultaneous subdivisions of the measure), blurring everything both harmonically and rhythmically so as to create an aural aggregate, thus highlighting the theoretical/aesthetic question of the next section as to whether fugue is a form or a texture. According to Tom Service, in this work, Ligeti\nBenjamin Britten used a fugue in the final part of \"The Young Person's Guide to the Orchestra\" (1946). The Henry Purcell's theme is triumphantly cited at the end making it a choral fugue.\nCanadian pianist and musical thinker Glenn Gould composed \"So You Want to Write a Fugue?\", a full-scale fugue set to a text that cleverly explicates its own musical form.\nOutside classical music.\nFugues (or fughettas/fugatos) have been incorporated into genres outside Western classical music. Several examples exist within jazz, such as \"Bach goes to Town\", composed by the Welsh composer Alec Templeton and recorded by Benny Goodman in 1938, and \"Concorde\" composed by John Lewis and recorded by the Modern Jazz Quartet in 1955.\nIn \"Fugue for Tinhorns\" from the Broadway musical Guys and Dolls, written by Frank Loesser, the characters Nicely-Nicely, Benny, and Rusty sing simultaneously about hot tips they each have in an upcoming horse race.\nA few examples also exist within progressive rock, such as the central movement of \"The Endless Enigma\" by Emerson, Lake &amp; Palmer and \"On Reflection\" by Gentle Giant.\nOn their EP of the same name, Vulfpeck has a composition called \"Fugue State\", which incorporates a fugue between Theo Katzman (guitar), Joe Dart (bass), and Woody Goss (Wurlitzer keyboard).\nDiscussion.\nMusical form or texture.\nA widespread view of the fugue is that it is not a musical form but rather a technique of composition.\nThe Austrian musicologist Erwin Ratz argues that the formal organization of a fugue involves not only the arrangement of its theme and episodes, but also its harmonic structure. In particular, the exposition and coda tend to emphasize the tonic key, whereas the episodes usually explore more distant tonalities. Ratz stressed, however, that this is the core, underlying form (\"Urform\") of the fugue, from which individual fugues may deviate.\nAlthough certain related keys are more commonly explored in fugal development, the overall structure of a fugue does not limit its harmonic structure. For example, a fugue may not even explore the dominant, one of the most closely related keys to the tonic. Bach's Fugue in B major from Book 1 of the \"Well Tempered Clavier\" explores the relative minor, the supertonic and the subdominant. This is unlike later forms such as the sonata, which clearly prescribes which keys are explored (typically the tonic and dominant in an ABA form). Then, many modern fugues dispense with traditional tonal harmonic scaffolding altogether, and either use serial (pitch-oriented) rules, or (as the Kyrie/Christe in Gy\u00f6rgy Ligeti's \"Requiem\", Witold Lutos\u0142awski works), use panchromatic, or even denser, harmonic spectra.\nPerceptions and aesthetics.\nThe fugue is the most complex of contrapuntal forms. In Ratz's words, \"fugal technique significantly burdens the shaping of musical ideas, and it was given only to the greatest geniuses, such as Bach and Beethoven, to breathe life into such an unwieldy form and make it the bearer of the highest thoughts.\" In presenting Bach's fugues as among the greatest of contrapuntal works, Peter Kivy points out that \"counterpoint itself, since time out of mind, has been associated in the thinking of musicians with the profound and the serious\" and argues that \"there seems to be some rational justification for their doing so.\"\nThis is related to the idea that restrictions create freedom for the composer, by directing their efforts. He also points out that fugal writing has its roots in improvisation, and was, during the Renaissance, practiced as an improvisatory art. Writing in 1555, Nicola Vicentino, for example, suggests that:"}
{"id": "10898", "revid": "13568690", "url": "https://en.wikipedia.org/wiki?curid=10898", "title": "Fugue state", "text": "Dissociative fugue, formerly fugue state or psychogenic fugue, is a dissociative disorder and a rare psychiatric disorder characterized by reversible amnesia for personal identity, including the memories, personality, and other identifying characteristics of individuality. The state can last days, months or longer. Dissociative fugue usually involves unplanned travel or wandering and is sometimes accompanied by the establishment of a new identity. It is a facet of dissociative amnesia, according to the fifth edition of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5).\nAfter recovery from a fugue state, previous memories usually return intact, and further treatment is unnecessary. Additionally, an episode of fugue is not characterized as attributable to a psychiatric disorder if it can be related to the ingestion of psychotropic substances, to physical trauma, to a general medical condition, or to dissociative identity disorder, delirium, or dementia. Fugues are precipitated by a series of long-term traumatic episodes. It is most commonly associated with childhood victims of sexual abuse who learn over time to dissociate memory of the abuse (dissociative amnesia).\nSigns and symptoms.\nSymptoms of a dissociative fugue include mild confusion and once the fugue ends, possible depression, grief, shame, and discomfort. People have also experienced a post-fugue anger. Another symptom of the fugue state can consist of loss of one's identity.\nDiagnosis.\nA doctor may suspect dissociative fugue when people seem confused about their identity or are puzzled about their past or when confrontations challenge their new identity or absence of one. The doctor reviews symptoms and does a physical examination to exclude physical disorders that may contribute to or cause memory loss.\nSometimes dissociative fugue cannot be diagnosed until people return to their pre-fugue identity and are distressed to find themselves in unfamiliar circumstances, sometimes with awareness of \"lost time\". The diagnosis is usually made retroactively when a doctor reviews the history and collects information that documents the circumstances before people left home, the travel itself, and the establishment of an alternative life.\nFunctional amnesia can also be situation-specific, varying from all forms and variations of traumas or generally violent experiences, with the person experiencing severe memory loss for a particular trauma. Committing homicide; experiencing or committing a violent crime such as rape or torture; experiencing combat violence; attempting suicide; and being in automobile accidents and natural disasters have all induced cases of situation-specific amnesia (Arrigo &amp; Pezdek, 1997; Kopelman, 2002a). As Kopelman (2002a) notes, however, care must be exercised in interpreting cases of psychogenic amnesia when there are compelling motives to feign memory deficits for legal or financial reasons. However, although some fraction of psychogenic amnesia cases can be explained in this fashion, it is generally acknowledged that true cases are not uncommon. Both global and situationally-specific amnesia are often distinguished from the organic amnesic syndrome, in that the capacity to store new memories and experiences remains intact. Given the very delicate and oftentimes dramatic nature of memory loss in such cases, there usually is a concerted effort to help the person recover their identity and history. This will allow the subject to be recovered sometimes spontaneously when particular cures are encountered.\nDefinition.\nThe cause of the fugue state is related to dissociative amnesia, (Code 300.12 of the DSM-IV codes) which has several other subtypes: selective amnesia, generalized amnesia, continuous amnesia, and systematized amnesia, in addition to the subtype \"dissociative fugue\".\nUnlike retrograde amnesia (which is popularly referred to simply as \"amnesia\", the state where someone forgets events before brain damage), dissociative amnesia is not due to the direct physiological effects of a substance (e.g., a drug of abuse, a medication, DSM-IV Codes 291.1 &amp; 292.83) or a neurological or other general medical condition (e.g., amnestic disorder due to a head trauma, DSM-IV Code 294.0). It is a complex neuropsychological process.\nAs the person experiencing a dissociative fugue may have recently suffered the reappearance of an event or person representing an earlier life trauma, the emergence of an armoring or defensive personality seems to be for some, a logical apprehension of the situation.\nTherefore, the terminology \"fugue state\" may carry a slight linguistic distinction from \"dissociative fugue\", the former implying a greater degree of \"motion\". For the purposes of this article, then, a \"fugue state\" occurs while one is \"acting out\" a \"dissociative fugue\".\nThe \"DSM-IV\" defines \"dissociative fugue\" as:\nThe \"Merck Manual\" defines \"dissociative fugue\" as:\nIn support of this definition, the \"Merck Manual\" further defines dissociative amnesia as:\nPrognosis.\nThe DSM-IV-TR states that the fugue may have a duration from days to months, and recovery is usually rapid. However, some cases may be refractory. An individual usually has only one episode."}
{"id": "10900", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=10900", "title": "Frames per second", "text": ""}
{"id": "10901", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10901", "title": "First-Person Shooter", "text": ""}
{"id": "10902", "revid": "19931234", "url": "https://en.wikipedia.org/wiki?curid=10902", "title": "Force", "text": "In physics, a force is any interaction that, when unopposed, will change the motion of an object. A force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described intuitively as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F.\nThe original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object.\nConcepts related to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the internal mechanical stress. Such internal mechanical stresses cause no acceleration of that body as the forces balance one another. Pressure, the distribution of many small forces applied over an area of a body, is a simple type of stress that if unbalanced can cause the body to accelerate. Stress usually causes deformation of solid materials, or flow in fluids.\nDevelopment of the concept.\nPhilosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Galileo Galilei and Sir Isaac Newton. With his mathematical insight, Sir Isaac Newton formulated laws of motion that were not improved for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia.\nWith modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.\nPre-Newtonian concepts.\nSince antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids.\nAristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different \"natural places\" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their \"natural place\" (e.g., for heavy bodies to fall), which led to \"natural motion\", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.\nAristotelian physics began facing criticism in medieval science, first by John Philoponus in the 6th century.\nThe shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.\nIn the early 17th century, before Newton's Principia, the term \"force\" () was applied to many physical and non-physical phenomena, e.g., for an acceleration of a point. The product of a point mass and the square of its velocity was named (live force) by Leibniz. The modern concept of force corresponds to the Newton's (accelerating force).\nNewtonian mechanics.\nSir Isaac Newton described the motion of all objects using the concepts of inertia and force, and in doing so he found they obey certain conservation laws. In 1687, Newton published his thesis \"Philosophi\u00e6 Naturalis Principia Mathematica\". In this work Newton set out three laws of motion that to this day are the way forces are described in physics.\nFirst law.\nNewton's first law of motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force (resultant force). This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium \"natural state\" in place of the Aristotelian idea of the \"natural state of rest\". That is, Newton's empirical first law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making \"rest\" physically indistinguishable from \"non-zero constant velocity\", Newton's first law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is \"in motion\" and which object is \"at rest\". The laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation.\nFor instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change as a result of its motion. If a person riding within the vehicle throws a ball straight up, that person will observe it rise vertically and fall vertically and not have to apply a force in the direction the vehicle is moving. Another person, observing the moving vehicle pass by, would observe the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction of the vehicle. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest.\nSecond law.\nA modern statement of Newton's second law is a vector equation:\nwhere formula_2 is the momentum of the system, and formula_3 is the net (vector sum) force. If a body is in equilibrium, there is zero \"net\" force by definition (balanced forces may be present nevertheless). In contrast, the second law states that if there is an \"unbalanced\" force acting on an object it will result in the object's momentum changing over time.\nBy the definition of momentum,\nwhere \"m\" is the mass and formula_5 is the velocity.\nIf Newton's second law is applied to a system of constant mass, \"m\" may be moved outside the derivative operator. The equation then becomes\nBy substituting the definition of acceleration, the algebraic version of Newton's second law is derived:\nNewton never explicitly stated the formula in the reduced form above.\nNewton's second law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of \"mass\" by writing the law as an equality; the relative units of force and mass then are fixed.\nThe use of Newton's second law as a \"definition\" of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach and Walter Noll.\nNewton's second law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.\nThird law.\nWhenever one body exerts a force on another, the latter simultaneously exerts an equal and opposite force on the first. In vector form, if formula_8 is the force of body 1 on body 2 and formula_9 that of body 2 on body 1, then\nThis law is sometimes referred to as the \"action-reaction law\", with formula_11 called the \"action\" and formula_12 the \"reaction\".\nNewton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are \"interactions\" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body.\nIn a system composed of object 1 and object 2, the net force on the system due to their mutual interactions is zero:\nMore generally, in a closed system of particles, all internal forces are balanced. The particles may accelerate with respect to each other but the center of mass of the system will not accelerate. If an external force acts on the system, it will make the center of mass accelerate in proportion to the magnitude of the external force divided by the mass of the system.\nCombining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. In a system of two particles, if formula_14 is the momentum of object 1 and formula_15 the momentum of object 2, then\nUsing similar arguments, this can be generalized to a system with an arbitrary number of particles. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.\nSpecial theory of relativity.\nIn the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law\nremains valid because it is a mathematical definition. But for relativistic momentum to be conserved, it must be redefined as:\nwhere formula_19 is the rest mass and formula_20 the speed of light.\nThe relativistic expression relating force and acceleration for a particle with constant non-zero rest mass formula_21 moving in the formula_22 direction is:\nwhere\nis called the Lorentz factor.\nIn the early history of relativity, the expressions formula_25 and formula_26 were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever-decreasing acceleration as the object approaches the speed of light. Note that formula_27 approaches asymptotically an infinite value and is undefined for an object with a non-zero rest mass as it approaches the speed of light, and the theory yields no prediction at that speed.\nIf formula_28 is very small compared to formula_20, then formula_30 is very close to 1 and\nis a close approximation. Even for use in relativity, however, one can restore the form of\nthrough the use of four-vectors. This relation is correct in relativity when formula_33 is the four-force, formula_21 is the invariant mass, and formula_35 is the four-acceleration.\nDescriptions.\nSince forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics.\nForces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as \"vector quantities\". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems.\nHistorically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the \"resultant\" (also called the \"net force\"), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body.\nFree-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force.\nAs well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.\nEquilibrium.\nEquilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque be zero.\nThere are two kinds of equilibrium: static equilibrium and dynamic equilibrium.\nStatic.\nStatic equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them.\nThe simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, a force is applied by the surface that resists the downward force with equal upward force (called a normal force). The situation produces zero net force and hence no acceleration.\nPushing against an object that rests on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force \"exactly\" balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.\nA static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the \"spring reaction force\", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.\nDynamic.\nDynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an \"absolute rest frame\" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a \"natural state\" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.\nMoreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity.\nA simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.\nForces in quantum mechanics.\nThe notion \"force\" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schr\u00f6dinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes \"quantized\", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of \"forces\". However, the potentials \"V\"(\"x\",\"y\",\"z\") or fields, from which the forces generally can be derived, are treated similarly to classical position variables, i.e., formula_36.\nThis becomes different only in the framework of quantum field theory, where these fields are also quantized.\nHowever, already in quantum mechanics there is one \"caveat\", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the \"spin\", and there is the Pauli exclusion principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a \"symmetric\" spin function (e.g. parallel spins) the spatial variables must be \"antisymmetric\" (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel \"spins\" the \"position variables\" must be symmetric (i.e. the apparent force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.\nThus the notion \"force\" loses already part of its meaning.\nFeynman diagrams.\nIn modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be \"fundamental interactions\". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex.\nThe utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.\nFundamental forces.\nAll of the known forces of the universe are classified into four fundamental interactions. The strong and the weak forces act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Pauli exclusion principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.\nThe fundamental theories for forces developed from the unification of different ideas. For example, Sir Isaac Newton unified, with his universal theory of gravitation, the force responsible for objects falling near the surface of the Earth with the force responsible for the falling of celestial bodies about the Earth (the Moon) and around the Sun (the planets). Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through a theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This Standard Model of particle physics assumes a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory, which was subsequently confirmed by observation. The complete formulation of the Standard Model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations suggest that the Standard Model is incomplete. A Grand Unified Theory that allows for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.\nGravitational.\nWhat we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as formula_37 and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of formula_21 will experience a force:\nFor an object in free-fall, this force is unopposed and the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reaction forces applied by their supports. For example, a person standing on the ground experiences zero net force, since a normal force (a reaction force) is exerted by the ground upward on the person that counterbalances his weight that is directed downward.\nNewton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion.\nNewton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration of a body due to gravity is proportional to the mass of the other attracting body. Combining these ideas gives a formula that relates the mass (formula_40) and the radius (formula_41) of the Earth to the gravitational acceleration:\nwhere formula_43 is the distance between the two objects' centers of mass and formula_44 is the unit vector pointed in the direction away from the center of the first object toward the center of the second object.\nThis formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.\nMercury's orbit, however, did not match that predicted by Newton's Law of Gravitation. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be inexact.\nSince then, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time \u2013 defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the \"ballistic trajectory\" of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as \"gravitational force\".\nElectromagnetic.\nThe electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement.\nSubsequent mathematicians and physicists found the construct of the \"electric field\" to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical \"test charge\" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as\nwhere formula_46 is the magnitude of the hypothetical test charge.\nMeanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be\nwhere formula_48 is the magnitude of the hypothetical test current and formula_49 is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle.\nThrough combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified \"electromagnetic force\" that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:\nwhere formula_51 is the electromagnetic force, formula_46 is the magnitude of the charge of the particle, formula_53 is the electric field, formula_54 is the velocity of the particle that is crossed with the magnetic field (formula_55).\nThe origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These \"Maxwell Equations\" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be \"self-generating\" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum.\nHowever, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave\u2013particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.\nStrong nuclear.\nThere are two \"nuclear forces\", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons.\nThe strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the \"strongest\" of the four fundamental forces.\nThe strong force only acts \"directly\" upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement.\nWeak nuclear.\nThe weak force is due to the exchange of the heavy W and Z bosons. Since the weak force is mediated by two types of bosons, it can be divided into two types of interaction or \"vertices\" \u2014 charged current, involving the electrically charged W+ and W\u2212 bosons, and neutral current, involving electrically neutral Z0 bosons. The most familiar effect of weak interaction is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. This is a type of charged-current interaction. The word \"weak\" derives from the fact that the field strength is some 1013 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 1015\u00a0kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.\nNon-fundamental forces.\nSome forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.\nNormal force.\nThe normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.\nFriction.\nFriction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction.\nThe static friction force (formula_56) will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction (formula_57) multiplied by the normal force (formula_58). In other words, the magnitude of the static friction force satisfies the inequality:\nThe kinetic friction force (formula_60) is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:\nwhere formula_62 is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.\nTension.\nTension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.\nElastic force.\nAn elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If formula_63 is the displacement, the force exerted by an ideal spring equals:\nwhere formula_65 is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.\nContinuum mechanics.\nNewton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:\nwhere formula_67 is the volume of the object in the fluid and formula_68 is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight.\nA specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called \"Stokes' drag\" the force is approximately proportional to the velocity, but opposite in direction:\nwhere:\nMore formally, forces in continuum mechanics are fully described by a stress\u2013tensor with terms that are roughly defined as\nwhere formula_73 is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.\nFictitious forces.\nThere are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as \"pseudo forces\".\nIn general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza\u2013Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.\nRotations and torque.\nForces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force formula_51 is defined relative to an arbitrary reference point as the cross-product:\nwhere\nTorque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:\nwhere\nThis provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation.\nEquivalently, the differential form of Newton's Second Law provides an alternative definition of torque:\nNewton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.\nCentripetal force.\nFor an object accelerating in circular motion, the unbalanced force acting on the object equals:\nwhere formula_21 is the mass of the object, formula_28 is the velocity of the object and formula_43 is the distance to the center of the circular path and formula_86 is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.\nKinematic integrals.\nForces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:\nwhich by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem).\nSimilarly, integrating with respect to position gives a definition for the work done by a force:\nwhich is equivalent to changes in kinetic energy (yielding the work energy theorem).\nPower \"P\" is the rate of change d\"W\"/d\"t\" of the work \"W\", as the trajectory is extended by a position change formula_89 in a time interval d\"t\":\nwith formula_91 the velocity.\nPotential energy.\nInstead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field formula_92 is defined as that field whose gradient is equal and opposite to the force produced at every point:\nForces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.\nConservative forces.\nA conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area.\nConservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector formula_76 emanating from spherically symmetric potentials. Examples of this follow:\nFor gravity:\nwhere formula_96 is the gravitational constant, and formula_97 is the mass of object \"n\".\nFor electrostatic forces:\nwhere formula_99 is electric permittivity of free space, and formula_100 is the electric charge of object \"n\".\nFor spring forces:\nwhere formula_65 is the spring constant.\nNonconservative forces.\nFor certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials.\nThe connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.\nUnits of measurement.\nThe SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or . The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or . A newton is thus equal to 100,000\u00a0dynes.\nThe gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of . The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force.\nAn alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law.\nThe pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at when subjected to a force of 1\u00a0kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sth\u00e8ne, which is equivalent to 1000\u00a0N, and the kip, which is equivalent to 1000\u00a0lbf.\nSee also Ton-force.\nForce measurement.\nSee force gauge, spring scale, load cell"}
{"id": "10905", "revid": "27556023", "url": "https://en.wikipedia.org/wiki?curid=10905", "title": "Family law", "text": "Family law (also called matrimonial law or the \"law of domestic relations\") is an area of the law that deals with family matters and domestic relations.\nOverview.\nSubjects that commonly fall under a nation's body of family law include:\nThis list is not exhaustive and varies depending on jurisdiction.\nConflict of laws.\nIssues may arise in family law where there is a question as to the laws of the jurisdiction that apply to the marriage relationship or to custody and divorce, and whether a divorce or child custody order is recognized under the laws of another jurisdiction. For child custody, many nations have joined the Hague Convention on the Civil Aspects of International Child Abduction in order to grant recognition to other member states' custody orders and avoid issues of parental kidnapping."}
{"id": "10906", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10906", "title": "Fractals", "text": ""}
{"id": "10908", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=10908", "title": "Finite state machines", "text": ""}
{"id": "10909", "revid": "155178", "url": "https://en.wikipedia.org/wiki?curid=10909", "title": "Foonly", "text": "Foonly Inc. was an American computer company formed by Dave Poole in 1976, that produced a series of \"DEC PDP-10\" compatible mainframe computers, named \"Foonly F1\" to \"Foonly F5\".\nThe first and most famous Foonly machine, the \"F1\", was the computer used by Triple-I to create some of the computer-generated imagery in the 1982 film \"Tron\".\nHistory.\nAt the beginning of the 1970s, the Stanford Artificial Intelligence Laboratory (SAIL) began to study the building of a new computer to replace their \"DEC PDP-10 KA-10\", by a far more powerful machine, with a funding of the DARPA. This project was named \"Super-Foonly\", and was developed by a team led by Phil Petit, Jack Holloway, and Dave Poole.\nIn 1974, the DARPA cut the funding, and a large part of the team went to DEC to develop the \"PDP-10 model KL10\", based on the \"Super-Foonly project\".\nBut Dave Poole, with Phil Petit and Jack Holloway, preferred to found the Foonly Company in 1976, to try to build a series of computers based on the \"Super-Foonly project\".\nDuring the early 1980s, after the releasing of their first and only F1, Foonly built and sold some F2, F4 and F5 low cost DEC PDP-10 compatible machines.\nIn 1983, after the cancellation of the Jupiter project, Foonly tried to propose a new \"Foonly F1\", but it was eclipsed by the \"SC Group\" company and their \"Mars project\", and the company never quite recovered.\nComputers.\nThe Foonly F1.\nThe Foonly F1 was the first and most powerful Foonly computer, but also the only one being built of its kind. It was based on the \"Super-Foonly project\" designs, aimed to be the fastest DEC PDP-10 compatible, but using ECL gates rather than TTL, and without the extended instruction set.\nIt was developed with the help of Triple-I, its first customer, and began operations in 1978.\nThe computer consisted of 4 cabinets :\nIt was able to reach 4.5 MIPS.\nThe F1 is mostly famous to have been the computer behind some of the Computer-generated imagery of the Disney 1982 Tron movie, and also Looker (1981).\nAfter that, the computer was bought by the Canadian Omnibus Computer Graphics company, and was used on some movies, such as TV logos for CBC, CTV, and Global Television Network channels, opening titles for the show Hockey Night in Canada,\n (1984), Flight of the Navigator (1986), Captain Power and the Soldiers of the Future TV series (1987), and Marilyn Monrobot.\nOther models.\nUnlike the F1, the other models (F2, F4, F4B, F5) were\nF2.\nFoonly described the F2 as \"a powerful mainframe at a minicomputer price,\" \"with an average execution speed about 25% of that of the DECSYSTEM-2060.\"\nPeripherals.\nStandard equipment:\nSoftware.\nThe Foonly machines, which could run the TENEX operating system, came with a derivative thereof, FOONEX.\nTymshare.\nTymshare attempted marketing the Foonly line, using the name \"Tymshare XX Series Computer Family\" of which the \"Tymshare System XXVI\" was the main focus."}
{"id": "10911", "revid": "1013145648", "url": "https://en.wikipedia.org/wiki?curid=10911", "title": "Functional group", "text": "In organic chemistry, a functional group is a substituent or moiety in a molecule that causes the molecule's characteristic chemical reactions. The same functional group will undergo the same or similar chemical reactions regardless of the rest of the molecule's composition. This enables systematic prediction of chemical reactions and behavior of chemical compounds and the design of chemical synthesis. The reactivity of a functional group can be modified by other functional groups nearby. Functional group interconversion can be used in retrosynthetic analysis to plan organic synthesis.\nA functional group is a group of atoms in a molecule with distinctive chemical properties, regardless of the other atoms in the molecule. The atoms in a functional group are linked to each other and to the rest of the molecule by covalent bonds. For repeating units of polymers, functional groups attach to their nonpolar core of carbon atoms and thus add chemical character to carbon chains. Functional groups can also be charged, e.g. in carboxylate salts (\u2013COO\u2212), which turns the molecule into a polyatomic ion or a complex ion. Functional groups binding to a central atom in a coordination complex are called \"ligands\". Complexation and solvation are also caused by specific interactions of functional groups. In the common rule of thumb \"like dissolves like\", it is the shared or mutually well-interacting functional groups which give rise to solubility. For example, sugar dissolves in water because both share the hydroxyl functional group (\u2013OH) and hydroxyls interact strongly with each other. Plus, when functional groups are more electronegative than atoms they attach to, the functional groups will become polar, and the otherwise nonpolar molecules containing these functional groups become polar and so become soluble in some aqueous environment.\nCombining the names of functional groups with the names of the parent alkanes generates what is termed a systematic nomenclature for naming organic compounds. In traditional nomenclature, the first carbon atom after the carbon that attaches to the functional group is called the alpha carbon; the second, beta carbon, the third, gamma carbon, etc. If there is another functional group at a carbon, it may be named with the Greek letter, e.g., the gamma-amine in gamma-aminobutyric acid is on the third carbon of the carbon chain attached to the carboxylic acid group. IUPAC conventions call for numeric labeling of the position, e.g. 4-aminobutanoic acid. In traditional names various qualifiers are used to label isomers, for example, isopropanol (IUPAC name: propan-2-ol) is an isomer of n-propanol (propan-1-ol). The term moiety has some overlap with the term \"functional group\". However, a moiety is an entire \"half\" of a molecule, which can be only a single functional group, but also a larger unit consisting of multiple functional groups. For example, an \"aryl moiety\" may be any group containing an aromatic ring, regardless of how many functional groups the said aryl has.\nTable of common functional groups.\nThe following is a list of common functional groups. In the formulas, the symbols R and R' usually denote an attached hydrogen, or a hydrocarbon side chain of any length, but may sometimes refer to any group of atoms.\nHydrocarbons.\nHydrocarbons are a class of molecule that is defined by functional groups called hydrocarbyls that contain only carbon and hydrogen, but vary in the number and order of double bonds. Each one differs in type (and scope) of reactivity.\nThere are also a large number of branched or ring alkanes that have specific names, e.g., tert-butyl, bornyl, cyclohexyl, etc. Hydrocarbons may form charged structures: positively charged carbocations or negative carbanions. Carbocations are often named \"-um\". Examples are tropylium and triphenylmethyl cations and the cyclopentadienyl anion.\nGroups containing halogen.\nHaloalkanes are a class of molecule that is defined by a carbon\u2013halogen bond. This bond can be relatively weak (in the case of an iodoalkane) or quite stable (as in the case of a fluoroalkane). In general, with the exception of fluorinated compounds, haloalkanes readily undergo nucleophilic substitution reactions or elimination reactions. The substitution on the carbon, the acidity of an adjacent proton, the solvent conditions, etc. all can influence the outcome of the reactivity.\nGroups containing oxygen.\nCompounds that contain C-O bonds each possess differing reactivity based upon the location and hybridization of the C-O bond, owing to the electron-withdrawing effect of sp-hybridized oxygen (carbonyl groups) and the donating effects of sp2-hybridized oxygen (alcohol groups).\nGroups containing nitrogen.\nCompounds that contain nitrogen in this category may contain C-O bonds, such as in the case of amides.\nGroups containing sulfur.\nCompounds that contain sulfur exhibit unique chemistry due to their ability to form more bonds than oxygen, their lighter analogue on the periodic table. Substitutive nomenclature (marked as prefix in table) is preferred over functional class nomenclature (marked as suffix in table) for sulfides, disulfides, sulfoxides and sulfones.\nGroups containing phosphorus.\nCompounds that contain phosphorus exhibit unique chemistry due to their ability to form more bonds than nitrogen, their lighter analogues on the periodic table.\nGroups containing boron.\nCompounds containing boron exhibit unique chemistry due to their having partially filled octets and therefore acting as Lewis acids.\nGroups containing metals.\n Fluorine is too electronegative to be bonded to magnesium; it becomes an ionic salt instead.\nNames of radicals or moieties.\nThese names are used to refer to the moieties themselves or to radical species, and also to form the names of halides and substituents in larger molecules.\nWhen the parent hydrocarbon is unsaturated, the suffix (\"-yl\", \"-ylidene\", or \"-ylidyne\") replaces \"-ane\" (e.g. \"ethane\" becomes \"ethyl\"); otherwise, the suffix replaces only the final \"-e\" (e.g. \"ethyne\" becomes \"ethynyl\").\nWhen used to refer to moieties, multiple single bonds differ from a single multiple bond. For example, a methylene bridge (methanediyl) has two single bonds, whereas a methylene group (methylidene) has one double bond. Suffixes can be combined, as in methylidyne (triple bond) vs. methylylidene (single bond and double bond) vs. methanetriyl (three double bonds).\nThere are some retained names, such as methylene for methanediyl, 1,x-phenylene for phenyl-1,x-diyl (where x is 2, 3, or 4), carbyne for methylidyne, and trityl for triphenylmethyl."}
{"id": "10913", "revid": "28331428", "url": "https://en.wikipedia.org/wiki?curid=10913", "title": "Fractal", "text": "In mathematics, a fractal is a subset of Euclidean space with a fractal dimension that strictly exceeds its topological dimension. Fractals appear the same at different scales, as illustrated in successive magnifications of the Mandelbrot set. Fractals exhibit similar patterns at increasingly smaller scales, a property called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge, it is called affine self-similar. Fractal geometry lies within the mathematical branch of measure theory.\nOne way that fractals are different from finite geometric figures is how they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). However, if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer. This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.\nAnalytically, most fractals are nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line \u2013 although it is still topologically 1-dimensional, its fractal dimension indicates that it also resembles a surface.\nStarting in the 17th century with notions of recursion, fractals have moved through increasingly rigorous mathematical treatment to the study of continuous but not differentiable functions in the 19th century by the seminal work of Bernard Bolzano, Bernhard Riemann, and Karl Weierstrass, and on to the coining of the word \"fractal\" in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 20th century. The term \"fractal\" was first used by mathematician Benoit Mandelbrot in 1975. Mandelbrot based it on the Latin , meaning \"broken\" or \"fractured\", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.\nThere is some disagreement among mathematicians about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as \"beautiful, damn hard, increasingly useful. That's fractals.\" More formally, in 1982 Mandelbrot defined \"fractal\" as follows: \"A fractal is by definition a set for which the Hausdorff\u2013Besicovitch dimension strictly exceeds the topological dimension.\" Later, seeing this as too restrictive, he simplified and expanded the definition to this: \"A fractal is a shape made of parts similar to the whole in some way.\" Still later, Mandelbrot proposed \"to use \"fractal\" without a pedantic definition, to use \"fractal dimension\" as a generic term applicable to \"all\" the variants\".\nThe consensus among mathematicians is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures, and sounds and found in nature, technology, art, architecture and law. Fractals are of particular relevance in the field of chaos theory because the graphs of most chaotic processes are fractals. Many real and model networks have been found to have fractal features such as self similarity.\nIntroduction.\nThe word \"fractal\" often has different connotations for the lay public as opposed to mathematicians, where the public is more likely to be familiar with fractal art than the mathematical concept. The mathematical concept is difficult to define formally, even for mathematicians, but key features can be understood with a little mathematical background.\nThe feature of \"self-similarity\", for instance, is easily understood by analogy to zooming in with a lens or other device that zooms in on digital images to uncover finer, previously invisible, new structure. If this is done on fractals, however, no new detail appears; nothing changes and the same pattern repeats over and over, or for some fractals, nearly the same pattern reappears over and over. Self-similarity itself is not necessarily counter-intuitive (e.g., people have pondered self-similarity informally such as in the infinite regress in parallel mirrors or the homunculus, the little man inside the head of the little man inside the head ...). The difference for fractals is that the pattern reproduced must be detailed.\nThis idea of being detailed relates to another feature that can be understood without much mathematical background: Having a fractal dimension greater than its topological dimension, for instance, refers to how a fractal scales compared to how geometric shapes are usually perceived. A straight line, for instance, is conventionally understood to be one-dimensional; if such a figure is rep-tiled into pieces each 1/3 the length of the original, then there are always three equal pieces. A solid square is understood to be two-dimensional; if such a figure is rep-tiled into pieces each scaled down by a factor of 1/3 in both dimensions, there are a total of 32 = 9 pieces. We see that for ordinary self-similar objects, being n-dimensional means that when it is rep-tiled into pieces each scaled down by a scale-factor of 1/\"r\", there are a total of \"r\"\"n\" pieces. Now, consider the Koch curve. It can be rep-tiled into four sub-copies, each scaled down by a scale-factor of 1/3. So, strictly by analogy, we can consider the \"dimension\" of the Koch curve as being the unique real number \"D\" that satisfies 3\"D\" = 4. This number is what mathematicians call the \"fractal dimension\" of the Koch curve; it is certainly \"not\" what is conventionally perceived as the dimension of a curve (this number is not even an integer!). The fact that the Koch curve has a fractal dimension differing from its \"conventionally understood\" dimension (that is, its topological dimension) is what makes it a fractal.\nThis also leads to understanding a third feature, that fractals as mathematical equations are \"nowhere differentiable\". In a concrete sense, this means fractals cannot be measured in traditional ways. To elaborate, in trying to find the length of a wavy non-fractal curve, one could find straight segments of some measuring tool small enough to lay end to end over the waves, where the pieces could get small enough to be considered to conform to the curve in the normal manner of measuring with a tape measure. But in measuring an infinitely \"wiggly\" fractal curve such as the Koch snowflake, one would never find a small enough straight segment to conform to the curve, because the jagged pattern would always re-appear, at arbitrarily small scales, essentially pulling a little more of the tape measure into the total length measured each time one attempted to fit it tighter and tighter to the curve. The result is that one must need infinite tape to perfectly cover the entire curve, i.e. the snowflake has an infinite perimeter.\nHistory.\nThe history of fractals traces a path from chiefly theoretical studies to modern applications in computer graphics, with several notable people contributing canonical fractal forms along the way. \nA common theme in ancient traditional African architecture is the use of fractal scaling, whereby small parts of the structure tend to look similar to larger parts, such as a circular village made of circular houses.\nAccording to Pickover, the mathematics behind fractals began to take shape in the 17th century when the mathematician and philosopher Gottfried Leibniz pondered recursive self-similarity (although he made the mistake of thinking that only the straight line was self-similar in this sense). In his writings, Leibniz used the term \"fractional exponents\", but lamented that \"Geometry\" did not yet know of them. Indeed, according to various historical accounts, after that point few mathematicians tackled the issues and the work of those who did remained obscured largely because of resistance to such unfamiliar emerging concepts, which were sometimes referred to as mathematical \"monsters\". Thus, it was not until two centuries had passed that on July 18, 1872 Karl Weierstrass presented the first definition of a function with a graph that would today be considered a fractal, having the non-intuitive property of being everywhere continuous but nowhere differentiable at the Royal Prussian Academy of Sciences. In addition, the quotient difference becomes arbitrarily large as the summation index increases. Not long after that, in 1883, Georg Cantor, who attended lectures by Weierstrass, published examples of subsets of the real line known as Cantor sets, which had unusual properties and are now recognized as fractals. Also in the last part of that century, Felix Klein and Henri Poincar\u00e9 introduced a category of fractal that has come to be called \"self-inverse\" fractals.\nOne of the next milestones came in 1904, when Helge von Koch, extending ideas of Poincar\u00e9 and dissatisfied with Weierstrass's abstract and analytic definition, gave a more geometric definition including hand-drawn images of a similar function, which is now called the Koch snowflake. Another milestone came a decade later in 1915, when Wac\u0142aw Sierpi\u0144ski constructed his famous triangle then, one year later, his carpet. By 1918, two French mathematicians, Pierre Fatou and Gaston Julia, though working independently, arrived essentially simultaneously at results describing what is now seen as fractal behaviour associated with mapping complex numbers and iterative functions and leading to further ideas about attractors and repellors (i.e., points that attract or repel other points), which have become very important in the study of fractals. Very shortly after that work was submitted, by March 1918, Felix Hausdorff expanded the definition of \"dimension\", significantly for the evolution of the definition of fractals, to allow for sets to have non-integer dimensions. The idea of self-similar curves was taken further by Paul L\u00e9vy, who, in his 1938 paper \"Plane or Space Curves and Surfaces Consisting of Parts Similar to the Whole\", described a new fractal curve, the L\u00e9vy C curve.\nDifferent researchers have postulated that without the aid of modern computer graphics, early investigators were limited to what they could depict in manual drawings, so lacked the means to visualize the beauty and appreciate some of the implications of many of the patterns they had discovered (the Julia set, for instance, could only be visualized through a few iterations as very simple drawings). That changed, however, in the 1960s, when Benoit Mandelbrot started writing about self-similarity in papers such as \"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\", which built on earlier work by Lewis Fry Richardson. In 1975 Mandelbrot solidified hundreds of years of thought and mathematical development in coining the word \"fractal\" and illustrated his mathematical definition with striking computer-constructed visualizations. These images, such as of his canonical Mandelbrot set, captured the popular imagination; many of them were based on recursion, leading to the popular meaning of the term \"fractal\".\nIn 1980, Loren Carpenter gave a presentation at the SIGGRAPH where he introduced his software for generating and rendering fractally generated landscapes.\nDefinition and characteristics.\nOne often cited description that Mandelbrot published to describe geometric fractals is \"a rough or fragmented geometric shape that can be split into parts, each of which is (at least approximately) a reduced-size copy of the whole\"; this is generally helpful but limited. Authors disagree on the exact definition of \"fractal\", but most usually elaborate on the basic ideas of self-similarity and the unusual relationship fractals have with the space they are embedded in.\nOne point agreed on is that fractal patterns are characterized by fractal dimensions, but whereas these numbers quantify complexity (i.e., changing detail with changing scale), they neither uniquely describe nor specify details of how to construct particular fractal patterns. In 1975 when Mandelbrot coined the word \"fractal\", he did so to denote an object whose Hausdorff\u2013Besicovitch dimension is greater than its topological dimension. However, this requirement is not met by space-filling curves such as the Hilbert curve.\nBecause of the trouble involved in finding one definition for fractals, some argue that fractals should not be strictly defined at all. According to Falconer, fractals should, in addition to being nowhere differentiable and able to have a fractal dimension, be only generally characterized by a gestalt of the following features;\nAs a group, these criteria form guidelines for excluding certain cases, such as those that may be self-similar without having other typically fractal features. A straight line, for instance, is self-similar but not fractal because it lacks detail, is easily described in Euclidean language, has the same Hausdorff dimension as topological dimension, and is fully defined without a need for recursion.\nCommon techniques for generating fractals.\nImages of fractals can be created by fractal generating programs. Because of the butterfly effect, a small change in a single variable can have an unpredictable outcome.\nSimulated fractals.\nFractal patterns have been modeled extensively, albeit within a range of scales rather than infinitely, owing to the practical limits of physical time and space. Models may simulate theoretical fractals or natural phenomena with fractal features. The outputs of the modelling process may be highly artistic renderings, outputs for investigation, or benchmarks for fractal analysis. Some specific applications of fractals to technology are listed elsewhere. Images and other outputs of modelling are normally referred to as being \"fractals\" even if they do not have strictly fractal characteristics, such as when it is possible to zoom into a region of the fractal image that does not exhibit any fractal properties. Also, these may include calculation or display artifacts which are not characteristics of true fractals.\nModeled fractals may be sounds, digital images, electrochemical patterns, circadian rhythms, etc.\nFractal patterns have been reconstructed in physical 3-dimensional space and virtually, often called \"in silico\" modeling. Models of fractals are generally created using fractal-generating software that implements techniques such as those outlined above. As one illustration, trees, ferns, cells of the nervous system, blood and lung vasculature, and other branching patterns in nature can be modeled on a computer by using recursive algorithms and L-systems techniques. The recursive nature of some patterns is obvious in certain examples\u2014a branch from a tree or a frond from a fern is a miniature replica of the whole: not identical, but similar in nature. Similarly, random fractals have been used to describe/create many highly irregular real-world objects. A limitation of modeling fractals is that resemblance of a fractal model to a natural phenomenon does not prove that the phenomenon being modeled is formed by a process similar to the modeling algorithms.\nNatural phenomena with fractal features.\nApproximate fractals found in nature display self-similarity over extended, but finite, scale ranges. The connection between fractals and leaves, for instance, is currently being used to determine how much carbon is contained in trees. Phenomena known to have fractal features include: \nIn creative works.\nSince 1999, more than 10 scientific groups have performed fractal analysis on over 50 of Jackson Pollock's (1912\u20131956) paintings which were created by pouring paint directly onto his horizontal canvases Recently, fractal analysis has been used to achieve a 93% success rate in distinguishing real from imitation Pollocks. Cognitive neuroscientists have shown that Pollock's fractals induce the same stress-reduction in observers as computer-generated fractals and Nature's fractals.\nDecalcomania, a technique used by artists such as Max Ernst, can produce fractal-like patterns. It involves pressing paint between two surfaces and pulling them apart.\nCyberneticist Ron Eglash has suggested that fractal geometry and mathematics are prevalent in African art, games, divination, trade, and architecture. Circular houses appear in circles of circles, rectangular houses in rectangles of rectangles, and so on. Such scaling patterns can also be found in African textiles, sculpture, and even cornrow hairstyles. Hokky Situngkir also suggested the similar properties in Indonesian traditional art, batik, and ornaments found in traditional houses.\nEthnomathematician Ron Eglash has discussed the planned layout of Benin city using fractals as the basis, not only in the city itself and the villages but even in the rooms of houses. He commented that \"When Europeans first came to Africa, they considered the architecture very disorganised and thus primitive. It never occurred to them that the Africans might have been using a form of mathematics that they hadn\u2019t even discovered yet.\" \nIn a 1996 interview with Michael Silverblatt, David Foster Wallace admitted that the structure of the first draft of \"Infinite Jest\" he gave to his editor Michael Pietsch was inspired by fractals, specifically the Sierpinski triangle (a.k.a. Sierpinski gasket), but that the edited novel is \"more like a lopsided Sierpinsky Gasket\".\nSome works by the Dutch artist M. C. Escher, such as Circle Limit III, contain shapes repeated to infinity that become smaller and smaller as they get near to the edges, in a pattern that would always look the same if zoomed in.\nPhysiological responses.\nHumans appear to be especially well-adapted to processing fractal patterns with D values between 1.3 and 1.5. When humans view fractal patterns with D values between 1.3 and 1.5, this tends to reduce physiological stress.\nApplications in technology.\nIon propulsion.\nWhen two-dimensional fractals are iterated many times, the perimeter of the fractal increases up to infinity, but the area may never exceed a certain value. A fractal in three-dimensional space is similar; such a fractal may have an infinite surface area, but never exceed a certain volume. This can be utilized to maximize the efficiency of ion propulsion when choosing electron emitter construction and material. If done correctly, the efficiency of the emission process can be maximized."}
{"id": "10915", "revid": "33333691", "url": "https://en.wikipedia.org/wiki?curid=10915", "title": "Fluid", "text": "In physics, a fluid is a substance that continually deforms (flows) under an applied shear stress, or external force. Fluids are a phase of matter and include liquids, gases and plasmas. They are substances with zero shear modulus, or, in simpler terms, substances which cannot resist any shear force applied to them. \nAlthough the term \"fluid\" includes both the liquid and gas phases, in common usage, \"fluid\" is often used synonymously with \"liquid\". This usage of the term is also common in medicine and in nutrition (\"take plenty of fluids\").\nLiquids form a free surface (that is, a surface not created by the container) while gases do not. Viscoelastic fluids like Silly Putty appear to behave similar to a solid when a sudden force is applied. Also substances with a very high viscosity such as pitch appear to behave like a solid (see pitch drop experiment).\nPhysics.\nFluids display properties such as:\nThese properties are typically a function of their inability to support a shear stress in static equilibrium. In contrast, solids respond to shear either with a spring-like restoring force, which means that deformations are reversible, or they require a certain initial stress before they deform (see plasticity).\nSolids respond with restoring forces to both shear stresses and to normal stresses\u2014both compressive and tensile. In contrast, ideal fluids only respond with restoring forces to normal stresses, called pressure: fluids can be subjected to both compressive stress, corresponding to positive pressure, and to tensile stress, corresponding to negative pressure. Both solids and liquids also have tensile strengths, which when exceeded in solids makes irreversible deformation and fracture, and in liquids causes the onset of cavitation. \nBoth solids and liquids have free surfaces, which cost some amount of free energy to form. In the case of solids, the amount of free energy to form a given unit of surface area is called surface energy, whereas for liquids the same quantity is called surface tension. The ability of liquids to flow results in different behaviour in response to surface tension than in solids, although in equilibrium both will try to minimise their surface energy: liquids tend to form rounded droplets, whereas pure solids tend to form crystals. Gases do not have free surfaces, and freely diffuse.\nModelling.\nIn a solid, shear stress is a function of strain, but in a fluid, shear stress is a function of strain rate. A consequence of this behavior is Pascal's law which describes the role of pressure in characterizing a fluid's state. \nDepending on the relationship between shear stress, and the rate of strain and its derivatives, fluids can be characterized as one of the following:\nThe behavior of fluids can be described by the Navier\u2013Stokes equations\u2014a set of partial differential equations which are based on:\nThe study of fluids is fluid mechanics, which is subdivided into fluid dynamics and fluid statics depending on whether the fluid is in motion."}
{"id": "10916", "revid": "164776", "url": "https://en.wikipedia.org/wiki?curid=10916", "title": "FAQ", "text": "A frequently asked questions (FAQ) forum is often used in articles, websites, email lists, and online forums where common questions tend to recur, for example through posts or queries by new users related to common knowledge gaps. The purpose of an FAQ is generally to provide information on frequent questions or concerns; however, the format is a useful means of organizing information, and text consisting of questions and their answers may thus be called an FAQ regardless of whether the questions are actually \"frequently\" asked.\nSince the acronym \"FAQ\" originated in textual media, its pronunciation varies. FAQ is most commonly pronounced as an initialism, \"F-A-Q\", but may also be pronounced as an acronym, \"FAQ\". Web designers often label a single list of questions as an \"FAQ\", such as on Google Search, while using \"FAQs\" to denote multiple lists of questions such as on United States Treasury sites. Use of \"FAQ\" to refer to a single frequently asked question, in and of itself, is less common.\nOrigins.\nWhile the name may be recent, the FAQ format itself is quite old. For example, Matthew Hopkins wrote \"The Discovery of Witches\" in 1648 as a list of questions and answers, introduced as \"Certain Queries answered\". Many old catechisms are in a question-and-answer (Q&amp;A) format. \"Summa Theologica\", written by Thomas Aquinas in the second half of the 13th century, is a series of common questions about Christianity to which he wrote a series of replies. Plato's dialogues are even older.\nOn the Internet.\nThe \"FAQ\" is an Internet textual tradition originating from the technical limitations of early mailing lists from NASA in the early 1980s. The first FAQ developed over several pre-Web years, starting from 1982 when storage was expensive. On ARPANET's SPACE mailing list, the presumption was that new users would download archived past messages through FTP. In practice this rarely happened, and the users tended to post questions to the mailing list instead of searching its archives. Repeating the \"right\" answers became tedious, and went against developing netiquette. A series of different measures were set up by loosely affiliated groups of computer system administrators, from regularly posted messages to netlib-like query email daemons. The acronym \"FAQ\" was developed between 1982 and 1985 by Eugene Miya of NASA for the SPACE mailing list.\nThe format was then picked up on other mailing lists and Usenet newsgroups.\nPosting frequency changed to monthly, and finally weekly and daily across a variety of mailing lists and newsgroups.\nThe first person to post a weekly FAQ was Jef Poskanzer to the Usenet net.graphics/comp.graphics newsgroups.\nEugene Miya experimented with the first daily FAQ.\nModern developments.\nNon-traditional FAQs.\nIn some cases, informative documents not in the traditional FAQ style have also been described as FAQs, particularly the video game FAQ, which is often a detailed description of gameplay, including tips, secrets, and beginning-to-end guidance. Rarely are videogame FAQs in a question-and-answer format, although they may contain a short section of questions and answers.\nOver time, the accumulated FAQs across all Usenet newsgroups sparked the creation of the \"*.answers\" moderated newsgroups such as comp.answers, misc.answers and sci.answers for crossposting and collecting FAQ across respective comp.*, misc.*, sci.* newsgroups.\nIn web design.\nThe FAQ has become an important component of websites, either as a stand-alone page or as a website section with multiple subpages per question or topic. Embedded links to FAQ pages have become commonplace in website navigation bars, bodies, or footers. The FAQ page is an important consideration in web design, in order to achieve several goals of customer service and search engine optimization (SEO), including\nCriticism.\nSome content providers discourage the use of FAQs in place of restructuring content under logical headings. For example, the UK Government Digital Service does not use FAQs."}
{"id": "10918", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=10918", "title": "Fibonacci number", "text": "In mathematics, the Fibonacci numbers, commonly denoted , form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is, \nformula_1\nand\nformula_2\nfor .\nThe beginning of the sequence is thus:\nformula_3\nUnder some older definitions, the value formula_4 is omitted, so that the sequence starts with formula_5 and the recurrence formula_6 is valid for . In his original definition, Fibonacci started the sequence with formula_7\nFibonacci numbers are strongly related to the golden ratio: Binet's formula expresses the th Fibonacci number in terms of and the golden ratio, and implies that the ratio of two consecutive Fibonacci numbers tends to the golden ratio as increases.\nFibonacci numbers are named after the Italian mathematician Leonardo of Pisa, later known as Fibonacci. In his 1202 book \"Liber Abaci\", Fibonacci introduced the sequence to Western European mathematics, although the sequence had been described earlier in Indian mathematics, as early as 200 BC in work by Pingala on enumerating possible patterns of Sanskrit poetry formed from syllables of two lengths. \nFibonacci numbers appear unexpectedly often in mathematics, so much so that there is an entire journal dedicated to their study, the \"Fibonacci Quarterly\". Applications of Fibonacci numbers include computer algorithms such as the Fibonacci search technique and the Fibonacci heap data structure, and graphs called Fibonacci cubes used for interconnecting parallel and distributed systems.\nThey also appear in biological settings, such as branching in trees, the arrangement of leaves on a stem, the fruit sprouts of a pineapple, the flowering of an artichoke, an uncurling fern, and the arrangement of a pine cone's bracts.\nFibonacci numbers are also closely related to Lucas numbers formula_8, in that the Fibonacci and Lucas numbers form a complementary pair of Lucas sequences: formula_9 and formula_10.\nHistory.\nThe Fibonacci sequence appears in Indian mathematics in connection with Sanskrit prosody, as pointed out by Parmanand Singh in 1986. In the Sanskrit poetic tradition, there was interest in enumerating all patterns of long (L) syllables of 2 units duration, juxtaposed with short (S) syllables of 1 unit duration. Counting the different patterns of successive L and S with a given total duration results in the Fibonacci numbers: the number of patterns of duration units is .\nKnowledge of the Fibonacci sequence was expressed as early as Pingala (\u00a0450\u00a0BC\u2013200\u00a0BC). Singh cites Pingala's cryptic formula \"misrau cha\" (\"the two are mixed\") and scholars who interpret it in context as saying that the number of patterns for beats () is obtained by adding one [S] to the cases and one [L] to the cases.\nBharata Muni also expresses knowledge of the sequence in the \"Natya Shastra\" (c.\u00a0100\u00a0BC\u2013c.\u00a0350\u00a0AD).\nHowever, the clearest exposition of the sequence arises in the work of Virahanka (c.\u00a0700 AD), whose own work is lost, but is available in a quotation by Gopala (c.\u00a01135):\nVariations of two earlier meters [is the variation]... For example, for [a meter of length] four, variations of meters of two [and] three being mixed, five happens. [works out examples 8, 13, 21]... In this way, the process should be followed in all \"m\u0101tr\u0101-v\u1e5bttas\" [prosodic combinations].\nHemachandra (c.\u00a01150) is credited with knowledge of the sequence as well, writing that \"the sum of the last and the one before the last is the number ... of the next m\u0101tr\u0101-v\u1e5btta.\"\nOutside India, the Fibonacci sequence first appears in the book \"Liber Abaci\" (1202) by Fibonacci where it is used to calculate the growth of rabbit populations. Fibonacci considers the growth of an idealized (biologically unrealistic) rabbit population, assuming that: a newly born breeding pair of rabbits are put in a field; each breeding pair mates at the age of one month, and at the end of their second month they always produce another pair of rabbits; and rabbits never die, but continue breeding forever. Fibonacci posed the puzzle: how many pairs will there be in one year?\nAt the end of the th month, the number of pairs of rabbits is equal to the number of mature pairs (that is, the number of pairs in month ) plus the number of pairs alive last month (month ). The number in the th month is the th Fibonacci number.\nThe name \"Fibonacci sequence\" was first used by the 19th-century number theorist \u00c9douard Lucas.\nSequence properties.\nThe first 21 Fibonacci numbers are:\nThe sequence can also be extended to negative index using the re-arranged recurrence relation\nformula_11\n which yields the sequence of \"negafibonacci\" numbers satisfying\nformula_12\nThus the bidirectional sequence is\nRelation to the golden ratio.\nClosed-form expression.\nLike every sequence defined by a linear recurrence with constant coefficients, the Fibonacci numbers have a closed form expression. It has become known as Binet's formula, named after French mathematician Jacques Philippe Marie Binet, though it was already known by Abraham de Moivre and Daniel Bernoulli:\nformula_13\nwhere\nformula_14\nis the golden ratio (), and\nformula_15\nSince formula_16, this formula can also be written as\nformula_17\nTo see this, note that and are both solutions of the equations\nformula_18\nso the powers of and satisfy the Fibonacci recursion. In other words,\nformula_19\nand\nformula_20\nIt follows that for any values and , the sequence defined by\nformula_21\nsatisfies the same recurrence\nformula_22\nIf and are chosen so that and then the resulting sequence must be the Fibonacci sequence. This is the same as requiring and satisfy the system of equations:\nformula_23\nwhich has solution\nformula_24\nproducing the required formula.\nTaking the starting values and to be arbitrary constants, a more general solution is:\nformula_25\nwhere\nformula_26\nformula_27\nComputation by rounding.\nSince\nformula_28\nfor all , the number is the closest integer to formula_29. Therefore, it can be found by rounding, using the nearest integer function:\nformula_30\nIn fact, the rounding error is very small, being less than 0.1 for , and less than 0.01 for .\nFibonacci number can also be computed by truncation, in terms of the floor function:\nformula_31\nAs the floor function is monotonic, the latter formula can be inverted for finding the index of the largest Fibonacci number that is not greater than a real number :\nformula_32\nwhere formula_33\nLimit of consecutive quotients.\nJohannes Kepler observed that the ratio of consecutive Fibonacci numbers converges. He wrote that \"as 5 is to 8 so is 8 to 13, practically, and as 8 is to 13, so is 13 to 21 almost\", and concluded that these ratios approach the golden ratio formula_34\nformula_35\nThis convergence holds regardless of the starting values, excluding 0 and 0, or any pair in the conjugate golden ratio, formula_36 This can be verified using Binet's formula. For example, the initial values 3 and 2 generate the sequence 3, 2, 5, 7, 12, 19, 31, 50, 81, 131, 212, 343, 555, ... The ratio of consecutive terms in this sequence shows the same convergence towards the golden ratio.\nDecomposition of powers.\nSince the golden ratio satisfies the equation\nformula_37\nthis expression can be used to decompose higher powers formula_38 as a linear function of lower powers, which in turn can be decomposed all the way down to a linear combination of formula_39 and 1. The resulting recurrence relationships yield Fibonacci numbers as the linear coefficients:\nformula_40\nThis equation can be proved by induction on \"n\".\nThis expression is also true for \"n\" &lt; 1 if the Fibonacci sequence \"Fn\" is extended to negative integers using the Fibonacci rule formula_41\nMatrix form.\nA 2-dimensional system of linear difference equations that describes the Fibonacci sequence is\nformula_42\nalternatively denoted\nformula_43\nwhich yields formula_44. The eigenvalues of the matrix are formula_45 and formula_46 corresponding to the respective eigenvectors \nformula_47 \nand\nformula_48\nAs the initial value is\nformula_49 \nit follows that the th term is\nformula_50\nFrom this, the th element in the Fibonacci series\nmay be read off directly as a closed-form expression:\nformula_51\nEquivalently, the same computation may performed by diagonalization of through use of its eigendecomposition:\nformula_52\nwhere formula_53 and formula_54\nThe closed-form expression for the th element in the Fibonacci series is therefore given by\nformula_55\nwhich again yields\nformula_56\nThe matrix has a determinant of \u22121, and thus it is a 2\u00d72 unimodular matrix.\nThis property can be understood in terms of the continued fraction representation for the golden ratio:\nformula_57\nThe Fibonacci numbers occur as the ratio of successive convergents of the continued fraction for , and the matrix formed from successive convergents of any continued fraction has a determinant of +1 or \u22121. The matrix representation gives the following closed-form expression for the Fibonacci numbers:\nformula_58\nTaking the determinant of both sides of this equation yields Cassini's identity,\nformula_59\nMoreover, since for any square matrix , the following identities can be derived (they are obtained from two different coefficients of the matrix product, and one may easily deduce the second one from the first one by changing into ),\nformula_60\nIn particular, with ,\nformula_61\nThese last two identities provide a way to compute Fibonacci numbers recursively in arithmetic operations and in time , where is the time for the multiplication of two numbers of digits. This matches the time for computing the th Fibonacci number from the closed-form matrix formula, but with fewer redundant steps if one avoids recomputing an already computed Fibonacci number (recursion with memoization).\nIdentification.\nThe question may arise whether a positive integer \"x\" is a Fibonacci number. This is true if and only if at least one of formula_62 or formula_63 is a perfect square. This is because Binet's formula above can be rearranged to give\nformula_64\nwhich allows one to find the position in the sequence of a given Fibonacci number.\nThis formula must return an integer for all \"n\", so the radical expression must be an integer (otherwise the logarithm does not even return a rational number).\nCombinatorial identities.\nMost identities involving Fibonacci numbers can be proved using combinatorial arguments using the fact that \"F\"\"n\" can be interpreted as the number of sequences of 1s and 2s that sum to \"n\"\u00a0\u2212\u00a01. This can be taken as the definition of \"F\"\"n\", with the convention that \"F\"0\u00a0=\u00a00, meaning no sum adds up to \u22121, and that \"F\"1\u00a0=\u00a01, meaning the empty sum \"adds up\" to 0. Here, the order of the summand matters. For example, 1\u00a0+\u00a02 and 2\u00a0+\u00a01 are considered two different sums.\nFor example, the recurrence relation\nor in words, the \"n\"th Fibonacci number is the sum of the previous two Fibonacci numbers, may be shown by dividing the \"F\"\"n\" sums of 1s and 2s that add to \"n\"\u00a0\u2212\u00a01 into two non-overlapping groups. One group contains those sums whose first term is 1 and the other those sums whose first term is 2. In the first group the remaining terms add to \"n\"\u00a0\u2212\u00a02, so it has \"F\"\"n\"-1 sums, and in the second group the remaining terms add to \"n\"\u00a0\u2212\u00a03, so there are \"F\"\"n\"\u22122 sums. So there are a total of \"F\"\"n\"\u22121 + \"F\"\"n\"\u22122 sums altogether, showing this is equal to \"F\"\"n\".\nSimilarly, it may be shown that the sum of the first Fibonacci numbers up to the \"n\"th is equal to the (\"n\" + 2)-nd Fibonacci number minus 1. In symbols:\nformula_66\nThis is done by dividing the sums adding to \"n\"\u00a0+\u00a01 in a different way, this time by the location of the first 2. Specifically, the first group consists of those sums that start with 2, the second group those that start 1\u00a0+\u00a02, the third 1\u00a0+\u00a01\u00a0+\u00a02, and so on, until the last group, which consists of the single sum where only 1's are used. The number of sums in the first group is \"F\"(\"n\"), \"F\"(\"n\"\u00a0\u2212\u00a01) in the second group, and so on, with 1 sum in the last group. So the total number of sums is \"F\"(\"n\")\u00a0+\u00a0\"F\"(\"n\"\u00a0\u2212\u00a01)\u00a0+\u00a0...\u00a0+\u00a0\"F\"(1)\u00a0+\u00a01 and therefore this quantity is equal to \"F\"(\"n\"\u00a0+\u00a02).\nA similar argument, grouping the sums by the position of the first 1 rather than the first\u00a02, gives two more identities:\nformula_67\nand\nformula_68\nIn words, the sum of the first Fibonacci numbers with odd index up to \"F\"2\"n\"\u22121 is the (2\"n\")th Fibonacci number, and the sum of the first Fibonacci numbers with even index up to \"F\"2\"n\" is the (2\"n\"\u00a0+\u00a01)th Fibonacci number minus\u00a01.\nA different trick may be used to prove\nformula_69\nor in words, the sum of the squares of the first Fibonacci numbers up to \"F\"\"n\" is the product of the \"n\"th and (\"n\"\u00a0+\u00a01)th Fibonacci numbers. In this case Fibonacci rectangle of size \"F\"\"n\" by \"F\"(\"n\"\u00a0+\u00a01) can be decomposed into squares of size \"F\"\"n\", \"F\"\"n\"\u22121, and so on to \"F\"1\u00a0=\u00a01, from which the identity follows by comparing areas.\nSymbolic method.\nThe sequence formula_70 is also considered using the symbolic method. More precisely, this sequence corresponds to a specifiable combinatorial class. The specification of this sequence is formula_71. Indeed, as stated above, the formula_72-th Fibonacci number equals the number of combinatorial compositions (ordered partitions) of formula_73 using terms 1 and 2.\nIt follows that the ordinary generating function of the Fibonacci sequence, i.e. formula_74, is the complex function formula_75.\nOther identities.\nNumerous other identities can be derived using various methods. Some of the most noteworthy are:\nCassini's and Catalan's identities.\nCassini's identity states that\nformula_76\nCatalan's identity is a generalization:\nformula_77\nd'Ocagne's identity.\nformula_78\nformula_79\nwhere \"L\"\"n\" is the \"n\"'th Lucas number. The last is an identity for doubling \"n\"; other identities of this type are\nformula_80\nby Cassini's identity.\nformula_81\nformula_82\nformula_83\nThese can be found experimentally using lattice reduction, and are useful in setting up the special number field sieve to factorize a Fibonacci number.\nMore generally,\nformula_84\nor alternatively\nformula_85\nPutting in this formula, one gets again the formulas of the end of above section Matrix form.\nPower series.\nThe generating function of the Fibonacci sequence is the power series\nformula_86\nThis series is convergent for formula_87 and its sum has a simple closed-form:\nformula_88\nThis can be proved by using the Fibonacci recurrence to expand each coefficient in the infinite sum:\nformula_89\nSolving the equation\nformula_90\nfor \"s\"(\"x\") results in the above closed form.\nSetting , the closed form of the series becomes\nformula_91\nIn particular, if is an integer greater than 1, then this series converges. Further setting yields\nformula_92\nfor all positive integers .\nSome math puzzle-books present as curious the particular value that comes from , which is formula_93 Similarly, gives\nformula_94\nReciprocal sums.\nInfinite sums over reciprocal Fibonacci numbers can sometimes be evaluated in terms of theta functions. For example, we can write the sum of every odd-indexed reciprocal Fibonacci number as\nformula_95\nand the sum of squared reciprocal Fibonacci numbers as\nformula_96\nIf we add 1 to each Fibonacci number in the first sum, there is also the closed form\nformula_97\nand there is a \"nested\" sum of squared Fibonacci numbers giving the reciprocal of the golden ratio,\nformula_98\nNo closed formula for the reciprocal Fibonacci constant\nformula_99\nis known, but the number has been proved irrational by Richard Andr\u00e9-Jeannin.\nThe Millin series gives the identity\nformula_100\nwhich follows from the closed form for its partial sums as \"N\" tends to infinity:\nformula_101\nPrimes and divisibility.\nDivisibility properties.\nEvery third number of the sequence is even and more generally, every \"k\"th number of the sequence is a multiple of \"Fk\". Thus the Fibonacci sequence is an example of a divisibility sequence. In fact, the Fibonacci sequence satisfies the stronger divisibility property\nformula_102\nAny three consecutive Fibonacci numbers are pairwise coprime, which means that, for every \"n\",\nEvery prime number \"p\" divides a Fibonacci number that can be determined by the value of \"p\" modulo\u00a05. If \"p\" is congruent to 1 or 4 (mod 5), then \"p\" divides \"F\"\"p\"\u00a0\u2212\u00a01, and if \"p\" is congruent to 2 or 3 (mod 5), then, \"p\" divides \"F\"\"p\"\u00a0+\u00a01. The remaining case is that \"p\"\u00a0=\u00a05, and in this case \"p\" divides \"F\"p.\nformula_103\nThese cases can be combined into a single, non-piecewise formula, using the Legendre symbol:\nformula_104\nPrimality testing.\nThe above formula can be used as a primality test in the sense that if\nformula_105\nwhere the Legendre symbol has been replaced by the Jacobi symbol, then this is evidence that \"n\" is a prime, and if it fails to hold, then \"n\" is definitely not a prime. If \"n\" is composite and satisfies the formula, then \"n\" is a \"Fibonacci pseudoprime\". When \"m\" is largesay a 500-bit numberthen we can calculate \"F\"\"m\" (mod \"n\") efficiently using the matrix form. Thus\nformula_106\nHere the matrix power \"A\"\"m\" is calculated using modular exponentiation, which can be adapted to matrices.\nFibonacci primes.\nA \"Fibonacci prime\" is a Fibonacci number that is prime. The first few are:\nFibonacci primes with thousands of digits have been found, but it is not known whether there are infinitely many.\n\"F\"\"kn\" is divisible by \"F\"\"n\", so, apart from \"F\"4 = 3, any Fibonacci prime must have a prime index. As there are arbitrarily long runs of composite numbers, there are therefore also arbitrarily long runs of composite Fibonacci numbers.\nNo Fibonacci number greater than \"F\"6 = 8 is one greater or one less than a prime number.\nThe only nontrivial square Fibonacci number is 144. Attila Peth\u0151 proved in 2001 that there is only a finite number of perfect power Fibonacci numbers. In 2006, Y. Bugeaud, M. Mignotte, and S. Siksek proved that 8 and 144 are the only such non-trivial perfect powers.\n1, 3, 21, 55 are the only triangular Fibonacci numbers, which was conjectured by Vern Hoggatt and proved by Luo Ming.\nNo Fibonacci number can be a perfect number. More generally, no Fibonacci number other than 1 can be multiply perfect, and no ratio of two Fibonacci numbers can be perfect.\nPrime divisors.\nWith the exceptions of 1, 8 and 144 (\"F\"1 = \"F\"2, \"F\"6 and \"F\"12) every Fibonacci number has a prime factor that is not a factor of any smaller Fibonacci number (Carmichael's theorem). As a result, 8 and 144 (\"F\"6 and \"F\"12) are the only Fibonacci numbers that are the product of other Fibonacci numbers .\nThe divisibility of Fibonacci numbers by a prime \"p\" is related to the Legendre symbol formula_107 which is evaluated as follows:\nformula_108\nIf \"p\" is a prime number then\nformula_109\nFor example,\nformula_110\nIt is not known whether there exists a prime \"p\" such that\nformula_111\nSuch primes (if there are any) would be called Wall\u2013Sun\u2013Sun primes.\nAlso, if \"p\" \u2260 5 is an odd prime number then:\nformula_112\nExample 1. \"p\" = 7, in this case \"p\" \u2261 3 (mod 4) and we have:\nformula_113\nformula_114\nformula_115\nExample 2. \"p\" = 11, in this case \"p\" \u2261 3 (mod 4) and we have:\nformula_116\nformula_117\nformula_118\nExample 3. \"p\" = 13, in this case \"p\" \u2261 1 (mod 4) and we have:\nformula_119\nformula_120\nformula_121\nExample 4. \"p\" = 29, in this case \"p\" \u2261 1 (mod 4) and we have:\nformula_122\nformula_123\nformula_124\nFor odd \"n\", all odd prime divisors of \"F\"\"n\" are congruent to 1 modulo 4, implying that all odd divisors of \"F\"\"n\" (as the products of odd prime divisors) are congruent to 1 modulo 4.\nFor example,\nformula_125\nAll known factors of Fibonacci numbers \"F\"(\"i\") for all \"i\" &lt; 50000 are collected at the relevant repositories.\nPeriodicity modulo \"n\".\nIf the members of the Fibonacci sequence are taken mod\u00a0\"n\", the resulting sequence is periodic with period at most\u00a0\"6n\". The lengths of the periods for various \"n\" form the so-called Pisano periods . Determining a general formula for the Pisano periods is an open problem, which includes as a subproblem a special instance of the problem of finding the multiplicative order of a modular integer or of an element in a finite field. However, for any particular \"n\", the Pisano period may be found as an instance of cycle detection.\nMagnitude.\nSince \"Fn\" is asymptotic to formula_126, the number of digits in \"F\"\"n\" is asymptotic to formula_127. As a consequence, for every integer \"d\" &gt; 1 there are either 4 or 5 Fibonacci numbers with \"d\" decimal digits.\nMore generally, in the base \"b\" representation, the number of digits in \"F\"\"n\" is asymptotic to formula_128\nGeneralizations.\nThe Fibonacci sequence is one of the simplest and earliest known sequences defined by a recurrence relation, and specifically by a linear difference equation. All these sequences may be viewed as generalizations of the Fibonacci sequence. In particular, Binet's formula may be generalized to any sequence that is a solution of a homogeneous linear difference equation with constant coefficients.\nSome specific examples that are close, in some sense, from Fibonacci sequence include:\nApplications.\nThe Fibonacci numbers occur in the sums of \"shallow\" diagonals in Pascal's triangle (see binomial coefficient):\nMathematics.\nformula_129\nThese numbers also give the solution to certain enumerative problems, the most common of which is that of counting the number of ways of writing a given number as an ordered sum of 1s and 2s (called compositions); there are ways to do this. For example, there are ways one can climb a staircase of 5 steps, taking one or two steps at a time:\nThe figure shows that 8 can be decomposed into 5 (the number of ways to climb 4 steps, followed by a single-step) plus 3 (the number of ways to climb 3 steps, followed by a double-step). The same reasoning is applied recursively until a single step, of which there is only one way to climb.\nThe Fibonacci numbers can be found in different ways among the set of binary strings, or equivalently, among the subsets of a given set.\nNature.\nFibonacci sequences appear in biological settings, such as branching in trees, arrangement of leaves on a stem, the fruitlets of a pineapple, the flowering of artichoke, an uncurling fern and the arrangement of a pine cone, and the family tree of honeybees. Kepler pointed out the presence of the Fibonacci sequence in nature, using it to explain the (golden ratio-related) pentagonal form of some flowers. Field daisies most often have petals in counts of Fibonacci numbers. In 1754, Charles Bonnet discovered that the spiral phyllotaxis of plants were frequently expressed in Fibonacci number series.\nPrzemys\u0142aw Prusinkiewicz advanced the idea that real instances can in part be understood as the expression of certain algebraic constraints on free groups, specifically as certain Lindenmayer grammars.\nA model for the pattern of florets in the head of a sunflower was proposed by in 1979. This has the form\nformula_131\nwhere is the index number of the floret and is a constant scaling factor; the florets thus lie on Fermat's spiral. The divergence angle, approximately 137.51\u00b0, is the golden angle, dividing the circle in the golden ratio. Because this ratio is irrational, no floret has a neighbor at exactly the same angle from the center, so the florets pack efficiently. Because the rational approximations to the golden ratio are of the form , the nearest neighbors of floret number are those at for some index , which depends on , the distance from the center. Sunflowers and similar flowers most commonly have spirals of florets in clockwise and counter-clockwise directions in the amount of adjacent Fibonacci numbers, typically counted by the outermost range of radii.\nFibonacci numbers also appear in the pedigrees of idealized honeybees, according to the following rules:\nThus, a male bee always has one parent, and a female bee has two. If one traces the pedigree of any male bee (1 bee), he has 1 parent (1 bee), 2 grandparents, 3 great-grandparents, 5 great-great-grandparents, and so on. This sequence of numbers of parents is the Fibonacci sequence. The number of ancestors at each level, , is the number of female ancestors, which is , plus the number of male ancestors, which is . This is under the unrealistic assumption that the ancestors at each level are otherwise unrelated.\nIt has been noticed that the number of possible ancestors on the human X chromosome inheritance line at a given ancestral generation also follows the Fibonacci sequence. A male individual has an X chromosome, which he received from his mother, and a Y chromosome, which he received from his father. The male counts as the \"origin\" of his own X chromosome (formula_132), and at his parents' generation, his X chromosome came from a single parent (formula_133). The male's mother received one X chromosome from her mother (the son's maternal grandmother), and one from her father (the son's maternal grandfather), so two grandparents contributed to the male descendant's X chromosome (formula_134). The maternal grandfather received his X chromosome from his mother, and the maternal grandmother received X chromosomes from both of her parents, so three great-grandparents contributed to the male descendant's X chromosome (formula_135). Five great-great-grandparents contributed to the male descendant's X chromosome (formula_136), etc. (This assumes that all ancestors of a given descendant are independent, but if any genealogy is traced far enough back in time, ancestors begin to appear on multiple lines of the genealogy, until eventually a population founder appears on all lines of the genealogy.)\nThe pathways of tubulins on intracellular microtubules arrange in patterns of 3, 5, 8 and 13.\nReferences.\nFootnotes\nCitations"}
{"id": "10921", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10921", "title": "Fighter Aircraft", "text": ""}
{"id": "10923", "revid": "3462140", "url": "https://en.wikipedia.org/wiki?curid=10923", "title": "Fontainebleau", "text": "Fontainebleau (; ) is a commune in the metropolitan area of Paris, France. It is located south-southeast of the centre of Paris. Fontainebleau is a sub-prefecture of the Seine-et-Marne department, and it is the seat of the \"arrondissement\" of Fontainebleau. The commune has the largest land area in the \u00cele-de-France region; it is the only one to cover a larger area than Paris itself.\nFontainebleau, together with the neighbouring commune of Avon and three other smaller communes, form an urban area of 39,713 inhabitants (according to the 2001 census). This urban area is a satellite of Paris.\nFontainebleau is renowned for the large and scenic forest of Fontainebleau, a favourite weekend getaway for Parisians, as well as for the historic Ch\u00e2teau de Fontainebleau, which once belonged to the kings of France. It is also the home of INSEAD, one of the world's most elite business schools.\nInhabitants of Fontainebleau are sometimes called \"Bellifontains\".\nHistory.\nFontainebleau was recorded in the Latinised forms \"Fons Bleaudi\", \"Fons Bliaudi\", and \"Fons Blaadi\" in the 12th and 13th centuries, as \"Fontem blahaud\" in 1137, as \"Fontaine belle eau\" (folk etymology \"fountain of beautiful water\") in the 16th century, as \"Fontainebleau\" and \"Fontaine belle eau\" in 1630, and as the invented, fanciful Latin \"Fons Bellaqueus\" in the 17th century, which is the origin of the fanciful name \"Bellifontains\" of the inhabitants. Contrary to the folk etymology, the name comes from the medieval compound noun of \"fontaine\", meaning spring (fountainhead) and fountain, and \"blitwald\", consisting of the Germanic personal name Blit and the Germanic word for forest.\nThis hamlet was endowed with a royal hunting lodge and a chapel by Louis VII in the middle of the twelfth century. A century later, Louis IX, also called Saint Louis, who held Fontainebleau in high esteem and referred to it as \"his wilderness\", had a country house and a hospital constructed there.\nPhilip the Fair was born there in 1268 and died there in 1314. In all, thirty-four sovereigns, from Louis VI, the Fat, (1081\u20131137) to Napoleon III (1808\u20131873), spent time at Fontainebleau.\nThe connection between the town of Fontainebleau and the French monarchy was reinforced with the transformation of the royal country house into a true royal palace, the Palace of Fontainebleau. This was accomplished by the great builder-king, Francis I (1494\u20131547), who, in the largest of his many construction projects, reconstructed, expanded, and transformed the royal ch\u00e2teau at Fontainebleau into a residence that became his favourite, as well as the residence of his mistress, Anne, duchess of \u00c9tampes.\nFrom the sixteenth to the eighteenth century, every monarch, from Francis I to Louis XV, made important renovations at the Palace of Fontainebleau, including demolitions, reconstructions, additions, and embellishments of various descriptions, all of which endowed it with a character that is a bit heterogeneous, but harmonious nonetheless.\nOn 18 October 1685, Louis XIV signed the \"Edict of Fontainebleau\" there. Also known as the \"Revocation of the Edict of Nantes\", this royal fiat reversed the permission granted to the Huguenots in 1598 to worship publicly in specified locations and hold certain other privileges. The result was that a large number of Protestants were forced to convert to the Catholic faith, killed, or forced into exile, mainly in the Low Countries, Prussia and in England.\nThe 1762 Treaty of Fontainebleau, a secret agreement between France and Spain concerning the Louisiana territory in North America, was concluded here. Also, preliminary negotiations, held before the 1763 Treaty of Paris was signed, ending the Seven Years' War, were at Fontainebleau.\nDuring the French Revolution, Fontainebleau was temporarily renamed Fontaine-la-Montagne, meaning \"Fountain by the Mountain\". (The mountain referred to is the series of rocky formations located in the forest of Fontainebleau.)\nOn 29 October 1807, Manuel Godoy, chancellor to the Spanish king, Charles IV and Napoleon signed the Treaty of Fontainebleau, which authorized the passage of French troops through Spanish territories so that they might invade Portugal.\nOn 20 June 1812, Pope Pius VII arrived at the ch\u00e2teau of Fontainebleau, after a secret transfer from Savona, accompanied by his personal physician, Balthazard Claraz. In poor health, the Pope was the prisoner of Napoleon, and he remained in his genteel prison at Fontainebleau for nineteen months. From June 1812 until 23 January 1814, the Pope never left his apartments.\nOn 20 April 1814, Napoleon Bonaparte, shortly before his first abdication, bid farewell to the Old Guard, the renowned \"grognards\" (gripers) who had served with him since his first campaigns, in the \"White Horse Courtyard\" (la cour du Cheval Blanc) at the Palace of Fontainebleau. (The courtyard has since been renamed the \"Courtyard of Goodbyes\".) According to contemporary sources, the occasion was very moving. The 1814 Treaty of Fontainebleau stripped Napoleon of his powers (but not his title as Emperor of the French) and sent him into exile on Elba.\nUntil the 19th century, Fontainebleau was a village and a suburb of Avon. Later, it developed as an independent residential city.\nFor the 1924 Summer Olympics, the town played host to the riding portion of the modern pentathlon event. This event took place near a golf course.\nIn July and August 1946, the town hosted the Franco-Vietnamese Conference, intended to find a solution to the long-contested struggle for Vietnam's independence from France, but the conference ended in failure.\nFontainebleau also hosted the general staff of the Allied Forces in Central Europe (Allied Forces Center or AFCENT) and the land forces command (LANDCENT); the air forces command (AIRCENT) was located nearby at Camp Guynemer. These facilities were in place from the inception of NATO until France's partial withdrawal from NATO in 1967 when the United States returned those bases to French control. NATO moved AFCENT to Brunssum in the Netherlands and AIRCENT to Ramstein in West Germany. (Note that the Supreme Headquarters Allied Powers Europe, also known as SHAPE, was located at Rocquencourt, west of Paris, quite a distance from Fontainebleau).\nIn 2008, The men's World Championship of Real Tennis (Jeu de Paume) was held in the tennis court of the Chateau. The real tennis World Championship is the oldest in sport and Fontainebleau has one of only two active courts in France.\nTourism.\nFontainebleau is a popular tourist destination; each year, 300,000 people visit the palace and more than 13 million people visit the forest.\nFontainebleau forest.\nThe forest of Fontainebleau surrounds the town and dozens of nearby villages. It is protected by France's \"Office National des For\u00eats\", and it is recognised as a French national park. It is managed in order that its wild plants and trees, such as the rare service tree of Fontainebleau, and its populations of birds, mammals, and butterflies, can be conserved. It is a former royal hunting park often visited by hikers and horse riders. The forest is also well regarded for bouldering and is particularly popular among climbers, as it is the biggest developed area of that kind in the world.\nRoyal Ch\u00e2teau de Fontainebleau.\nThe Royal Ch\u00e2teau de Fontainebleau is a large palace where the kings of France took their ease. It is also the site where the French royal court, from 1528 onwards, entertained the body of new ideas that became known as the Renaissance.\nINSEAD.\nThe European (and historic) campus of the INSEAD business school is located at the edge of Fontainebleau, by the Lycee Francois Couperin. INSEAD students live in local accommodations around the Fontainebleau area, and especially in the surrounding towns.\nOther notables.\nThe graves of G. I. Gurdjieff and Katherine Mansfield can be found in the cemetery at Avon.\nTransport.\nFontainebleau is served by two stations on the Transilien Paris\u2013Lyon rail line: Fontainebleau\u2013Avon and Thomery. Fontainebleau\u2013Avon station, the station closest to the centre of Fontainebleau, is located near the dividing-line between the commune of Fontainebleau and the commune of Avon, on the Avon side of the border.\nHospital.\nFontainebleau has a campus of the Centre hospitalier Sud Seine et Marne.\nTwinning.\nFontainebleau is twinned with the following cities:"}
{"id": "10924", "revid": "3113", "url": "https://en.wikipedia.org/wiki?curid=10924", "title": "Fermats little theorom", "text": ""}
{"id": "10927", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=10927", "title": "Fossil fuels", "text": ""}
{"id": "10929", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=10929", "title": "Fighter aircraft", "text": "Fighter aircraft are fixed-wing military aircraft designed primarily for air-to-air combat. In military conflict, the role of fighter aircraft is to establish air superiority of the battlespace. Domination of the airspace above a battlefield permits bombers and attack aircraft to engage in tactical and strategic bombing of enemy targets. \nThe key performance features of a fighter include not only its firepower but also its high speed and maneuverability relative to the target aircraft. The success or failure of a combatant's efforts to gain air superiority hinges on several factors including the skill of its pilots, the tactical soundness of its doctrine for deploying its fighters, and the numbers and performance of those fighters.\nMany modern fighter aircraft have secondary capabilities such as ground attack and some types, such as fighter-bombers, are designed from the outset for dual roles. Other fighter designs are highly specialized while still filling the main air superiority role, these include the interceptor, heavy fighter, and night fighter.\nClassification.\nA fighter aircraft is primarily designed for air-to-air combat. A given type may be designed for specific combat conditions, and in some cases for additional roles such as air-to-ground fighting. Historically the British Royal Flying Corps and Royal Air Force referred to them as \"scouts\" until the early 1920s, while the U.S. Army called them \"pursuit\" aircraft until the late 1940s. The UK changed to calling them fighters in the 1920s, while the US Army did so in the 1940s. A short-range fighter designed to defend against incoming enemy aircraft is known as an interceptor.\nRecognised classes of fighter include:\nOf these, the Fighter-bomber, reconnaissance fighter and strike fighter classes are dual-role, possessing qualities of the fighter alongside some other battlefield role. Some fighter designs may be developed in variants performing other roles entirely, such as ground attack or unarmed reconnaissance. This may be for political or national security reasons, for advertising purposes, or other reasons.\nThe Sopwith Camel and other \"fighting scouts\" of World War I performed a great deal of ground-attack work. In World War II, the USAAF and RAF often favored fighters over dedicated light bombers or dive bombers, and types such as the Republic P-47 Thunderbolt and Hawker Hurricane that were no longer competitive as aerial combat fighters were relegated to ground attack. Several aircraft, such as the F-111 and F-117, have received fighter designations though they had no fighter capability due to political or other reasons. The F-111B variant was originally intended for a fighter role with the U.S. Navy, but it was canceled. This blurring follows the use of fighters from their earliest days for \"attack\" or \"strike\" operations against ground targets by means of strafing or dropping small bombs and incendiaries. Versatile multi role fighter-bombers such as the McDonnell Douglas F/A-18 Hornet are a less expensive option than having a range of specialized aircraft types.\nSome of the most expensive fighters such as the US Grumman F-14 Tomcat, McDonnell Douglas F-15 Eagle, Lockheed Martin F-22 Raptor and Russian Sukhoi Su-27 were employed as all-weather interceptors as well as air superiority fighter aircraft, while commonly developing air-to-ground roles late in their careers. An interceptor is generally an aircraft intended to target (or intercept) bombers and so often trades maneuverability for climb rate.\nAs a part of military nomenclature, a letter is often assigned to various types of aircraft to indicate their use, along with a number to indicate the specific aircraft. The letters used to designate a fighter differ in various countries\u00a0\u2013 in the English-speaking world, \"F\" is now used to indicate a fighter (e.g. Lockheed Martin F-35 Lightning II or Supermarine Spitfire F.22), though when the pursuit designation was used in the US, they were \"P\" types (e.g. Curtiss P-40 Warhawk). In Russia \"I\" was used (Polikarpov I-16), while the French continue to use \"C\" (Nieuport 17 C.1).\nAir superiority fighter.\nAs fighter types have proliferated, the air superiority fighter emerged as a specific role at the pinnacle of speed, maneuverability, and air-to-air weapon systems \u2013 able to hold its own against all other fighters and establish its dominance in the skies above the battlefield.\nInterceptor.\nThe interceptor is a fighter designed specifically to intercept and engage approaching enemy aircraft. There are two general classes of interceptor: relatively lightweight aircraft in the point-defence role, built for fast reaction, high performance and with a short range, and heavier aircraft with more comprehensive avionics and designed to fly at night or in all weathers and to operate over longer ranges. Originating during World War I, by 1929 this class of fighters had become known as the interceptor.\nNight and all-weather fighters.\nThe equipment necessary for daytime flight is inadequate when flying at night or in poor visibility. The night fighter was developed during World War I with additional equipment to aid the pilot in flying straight, navigating and finding the target. From modified variants of the Royal Aircraft Factory B.E.2c in 1915, the night fighter has evolved into the highly capable all-weather fighter.\nStrategic fighters.\nThe strategic fighter is a fast, heavily armed and long-range type, able to act as an escort fighter protecting bombers, to carry out offensive sorties of its own as a penetration fighter and maintain standing patrols at significant distance from its home base.\nBombers are vulnerable due to their low speed and poor maneuvrability. The escort fighter was developed during World War II to come between the bombers and enemy attackers as a protective shield. The primary requirement was for long range, with several heavy fighters given the role. However they too proved unwieldy and vulnerable, so as the war progressed techniques such as drop tanks were developed to extend the range of more nimble conventional fighters.\nThe penetration fighter is typically also fitted for the ground-attack role, and so is able to defend itself while conducting attack sorties.\nHistorical overview.\nSince World War I, achieving and maintaining air superiority has been considered essential for victory in conventional warfare.\nFighters continued to be developed throughout World War I, to deny enemy aircraft and dirigibles the ability to gather information by reconnaissance over the battlefield. Early fighters were very small and lightly armed by later standards, and most were biplanes built with a wooden frame covered with fabric, and a maximum airspeed of about . As control of the airspace over armies became increasingly important, all of the major powers developed fighters to support their military operations. Between the wars, wood was largely replaced in part or whole by metal tubing, and finally aluminum stressed skin structures (monocoque) began to predominate.\nBy World War II, most fighters were all-metal monoplanes armed with batteries of machine guns or cannons and some were capable of speeds approaching . Most fighters up to this point had one engine, but a number of twin-engine fighters were built; however they were found to be outmatched against single-engine fighters and were relegated to other tasks, such as night fighters equipped with primitive radar sets.\nBy the end of the war, turbojet engines were replacing piston engines as the means of propulsion, further increasing aircraft speed. Since the weight of the turbojet engine was far less than a piston engine, having two engines was no longer a handicap and one or two were used, depending on requirements. This in turn required the development of ejection seats so the pilot could escape, and G-suits to counter the much greater forces being applied to the pilot during maneuvers.\nIn the 1950s, radar was fitted to day fighters, since due to ever increasing air-to-air weapon ranges, pilots could no longer see far enough ahead to prepare for the opposition. Subsequently, radar capabilities grew enormously and are now the primary method of target acquisition. Wings were made thinner and swept back to reduce transonic drag, which required new manufacturing methods to obtain sufficient strength. Skins were no longer sheet metal riveted to a structure, but milled from large slabs of alloy. The sound barrier was broken, and after a few false starts due to required changes in controls, speeds quickly reached Mach 2, past which aircraft cannot maneuver sufficiently to avoid attack.\nAir-to-air missiles largely replaced guns and rockets in the early 1960s since both were believed unusable at the speeds being attained, however the Vietnam War showed that guns still had a role to play, and most fighters built since then are fitted with cannon (typically between 20 and 30\u00a0mm in caliber) in addition to missiles. Most modern combat aircraft can carry at least a pair of air-to-air missiles.\nIn the 1970s, turbofans replaced turbojets, improving fuel economy enough that the last piston engined support aircraft could be replaced with jets, making multi-role combat aircraft possible. Honeycomb structures began to replace milled structures, and the first composite components began to appear on components subjected to little stress. Needless to say, earlier generations of engines consumed much less fuel; today, a fighter aircraft consumes as much fuel in one hour as an average motorist does in two whole years.\nWith the steady improvements in computers, defensive systems have become increasingly efficient. To counter this, stealth technologies have been pursued by the United States, Russia, India and China. The first step was to find ways to reduce the aircraft's reflectivity to radar waves by burying the engines, eliminating sharp corners and diverting any reflections away from the radar sets of opposing forces. Various materials were found to absorb the energy from radar waves, and were incorporated into special finishes that have since found widespread application. Composite structures have become widespread, including major structural components, and have helped to counterbalance the steady increases in aircraft weight\u2014most modern fighters are larger and heavier than World War II medium bombers.\nBecause of the importance of air superiority, since the early days of aerial combat armed forces have constantly competed to develop technologically superior fighters and to deploy these fighters in greater numbers, and fielding a viable fighter fleet consumes a substantial proportion of the defense budgets of modern armed forces.\nThe global combat aircraft market was worth $45.75\u00a0billion in 2017 and is projected by Frost &amp; Sullivan at $47.2\u00a0billion in 2026: 35% modernization programs and 65% aircraft purchases, dominated by the Lockheed Martin F-35 with 3,000 deliveries over 20 years.\nPiston engine fighters.\nWorld War I.\nThe word \"fighter\" was first used to describe a two-seater aircraft with sufficient lift to carry a machine gun and its operator as well as the pilot. Some of the first such \"fighters\" belonged to the \"gunbus\" series of experimental gun carriers of the British Vickers company that culminated in the Vickers F.B.5 Gunbus of 1914. The main drawback of this type of aircraft was its lack of speed. Planners quickly realized that an aircraft intended to destroy its kind in the air had to be fast enough to catch its quarry.\nOne of the first companies to develop an armed aircraft was Vickers. Their Type 18 Destroyer of 1913 was a two-seat pusher type, with the pilot behind and an observer/gunner in front and a machine gun fitted in the nose on a pivoting mount. It would be developed as the F.B.5 \"Gunbus\" and introduced into service in 1915.\nHowever at the outbreak of World War I, front-line aircraft were unarmed and used almost entirely for reconnaissance. On 15 August 1914, Miodrag Tomi\u0107 encountered an enemy plane while conducting a reconnaissance flight over Austria-Hungary. The enemy pilot shot at Tomi\u0107's plane with a revolver. Tomi\u0107 produced a pistol of his own and fired back. It was considered the first exchange of fire between aircraft in history. Within weeks, all Serbian and Austro-Hungarian aircraft were armed. Machine guns were soon fitted to existing reconnaissance types for use by the observer, but none of these were true fighter planes.\nAnother type of military aircraft was to form the basis for an effective \"fighter\" in the modern sense of the word. It was based on the small fast aircraft developed before the war for such air races as the Gordon Bennett Cup and Schneider Trophy. The military scout airplane was not expected to carry serious armament, but rather to rely on its speed to reach the scout or reconnoiter location and return quickly to report, making it essentially an aerial horse. British scout aircraft, in this sense, included the Sopwith Tabloid and Bristol Scout. French equivalents included the Morane-Saulnier N.\nThe next advance came with the fixed forward-firing machine gun, so that the pilot pointed the whole plane at the target and fired the gun, instead of relying on a second gunner. Roland Garros (aviator) bolted metal deflector plates to the propeller so that it would not shoot itself out of the sky and a number of Morane-Saulnier Ns were modified. The technique proved effective, however the deflected bullets were still highly dangerous.\nThe next fighter manufactured in any quantity was the Fokker E.I \"Eindecker\" and its derivatives, whose introduction in 1915, only a few months after the appearance of the slower Gunbus, ushered in what the Allies came to call the \"Fokker scourge\" and a period of air superiority for the German forces. Although it still had mediocre flying qualities, the Fokker's unique innovation was an interrupter gear which allowed the gun to fire through the propeller arc without hitting the blades.\nSoon after the commencement of the war, pilots armed themselves with pistols, carbines, grenades, and an assortment of improvised weapons. Many of these proved ineffective as the pilot had to fly his airplane while attempting to aim a handheld weapon and make a difficult deflection shot. The first step in finding a real solution was to mount the weapon on the aircraft, but the propeller remained a problem since the best direction to shoot is straight ahead. Numerous solutions were tried. A second crew member behind the pilot could aim and fire a swivel-mounted machine gun at enemy airplanes; however, this limited the area of coverage chiefly to the rear hemisphere, and effective coordination of the pilot's maneuvering with the gunner's aiming was difficult. This option was chiefly employed as a defensive measure on two-seater reconnaissance aircraft from 1915 on. Both the SPAD S.A and the Royal Aircraft Factory B.E.9 added a second crewman ahead of the engine in a pod but this was both hazardous to the second crewman and limited performance. The Sopwith L.R.T.Tr. similarly added a pod on the top wing with no better luck.\nAn alternative was to build a \"pusher\" scout such as the Airco DH.2, with the propeller mounted behind the pilot. The main drawback was that the high drag of a pusher type's tail structure made it slower than a similar \"tractor\" aircraft.\nA better solution for a single seat scout was to mount the machine gun (rifles and pistols having been dispensed with) to fire forwards but outside the propeller arc. Wing guns were tried but the unreliable weapons available required frequent clearing of jammed rounds and misfires and remained impractical until after the war. Mounting the machine gun over the top wing worked well and was used long after the ideal solution was found. The Nieuport 11 of 1916 and Royal Aircraft Factory S.E.5 of 1918 both used this system with considerable success; however, this placement made aiming difficult and the location made it difficult for a pilot to both maneuver and have access to the gun's breech. The British Foster mounting was specifically designed for this kind of application, fitted with the Lewis Machine gun, which due to its design was unsuitable for synchronizing.\nThe need to arm a tractor scout with a forward-firing gun whose bullets passed through the propeller arc was evident even before the outbreak of war and inventors in both France and Germany devised mechanisms that could time the firing of the individual rounds to avoid hitting the propeller blades. Franz Schneider, a Swiss engineer, had patented such a device in Germany in 1913, but his original work was not followed up. French aircraft designer Raymond Saulnier patented a practical device in April 1914, but trials were unsuccessful because of the propensity of the machine gun employed to hang fire due to unreliable ammunition.\nIn December 1914, French aviator Roland Garros asked Saulnier to install his synchronization gear on Garros' Morane-Saulnier Type L. Unfortunately the gas-operated Hotchkiss machine gun he was provided had an erratic rate of fire and it was impossible to synchronize it with a spinning propeller. As an interim measure, the propeller blades were armored and fitted with metal wedges to protect the pilot from ricochets. Garros' modified monoplane was first flown in March 1915 and he began combat operations soon thereafter. Garros scored three victories in three weeks before he himself was downed on 18 April and his airplane, along with its synchronization gear and propeller was captured by the Germans.\nMeanwhile, the synchronization gear (called the \"Stangensteuerung\" in German, for \"pushrod control system\") devised by the engineers of Anthony Fokker's firm was the first system to see production contracts, and would make the Fokker \"Eindecker\" monoplane a feared name over the Western Front, despite its being an adaptation of an obsolete pre-war French Morane-Saulnier racing airplane, with a mediocre performance and poor flight characteristics. The first victory for the \"Eindecker\" came on 1 July 1915, when \"Leutnant\" Kurt Wintgens, flying with the \"Feldflieger Abteilung 6\" unit on the Western Front, forced down a Morane-Saulnier Type L two-seat \"parasol\" monoplane just east of Luneville. Wintgens' aircraft, one of the five Fokker M.5K/MG production prototype examples of the \"Eindecker\", was armed with a synchronized, air-cooled aviation version of the Parabellum MG14 machine gun.\nThe success of the \"Eindecker\" kicked off a competitive cycle of improvement among the combatants, both sides striving to build ever more capable single-seat fighters. The Albatros D.I and Sopwith Pup of 1916 set the classic pattern followed by fighters for about twenty years. Most were biplanes and only rarely monoplanes or triplanes. The strong box structure of the biplane provided a rigid wing that allowed the accurate lateral control essential for dogfighting. They had a single operator, who flew the aircraft and also controlled its armament. They were armed with one or two Maxim or Vickers machine guns, which were easier to synchronize than other types, firing through the propeller arc. Gun breeches were directly in front of the pilot, with obvious implications in case of accidents, but jams could be cleared in flight, while aiming was simplified.\nThe use of metal aircraft structures was pioneered before World War I by Breguet but would find its biggest proponent in Anthony Fokker, who used chrome-molybdenum steel tubing for the fuselage structure of all his fighter designs, while the innovative German engineer Hugo Junkers developed two all-metal, single-seat fighter monoplane designs with cantilever wings: the strictly experimental Junkers J 2 private-venture aircraft, made with steel, and some forty examples of the Junkers D.I, made with corrugated duralumin, all based on his experience in creating the pioneering Junkers J\u00a01 all-metal airframe technology demonstration aircraft of late 1915. While Fokker would pursue steel tube fuselages with wooden wings until the late 1930s, and Junkers would focus on corrugated sheet metal, Dornier was the first to build a fighter (The Dornier-Zeppelin D.I) made with pre-stressed sheet aluminum and having cantilevered wings, a form that would replace all others in the 1930s.\nAs collective combat experience grew, the more successful pilots such as Oswald Boelcke, Max Immelmann, and Edward Mannock developed innovative tactical formations and maneuvers to enhance their air units' combat effectiveness.\nAllied and\u00a0\u2013 before 1918\u00a0\u2013 German pilots of World War I were not equipped with parachutes, so in-flight fires or structural failure were often fatal. Parachutes were well-developed by 1918 having previously been used by balloonists, and were adopted by the German flying services during the course of that year. The well known and feared Manfred von Richthofen, the \"Red Baron\", was wearing one when he was killed, but the allied command continued to oppose their use on various grounds.\nIn April 1917, during a brief period of German aerial supremacy a British pilot's average life expectancy was 93 flying hours, or about three weeks of active service. More than 50,000 airmen from both sides died during the war.\nInter-war period (1919\u201338).\nFighter development stagnated between the wars, especially in the United States and the United Kingdom, where budgets were small. In France, Italy and Russia, where large budgets continued to allow major development, both monoplanes and all metal structures were common. By the end of the 1920s, however, those countries overspent themselves and were overtaken in the 1930s by those powers that hadn't been spending heavily, namely the British, the Americans and the Germans.\nGiven limited defense budgets, air forces tended to be conservative in their aircraft purchases, and biplanes remained popular with pilots because of their agility, and remained in service long after they had ceased to be competitive. Designs such as the Gloster Gladiator, Fiat CR.42 Falco, and Polikarpov I-15 were common even in the late 1930s, and many were still in service as late as 1942. Up until the mid-1930s, the majority of fighters in the US, the UK, Italy and Russia remained fabric-covered biplanes.\nFighter armament eventually began to be mounted inside the wings, outside the arc of the propeller, though most designs retained two synchronized machine guns directly ahead of the pilot, where they were more accurate (that being the strongest part of the structure, reducing the vibration to which the guns were subjected to). Shooting with this traditional arrangement was also easier for the further reason that the guns shot directly ahead in the direction of the aircraft's flight, up to the limit of the guns range; unlike wing-mounted guns which to be effective required to be harmonised, that is, preset to shoot at an angle by ground crews so that their bullets would converge on a target area a set distance ahead of the fighter. Rifle-caliber .30 and .303 in (7.62\u00a0mm) caliber guns remained the norm, with larger weapons either being too heavy and cumbersome or deemed unnecessary against such lightly built aircraft. It was not considered unreasonable to use World War I-style armament to counter enemy fighters as there was insufficient air-to-air combat during most of the period to disprove this notion.\nThe rotary engine, popular during World War I, quickly disappeared, its development having reached the point where rotational forces prevented more fuel and air from being delivered to the cylinders, which limited horsepower. They were replaced chiefly by the stationary radial engine though major advances led to inline engines, which gained ground with several exceptional engines\u2014including the V-12 Curtiss D-12. Aircraft engines increased in power several-fold over the period, going from a typical in the 900-kg Fokker D.VII of 1918 to in the 2,500-kg Curtiss P-36 of 1936. The debate between the sleek in-line engines versus the more reliable radial models continued, with naval air forces preferring the radial engines, and land-based forces often choosing in-line units. Radial designs did not require a separate (and vulnerable) cooling system, but had increased drag. In-line engines often had a better power-to-weight ratio, but there were radial engines that kept working even after having suffered significant battle damage.\nSome air forces experimented with \"heavy fighters\" (called \"destroyers\" by the Germans). These were larger, usually twin-engined aircraft, sometimes adaptations of light or medium bomber types. Such designs typically had greater internal fuel capacity (thus longer range) and heavier armament than their single-engine counterparts. In combat, they proved vulnerable to more agile single-engine fighters.\nThe primary driver of fighter innovation, right up to the period of rapid re-armament in the late 1930s, were not military budgets, but civilian aircraft racing. Aircraft designed for these races introduced innovations like streamlining and more powerful engines that would find their way into the fighters of World War II. The most significant of these was the Schneider Trophy races, where competition grew so fierce, only national governments could afford to enter.\nAt the very end of the inter-war period in Europe came the Spanish Civil War. This was just the opportunity the German \"Luftwaffe\", Italian \"Regia Aeronautica\", and the Soviet Union's Red Air Force needed to test their latest aircraft. Each party sent numerous aircraft types to support their sides in the conflict. In the dogfights over Spain, the latest Messerschmitt Bf 109 fighters did well, as did the Soviet Polikarpov I-16. The German design had considerably more room for development however and the lessons learned led to greatly improved models in World War II. The Russians, whose side lost, failed to keep up and despite newer models coming into service, I-16s were outfought by the improved Bf 109s in World War II, while remaining the most common Soviet front-line fighter into 1942. For their part, the Italians developed several monoplanes such as the Fiat G.50 Freccia, but being short on funds, were forced to continue operating obsolete Fiat CR.42 Falco biplanes.\nFrom the early 1930s the Japanese had been at war against both the Chinese Nationalists and the Russians in China, and used the experience to improve both training and aircraft, replacing biplanes with modern cantilever monoplanes and creating a cadre of exceptional pilots for use in the Pacific War. In the United Kingdom, at the behest of Neville Chamberlain, (more famous for his 'peace in our time' speech) the entire British aviation industry was retooled, allowing it to change quickly from fabric covered metal framed biplanes to cantilever stressed skin monoplanes in time for the war with Germany.\nThe period of improving the same biplane design over and over was now coming to an end, and the Hawker Hurricane and Supermarine Spitfire finally started to supplant the Gloster Gladiator and Hawker Fury biplanes but many of the former remained in front-line service well past the start of World War II. While not a combatant themselves in Spain, they absorbed many of the lessons learned in time to use them.\nThe Spanish Civil War also provided an opportunity for updating fighter tactics. One of the innovations to result from the aerial warfare experience this conflict provided was the development of the \"finger-four\" formation by the German pilot Werner M\u00f6lders. Each fighter squadron (German: \"Staffel\") was divided into several flights (\"Schw\u00e4rme\") of four aircraft. Each \"Schwarm\" was divided into two \"Rotten\", which was a pair of aircraft. Each \"Rotte\" was composed of a leader and a wingman. This flexible formation allowed the pilots to maintain greater situational awareness, and the two \"Rotten\" could split up at any time and attack on their own. The finger-four would become widely adopted as the fundamental tactical formation over the course of World War.\nWorld War II.\nWorld War II featured fighter combat on a larger scale than any other conflict to date. German Field Marshal Erwin Rommel noted the effect of airpower: \"Anyone who has to fight, even with the most modern weapons, against an enemy in complete command of the air, fights like a savage against modern European troops, under the same handicaps and with the same chances of success.\" Throughout the war, fighters performed their conventional role in establishing air superiority through combat with other fighters and through bomber interception, and also often performed roles such as tactical air support and reconnaissance.\nFighter design varied widely among combatants. The Japanese and Italians favored lightly armed and armored but highly maneuverable designs such as the Japanese Nakajima Ki-27, Nakajima Ki-43 and Mitsubishi A6M Zero and the Italian Fiat G.50 Freccia and Macchi MC.200. In contrast, designers in the United Kingdom, Germany, the Soviet Union, and the United States believed that the increased speed of fighter aircraft would create \"g\"-forces unbearable to pilots who attempted maneuvering dogfights typical of the First World War, and their fighters were instead optimized for speed and firepower. In practice, while light, highly maneuverable aircraft did possess some advantages in fighter-versus-fighter combat, those could usually be overcome by sound tactical doctrine, and the design approach of the Italians and Japanese made their fighters ill-suited as interceptors or attack aircraft.\nEuropean theater.\nDuring the invasion of Poland and the Battle of France, Luftwaffe fighters\u2014primarily the Messerschmitt Bf 109\u2014held air superiority, and the Luftwaffe played a major role in German victories in these campaigns. During the Battle of Britain, however, British Hurricanes and Spitfires proved roughly equal to Luftwaffe fighters. Additionally Britain's radar-based Dowding system directing fighters onto German attacks and the advantages of fighting above Britain's home territory allowed the RAF to deny Germany air superiority, saving the UK from possible German invasion and dealing the Axis a major defeat early in the Second World War.\nOn the Eastern Front, Soviet fighter forces were overwhelmed during the opening phases of Operation Barbarossa. This was a result of the tactical surprise at the outset of the campaign, the leadership vacuum within the Soviet military left by the Great Purge, and the general inferiority of Soviet designs at the time, such as the obsolescent I-15 biplane and the I-16. More modern Soviet designs, including the MiG-3, LaGG-3 and Yak-1, had not yet arrived in numbers and in any case were still inferior to the Messerschmitt Bf 109. As a result, during the early months of these campaigns, Axis air forces destroyed large numbers of Red Air Force aircraft on the ground and in one-sided dogfights.\nIn the later stages on the Eastern Front, Soviet training and leadership improved, as did their equipment. Since 1942 Soviet designs such as the Yakovlev Yak-9 and Lavochkin La-5 had performance comparable to the German Bf 109 and Focke-Wulf Fw 190. Also, significant numbers of British, and later U.S., fighter aircraft were supplied to aid the Soviet war effort as part of Lend-Lease, with the Bell P-39 Airacobra proving particularly effective in the lower-altitude combat typical of the Eastern Front. The Soviets were also helped indirectly by the American and British bombing campaigns, which forced the Luftwaffe to shift many of its fighters away from the Eastern Front in defense against these raids. The Soviets increasingly were able to challenge the Luftwaffe, and while the Luftwaffe maintained a qualitative edge over the Red Air Force for much of the war, the increasing numbers and efficacy of the Soviet Air Force were critical to the Red Army's efforts at turning back and eventually annihilating the Wehrmacht.\nMeanwhile, air combat on the Western Front had a much different character. Much of this combat focused on the strategic bombing campaigns of the RAF and the USAAF against German industry intended to wear down the Luftwaffe. Axis fighter aircraft focused on defending against Allied bombers while Allied fighters' main role was as bomber escorts. The RAF raided German cities at night, and both sides developed radar-equipped night fighters for these battles. The Americans, in contrast, flew daylight bombing raids into Germany. Unescorted Consolidated B-24 Liberators and Boeing B-17 Flying Fortress bombers, however, proved unable to fend off German interceptors (primarily Bf 109s and Fw 190s). With the later arrival of long range fighters, particularly the North American P-51 Mustang, American fighters were able to escort far into Germany on daylight raids and established control of the skies over Western Europe.\nBy the time of Operation Overlord in June 1944, the Allies had gained near complete air superiority over the Western Front. This cleared the way both for intensified strategic bombing of German cities and industries, and for the tactical bombing of battlefield targets. With the Luftwaffe largely cleared from the skies, Allied fighters increasingly served as attack aircraft.\nAllied fighters, by gaining air superiority over the European battlefield, played a crucial role in the eventual defeat of the Axis, which Reichmarshal Hermann G\u00f6ring, commander of the German \"Luftwaffe\" summed up when he said: \"When I saw Mustangs over Berlin, I knew the jig was up.\"\nPacific theater.\nMajor air combat during the war in the Pacific began with the entry of the Western Allies following Japan's attack against Pearl Harbor. The Imperial Japanese Navy Air Service primarily operated the Mitsubishi A6M Zero, and the Imperial Japanese Army Air Service flew the Nakajima Ki-27 and the Nakajima Ki-43, initially enjoying great success, as these fighters generally had better range, maneuverability, speed and climb rates than their Allied counterparts. Additionally, Japanese pilots had received excellent training and many were combat veterans from Japan's campaigns in China. They quickly gained air superiority over the Allies, who at this stage of the war were often disorganized, under-trained and poorly equipped, and Japanese air power contributed significantly to their successes in the Philippines, Malaysia and Singapore, the Dutch East Indies and Burma.\nBy mid-1942, the Allies began to regroup and while some Allied aircraft such as the Brewster Buffalo and the P-39 were hopelessly outclassed by fighters like Japan's Zero, others such as the Army's P-40 and the Navy's Wildcat possessed attributes such as superior firepower, ruggedness and dive speed, and the Allies soon developed tactics (such as the Thach Weave) to take advantage of these strengths. These changes soon paid dividends, as the Allied ability to deny Japan air superiority was critical to their victories at Coral Sea, Midway, Guadalcanal and New Guinea. In China, the Flying Tigers also used the same tactics with some success, although they were unable to stem the tide of Japanese advances there.\nBy 1943, the Allies began to gain the upper hand in the Pacific Campaign's air campaigns. Several factors contributed to this shift. First, the P-38 and second-generation Allied fighters such as the Hellcat and later the Corsair, the P-47 and the P-51, began arriving in numbers. These fighters outperformed Japanese fighters in all respects except maneuverability. Other problems with Japan's fighter aircraft also became apparent as the war progressed, such as their lack of armor and light armament, which made them inadequate as bomber interceptors or ground-attack planes\u00a0\u2013 roles Allied fighters excelled at. Most importantly, Japan's training program failed to provide enough well-trained pilots to replace losses. In contrast, the Allies improved both the quantity and quality of pilots graduating from their training programs.\nBy mid-1944, Allied fighters had gained air superiority throughout the theater, which would not be contested again during the war. The extent of Allied quantitative and qualitative superiority by this point in the war was demonstrated during the Battle of the Philippine Sea, a lopsided Allied victory in which Japanese fliers were downed in such numbers and with such ease that American fighter pilots likened it to a great turkey shoot.\nLate in the war, Japan did begin to produce new fighters such as the Nakajima Ki-84 and the Kawanishi N1K to replace the venerable Zero, but these were produced only in small numbers, and in any case by that time Japan lacked trained pilots or sufficient fuel to mount a sustained challenge to Allied fighters. During the closing stages of the war, Japan's fighter arm could not seriously challenge raids over Japan by American B-29s, and was largely relegated to Kamikaze tactics.\nTechnological innovations.\nFighter technology advanced rapidly during the Second World War. Piston-engines, which powered the vast majority of World War II fighters, grew more powerful: at the beginning of the war fighters typically had engines producing between and , while by the end of the war many could produce over . For example, the Spitfire, one of the few fighters in continuous production throughout the war, was in 1939 powered by a Merlin II, while variants produced in 1945 were equipped with the Griffon 61. Nevertheless, these fighters could only achieve modest increases in top speed due to problems of compressibility created as aircraft and their propellers approached the sound barrier, and it was apparent that propeller-driven aircraft were approaching the limits of their performance. German jet and rocket-powered fighters entered combat in 1944, too late to impact the war's outcome. The same year the Allies' only operational jet fighter, the Gloster Meteor, also entered service.\nWorld War II fighters also increasingly featured monocoque construction, which improved their aerodynamic efficiency while adding structural strength. Laminar flow wings, which improved high speed performance, also came into use on fighters such as the P-51, while the Messerschmitt Me 262 and the Messerschmitt Me 163 featured swept wings that dramatically reduced drag at high subsonic speeds.\nArmament also advanced during the war. The rifle-caliber machine guns that were common on prewar fighters could not easily down the more rugged warplanes of the era. Air forces began to replace or supplement them with cannons, which fired explosive shells that could blast a hole in an enemy aircraft\u00a0\u2013 rather than relying on kinetic energy from a solid bullet striking a critical component of the aircraft, such as a fuel line or control cable, or the pilot. Cannons could bring down even heavy bombers with just a few hits, but their slower rate of fire made it difficult to hit fast-moving fighters in a dogfight. Eventually, most fighters mounted cannons, sometimes in combination with machine guns.\nThe British epitomized this shift. Their standard early war fighters mounted eight caliber machine guns, but by mid-war they often featured a combination of machine guns and 20\u00a0mm cannons, and late in the war often only cannons. The Americans, in contrast, had problems producing a native cannon design, so instead placed multiple .50\u00a0caliber (12.7\u00a0mm) heavy machine guns on their fighters. Fighters were also increasingly fitted with bomb racks and air-to-surface ordnance such as bombs or rockets beneath their wings, and pressed into close air support roles as fighter-bombers. Although they carried less ordnance than light and medium bombers, and generally had a shorter range, they were cheaper to produce and maintain and their maneuverability made it easier for them to hit moving targets such as motorized vehicles. Moreover, if they encountered enemy fighters, their ordnance (which reduced lift and increased drag and therefore decreased performance) could be jettisoned and they could engage the enemy fighters, which eliminated the need for the fighter escorts that bombers required. Heavily armed and sturdily constructed fighters such as Germany's Focke-Wulf Fw 190, Britain's Hawker Typhoon and Hawker Tempest, and America's P-40, Corsair, P-47 and P-38 all excelled as fighter-bombers, and since the Second World War ground attack has been an important secondary capability of many fighters.\nWorld War II also saw the first use of airborne radar on fighters. The primary purpose of these radars was to help night fighters locate enemy bombers and fighters. Because of the bulkiness of these radar sets, they could not be carried on conventional single-engined fighters and instead were typically retrofitted to larger heavy fighters or light bombers such as Germany's Messerschmitt Bf 110 and Junkers Ju 88, Britain's Mosquito and Beaufighter, and America's A-20, which then served as night fighters. The Northrop P-61 Black Widow, a purpose-built night fighter, was the only fighter of the war that incorporated radar into its original design. Britain and America cooperated closely in the development of airborne radar, and Germany's radar technology generally lagged slightly behind Anglo-American efforts, while other combatants developed few radar-equipped fighters.\nPost\u2013World War II period.\nSeveral prototype fighter programs begun early in 1945 continued on after the war and led to advanced piston-engine fighters that entered production and operational service in 1946. A typical example is the Lavochkin La-9 'Fritz', which was an evolution of the successful wartime Lavochkin La-7 'Fin'. Working through a series of prototypes, the La-120, La-126 and La-130, the Lavochkin design bureau sought to replace the La-7's wooden airframe with a metal one, as well as fit a laminar-flow wing to improve maneuver performance, and increased armament. The La-9 entered service in August 1946 and was produced until 1948; it also served as the basis for the development of a long-range escort fighter, the La-11 'Fang', of which nearly 1200 were produced 1947\u20131951. Over the course of the Korean War, however, it became obvious that the day of the piston-engined fighter was coming to a close and that the future would lie with the jet fighter.\nThis period also witnessed experimentation with jet-assisted piston engine aircraft. La-9 derivatives included examples fitted with two underwing auxiliary pulsejet engines (the La-9RD) and a similarly mounted pair of auxiliary ramjet engines (the La-138); however, neither of these entered service. One that did enter service \u2013 with the U.S. Navy in March 1945 \u2013 was the Ryan FR-1 Fireball; production was halted with the war's end on VJ-Day, with only 66 having been delivered, and the type was withdrawn from service in 1947. The USAAF had ordered its first 13 mixed turboprop-turbojet-powered pre-production prototypes of the Consolidated Vultee XP-81 fighter, but this program was also canceled by VJ Day, with 80% of the engineering work completed.\nRocket-powered fighters.\nThe first rocket-powered aircraft was the Lippisch Ente, which made a successful maiden flight in March 1928. The only pure rocket aircraft ever mass-produced was the Messerschmitt Me 163B \"Komet\" in 1944, one of several German World War II projects aimed at developing high speed, point-defense aircraft. Later variants of the Me 262 (C-1a and C-2b) were also fitted with \"mixed-power\" jet/rocket powerplants, while earlier models were fitted with rocket boosters, but were not mass-produced with these modifications.\nThe USSR experimented with a rocket-powered interceptor in the years immediately following World War II, the Mikoyan-Gurevich I-270. Only two were built.\nIn the 1950s, the British developed mixed-power jet designs employing both rocket and jet engines to cover the performance gap that existed in turbojet designs. The rocket was the main engine for delivering the speed and height required for high-speed interception of high-level bombers and the turbojet gave increased fuel economy in other parts of flight, most notably to ensure the aircraft was able to make a powered landing rather than risking an unpredictable gliding return.\nThe Saunders-Roe SR.53 was a successful design, and was planned for production when economics forced the British to curtail most aircraft programs in the late 1950s. Furthermore, rapid advancements in jet engine technology rendered mixed-power aircraft designs like Saunders-Roe's SR.53 (and the following SR.177) obsolete. The American Republic XF-91 Thunderceptor \u2013the first U.S. fighter to exceed Mach 1 in level flight\u2013 met a similar fate for the same reason, and no hybrid rocket-and-jet-engine fighter design has ever been placed into service.\nThe only operational implementation of mixed propulsion was Rocket-Assisted Take Off (RATO), a system rarely used in fighters, such as with the zero-length launch, RATO-based takeoff scheme from special launch platforms, tested out by both the United States and the Soviet Union, and made obsolete with advancements in surface-to-air missile technology.\nJet-powered fighters.\nIt has become common in the aviation community to classify jet fighters by \"generations\" for historical purposes. No official definitions of these generations exist; rather, they represent the notion of stages in the development of fighter-design approaches, performance capabilities, and technological evolution. Different authors have packed jet fighters into different generations. For example, Richard P. Hallion of the Secretary of the Air Force's Action Group classified the F-16 as a sixth-generation jet fighter.\nThe timeframes associated with each generation remain inexact and are only indicative of the period during which their design philosophies and technology employment enjoyed a prevailing influence on fighter design and development. These timeframes also encompass the peak period of service entry for such aircraft.\nFirst-generation subsonic jet fighters (mid-1940s to mid-1950s).\nThe first generation of jet fighters comprised the initial, subsonic jet-fighter designs introduced late in World War II (1939\u20131945) and in the early post-war period. They differed little from their piston-engined counterparts in appearance, and many employed unswept wings. Guns and cannon remained the principal armament. The need to obtain a decisive advantage in maximum speed pushed the development of turbojet-powered aircraft forward. Top speeds for fighters rose steadily throughout World War II as more powerful piston engines developed, and they approached transonic flight-speeds where the efficiency of propellers drops off, making further speed increases nearly impossible.\nThe first jets developed during World War II and saw combat in the last two years of the war. Messerschmitt developed the first operational jet fighter, the Me 262A, primarily serving with the Luftwaffe's JG 7, the world's first jet-fighter wing. It was considerably faster than contemporary piston-driven aircraft, and in the hands of a competent pilot, proved quite difficult for Allied pilots to defeat. The Luftwaffe never deployed the design in numbers sufficient to stop the Allied air campaign, and a combination of fuel shortages, pilot losses, and technical difficulties with the engines kept the number of sorties low. Nevertheless, the Me 262 indicated the obsolescence of piston-driven aircraft. Spurred by reports of the German jets, Britain's Gloster Meteor entered production soon after, and the two entered service around the same time in 1944. Meteors commonly served to intercept the V-1 flying bomb, as they were faster than available piston-engined fighters at the low altitudes used by the flying bombs. Nearer the end of World War II, the first military jet-powered light-fighter design, the Luftwaffe intended the Heinkel He 162A \"Spatz\" (sparrow) to serve as a simple jet fighter for German home defense, with a few examples seeing squadron service with JG 1 by April 1945. By the end of the war almost all work on piston-powered fighters had ended. A few designs combining piston- and jet-engines for propulsion \u2013 such as the Ryan FR Fireball \u2013 saw brief use, but by the end of the 1940s virtually all new fighters were jet-powered.\nDespite their advantages, the early jet-fighters were far from perfect. The operational lifespan of turbines were very short and engines were temperamental, while power could be adjusted only slowly and acceleration was poor (even if top speed was higher) compared to the final generation of piston fighters. Many squadrons of piston-engined fighters remained in service until the early to mid-1950s, even in the air forces of the major powers (though the types retained were the best of the World War II designs). Innovations including ejection seats, air brakes and all-moving tailplanes became widespread in this period.\nThe Americans began using jet fighters operationally after World War II, the wartime Bell P-59 having proven a failure. The Lockheed P-80 Shooting Star (soon re-designated F-80) was less elegant than the swept-wing Me 262, but had a cruise speed () as high as the maximum speed attainable by many piston-engined fighters. The British designed several new jets, including the distinctive single-engined twin boom de Havilland Vampire which Britain sold to the air forces of many nations.\nThe British transferred the technology of the Rolls-Royce Nene jet-engine to the Soviets, who soon put it to use in their advanced Mikoyan-Gurevich MiG-15 fighter, which used fully swept wings that allowed flying closer to the speed of sound than straight-winged designs such as the F-80. The MiG-15s' top speed of proved quite a shock to the American F-80 pilots who encountered them in the Korean War, along with their armament of two 23\u00a0mm cannons and a single 37\u00a0mm cannon. Nevertheless, in the first jet-versus-jet dogfight, which occurred during the Korean War on 8 November 1950, an F-80 shot down two North Korean MiG-15s\nThe Americans responded by rushing their own swept-wing fighter \u2013 the North American F-86 Sabre \u2013 into battle against the MiGs, which had similar transsonic performance. The two aircraft had different strengths and weaknesses, but were similar enough that victory could go either way. While the Sabres focused primarily on downing MiGs and scored favorably against those flown by the poorly-trained North Koreans, the MiGs in turn decimated US bomber formations and forced the withdrawal of numerous American types from operational service.\nThe world's navies also transitioned to jets during this period, despite the need for catapult-launching of the new aircraft. The U.S. Navy adopted the Grumman F9F Panther as their primary jet fighter in the Korean War period, and it was one of the first jet fighters to employ an afterburner. The de Havilland Sea Vampire became the Royal Navy's first jet fighter. Radar was used on specialized night-fighters such as the Douglas F3D Skyknight, which also downed MiGs over Korea, and later fitted to the McDonnell F2H Banshee and swept-wing Vought F7U Cutlass and McDonnell F3H Demon as all-weather / night fighters. Early versions of Infra-red (IR) air-to-air missiles (AAMs) such as the AIM-9 Sidewinder and radar-guided missiles such as the AIM-7 Sparrow whose descendants remain in use , were first introduced on swept-wing subsonic Demon and Cutlass naval fighters.\nSecond-generation jet fighters (mid-1950s to early 1960s).\nTechnological breakthroughs, lessons learned from the aerial battles of the Korean War, and a focus on conducting operations in a nuclear warfare environment shaped the development of second-generation fighters. Technological advances in aerodynamics, propulsion and aerospace building-materials (primarily aluminum alloys) permitted designers to experiment with aeronautical innovations such as swept wings, delta wings, and area-ruled fuselages. Widespread use of afterburning turbojet engines made these the first production aircraft to break the sound barrier, and the ability to sustain supersonic speeds in level flight became a common capability amongst fighters of this generation.\nFighter designs also took advantage of new electronics technologies that made effective radars small enough to carry aboard smaller aircraft. Onboard radars permitted detection of enemy aircraft beyond visual range, thereby improving the handoff of targets by longer-ranged ground-based warning- and tracking-radars. Similarly, advances in guided-missile development allowed air-to-air missiles to begin supplementing the gun as the primary offensive weapon for the first time in fighter history. During this period, passive-homing infrared-guided (IR) missiles became commonplace, but early IR missile sensors had poor sensitivity and a very narrow field of view (typically no more than 30\u00b0), which limited their effective use to only close-range, tail-chase engagements. Radar-guided (RF) missiles were introduced as well, but early examples proved unreliable. These semi-active radar homing (SARH) missiles could track and intercept an enemy aircraft \"painted\" by the launching aircraft's onboard radar. Medium- and long-range RF air-to-air missiles promised to open up a new dimension of \"beyond-visual-range\" (BVR) combat, and much effort concentrated on further development of this technology.\nThe prospect of a potential third world war featuring large mechanized armies and nuclear-weapon strikes led to a degree of specialization along two design approaches: interceptors, such as the English Electric Lightning and Mikoyan-Gurevich MiG-21F; and fighter-bombers, such as the Republic F-105 Thunderchief and the Sukhoi Su-7B. Dogfighting, \"per se\", became de-emphasized in both cases. The interceptor was an outgrowth of the vision that guided missiles would completely replace guns and combat would take place at beyond-visual ranges. As a result, strategists designed interceptors with a large missile-payload and a powerful radar, sacrificing agility in favor of high speed, altitude ceiling and rate of climb. With a primary air-defense role, emphasis was placed on the ability to intercept strategic bombers flying at high altitudes. Specialized point-defense interceptors often had limited range and few, if any, ground-attack capabilities. Fighter-bombers could swing between air-superiority and ground-attack roles, and were often designed for a high-speed, low-altitude dash to deliver their ordnance. Television- and IR-guided air-to-surface missiles were introduced to augment traditional gravity bombs, and some were also equipped to deliver a nuclear bomb.\nThird-generation jet fighters (early 1960s to circa 1970).\nThe third generation witnessed continued maturation of second-generation innovations, but it is most marked by renewed emphases on maneuverability and on traditional ground-attack capabilities. Over the course of the 1960s, increasing combat experience with guided missiles demonstrated that combat would devolve into close-in dogfights. Analog avionics began to appear, replacing older \"steam-gauge\" cockpit instrumentation. Enhancements to the aerodynamic performance of third-generation fighters included flight control surfaces such as canards, powered slats, and blown flaps. A number of technologies would be tried for vertical/short takeoff and landing, but thrust vectoring would be successful on the Harrier.\nGrowth in air-combat capability focused on the introduction of improved air-to-air missiles, radar systems, and other avionics. While guns remained standard equipment (early models of F-4 being a notable exception), air-to-air missiles became the primary weapons for air-superiority fighters, which employed more sophisticated radars and medium-range RF AAMs to achieve greater \"stand-off\" ranges, however, kill probabilities proved unexpectedly low for RF missiles due to poor reliability and improved electronic countermeasures (ECM) for spoofing radar seekers. Infrared-homing AAMs saw their fields of view expand to 45\u00b0, which strengthened their tactical usability. Nevertheless, the low dogfight loss-exchange ratios experienced by American fighters in the skies over Vietnam led the U.S. Navy to establish its famous \"TOPGUN\" fighter-weapons school, which provided a graduate-level curriculum to train fleet fighter-pilots in advanced Air Combat Maneuvering (ACM) and Dissimilar air combat training (DACT) tactics and techniques.\nThis era also saw an expansion in ground-attack capabilities, principally in guided missiles, and witnessed the introduction of the first truly effective avionics for enhanced ground attack, including terrain-avoidance systems. Air-to-surface missiles (ASM) equipped with electro-optical (E-O) contrast seekers \u2013 such as the initial model of the widely used AGM-65 Maverick \u2013 became standard weapons, and laser-guided bombs (LGBs) became widespread in an effort to improve precision-attack capabilities. Guidance for such precision-guided munitions (PGM) was provided by externally-mounted targeting pods, which were introduced in the mid-1960s.\nThe third generation also led to the development of new automatic-fire weapons, primarily chain-guns that use an electric motor to drive the mechanism of a cannon. This allowed a plane to carry a single multi-barrel weapon (such as the 20\u00a0mm Vulcan), and provided greater accuracy and rates of fire. Powerplant reliability increased, and jet engines became \"smokeless\" to make it harder to sight aircraft at long distances.\nDedicated ground-attack aircraft (like the Grumman A-6 Intruder, SEPECAT Jaguar and LTV A-7 Corsair II) offered longer range, more sophisticated night-attack systems or lower cost than supersonic fighters. With variable-geometry wings, the supersonic F-111 introduced the Pratt &amp; Whitney TF30, the first turbofan equipped with afterburner. The ambitious project sought to create a versatile common fighter for many roles and services. It would serve well as an all-weather bomber, but lacked the performance to defeat other fighters. The McDonnell F-4 Phantom was designed to capitalize on radar and missile technology as an all-weather interceptor, but emerged as a versatile strike-bomber nimble enough to prevail in air combat, adopted by the U.S. Navy, Air Force and Marine Corps. Despite numerous shortcomings that would be not be fully addressed until newer fighters, the Phantom claimed 280 aerial kills (more than any other U.S. fighter) over Vietnam. With range and payload capabilities that rivaled that of World War II bombers such as B-24 Liberator, the Phantom would become a highly successful multirole aircraft.\nFourth-generation jet fighters (circa 1970 to mid-1990s).\nFourth-generation fighters continued the trend towards multirole configurations, and were equipped with increasingly sophisticated avionics- and weapon-systems. Fighter designs were significantly influenced by the Energy-Maneuverability (E-M) theory developed by Colonel John Boyd and mathematician Thomas Christie, based upon Boyd's combat experience in the Korean War and as a fighter-tactics instructor during the 1960s. E-M theory emphasized the value of aircraft-specific energy maintenance as an advantage in fighter combat. Boyd perceived maneuverability as the primary means of getting \"inside\" an adversary's decision-making cycle, a process Boyd called the \"OODA loop\" (for \"Observation-Orientation-Decision-Action\"). This approach emphasized aircraft designs capable of performing \"fast transients\" \u2013 quick changes in speed, altitude, and direction \u2013 as opposed to relying chiefly on high speed alone.\nE-M characteristics were first applied to the McDonnell Douglas F-15 Eagle, but Boyd and his supporters believed these performance parameters called for a small, lightweight aircraft with a larger, higher-lift wing. The small size would minimize drag and increase the thrust-to-weight ratio, while the larger wing would minimize wing loading; while the reduced wing loading tends to lower top speed and can cut range, it increases payload capacity and the range reduction can be compensated for by increased fuel in the larger wing. The efforts of Boyd's \"Fighter mafia\" would result in the General Dynamics F-16 Fighting Falcon (now Lockheed Martin's).\nThe F-16's maneuverability was further enhanced by its slight aerodynamic instability. This technique, called \"relaxed static stability\" (RSS), was made possible by introduction of the \"fly-by-wire\" (FBW) flight-control system (FLCS), which in turn was enabled by advances in computers and in system-integration techniques. Analog avionics, required to enable FBW operations, became a fundamental requirement, but began to be replaced by digital flight-control systems in the latter half of the 1980s. Likewise, Full Authority Digital Engine Controls (FADEC) to electronically manage powerplant performance was introduced with the Pratt &amp; Whitney F100 turbofan. The F-16's sole reliance on electronics and wires to relay flight commands, instead of the usual cables and mechanical linkage controls, earned it the sobriquet of \"the electric jet\". Electronic FLCS and FADEC quickly became essential components of all subsequent fighter designs.\nOther innovative technologies introduced in fourth-generation fighters included pulse-Doppler fire-control radars (providing a \"look-down/shoot-down\" capability), head-up displays (HUD), \"hands on throttle-and-stick\" (HOTAS) controls, and multi-function displays (MFD), all essential equipment . Aircraft designers began to incorporate composite materials in the form of bonded-aluminum honeycomb structural elements and graphite epoxy laminate skins to reduce weight. Infrared search-and-track (IRST) sensors became widespread for air-to-ground weapons delivery, and appeared for air-to-air combat as well. \"All-aspect\" IR AAM became standard air superiority weapons, which permitted engagement of enemy aircraft from any angle (although the field of view remained relatively limited). The first long-range active-radar-homing RF AAM entered service with the AIM-54 Phoenix, which solely equipped the Grumman F-14 Tomcat, one of the few variable-sweep-wing fighter designs to enter production. Even with the tremendous advancement of air-to-air missiles in this era, internal guns were standard equipment.\nAnother revolution came in the form of a stronger reliance on ease of maintenance, which led to standardization of parts, reductions in the numbers of access panels and lubrication points, and overall parts reduction in more complicated equipment like the engines. Some early jet fighters required 50 man-hours of work by a ground crew for every hour the aircraft was in the air; later models substantially reduced this to allow faster turn-around times and more sorties in a day. Some modern military aircraft only require 10-man-hours of work per hour of flight time, and others are even more efficient.\nAerodynamic innovations included variable-camber wings and exploitation of the vortex lift effect to achieve higher angles of attack through the addition of leading-edge extension devices such as strakes.\nUnlike interceptors of the previous eras, most fourth-generation air-superiority fighters were designed to be agile dogfighters (although the Mikoyan MiG-31 and Panavia Tornado ADV are notable exceptions). The continually rising cost of fighters, however, continued to emphasize the value of multirole fighters. The need for both types of fighters led to the \"high/low mix\" concept, which envisioned a high-capability and high-cost core of dedicated air-superiority fighters (like the F-15 and Su-27) supplemented by a larger contingent of lower-cost multi-role fighters (such as the F-16 and MiG-29).\nMost fourth-generation fighters, such as the McDonnell Douglas F/A-18 Hornet, HAL Tejas, JF-17 and Dassault Mirage 2000, are true multirole warplanes, designed as such from the start. This was facilitated by multimode avionics that could switch seamlessly between air and ground modes. The earlier approaches of adding on strike capabilities or designing separate models specialized for different roles generally became \"pass\u00e9\" (with the Panavia Tornado being an exception in this regard). Attack roles were generally assigned to dedicated ground-attack aircraft such as the Sukhoi Su-25 and the A-10 Thunderbolt II.\nA typical US Air Force fighter wing of the period might contain a mix of one air superiority squadron (F-15C), one strike fighter squadron (F-15E), and two multirole fighter squadrons (F-16C).\nPerhaps the most novel technology introduced for combat aircraft was \"stealth\", which involves the use of special \"low-observable\" (L-O) materials and design techniques to reduce the susceptibility of an aircraft to detection by the enemy's sensor systems, particularly radars. The first stealth aircraft introduced were the Lockheed F-117 Nighthawk attack aircraft (introduced in 1983) and the Northrop Grumman B-2 Spirit bomber (first flew in 1989). Although no stealthy fighters per se appeared among the fourth generation, some radar-absorbent coatings and other L-O treatments developed for these programs are reported to have been subsequently applied to fourth-generation fighters.\n4.5-generation jet fighters (1990s to 2000s).\nThe end of the Cold War in 1992 led many governments to significantly decrease military spending as a \"peace dividend\". Air force inventories were cut. Research and development programs working on \"fifth-generation\" fighters took serious hits. Many programs were canceled during the first half of the 1990s, and those that survived were \"stretched out\". While the practice of slowing the pace of development reduces annual investment expenses, it comes at the penalty of increased overall program and unit costs over the long-term. In this instance, however, it also permitted designers to make use of the tremendous achievements being made in the fields of computers, avionics and other flight electronics, which had become possible largely due to the advances made in microchip and semiconductor technologies in the 1980s and 1990s. This opportunity enabled designers to develop fourth-generation designs \u2013 or redesigns \u2013 with significantly enhanced capabilities. These improved designs have become known as \"Generation 4.5\" fighters, recognizing their intermediate nature between the 4th and 5th generations, and their contribution in furthering development of individual fifth-generation technologies.\nThe primary characteristics of this sub-generation are the application of advanced digital avionics and aerospace materials, modest signature reduction (primarily RF \"stealth\"), and highly integrated systems and weapons. These fighters have been designed to operate in a \"network-centric\" battlefield environment and are principally multirole aircraft. Key weapons technologies introduced include beyond-visual-range (BVR) AAMs; Global Positioning System (GPS)\u2013guided weapons, solid-state phased-array radars; helmet-mounted sights; and improved secure, jamming-resistant datalinks. Thrust vectoring to further improve transient maneuvering capabilities has also been adopted by many 4.5th generation fighters, and uprated powerplants have enabled some designs to achieve a degree of \"supercruise\" ability. Stealth characteristics are focused primarily on frontal-aspect radar cross section (RCS) signature-reduction techniques including radar-absorbent materials (RAM), L-O coatings and limited shaping techniques.\n\"Half-generation\" designs are either based on existing airframes or are based on new airframes following similar design theory to previous iterations; however, these modifications have introduced the structural use of composite materials to reduce weight, greater fuel fractions to increase range, and signature reduction treatments to achieve lower RCS compared to their predecessors. Prime examples of such aircraft, which are based on new airframe designs making extensive use of carbon-fiber composites, include the Eurofighter Typhoon, Dassault Rafale, Saab JAS 39 Gripen, and HAL Tejas Mark 1A.\nApart from these fighter jets, most of the 4.5 generation aircraft are actually modified variants of existing airframes from the earlier fourth generation fighter jets. Such fighter jets are generally heavier and examples include the Boeing F/A-18E/F Super Hornet, which is an evolution of the F/A-18 Hornet, the F-15E Strike Eagle, which is a ground-attack/multi-role variant of the F-15 Eagle, the Su-30SM and Su-35S modified variants of the Sukhoi Su-27, and the MiG-35 upgraded version of the Mikoyan MiG-29. The Su-30SM/Su-35S and MiG-35 feature thrust vectoring engine nozzles to enhance maneuvering. The upgraded version of F-16 is also considered a member of the 4.5 generation aircraft.\nGeneration 4.5 fighters first entered service in the early 1990s, and most of them are still being produced and evolved. It is quite possible that they may continue in production alongside fifth-generation fighters due to the expense of developing the advanced level of stealth technology needed to achieve aircraft designs featuring very low observables (VLO), which is one of the defining features of fifth-generation fighters. Of the 4.5th generation designs, the Strike Eagle, Super Hornet, Typhoon, Gripen, and Rafale have been used in combat.\nThe U.S. government has defined 4.5 generation fighter aircraft as those that \"(1) have advanced capabilities, including\u2014 (A) AESA radar; (B) high capacity data-link; and (C) enhanced avionics; and (2) have the ability to deploy current and reasonably foreseeable advanced armaments.\"\nFifth-generation jet fighters (2005 to 2018).\nCurrently the cutting edge of fighter design, fifth-generation fighters are characterized by being designed from the start to operate in a network-centric combat environment, and to feature extremely low, all-aspect, multi-spectral signatures employing advanced materials and shaping techniques. They have multifunction AESA radars with high-bandwidth, low-probability of intercept (LPI) data transmission capabilities. The infra-red search and track sensors incorporated for air-to-air combat as well as for air-to-ground weapons delivery in the 4.5th generation fighters are now fused in with other sensors for Situational Awareness IRST or SAIRST, which constantly tracks all targets of interest around the aircraft so the pilot need not guess when he glances. These sensors, along with advanced avionics, glass cockpits, helmet-mounted sights (not currently on F-22), and improved secure, jamming-resistant LPI datalinks are highly integrated to provide multi-platform, multi-sensor data fusion for vastly improved situational awareness while easing the pilot's workload. Avionics suites rely on extensive use of very high-speed integrated circuit (VHSIC) technology, common modules, and high-speed data buses. Overall, the integration of all these elements is claimed to provide fifth-generation fighters with a \"first-look, first-shot, first-kill capability\".\nA key attribute of fifth-generation fighters is a small radar cross-section. Great care has been taken in designing its layout and internal structure to minimize RCS over a broad bandwidth of detection and tracking radar frequencies; furthermore, to maintain its VLO signature during combat operations, primary weapons are carried in internal weapon bays that are only briefly opened to permit weapon launch. Furthermore, stealth technology has advanced to the point where it can be employed without a tradeoff with aerodynamics performance, in contrast to previous stealth efforts. Some attention has also been paid to reducing IR signatures, especially on the F-22. Detailed information on these signature-reduction techniques is classified, but in general includes special shaping approaches, thermoset and thermoplastic materials, extensive structural use of advanced composites, conformal sensors, heat-resistant coatings, low-observable wire meshes to cover intake and cooling vents, heat ablating tiles on the exhaust troughs (seen on the Northrop YF-23), and coating internal and external metal areas with radar-absorbent materials and paint (RAM/RAP).\nThe AESA radar offers unique capabilities for fighters (and it is also quickly becoming essential for Generation 4.5 aircraft designs, as well as being retrofitted onto some fourth-generation aircraft). In addition to its high resistance to ECM and LPI features, it enables the fighter to function as a sort of \"mini-AWACS\", providing high-gain electronic support measures (ESM) and electronic warfare (EW) jamming functions. Other technologies common to this latest generation of fighters includes integrated electronic warfare system (INEWS) technology, integrated communications, navigation, and identification (CNI) avionics technology, centralized \"vehicle health monitoring\" systems for ease of maintenance, fiber optics data transmission, stealth technology and even hovering capabilities. Maneuver performance remains important and is enhanced by thrust-vectoring, which also helps reduce takeoff and landing distances. Supercruise may or may not be featured; it permits flight at supersonic speeds without the use of the afterburner \u2013 a device that significantly increases IR signature when used in full military power.\nSuch aircraft are sophisticated and expensive. The fifth generation was ushered in by the Lockheed Martin/Boeing F-22 Raptor in late 2005. The U.S. Air Force originally planned to acquire 650 F-22s, but now only 187 will be built. As a result, its unit flyaway cost (FAC) is around US$150\u00a0million. To spread the development costs \u2013 and production base \u2013 more broadly, the Joint Strike Fighter (JSF) program enrolls eight other countries as cost- and risk-sharing partners. Altogether, the nine partner nations anticipate procuring over 3,000 Lockheed Martin F-35 Lightning II fighters at an anticipated average FAC of $80\u201385\u00a0million. The F-35, however, is designed to be a family of three aircraft, a conventional take-off and landing (CTOL) fighter, a short take-off and vertical landing (STOVL) fighter, and a Catapult Assisted Take Off But Arrested Recovery (CATOBAR) fighter, each of which has a different unit price and slightly varying specifications in terms of fuel capacity (and therefore range), size and payload.\nOther countries have initiated fifth-generation fighter development projects, with Russia's Sukhoi Su-57 and Mikoyan LMFS. In December 2010, it was discovered that China is developing the 5th generation fighter Chengdu J-20. The J-20 took its maiden flight in January 2011. The Shenyang J-31 took its maiden flight on 31 October 2012. Japan is exploring its technical feasibility to produce fifth-generation fighters. India is developing the Advanced Medium Combat Aircraft (AMCA), a medium weight stealth fighter jet designated to enter into serial production by late 2030s. India also had initiated a joint fifth generation heavy fighter with Russia called the FGFA. May, the project is suspected to have not yielded desired progress or results for India and has been put on hold or dropped altogether. Other countries considering fielding an indigenous or semi-indigenous advanced fifth generation aircraft include Korea, Sweden, Turkey and Pakistan.\nSixth-generation jet fighters (2018 to the Present).\nAs of November 2018, France, Germany, Japan, Russia, India, the United Kingdom and the United States have announced the development of a sixth-generation aircraft program.\nFrance and Germany will develop a joint sixth-generation fighter to replace their current fleet of Dassault Rafales, Eurofighter Typhoons, and Panavia Tornados by 2035. The overall development will be led by a collaboration of Dassault and Airbus, while the engines will reportedly be jointly developed by Safran and MTU Aero Engines. Thales and MBDA are also seeking a stake in the project. Spain is reportedly planning to join the program in the later stages and is expected to sign a letter of intent in early 2019.\nCurrently at the concept stage, the first sixth-generation jet fighter is expected to enter service in the United States Navy in 2025\u201330 period. The USAF seeks a new fighter for the 2030\u201350 period named the \"Next Generation Tactical Aircraft\" (\"Next Gen TACAIR\"). The US Navy looks to replace its F/A-18E/F Super Hornets beginning in 2025 with the Next Generation Air Dominance air superiority fighter.\nThe United Kingdom's proposed stealth fighter is being developed by a European consortium called \"Team Tempest\", consisting of BAE Systems, Rolls-Royce, Leonardo S.p.A. and MBDA. The aircraft is intended to enter service in 2035.\nFighter weapons.\nFighters were typically armed with guns only for air to air combat up through the late 1950s, though unguided rockets for mostly air to ground use and limited air to air use were deployed in WWII. From the late 1950s forward guided missiles came into use for air to air combat. Throughout this history fighters which by surprise or maneuver attain a good firing position have achieved the kill about one third to one half the time, no matter what weapons were carried. The only major historic exception to this has been the low effectiveness shown by guided missiles in the first one to two decades of their existence.\nFrom WWI to the present, fighter aircraft have featured machine guns and automatic cannons as weapons, and they are still considered as essential back-up weapons today. The power of air-to-air guns has increased greatly over time, and has kept them relevant in the guided missile era. In WWII rifle caliber machine guns was the typical armament producing a weight of fire of about per second. The standard WWII American fighter armament of six 0.50-cal (12.7mm) machine guns fired a bullet weight of approximately 3.7\u00a0kg/sec (8.1\u00a0lbs/sec), at a muzzle velocity of 856\u00a0m/s (2,810\u00a0ft/s). British and German aircraft tended to use a mix of machine guns and autocannon, the latter firing explosive projectiles. The modern M61 Vulcan 20\u00a0mm rotating barrel Gatling gun that is standard on current American fighters fires a projectile weight of about 10\u00a0kg/s (22\u00a0lb/s), nearly three times that of six 0.50-cal machine guns, with higher velocity of 1,052\u00a0m/s (3450\u00a0ft/s) supporting a flatter trajectory, and with exploding projectiles. Modern fighter gun systems also feature ranging radar and lead computing electronic gun sights to ease the problem of aim point to compensate for projectile drop and time of flight (target lead) in the complex three dimensional maneuvering of air-to-air combat. However, getting in position to use the guns is still a challenge. The range of guns is longer than in the past but still quite limited compared to missiles, with modern gun systems having a maximum effective range of approximately 1,000 meters. High probability of kill also requires firing to usually occur from the rear hemisphere of the target. Despite these limits, when pilots are well trained in air-to-air gunnery and these conditions are satisfied, gun systems are tactically effective and highly cost efficient. The cost of a gun firing pass is far less than firing a missile, and the projectiles are not subject to the thermal and electronic countermeasures than can sometimes defeat missiles. When the enemy can be approached to within gun range, the lethality of guns is approximately a 25% to 50% chance of \"kill per firing pass\".\nThe range limitations of guns, and the desire to overcome large variations in fighter pilot skill and thus achieve higher force effectiveness, led to the development of the guided air-to-air missile. There are two main variations, heat-seeking (infrared homing), and radar guided. Radar missiles are typically several times heavier and more expensive than heat-seekers, but with longer range, greater destructive power, and ability to track through clouds.\nThe highly successful AIM-9 Sidewinder heat-seeking (infrared homing) short-range missile was developed by the United States Navy in the 1950s. These small missiles are easily carried by lighter fighters, and provide effective ranges of approximately 10 to 35\u00a0km (~6 to 22 miles). Beginning with the AIM-9L in 1977, subsequent versions of Sidewinder have added all-aspect capability, the ability to use the lower heat of air to skin friction on the target aircraft to track from the front and sides. The latest (2003 service entry) AIM-9X also features \"off-boresight\" and \"lock on after launch\" capabilities, which allow the pilot to make a quick launch of a missile to track a target anywhere within the pilot's vision. The AIM-9X development cost was U.S. $3\u00a0billion in mid to late 1990s dollars, and 2015 per unit procurement cost is $0.6\u00a0million each. The missile weighs 85.3\u00a0kg (188\u00a0lbs), and has a maximum range of 35\u00a0km (22 miles) at higher altitudes. Like most air-to-air missiles, lower altitude range can be as limited as only about one third of maximum due to higher drag and less ability to coast downward.\nThe effectiveness of heat-seeking missiles was only 7% early in the Vietnam War, but improved to approximately 15%\u201340% over the course of the war. The AIM-4 Falcon used by the USAF had kill rates of approximately 7% and was considered a failure. The AIM-9B Sidewinder introduced later achieved 15% kill rates, and the further improved AIM-9D and J models reached 19%. The AIM-9G used in the last year of the Vietnam air war achieved 40%. Israel used almost totally guns in the 1967 Six-Day War, achieving 60 kills and 10 losses. However, Israel made much more use of steadily improving heat-seeking missiles in the 1973 Yom Kippur War. In this extensive conflict Israel scored 171 of out of 261 total kills with heat-seeking missiles (65.5%), 5 kills with radar guided missiles (1.9%), and 85 kills with guns (32.6%). The AIM-9L Sidewinder scored 19 kills out of 26 fired missiles (73%) in the 1982 Falklands War. But, in a conflict against opponents using thermal countermeasures, the United States only scored 11 kills out of 48 fired (Pk = 23%) with the follow-on AIM-9M in the 1991 Gulf War.\nRadar guided missiles fall into two main missile guidance types. In the historically more common semi-active radar homing case the missile homes in on radar signals transmitted from launching aircraft and reflected from the target. This has the disadvantage that the firing aircraft must maintain radar lock on the target and is thus less free to maneuver and more vulnerable to attack. A widely deployed missile of this type was the AIM-7 Sparrow, which entered service in 1954 and was produced in improving versions until 1997. In more advanced active radar homing the missile is guided to the vicinity of the target by internal data on its projected position, and then \"goes active\" with an internally carried small radar system to conduct terminal guidance to the target. This eliminates the requirement for the firing aircraft to maintain radar lock, and thus greatly reduces risk. A prominent example is the AIM-120 AMRAAM, which was first fielded in 1991 as the AIM-7 replacement, and which has no firm retirement date . The current AIM-120D version has a maximum high altitude range of greater than 160\u00a0km (&gt;99 miles), and cost approximately $2.4\u00a0million each (2016). As is typical with most other missiles, range at lower altitude may be as little as one third that of high altitude.\nIn the Vietnam air war radar missile kill reliability was approximately 10% at shorter ranges, and even worse at longer ranges due to reduced radar return and greater time for the target aircraft to detect the incoming missile and take evasive action. At one point in the Vietnam war, the U.S. Navy fired 50 AIM-7 Sparrow radar guided missiles in a row without a hit. Between 1958 and 1982 in five wars there were 2,014 combined heat-seeking and radar guided missile firings by fighter pilots engaged in air-to-air combat, achieving 528 kills, of which 76 were radar missile kills, for a combined effectiveness of 26%. However, only four of the 76 radar missile kills were in the beyond-visual-range mode intended to be the strength of radar guided missiles. The United States invested over $10\u00a0billion in air-to-air radar missile technology from the 1950s to the early 1970s. Amortized over actual kills achieved by the U.S. and its allies, each radar guided missile kill thus cost over $130\u00a0million. The defeated enemy aircraft were for the most part older MiG-17s, \u221219s, and \u221221s, with new cost of $0.3\u00a0million to $3\u00a0million each. Thus, the radar missile investment over that period far exceeded the value of enemy aircraft destroyed, and furthermore had very little of the intended BVR effectiveness.\nHowever, continuing heavy development investment and rapidly advancing electronic technology led to significant improvement in radar missile reliabilities from the late 1970s onward. Radar guided missiles achieved 75% Pk (9 kills out of 12 shots) in operations in the Gulf War in 1991. The percentage of kills achieved by radar guided missiles also surpassed 50% of total kills for the first time by 1991. Since 1991, 20 of 61 kills worldwide have been beyond-visual-range using radar missiles. Discounting an accidental friendly fire kill, in operational use the AIM-120D (the current main American radar guided missile) has achieved 9 kills out of 16 shots for a 56% Pk. Six of these kills were BVR, out of 13 shots, for a 46% BVR Pk. Though all these kills were against less capable opponents who were not equipped with operating radar, electronic countermeasures, or a comparable weapon themselves, the BVR Pk was a significant improvement from earlier eras. However, a current concern is electronic countermeasures to radar missiles, which are thought to be reducing the effectiveness of the AIM-120D. Some experts believe that the European Meteor missile, the Russian R-37M, and the Chinese PL-15 are more resistant to countermeasures and more effective than the AIM-120D.\nNow that higher reliabilities have been achieved, both types of missiles allow the fighter pilot to often avoid the risk of the short-range dogfight, where only the more experienced and skilled fighter pilots tend to prevail, and where even the finest fighter pilot can simply get unlucky. Taking maximum advantage of complicated missile parameters in both attack and defense against competent opponents does take considerable experience and skill, but against surprised opponents lacking comparable capability and countermeasures, air-to-air missile warfare is relatively simple. By partially automating air-to-air combat and reducing reliance on gun kills mostly achieved by only a small expert fraction of fighter pilots, air-to-air missiles now serve as highly effective force multipliers."}
{"id": "10930", "revid": "35875811", "url": "https://en.wikipedia.org/wiki?curid=10930", "title": "February 25", "text": ""}
{"id": "10931", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=10931", "title": "Finite-state machine", "text": "A finite-state machine (FSM) or finite-state automaton (FSA, plural: \"automata\"), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of \"states\" at any given time. The FSM can change from one state to another in response to some inputs; the change from one state to another is called a \"transition\". An FSM is defined by a list of its states, its initial state, and the inputs that trigger each transition. Finite-state machines are of two types\u2014deterministic finite-state machines and non-deterministic finite-state machines. A deterministic finite-state machine can be constructed equivalent to any non-deterministic one.\nThe behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, whose sequence of stops is determined by the floors requested by riders, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of a sequence of numbers in the proper order.\nThe finite-state machine has less computational power than some other models of computation such as the Turing machine. The computational power distinction means there are computational tasks that a Turing machine can do but an FSM cannot. This is because an FSM's memory is limited by the number of states it has. A finite-state machine has the same computational power as a Turing machine that is restricted such that its head may only perform \"read\" operations, and always has to move from left to right. FSMs are studied in the more general field of automata theory.\nExample: coin-operated turnstile.\nAn example of a simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.\nConsidered as a state machine, the turnstile has two possible states: Locked and Unlocked. There are two possible inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push). In the locked state, pushing on the arm has no effect; no matter how many times the input push is given, it stays in the locked state. Putting a coin in \u2013 that is, giving the machine a coin input \u2013 shifts the state from Locked to Unlocked. In the unlocked state, putting additional coins in has no effect; that is, giving additional coin inputs does not change the state. However, a customer pushing through the arms, giving a push input, shifts the state back to Locked.\nThe turnstile state machine can be represented by a state-transition table, showing for each possible state, the transitions between them (based upon the inputs given to the machine) and the outputs resulting from each input:\nThe turnstile state machine can also be represented by a directed graph called a state diagram \"(above)\". Each state is represented by a node (\"circle\"). Edges (\"arrows\") show the transitions from one state to another. Each arrow is labeled with the input that triggers that transition. An input that doesn't cause a change of state (such as a coin input in the Unlocked state) is represented by a circular arrow returning to the original state. The arrow into the Locked node from the black dot indicates it is the initial state.\nConcepts and terminology.\nA \"state\" is a description of the status of a system that is waiting to execute a \"transition\". A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.\nFor example, when using an audio system to listen to the radio (the system is in the \"radio\" state), receiving a \"next\" stimulus results in moving to the next station. When the system is in the \"CD\" state, the \"next\" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state.\nIn some finite-state machine representations, it is also possible to associate actions with a state:\nRepresentations.\nState/Event table.\nSeveral state-transition table types are used. The most common representation is shown below: the combination of current state (e.g. B) and input (e.g. Y) shows the next state (e.g. C). The complete action's information is not directly described in the table and can only be added using footnotes. An FSM definition including the full actions information is possible using state tables (see also virtual finite-state machine).\nUML state machines.\nThe Unified Modeling Language has a notation for describing state machines. UML state machines overcome the limitations of traditional finite-state machines while retaining their main benefits. UML state machines introduce the new concepts of hierarchically nested states and orthogonal regions, while extending the notion of actions. UML state machines have the characteristics of both Mealy machines and Moore machines. They support actions that depend on both the state of the system and the triggering event, as in Mealy machines, as well as entry and exit actions, which are associated with states rather than transitions, as in Moore machines.\nSDL state machines.\nThe Specification and Description Language is a standard from ITU that includes graphical symbols to describe actions in the transition:\nSDL embeds basic data types called \"Abstract Data Types\", an action language, and an execution semantic in order to make the finite-state machine executable.\nOther state diagrams.\nThere are a large number of variants to represent an FSM such as the one in figure 3.\nUsage.\nIn addition to their use in modeling reactive systems presented here, finite-state machines are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, video game programming, and logic. Finite-state machines are a class of automata studied in automata theory and the theory of computation.\nIn computer science, finite-state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.\nClassification.\nFinite-state machines can be subdivided into acceptors, classifiers, transducers and sequencers.\nAcceptors.\nAcceptors (also called detectors or recognizers) produce binary output, indicating whether or not the received input is accepted. Each state of an acceptor is either \"accepting\" or \"non accepting\". Once all input has been received, if the current state is an accepting state, the input is accepted; otherwise it is rejected. As a rule, input is a sequence of symbols (characters); actions are not used. The start state can also be an accepting state, in which case the acceptor accepts the empty string. The example in figure 4 shows an acceptor that accepts the string \"nice\". In this acceptor, the only accepting state is state 7.\nA (possibly infinite) set of symbol sequences, called a formal language, is a regular language if there is some acceptor that accepts \"exactly\" that set. For example, the set of binary strings with an even number of zeroes is a regular language (cf. Fig. 5), while the set of all strings whose length is a prime number is not.\nAn acceptor could also be described as defining a language that would contain every string accepted by the acceptor but none of the rejected ones; that language is \"accepted\" by the acceptor. By definition, the languages accepted by acceptors are the regular languages.\nThe problem of determining the language accepted by a given acceptor is an instance of the algebraic path problem\u2014itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring.\nAn example of an accepting state appears in Fig. 5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s.\n\"S\"1 (which is also the start state) indicates the state at which an even number of 0s has been input. S1 is therefore an accepting state. This acceptor will finish in an accept state, if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this acceptor are \u03b5 (the empty string), 1, 11, 11..., 00, 010, 1010, 10110, etc.\nClassifiers.\nClassifiers are a generalization of acceptors that produce \"n\"-ary output where \"n\" is strictly greater than two.\nTransducers.\nTransducers produce output based on a given input and/or a state using actions. They are used for control applications and in the field of computational linguistics.\nIn control applications, two types are distinguished:\nSequencers.\nSequencers (also called generators) are a subclass of acceptors and transducers that have a single-letter input alphabet. They produce only one sequence which can be seen as an output sequence of acceptor or transducer outputs.\nDeterminism.\nA further distinction is between deterministic (DFA) and non-deterministic (NFA, GNFA) automata. In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.\nA finite-state machine with only one state is called a \"combinatorial FSM\". It only allows actions upon transition \"into\" a state. This concept is useful in cases where a number of finite-state machines are required to work together, and when it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.\nAlternative semantics.\nThere are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers. They combine hierarchical state machines (which usually have more than one current state), flow graphs, and truth tables into one language, resulting in a different formalism and set of semantics. These charts, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.\nMathematical model.\nIn accordance with the general classification, the following formal definitions are found.\nA \"deterministic finite-state machine\" or \"deterministic finite-state acceptor\" is a quintuple formula_1, where:\nFor both deterministic and non-deterministic FSMs, it is conventional to allow formula_6 to be a partial function, i.e. formula_13 does not have to be defined for every combination of formula_14 and formula_15. If an FSM formula_16 is in a state formula_17, the next symbol is formula_18 and formula_13 is not defined, then formula_16 can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions.\nA finite-state machine has the same computational power as a Turing machine that is restricted such that its head may only perform \"read\" operations, and always has to move from left to right. That is, each formal language accepted by a finite-state machine is accepted by such a kind of restricted Turing machine, and vice versa.\nA \"finite-state transducer\" is a sextuple formula_21, where:\nIf the output function depends on the state and input symbol (formula_30) that definition corresponds to the \"Mealy model\", and can be modelled as a Mealy machine. If the output function depends only on the state (formula_31) that definition corresponds to the \"Moore model\", and can be modelled as a Moore machine. A finite-state machine with no output function at all is known as a semiautomaton or transition system.\nIf we disregard the first output symbol of a Moore machine, formula_32, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state. The converse transformation is less straightforward because a Mealy machine state may have different output labels on its incoming transitions (edges). Every such state needs to be split in multiple Moore machine states, one for every incident output symbol.\nOptimization.\nOptimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm. Other techniques include using an implication table, or the Moore reduction procedure. Additionally, acyclic FSAs can be minimized in linear time.\nImplementation.\nHardware applications.\nIn a digital circuit, an FSM may be built using a programmable logic device, a programmable logic controller, logic gates and flip flops or relays. More specifically, a hardware implementation requires a register to store state variables, a block of combinational logic that determines the state transition, and a second block of combinational logic that determines the output of an FSM. One of the classic hardware implementations is the Richards controller.\nIn a \"Medvedev machine\", the output is directly connected to the state flip-flops minimizing the time delay between flip-flops and output.\nThrough state encoding for low power state machines may be optimized to minimize power consumption.\nSoftware applications.\nThe following concepts are commonly used to build software applications with finite-state machines:\nFinite-state machines and compilers.\nFinite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite-state machines that implement a lexical analyzer and a parser.\nStarting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.\nFurther reading.\nFinite Markov chain processes.\nFinite Markov-chain processes are also known as subshifts of finite type."}
{"id": "10932", "revid": "11231087", "url": "https://en.wikipedia.org/wiki?curid=10932", "title": "Finite state automaton", "text": ""}
{"id": "10933", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=10933", "title": "Functional programming", "text": "In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\nIn functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner.\nFunctional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming which treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.\nFunctional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme, Clojure, Wolfram Language, Racket, Erlang, Elixir, OCaml, Haskell, and F#. Functional programming is also key to some languages that have found success in specific domains, like JavaScript in the Web, R in statistics, J, K and Q in financial analysis, and XQuery/XSLT for XML. Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values. In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, Kotlin, Perl, PHP, Python, Go, Rust, Raku, and Scala.\nHistory.\nThe lambda calculus, developed in the 1930s by Alonzo Church, is a formal system of computation built from function application. In 1937 Alan Turing proved that the lambda calculus and Turing machines are equivalent models of computation, showing that the lambda calculus is Turing complete. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation, combinatory logic, was developed by Moses Sch\u00f6nfinkel and Haskell Curry in the 1920s and 1930s.\nChurch later developed a weaker system, the simply-typed lambda calculus, which extended the lambda calculus by assigning a type to all terms. This forms the basis for statically-typed functional programming.\nThe first functional programming language, LISP, was developed in the late 1950s for the IBM 700/7000 series of scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT). LISP functions were defined using Church's lambda notation, extended with a label construct to allow recursive functions. Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.\nInformation Processing Language (IPL), 1956, is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of \"generator\", which amounts to a function that accepts a function as an argument, and, since it is an assembly-level language, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on the mutating list structure and similar imperative features.\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book \"A Programming Language\" (). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\nJohn Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\". He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style now associated with functional programming.\nThe 1973 language ML was created by Robin Milner at the University of Edinburgh, and David Turner developed the language SASL at the University of St Andrews. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML.\nIn the 1970s, Guy L. Steele and Gerald Jay Sussman developed Scheme, as described in the Lambda Papers and the 1985 textbook \"Structure and Interpretation of Computer Programs\". Scheme was the first dialect of lisp to use lexical scoping and to require tail-call optimization, features that encourage functional programming.\nIn the 1980s, Per Martin-L\u00f6f developed intuitionistic type theory (also called \"constructive\" type theory), which associated functional programs with constructive proofs expressed as dependent types. This led to new approaches to interactive theorem proving and has influenced the development of subsequent functional programming languages.\nThe lazy functional language, Miranda, developed by David Turner, initially appeared in 1985 and had a strong influence on Haskell. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\nMore recently it has found use in niches such as parametric CAD courtesy of the OpenSCAD language built on the CSG geometry framework, although its restriction on reassigning values (all values are treated as constants) has led to confusion among users who are unfamiliar with functional programming as a concept.\nFunctional programming continues to be used in commercial settings.\nConcepts.\nA number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages often cater to several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.\nFirst-class and higher-order functions.\nHigher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator formula_1, which returns the derivative of a function formula_2.\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\nHigher-order functions enable partial application or currying, a technique that applies a function to its arguments one at a time, with each application returning a new function that accepts the next argument. This lets a programmer succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\nPure functions.\nPure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\nWhile most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also lets functions be designated \"pure\". C++11 added codice_1 keyword with similar semantics.\nRecursion.\nIteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, letting an operation be repeated until it reaches the base case. In general, recursion requires maintaining a stack, which consumes space in a linear amount to the depth of recursion. This could make recursion prohibitively expensive to use instead of imperative loops. However, a special form of recursion known as tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\nThe Scheme language standard requires implementations to support proper tail recursion, meaning they must allow an unbounded number of active tail calls. Proper tail recursion is not simply an optimization; it is a language feature that assures users that they can use recursion to express a loop and doing so would be safe-for-space. Moreover, contrary to its name, it accounts for all tail calls, not just tail recursion. While proper tail recursion is usually implemented by turning code into imperative loops, implementations might implement it in other ways. For example, CHICKEN intentionally maintains a stack and lets the stack overflow. However, when this happens, its garbage collector will claim space back, allowing an unbounded number of active tail calls even though it does not turn tail recursion into a loop.\nCommon patterns of recursion can be abstracted away using higher-order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples. Such recursion schemes play a role analogous to built-in control structures such as loops in imperative languages.\nMost general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.\nStrict versus non-strict evaluation.\nFunctional languages can be categorized by whether they use \"strict (eager)\" or \"non-strict (lazy)\" evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm fails. For example, the expression:\n print length([2+1, 3*2, 1/0, 5-4])\nfails under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function returns the value 4 (i.e., the number of items in the list), since evaluating it does not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\nThe usual implementation strategy for lazy evaluation in functional languages is graph reduction. Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.\n argues for lazy evaluation as a mechanism for improving program modularity through separation of concerns, by easing independent implementation of producers and consumers of data streams. Launchbury 1993 describes some difficulties that lazy evaluation introduces, particularly in analyzing a program's storage requirements, and proposes an operational semantics to aid in such analysis. Harper 2009 proposes including both strict and lazy evaluation in the same language, using the language's type system to distinguish them.\nType systems.\nEspecially since the development of Hindley\u2013Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), though they reject all invalid programs at runtime when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.\nSome research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which lets types depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with. But dependent types can express arbitrary propositions in higher-order logic. Through the Curry\u2013Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.\nA limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience. GADT's are available in the Glasgow Haskell Compiler, in OCaml and in Scala, and have been proposed as additions to other languages including Java and C#.\nReferential transparency.\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.\nConsider C assignment statement codice_2, this changes the value assigned to the variable codice_3. Let us say that the initial value of codice_3 was codice_5, then two consecutive evaluations of the variable codice_3 yields codice_7 and codice_8 respectively. Clearly, replacing codice_2 with either codice_7 or codice_8 gives a program a different meaning, and so the expression \"is not\" referentially transparent. In fact, assignment statements are never referentially transparent.\nNow, consider another function such as int plusone(int x) {return x+1;} \"is\" transparent, as it does not implicitly change the input x and thus has no such side effects.\nFunctional programs exclusively use this type of function and are therefore referentially transparent.\nData structures.\nPurely functional data structures are often represented in a different way than their imperative counterparts. For example, the array with constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the hash table and binary heap, are based on arrays. Arrays can be replaced by maps or random access lists, which admit purely functional implementation, but have logarithmic access and update times. Purely functional data structures have persistence, a property of keeping previous versions of the data structure unmodified. In Clojure, persistent data structures are used as functional alternatives to their imperative counterparts. Persistent vectors, for example, use trees for partial updating. Calling the insert method will result in some but not all nodes being created.\nComparison to imperative programming.\nFunctional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order \u201cmap\u201d function that takes a function and a list, generating and returning a new list by applying the function to each list item.\nSide-by-side comparison of imperative vs. functional programming.\nThe following two examples (written in JavaScript) achieve the same effect: they multiply all even numbers in an array by 10 and add them all, storing the final sum in the variable \"result\".\nTraditional Imperative Loop:\nconst numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\nlet result = 0;\nfor (let i = 0; i &lt; numList.length; i++) {\n if (numList[i] % 2 === 0) {\n result += numList[i] * 10;\nFunctional Programming with higher-order functions:\nconst result = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n .filter(n =&gt; n % 2 === 0)\n .map(a =&gt; a * 10)\n .reduce((a, b) =&gt; a + b);\nSimulating state.\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\nThe pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).\nFunctional languages also simulate states by passing around immutable states. This can be done by making a function accept the state as one of its parameters, and return a new state together with the result, leaving the old state unchanged.\nImpure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.\nAlternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.\nEfficiency issues.\nFunctional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal. This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware. Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.\nLazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan \"et al.\" 2008 give some practical advice for analyzing and fixing them.\nHowever, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles) .\nFunctional programming in non-functional languages.\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.\nJavaScript, Lua, Python and Go had first class functions from their inception. Python had support for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2, though Python 3 relegated \"reduce\" to the codice_12 standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, C++11, and Kotlin.\nIn PHP, anonymous classes, closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.\nIn Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes.\nIn C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array, and Object.freeze() in JavaScript.\nApplications.\nAcademia.\nFunctional programming is an active area of research in the field of programming language theory. There are several peer-reviewed publication venues focusing on functional programming, including the International Conference on Functional Programming, the Journal of Functional Programming, and the Symposium on Trends in Functional Programming.\nIndustry.\nFunctional programming has seen use in a wide variety of industrial applications. For example, Erlang, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems, but has since become popular for building a range of applications at companies such as Nortel, Facebook, \u00c9lectricit\u00e9 de France and WhatsApp. Scheme, a dialect of Lisp, was used as the basis for several applications on early Apple Macintosh computers, and has been applied to problems such as training simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming, and static analysis of embedded software. Haskell, though initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.\nOther functional programming languages that have seen use in industry include Scala, F#, Wolfram Language, Lisp, Standard ML, and Clojure.\nFunctional \"platforms\" have been popular in finance for risk analytics (particularly with the larger investment banks). Risk factors are coded as functions that form interdependent graphs (categories) to measure correlations in market shifts not unlike Gr\u00f6bner basis optimizations but also for regulatory compliance such as Comprehensive Capital Analysis and Review. Given the use of OCAML or CAML variations in finance, these systems are sometimes considered related to a categorical abstract machine or CAM. Indeed, functional programming is heavily influenced by category theory.\nEducation.\nMany universities teach or have taught functional programming as part of their undergraduate Computer Science degrees. Some use it as their introduction to programming, while others teach it after teaching imperative programming. \nOutside of computer science, functional programming is being used as a method to teach problem solving, algebra and geometric concepts.\nIt has also been used as a tool to teach classical mechanics in \"Structure and Interpretation of Classical Mechanics\"."}
{"id": "10934", "revid": "76", "url": "https://en.wikipedia.org/wiki?curid=10934", "title": "Functional programming language", "text": ""}
{"id": "10936", "revid": "34626170", "url": "https://en.wikipedia.org/wiki?curid=10936", "title": "February 29", "text": "February 29, also known as leap day or leap year day, is a date added to leap years. A leap day is added in various solar calendars (calendars based on the Earth's revolution around the Sun), including the Gregorian calendar standard in most of the world. Lunisolar calendars (whose months are based on the phases of the Moon) instead add a leap or intercalary month. It is the 60th day of a leap year in the Gregorian calendar, and 306 days remain until the end of the leap year. It is also the last day of February on leap years."}
{"id": "10937", "revid": "1942604", "url": "https://en.wikipedia.org/wiki?curid=10937", "title": "Francis Scott Key", "text": "Francis Scott Key (August 1, 1779January 11, 1843) was an American lawyer, author, and amateur poet from Frederick, Maryland, who is best known for writing the lyrics for the American national anthem \"The Star-Spangled Banner\".\nKey observed the British bombardment of Fort McHenry in 1814 during the War of 1812. He was inspired upon seeing the American flag still flying over the fort at dawn and wrote the poem \"Defence of Fort M'Henry\"; it was published within a week with the suggested tune of the popular song \"To Anacreon in Heaven\". The song with Key's lyrics became known as \"The Star-Spangled Banner\" and slowly gained in popularity as an unofficial anthem, finally achieving official status more than a century later under President Herbert Hoover as the national anthem. \nKey was a lawyer in Maryland and Washington D.C. for four decades and worked on important cases, including the Burr conspiracy trial, and he argued numerous times before the Supreme Court. He was nominated for District Attorney for the District of Columbia by President Andrew Jackson, where he served from 1833 to 1841. Key was a devout Episcopalian.\nKey owned slaves from 1800, during which time abolitionists ridiculed his words, claiming that America was more like the \"Land of the Free and Home of the Oppressed\". As District Attorney, he suppressed abolitionists and did not support an immediate end to slavery. He was also a leader of the American Colonization Society which sent freed slaves to Africa. He freed some of his slaves in the 1830s, paying one ex-slave as his farm foreman. He publicly criticized slavery and gave free legal representation to some slaves seeking freedom, but he also represented owners of runaway slaves.\nEarly life.\nKey's father John Ross Key was a lawyer, a commissioned officer in the Continental Army, and a judge of English descent. His mother Ann Phoebe Dagworthy Charlton was born (February 6, 1756 \u2013 1830), to Arthur Charlton, a tavern keeper, and his wife, Eleanor Harrison of Frederick in the colony of Maryland.\nKey grew up on the family plantation Terra Rubra in Frederick County, Maryland (now Carroll County). He graduated from St.John's College, Annapolis, Maryland, in 1796 and read law under his uncle Philip Barton Key who was loyal to the British Crown during the War of Independence. He married Mary Tayloe Lloyd on January 1, 1802, daughter of Edward Lloyd IV of Wye House and Elizabeth Tayloe, daughter of John Tayloe II of Mount Airy and sister of John Tayloe III of The Octagon House.\n\"The Star-Spangled Banner\".\nDuring the War of 1812, Key and British Prisoner Exchange Agent Colonel John Stuart Skinner dined aboard as the guests of Vice Admiral Alexander Cochrane, Rear Admiral George Cockburn, and Major General Robert Ross. Skinner and Key were there to negotiate the release of prisoners, one of whom was William Beanes, a resident of Upper Marlboro, Maryland, who had been arrested after jailing British troops who were taking food from local farms. Skinner, Key, and Beanes were not allowed to return to their own sloop because they had become familiar with the strength and position of the British units and their intention to launch an attack upon Baltimore, and Key was unable to do anything but watch the bombarding of the American forces at Fort McHenry during the Battle of Baltimore on the night of September 1314,1814.\nAt dawn, Key was able to see an American flag waving, and he later wrote a poem about his experience entitled \"Defence of Fort M'Henry\" which was published in William Pechin's \"American and Commercial Daily Advertiser\" on September 21, 1814. He took it to music publisher Thomas Carr, who adapted it to the rhythms of composer John Stafford Smith's \"To Anacreon in Heaven\", a popular tune that Key had already used as a setting for his 1805 song \"When the Warrior Returns\", celebrating American heroes of the First Barbary War. It was somewhat difficult to sing, yet it became increasingly popular, competing with \"Hail, Columbia\" (1796) as the de facto national anthem by the time of the Mexican\u2013American War and the American Civil War. The song was finally adopted as the American national anthem more than a century after its first publication, first by an Executive Order from President Woodrow Wilson in1916, and then by a Congressional resolution in1931 signed by President Herbert Hoover.\nThe third stanza of the Star-Spangled Banner makes disparaging mention of blacks and demonstrates Key's opinion of their seeking freedom at the time by escaping to the British, who promised them freedom from American enslavement.\nLegal career.\nKey was a leading attorney in Frederick, Maryland, and Washington, D.C., for many years, with an extensive real estate and trial practice. He and his family settled in Georgetown in 1805 or 1806, near the new national capital. He assisted his uncle Philip Barton Key in the sensational conspiracy trial of Aaron Burr and in the expulsion of Senator John Smith of Ohio. He made the first of his many arguments before the United States Supreme Court in 1807. In 1808, he assisted President Thomas Jefferson's attorney general in \"United Statesv.Peters\".\nIn 1829, Key assisted in the prosecution of Tobias Watkins, former U.S. Treasury auditor under President John Quincy Adams, for misappropriating public funds. He also handled the Petticoat affair concerning Secretary of War John Eaton, and he served as the attorney for Sam Houston in 1832 during his trial for assaulting Representative William Stanbery of Ohio. After years as an adviser to President Jackson, Key was nominated by the President to District Attorney for the District of Columbia in 1833. He served from 1833 to 1841 while also handling his own private legal cases. In 1835, he prosecuted Richard Lawrence for his attempt to assassinate President Jackson at the top steps of the Capitol, the first attempt to kill an American president.\nKey and slavery.\nKey purchased his first slave in 1800 or 1801 and owned six slaves in 1820. He freed seven slaves in the 1830s, one of whom continued to work for him for wages as his farm's foreman, supervising several slaves. Key also represented several slaves seeking their freedom, as well as several slave-owners seeking return of their runaway slaves. Key was one of the executors of John Randolph of Roanoke's will, which freed his 400 slaves, and Key fought to enforce the will for the next decade and to provide the freed slaves with land to support themselves.\nKey is known to have publicly criticized slavery's cruelties, and a newspaper editorial stated that \"he often volunteered to defend the downtrodden sons and daughters of Africa.\" The editor said that Key \"convinced me that slavery was wrong\u2014radically wrong\".\nA quote increasingly credited to Key stating that free blacks are \"a distinct and inferior race of people, which all experience proves to be the greatest evil that afflicts a community\" is erroneous. The quote is taken from an 1838 letter that Key wrote to Reverend Benjamin Tappan of Maine who had sent Key a questionnaire about the attitudes of Southern religious institutions about slavery. Rather than representing a statement by Key identifying his personal thoughts, the words quoted are offered by Key to describe the attitudes of others who assert that formerly enslaved blacks could not remain in the U.S. as paid laborers. This was the official policy of the American Colonization Society. Key was an ACS leader and fundraiser for the organization, but he himself did not send the men and women he freed to Africa upon their emancipation. The original confusion around this quote arises from ambiguities in the 1937 biography of Key by Edward S. Delaplaine.\nKey was a founding member and active leader of the American Colonization Society (ACS), whose primary goal was to send free blacks to Africa. Though many free blacks were born in the United States by this time, historians argue that upper-class American society, of which Key was a part, could never \"envision a multiracial society\". The ACS was not supported by most abolitionists or free blacks of the time, but the organization's work would eventually lead to the creation of Liberia in 1847.\nAnti-abolitionism.\nIn the early 1830s American thinking on slavery changed quite abruptly. Considerable opposition to the American Colonization Society's project emerged. Led by newspaper editor and publisher Wm. Lloyd Garrison, a growing portion of the population noted that only a very small number of free blacks were actually moved, and they faced brutal conditions in West Africa, with very high mortality. Free blacks made it clear that few of them wanted to move, and if they did, it would be to Canada, Mexico, or Central America, not Africa. The leaders of the American Colonization Society, including Key, were predominantly slaveowners. The Society was intended to preserve slavery, rather than eliminate it. In the words of philanthropist Gerrit Smith, it was \"quite as much an Anti-Abolition, as Colonization Society\". \"This Colonization Society had, by an invisible process, half conscious, half unconscious, been transformed into a serviceable organ and member of the Slave Power.\"\nThe alternative to the colonization of Africa, project of the American Colonization Society, was the total and immediate abolition of slavery in the United States. This Key was firmly against, with or without slaveowner compensation, and he used his position as District Attorney to attack abolitionists. In 1833, he secured a grand jury indictment against Benjamin Lundy, editor of the anti-slavery publication \"Genius of Universal Emancipation\", and his printer William Greer, for libel after Lundy published an article that declared, \"There is neither mercy nor justice for colored people in this district [of Columbia]\". Lundy's article, Key said in the indictment, \"was intended to injure, oppress, aggrieve, and vilify the good name, fame, credit &amp; reputation of the Magistrates and constables\" of Washington. Lundy left town rather than face trial; Greer was acquitted.\nProsecution of Reuben Crandall.\nIn a larger unsuccessful prosecution, in August 1836 Key obtained an indictment against Reuben Crandall, brother of controversial Connecticut teacher Prudence Crandall, who had recently moved to Washington, D.C. It accused Crandall of \"seditious libel\" after two marshals (who operated as slave catchers in their off hours) found Crandall had a trunk full of anti-slavery publications in his Georgetown residence/office, five days after the Snow riot, caused by rumors that a mentally ill slave had attempted to kill an elderly white woman. In an April 1837 trial that attracted nationwide attention and that congressmen attended, Key charged that Crandall's publications instigated slaves to rebel. Crandall's attorneys acknowledged he opposed slavery, but denied any intent or actions to encourage rebellion. Evidence was introduced that the anti-slavery publications were packing materials used by his landlady in shipping his possessions to him. He had not \"published\" anything; he had given one copy to one man who had asked for it.\nKey, in his final address to the jury said:\nThe jury acquitted Crandall of all charges. This public and humiliating defeat, as well as family tragedies in 1835, diminished Key's political ambition. He resigned as District Attorney in 1840. He remained a staunch proponent of African colonization and a strong critic of the abolition movement until his death.\nCrandall died shortly after his acquittal of pneumonia contracted in the Washington jail.\nReligion.\nKey was a devout and prominent Episcopalian. In his youth, he almost became an Episcopal priest rather than a lawyer. Throughout his life he sprinkled biblical references in his correspondence. He was active in All Saints Parish in Frederick, Maryland, near his family's home. He also helped found or financially support several parishes in the new national capital, including St. John's Episcopal Church in Georgetown and Christ Church in Alexandria (at the time, in the District of Columbia).\nFrom 1818 until his death in 1843, Key was associated with the American Bible Society. He successfully opposed an abolitionist resolution presented to that group around 1838.\nKey also helped found two Episcopal seminaries, one in Baltimore and the other across the Potomac River in Alexandria (the Virginia Theological Seminary). Key also published a prose work called \"The Power of Literature, and Its Connection with Religion\" in 1834.\nDeath and legacy.\nOn January 11, 1843, Key died at the home of his daughter Elizabeth Howard in Baltimore from pleurisy at age 63. He was initially interred in Old Saint Paul's Cemetery in the vault of John Eager Howard but in 1866, his body was moved to his family plot in Frederick at Mount Olivet Cemetery.\nThe Key Monument Association erected a memorial in 1898 and the remains of both Francis Scott Key and his wife, Mary Tayloe Lloyd, were placed in a crypt in the base of the monument.\nDespite several efforts to preserve it, the Francis Scott Key residence was ultimately dismantled in1947. The residence had been located at 351618MStreet in Georgetown.\nThough Key had written poetry from time to time, often with heavily religious themes, these works were not collected and published until 14years after his death. Two of his religious poems used as Christian hymns include \"Before the Lord We Bow\" and \"Lord, with Glowing Heart I'd Praise Thee\".\nIn1806, Key's sister, Anne Phoebe Charlton Key, married Roger\u00a0B.\u00a0Taney, who would later become Chief Justice of the United States. In 1846\u00a0one daughter, Alice, married U.S.\u00a0Senator George H. Pendleton and another, Ellen Lloyd, married Simon F. Blunt. In1859, Key's son Philip Barton Key II, who also served as United States Attorney for the District of Columbia, was shot and killed by Daniel Sicklesa U.S.Representative from New York who would serve as a general in the American Civil Warafter he discovered that Philip Barton Key was having an affair with his wife. Sickles was acquitted in the first use of the temporary insanity defense. In1861, Key's grandson Francis Key Howard was imprisoned in Fort\u00a0McHenry with the Mayor of Baltimore George William Brown and other locals deemed to be Confederate sympathizers.\nKey was a distant cousin and the namesake of F.\u00a0Scott Fitzgerald, whose full name was Francis Scott Key Fitzgerald. His direct descendants include geneticist Thomas Hunt Morgan, guitarist Dana Key, and American fashion designer and socialite Pauline de Rothschild."}
{"id": "10938", "revid": "3131428", "url": "https://en.wikipedia.org/wiki?curid=10938", "title": "FSU", "text": "FSU may refer to:"}
{"id": "10939", "revid": "14771271", "url": "https://en.wikipedia.org/wiki?curid=10939", "title": "Formal language", "text": "In mathematics, computer science, and linguistics, a formal language consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.\nThe alphabet of a formal language consists of symbols, letters, or tokens that concatenate into strings of the language. Each string concatenated from symbols of this alphabet is called a word, and the words that belong to a particular formal language are sometimes called \"well-formed words\" or \"well-formed formulas\". A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, which consists of its formation rules.\nThe field of formal language theory studies primarily the purely syntactical aspects of such languages\u2014that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.\nIn computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.\nHistory.\nThe first use of formal language is thought to be Gottlob Frege's 1879 \"Begriffsschrift\", meaning \"concept writing\", which described a \"formal language of pure language.\"\nAxel Thue's early semi-Thue system, which can be used for rewriting strings, was influential on formal grammars.\nWords over an alphabet.\nAn alphabet, in the context of formal languages, can be any set, although it often makes sense to use an alphabet in the usual sense of the word, or more generally a character set such as ASCII or Unicode. The elements of an alphabet are called its letters. An alphabet may contain an infinite number of elements; however, most definitions in formal language theory specify alphabets with a finite number of elements, and most results apply only to them.\nA word over an alphabet can be any finite sequence (i.e., string) of letters. The set of all words over an alphabet \u03a3 is usually denoted by \u03a3* (using the Kleene star). The length of a word is the number of letters it is composed of. For any alphabet, there is only one word of length 0, the \"empty word\", which is often denoted by e, \u03b5, \u03bb or even \u039b. By concatenation one can combine two words to form a new word, whose length is the sum of the lengths of the original words. The result of concatenating a word with the empty word is the original word.\nIn some applications, especially in logic, the alphabet is also known as the \"vocabulary\" and words are known as \"formulas\" or \"sentences\"; this breaks the letter/word metaphor and replaces it by a word/sentence metaphor.\nDefinition.\nA formal language \"L\" over an alphabet \u03a3 is a subset of \u03a3*, that is, a set of words over that alphabet. Sometimes the sets of words are grouped into expressions, whereas rules and constraints may be formulated for the creation of 'well-formed expressions'.\nIn computer science and mathematics, which do not usually deal with natural languages, the adjective \"formal\" is often omitted as redundant.\nWhile formal language theory usually concerns itself with formal languages that are described by some syntactical rules, the actual definition of the concept \"formal language\" is only as above: a (possibly infinite) set of finite-length strings composed from a given alphabet, no more and no less. In practice, there are many languages that can be described by rules, such as regular languages or context-free languages. The notion of a formal grammar may be closer to the intuitive concept of a \"language,\" one described by syntactic rules. By an abuse of the definition, a particular formal language is often thought of as being equipped with a formal grammar that describes it.\nExamples.\nThe following rules describe a formal language\u00a0 over the alphabet \u03a3\u00a0=\u00a0{0,\u20091,\u20092,\u20093,\u20094,\u20095,\u20096,\u20097,\u20098,\u20099,\u2009+,\u2009=}:\nUnder these rules, the string \"23+4=555\" is in\u00a0, but the string \"=234=+\" is not. This formal language expresses natural numbers, well-formed additions, and well-formed addition equalities, but it expresses only what they look like (their syntax), not what they mean (semantics). For instance, nowhere in these rules is there any indication that \"0\" means the number zero, \"+\" means addition, \"23+4=555\" is false, etc.\nConstructions.\nFor finite languages, one can explicitly enumerate all well-formed words. For example, we can describe a language\u00a0 as just \u00a0=\u00a0{a,\u2009b,\u2009ab,\u2009cba}. The degenerate case of this construction is the empty language, which contains no words at all (\u00a0=\u00a0\u2205).\nHowever, even over a finite (non-empty) alphabet such as \u03a3\u00a0=\u00a0{a,\u00a0b} there are an infinite number of finite-length words that can potentially be expressed: \"a\", \"abb\", \"ababba\", \"aaababbbbaab\",\u00a0... Therefore, formal languages are typically infinite, and describing an infinite formal language is not as simple as writing \"L\"\u00a0=\u00a0{a,\u2009b,\u2009ab,\u2009cba}. Here are some examples of formal languages:\nLanguage-specification formalisms.\nFormal languages are used as tools in multiple disciplines. However, formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as\nTypical questions asked about such formalisms include:\nSurprisingly often, the answer to these decision problems is \"it cannot be done at all\", or \"it is extremely expensive\" (with a characterization of how expensive). Therefore, formal language theory is a major application area of computability theory and complexity theory. Formal languages may be classified in the Chomsky hierarchy based on the expressive power of their generative grammar as well as the complexity of their recognizing automaton. Context-free grammars and regular grammars provide a good compromise between expressivity and ease of parsing, and are widely used in practical applications.\nOperations on languages.\nCertain operations on languages are common. This includes the standard set operations, such as union, intersection, and complement. Another class of operation is the element-wise application of string operations.\nExamples: suppose formula_1 and formula_2 are languages over some common alphabet formula_3.\nSuch string operations are used to investigate closure properties of classes of languages. A class of languages is closed under a particular operation when the operation, applied to languages in the class, always produces a language in the same class again. For instance, the context-free languages are known to be closed under union, concatenation, and intersection with regular languages, but not closed under intersection or complement. The theory of trios and abstract families of languages studies the most common closure properties of language families in their own right.\nApplications.\nProgramming languages.\nA compiler usually has two distinct components. A lexical analyzer, sometimes generated by a tool like codice_1, identifies the tokens of the programming language grammar, e.g. identifiers or keywords, numeric and string literals, punctuation and operator symbols, which are themselves specified by a simpler formal language, usually by means of regular expressions. At the most basic conceptual level, a parser, sometimes generated by a parser generator like codice_2, attempts to decide if the source program is syntactically valid, that is if it is well formed with respect to the programming language grammar for which the compiler was built.\nOf course, compilers do more than just parse the source code \u2013 they usually translate it into some executable format. Because of this, a parser usually outputs more than a yes/no answer, typically an abstract syntax tree. This is used by subsequent stages of the compiler to eventually generate an executable containing machine code that runs directly on the hardware, or some intermediate code that requires a virtual machine to execute.\nFormal theories, systems, and proofs.\nIn mathematical logic, a \"formal theory\" is a set of sentences expressed in a formal language.\nA \"formal system\" (also called a \"logical calculus\", or a \"logical system\") consists of a formal language together with a deductive apparatus (also called a \"deductive system\"). The deductive apparatus may consist of a set of transformation rules, which may be interpreted as valid rules of inference, or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems formula_24 and formula_25 may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).\nA \"formal proof\" or \"derivation\" is a finite sequence of well-formed formulas (which may be interpreted as sentences, or propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.\nInterpretations and models.\nFormal languages are entirely syntactic in nature but may be given semantics that give meaning to the elements of the language. For instance, in mathematical logic, the set of possible formulas of a particular logic is a formal language, and an interpretation assigns a meaning to each of the formulas\u2014usually, a truth value.\nThe study of interpretations of formal languages is called formal semantics. In mathematical logic, this is often done in terms of model theory. In model theory, the terms that occur in a formula are interpreted as objects within mathematical structures, and fixed compositional interpretation rules determine how the truth value of the formula can be derived from the interpretation of its terms; a \"model\" for a formula is an interpretation of terms such that the formula becomes true."}
{"id": "10940", "revid": "11521989", "url": "https://en.wikipedia.org/wiki?curid=10940", "title": "Free to Choose", "text": "Free to Choose: A Personal Statement (1980) is a book by economists Milton and Rose D. Friedman, accompanied by a ten-part series broadcast on public television, that advocates free market principles. It was primarily a response to an earlier landmark book and television series \"The Age of Uncertainty\", by the noted economist John Kenneth Galbraith. Milton Friedman won the Nobel Memorial Prize in Economics in 1976.\nOverview.\n\"Free to Choose: A Personal Statement\" maintains that the free market works best for all members of a society, provides examples of how the free market engenders prosperity, and maintains that it can solve problems where other approaches have failed. Published in January 1980, the 297 page book contains 10 chapters. The book was on the United States best sellers list for 5 weeks.\nPBS broadcast the programs, beginning in January 1980. It was filmed at the invitation of Robert Chitester, the owner of WQLN-TV. It was based on a 15-part series of taped public lectures and question-and-answer sessions. The general format was that of Milton Friedman visiting and narrating a number of success and failure stories in history, which he attributes to free-market capitalism or the lack thereof (e.g., Hong Kong is commended for its free markets, while India is excoriated for relying on centralized planning especially for its protection of its traditional textile industry). Following the primary show, Friedman would engage in discussion moderated by Robert McKenzie with a number of selected debaters drawn from trade unions, academy and the business community, such as Donald Rumsfeld (then of G.D. Searle &amp; Company) and Frances Fox Piven of City University of New York. The interlocutors would offer objections to or support for the proposals put forward by Friedman, who would in turn respond. After the final episode, Friedman sat down for an interview with Lawrence Spivak.\nThe series was rebroadcast in 1990 with Linda Chavez moderating the episodes. Arnold Schwarzenegger, Ronald Reagan, Steve Allen and George Shultz give personal introductions for each episode. This time, after the documentary part, Friedman sits down with a single opponent to debate the issues raised in the episode.\nGuest debaters.\nGuest debaters included:\nPositions advocated.\nThe Friedmans advocate \"laissez-faire\" economic policies, often criticizing interventionist government policies and their cost in personal freedoms and economic efficiency in the United States and abroad. They argue that international free trade has been restricted through tariffs and protectionism while domestic free trade and freedom have been limited through high taxation and regulation. They cite the 19th-century United Kingdom, the United States before the Great Depression, and modern Hong Kong as ideal examples of a minimalist economic policy. They contrast the economic growth of Japan after the Meiji Restoration and the economic stagnation of India after its independence from the British Empire, and argue that India has performed worse despite its superior economic potential due to its centralized planning. They argue that even countries with command economies, including the Soviet Union and Yugoslavia, have been forced to adopt limited market mechanisms in order to operate. The authors argue against government taxation on gas and tobacco and government regulation of the public school systems. The Friedmans argue that the Federal Reserve exacerbated the Great Depression by neglecting to prevent the decline of the money supply in the years leading up to it. They further argue that the American public falsely perceived the Depression to be a result of a failure of capitalism rather than the government, and that the Depression allowed the Federal Reserve Board to centralize its control of the monetary system despite its responsibility for it. \nOn the subject of welfare, the Friedmans argue that the United States has maintained a higher degree of freedom and productivity by avoiding the nationalizations and extensive welfare systems of Western European countries such as the United Kingdom and Sweden. However, they also argue that welfare practices since the New Deal under \"the HEW empire\" have been harmful. They argue that public assistance programs have become larger than originally envisioned and are creating \"wards of the state\" as opposed to \"self-reliant individuals.\" They also argue that the Social Security System is fundamentally flawed, that urban renewal and public housing programs have contributed to racial inequality and diminished quality of low-income housing, and that Medicare and Medicaid are responsible for rising healthcare prices in the United States. They suggest completely replacing the welfare state with a negative income tax as a less harmful alternative. \nThe Friedmans also argue that declining academic performance in the United States is the result of increasing government control of the American education system tracing back to the 1840s, but suggest a voucher system as a politically feasible solution. They blame the 1970s recession and lower quality of consumer goods on extensive business regulations since the 1960s, and advocate abolishing the Food and Drug Administration, the Interstate Commerce Commission, the Consumer Product Safety Commission, Amtrak, and Conrail. They argue that the energy crisis would be resolved by abolishing the Department of Energy and price floors on crude oil. They recommend replacing the Environmental Protection Agency and environmental regulation with an effluent charge. They criticize labor unions for raising prices and lowering demand by enforcing high wage levels, and for contributing to unemployment by limiting jobs. They argue that inflation is caused by excessive government spending, the Federal Reserve's attempts to control interest rates, and full employment policy. They call for tighter control of Fed money supply despite the fact that it will result in a temporary period of high unemployment and low growth due to the interruption of the wage-price spiral. In the final chapter, they take note of recent current events that seem to suggest a return to free-market principles in academic thought and public opinion, and argue in favor of an \"economic Bill of Rights\" to cement the changes. "}
{"id": "10941", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10941", "title": "Foreign Service Level 3", "text": ""}
{"id": "10942", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10942", "title": "FORTRAN", "text": ""}
{"id": "10945", "revid": "32316857", "url": "https://en.wikipedia.org/wiki?curid=10945", "title": "Albert Park Circuit", "text": "The Albert Park Circuit is a motorsport street circuit around Albert Park Lake, three kilometres south of central Melbourne. It is used annually as a circuit for the traditional Formula One season-opening Australian Grand Prix, the supporting Supercars Championship Melbourne 400 and other associated support races. The circuit has an FIA Grade 1 license. Although the entire track consists of normally public roads, each sector includes medium to high-speed characteristics more commonly associated with dedicated racetracks facilitated by grass and gravel run-off safety zones that are reconstructed annually. However, the circuit also has characteristics of a street circuit's enclosed nature due to concrete barriers annually built along the Lakeside Drive curve, in particular, where run-off is not available due to the proximity of the lake shore.\nDesign.\nThe circuit uses everyday sections of road that circle Albert Park Lake, a small man-altered lake (originally a large lagoon formed as part of the ancient Yarra River course) just south of the Central Business District of Melbourne. The road sections that are used were rebuilt before the inaugural event in 1996 to ensure consistency and smoothness. As a result, compared to other circuits that are held on public roads, the Albert Park track has quite a smooth surface. Before 2007 there existed only a few other places on the Formula 1 calendar with a body of water close to the track. Many of the new tracks, such as Valencia, Singapore and Abu Dhabi are close to a body of water.\nThe course is considered to be quite fast and relatively easy to drive, drivers having commented that the consistent placement of corners allows them to easily learn the circuit and achieve competitive times. However, the flat terrain around the lake, coupled with a track design that features few true straights, means that the track is not conducive to overtaking or easy spectating unless in possession of a grandstand seat.\nEach year, most of the trackside fencing, pedestrian overpasses, grandstands, and other motorsport infrastructure are erected approximately two months before the Grand Prix weekend and removed within 6 weeks after the event. The land around the circuit (including a large aquatic centre, a golf course, a Lakeside Stadium, some restaurants, and rowing boathouses) has restricted access during that entire period. Dissent is still prevalent among nearby residents and users of those other facilities, and some still maintain a silent protest against the event. Nevertheless, the event is reasonably popular in Melbourne and Australia (with a large European population and a general interest in motorsport). Middle Park, the home of South Melbourne FC was demolished in 1994 due to expansion at Albert Park.\nOn 4 July 2008, F1 announced that more than 300,000 people attended the four-day Melbourne Grand Prix, though actual ticket sales were later disputed by the local media. There has never been a night race at Albert Park, however, the 2009 and 2010 events both started at 5:00\u00a0p.m. local time. The current contract for the Grand Prix at the circuit concludes in 2025.\nFollowing the postponement of the Australian Grand Prix in 2021, due to the COVID-19 pandemic, the track underwent layout changes, the most notable part is the modification of the Turn 9-10 complex from a heavy right-left corner to a fast-sweeping right-left corner into Turns 11 and 12. Further modifications include the widening of the pit lane by 2 metres and the reprofiling of Turn 13.\nA lap in a Formula One car.\nStarting on the Walker Straight, Turn 1 is a tight right-hander, followed by a quick flick to the left in Turn 2. A short straight follows, in which the cars accelerate up to 300\u00a0km/h, before Turn 3 is taken at a third of that speed. Two successive short straights lead to the left and right-handers of Turns 4 and 5, and the end of Sector 1. The next straight lasts for half a kilometre, before a 90-degree right in Turn 6. Turn 7, a small leftwards kink, leads onto a sweeping right-hander. Turns 9 and 10, a car park when not in use, form the Clark Chicane, before the long lakeside sweep right up to Turn 11, on which the cars reach 300\u00a0km/h. The next two corners are the fastest on the circuit, with drivers taking this chicane at up to 225\u00a0km/h, sustaining g-forces up to 3.5g. The straight that follows, with a small right kink in the middle, is a DRS zone (the detection point was shortly before Turn 11), before another 90-degree right in the form of Turn 13, called Ascari. After another short straight, the right-hander of Turn 14 leads to the slowest corner on the track, a tight left. The pit lane entry is located halfway between this corner and the next, a faster right-hander, which together form an extended chicane, and lead back onto the Walker Straight. The track is known for being bumpy, and in even slightly wet weather, is notoriously slippery.\nEveryday access.\nDuring the nine months of the year when the track is not required for Grand Prix preparation or the race weekend, most of the track can be driven by ordinary street-registered vehicles either clockwise or anti-clockwise.\nOnly the sections between turns 3, 4, and 5, then 5 and 6, differ significantly from the race track configuration. Turn 4 is replaced by a car park access road running directly from turns 3 to 5. Between turns 5 and 6, the road is blocked. It is possible to drive from turn 5 on to Albert Road and back on to the track at turn 7 though three sets of lights control the flow of this option. The only set of lights on the actual track is halfway between turns 12 and 13, where drivers using Queens Road are catered for. The chicanes at turns 11 and 12 are considerably more open than that used in the Grand Prix, using the escape roads. Turn 9 is also a car park and traffic is directed down another escape road.\nThe speed limit is generally , while some short sections have a speed limit of , which is still slower than an F1 car under pit lane speed restrictions. The back of the track, turns 7 to 13 inclusive, is known as Lakeside Drive. Double lines separate the two-way traffic along most of Lakeside Drive with short road islands approximately every 50 metres which means overtaking is illegal here. Black Swans live and breed in Albert Park, and frequently cross the road causing traffic delays, sometimes with up to five cygnets (young swans).\nApproximately 80% of the track edge is lined with short parkland-style chain-linked fencing leaving normal drivers less room for error than F1 drivers have during race weekend. There is however substantial shoulder room between the outside of each lane and the fencing, which is used as parking along Aughtie Drive during the other nine months.\nHistory.\nAlbert Park Circuit (1953 to 1958).\nAlbert Park has the distinction of being the only venue to host the Australian Grand Prix in both World Championship and non-World Championship formats with an earlier configuration of the current circuit used for the race on two occasions during the 1950s. During this time racing was conducted in an anti-clockwise direction as opposed to the current circuit which runs clockwise.\nKnown as the Albert Park Circuit, the original course hosted a total of six race meetings:\nRace lap records.\nAs of 16 March 2019."}
{"id": "10946", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10946", "title": "Monaco Grand Prix", "text": "The Monaco Grand Prix () is a Formula One motor race held annually on the Circuit de Monaco on the last weekend in May. Run since 1929, it is widely considered to be one of the most important and prestigious automobile races in the world, and is one of the races\u2014along with the Indianapolis 500 and the 24 Hours of Le Mans\u2014that form the Triple Crown of Motorsport. The circuit has been called \"an exceptional location of glamour and prestige\".\nThe race is held on a narrow course laid out in the streets of Monaco, with many elevation changes and tight corners as well as a tunnel, making it one of the most demanding tracks in Formula One. In spite of the relatively low average speeds, the Monaco circuit is a dangerous place to race and often involves the intervention of a safety car. It is the only Grand Prix that does not adhere to the FIA's mandated minimum race distance for F1 races.\nThe Monaco Grand Prix was part of the pre-Second World War European Championship and was included in the first World Championship of Drivers in 1950. It was designated the European Grand Prix two times, 1955 and 1963, when this title was an honorary designation given each year to one Grand Prix race in Europe. Graham Hill was known as \"Mr. Monaco\" due to his five Monaco wins in the 1960s. Ayrton Senna won the race more times than any other driver, with six victories, winning five races consecutively between 1989 and 1993.\nHistory.\nOrigins.\nLike many European races, the Monaco Grand Prix predates the current World Championship. The principality's first Grand Prix was organised in 1929 by Antony Nogh\u00e8s, under the auspices of Prince Louis II, through the Automobile Club de Monaco (ACM), of which he was president. The ACM organised the Rallye Automobile Monte Carlo, and in 1928 applied to the \"Association Internationale des Automobiles Clubs Reconnus\" (AIACR), the international governing body of motorsport, to be upgraded from a regional French club to full national status. Their application was refused due to the lack of a major motorsport event held wholly within Monaco's boundaries. The rally could not be considered as it mostly used the roads of other European countries.\nTo attain full national status, Nogh\u00e8s proposed the creation of an automobile Grand Prix in the streets of Monte Carlo. He obtained the official sanction of Prince Louis II, and the support of Mon\u00e9gasque \"Grand Prix\" driver Louis Chiron. Chiron thought Monaco's topography well-suited to setting up a race track.\nThe first race, held on 14 April 1929, was won by William Grover-Williams (using the pseudonym \"Williams\"), driving a works Bugatti Type 35B. It was an invitation-only event, but not all of those invited decided to attend. The leading Maserati and Alfa Romeo drivers decided not to compete, but Bugatti was well represented. Mercedes sent their leading driver, Rudolf Caracciola. Starting fifteenth, Caracciola drove a fighting race, taking his SSK into the lead before wasting 4\u00bd minutes on refuelling and a tyre change to finish second. Another driver who competed using a pseudonym was \"Georges Philippe\", the Baron Philippe de Rothschild. Chiron was unable to compete, having a prior commitment to compete in the Indianapolis 500 on the same day.\nCaracciola's SSK was refused permission to race the following year, but Chiron did compete (in the works Bugatti Type 35C), when he was beaten by privateer Ren\u00e9 Dreyfus and his Bugatti Type 35B, and finished second. Chiron took victory in the 1931 race driving a Bugatti. , he remains the only native of Monaco to have won the event.\nPre-war.\nThe race quickly grew in importance after its inception. Because of the high number of races which were being termed 'Grands Prix', the AIACR formally recognised the most important race of each of its affiliated national automobile clubs as International Grands Prix, or \"Grandes \u00c9preuves\", and in 1933 Monaco was ranked as such alongside the French, Belgian, Italian, and Spanish Grands Prix. That year's race was the first Grand Prix in which grid positions were decided, as they are now, by practice time rather than the established method of balloting. The race saw Achille Varzi and Tazio Nuvolari exchange the lead many times before the race being settled in Varzi's favour on the final lap when Nuvolari's car caught fire.\nThe race became a round of the new European Championship in 1936, when stormy weather and a broken oil line led to a series of crashes, eliminating the Mercedes-Benzes of Chiron, Fagioli, and von Brauchitsch, as well as Bernd Rosemeyer's \"Typ C\" for newcomer Auto Union; Rudolf Caracciola, proving the truth of his nickname, \"Regenmeister\" (Rainmaster), went on to win. In 1937, von Brauchitsch duelled Caracciola before coming out on top. It was the last prewar \"Grand Prix\" at Monaco, for in 1938, the demand for \u00a3500 (about US$2450) in appearance money per top entrant led AIACR to cancel the event, while looming war overtook it in 1939, and the Second World War ended organised racing in Europe until 1945.\nPost-war Grand Prix.\nRacing in Europe started again on 9 September 1945 at the Bois de Boulogne Park in the city of Paris, four months and one day after the end of the war in Europe. However, the Monaco Grand Prix was not run between 1945 and 1947 due to financial reasons. In 1946 a new premier racing category, Grand Prix, was defined by the F\u00e9d\u00e9ration Internationale de l'Automobile (FIA), the successor of the AIACR, based on the pre-war voiturette class. A Monaco Grand Prix was run to this formula in 1948, won by the future world champion Nino Farina in a Maserati 4CLT.\nFormula One.\nEarly championship days.\nThe 1949 event was cancelled due to the death of Prince Louis II; it was included in the new Formula One World Drivers' Championship the following year. The race provided future five-time world champion Juan Manuel Fangio with his first win in a World Championship race, as well as third place for the 51-year-old Louis Chiron, his best result in the World Championship era. However, there was no race in 1951. In 1952, the first of the two years in which the World Drivers' Championship was run to less powerful Formula Two regulations, the race was run to sports car rules instead, and it did not form part of the World Championship.\nNo races were held in 1953 or 1954.\nThe Monaco Grand Prix returned in 1955, again as part of the Formula One World Championship, and this would begin a streak of 64 consecutive years in which the race was held. In the 1955 race, Maurice Trintignant won in Monte Carlo for the first time and Chiron again scored points and at 56 became the oldest driver to compete in a Formula One Grand Prix. It was not until 1957, when Fangio won again, that the Grand Prix saw a double winner. Between 1954 and 1961 Fangio's former Mercedes colleague, Stirling Moss, went one better, as did Trintignant, who won the race again in 1958 driving a Cooper. The 1961 race saw Moss fend off three works Ferrari 156s in a year-old privateer Rob Walker Racing Team Lotus 18, to take his third Monaco victory.\nGraham Hill's era.\nBritain's Graham Hill won the race five times in the 1960s and became known as \"King of Monaco\" and \"Mr. Monaco\". He first won in 1963, and then won the next two years. In the 1965 race he took pole position and led from the start, but went up an escape road on lap\u00a025 to avoid hitting a slow backmarker. Re-joining in fifth place, Hill set several new lap records on the way to winning. The race was also notable for Jim Clark's absence (he was doing the Indianapolis 500), and for Paul Hawkins's Lotus ending up in the harbour. Hill's teammate, Briton Jackie Stewart, won in 1966 and New Zealander Denny Hulme won in 1967, but Hill won the next two years, the 1969 event being his final Formula One championship victory, by which time he was a double Formula One world champion.\nTrack alterations, safety, and increasing business interests.\nBy the start of the 1970s, efforts by Jackie Stewart saw several Formula One events cancelled because of safety concerns. For the 1969 event, Armco barriers were placed at specific points for the first time in the circuit's history. Before that, the circuit's conditions were (aside from the removal of people's production cars parked on the side of the road) virtually identical to everyday road use. If a driver went off, he had a chance to crash into whatever was next to the track (buildings, trees, lamp posts, glass windows, and even a train station), and in Alberto Ascari's and Paul Hawkins's cases, the harbour water, because the concrete road the course used had no Armco to protect the drivers from going off the track and into the Mediterranean. The circuit gained more Armco in specific points for the next two races, and by 1972, the circuit was almost completely Armco-lined. For the first time in its history, the Monaco circuit was altered in 1972 as the pits were moved next to the waterfront straight between the chicane and Tabac and the chicane was moved further forward right before Tabac becoming the junction point between the pits and the course. The course was changed again for the 1973 race. The Rainier III Nautical Stadium was constructed where the straight that went behind the pits was and the circuit introduced a double chicane that went around the new swimming pool (this chicane complex is known today as \"Swimming Pool\"). This created space for a whole new pit facility and in 1976 the course was altered yet again; the Sainte Devote corner was made slower and a chicane was placed right before the pit straight.\nBy the early 1970s, as Brabham team owner Bernie Ecclestone started to marshal the collective bargaining power of the Formula One Constructors Association (FOCA), Monaco was prestigious enough to become an early bone of contention. Historically the number of cars permitted in a race was decided by the race organiser, in this case the ACM, which had always set a low number of around 16. In 1972 Ecclestone started to negotiate deals which relied on FOCA guaranteeing at least 18 entrants for every race. A stand-off over this issue left the 1972 race in jeopardy until the ACM gave in and agreed that 26 cars could participate\u00a0\u2013 the same number permitted at most other circuits. Two years later, in 1974, the ACM got the numbers back down to 18.\nBecause of its tight confines, slow average speeds and punishing nature, Monaco has often thrown up unexpected results. In the 1982 race Ren\u00e9 Arnoux led the first 15\u00a0laps, before retiring. Alain Prost then led until four laps from the end, when he spun off on the wet track, hit the barriers and lost a wheel, giving Riccardo Patrese the lead. Patrese himself spun with only a lap and a half to go, letting Didier Pironi through to the front, followed by Andrea de Cesaris. On the last lap, Pironi ran out of fuel in the tunnel, but De Cesaris also ran out of fuel before he could overtake. In the meantime, Patrese had bump-started his car and went through to score his first Grand Prix win.\nIn 1983 the ACM became entangled in the disagreements between F\u00e9d\u00e9ration Internationale du Sport Automobile (FISA) and FOCA. The ACM, with the agreement of Bernie Ecclestone, negotiated an individual television rights deal with ABC in the United States. This broke an agreement enforced by FISA for a single central negotiation of television rights. Jean-Marie Balestre, president of FISA, announced that the Monaco Grand Prix would not form part of the Formula One world championship in 1985. The ACM fought their case in the French courts. They won the case and the race was eventually reinstated.\nProst/Senna era.\nFor the decade from 1984 to 1993 the race was won by only two drivers, arguably the two best drivers in Formula One at the time \u2013 Frenchman Alain Prost and Brazilian Ayrton Senna. Prost, already a winner of the support race for Formula Three cars in 1979, took his first Monaco win at the 1984 race. The race started 45\u00a0minutes late after heavy rain. Prost led briefly before Nigel Mansell overtook him on lap 11. Mansell crashed out five laps later, letting Prost back into the lead. On lap\u00a027, Prost led from Ayrton Senna's Toleman and Stefan Bellof's Tyrrell. Senna was catching Prost and Bellof was catching both of them in the only naturally aspirated car in the race. However, on lap 31, the race was controversially stopped with conditions deemed to be undriveable. Later, FISA fined the clerk of the course, Jacky Ickx, $6,000 and suspended his licence for not consulting the stewards before stopping the race. The drivers received only half of the points that would usually be awarded, as the race had been stopped before two-thirds of the intended race distance had been completed.\nProst won 1985 after polesitter Senna retired with a blown Renault engine in his Lotus after over-revving it at the start, and Michele Alboreto in the Ferrari retook the lead twice, but he went off the track at Sainte-Devote, where Brazilian Nelson Piquet and Italian Riccardo Patrese had a huge accident only a few laps previously and oil and debris littered the track. Prost passed Alboreto, who retook the Frenchman, and then he punctured a tyre after running over bodywork debris from the Piquet/Patrese accident, which dropped him to 4th. He was able to pass his Roman countrymen Andrea De Cesaris and Elio de Angelis, but finished 2nd behind Prost. The French Prost dominated 1986 after starting from pole position, a race where the Nouvelle Chicane had been changed on the grounds of safety.\nSenna holds the record for the most victories in Monaco, with six, including five consecutive wins between 1989 and 1993, as well as eight podium finishes in ten starts. His 1987 win was the first time a car with an active suspension had won a Grand Prix. He won this race after Briton Nigel Mansell in a Williams-Honda went out with a broken exhaust. His win was very popular with the people of Monaco, and when he was arrested on the Monday following the race, for riding a motorcycle without wearing a helmet, he was released by the officers after they realised who he was. Senna dominated 1988, and was able to get ahead of his teammate Prost while the Frenchman was held up for most of the race by Austrian Gerhard Berger in a Ferrari. By the time Prost got past Berger, he pushed as hard as he could and set a lap some 6 seconds faster than Senna's; Senna then set 2 fastest laps, and while pushing as hard as possible, he touched the barrier at the Portier corner and crashed into the Armco separating the road from the Mediterranean. Senna was so upset that he went back to his Monaco flat and was not heard from until the evening. Prost went on to win for the fourth time.\nSenna dominated 1989 while Prost was stuck behind backmarker Rene Arnoux and others; the Brazilian also dominated 1990 and 1991. At the 1992 event Nigel Mansell, who had won all five races held to that point in the season, took pole and dominated the race in his Williams FW14B-Renault. However, with seven laps remaining, Mansell suffered a loose wheel nut and was forced into the pits, emerging behind Ayrton Senna's McLaren-Honda, who was on worn tyres. Mansell, on fresh tyres, set a lap record almost two seconds quicker than Senna's and closed from 5.2 to 1.9 seconds in only two laps. The pair duelled around Monaco for the final four laps but Mansell could find no way past, finishing just two-tenths of a second behind the Brazilian. It was Senna's fifth win at Monaco, equalling Graham Hill's record. Senna had a poor start to the 1993 event, crashing in practice and qualifying 3rd behind pole-sitter Prost and the rising German star Michael Schumacher. Both of them beat Senna to the first corner, but Prost had to serve a time penalty for jumping the start and Schumacher retired after suspension problems, so Senna took his sixth win to break Graham Hill's record for most wins at the Monaco Grand Prix. Runner-up Damon Hill commented, \"If my father was around now, he would be the first to congratulate Ayrton.\"\nModern times.\nThe 1994 race was an emotional and tragic affair. It came two weeks after the race at Imola in which Austrian Roland Ratzenberger and Ayrton Senna both died from massive head injuries from on-track accidents on successive days. During the Monaco event, Austrian Karl Wendlinger had an accident in his Sauber in the tunnel; he went into a coma and was to miss the rest of the season. The German Michael Schumacher won the 1994 Monaco event. The 1996 race saw Michael Schumacher take pole position before crashing out on the first lap after being overtaken by Damon Hill. Hill led the first 40\u00a0laps before his engine expired in the tunnel. Jean Alesi took the lead but suffered suspension failure 20\u00a0laps later. Olivier Panis, who started in 14th place, moved into the lead and stayed there until the end of the race, being pushed all the way by David Coulthard. It was Panis's only win, and the last for his Ligier team. Only three cars crossed the finish line, but seven were classified.\nSeven-time world champion Schumacher would eventually win the race five times, matching Graham Hill's record. In his appearance at the 2006 event, he attracted criticism when, while provisionally holding pole position and with the qualifying session drawing to a close, he stopped his car at the Rascasse hairpin, blocking the track and obliging competitors to slow down. Although Schumacher claimed it was the unintentional result of a genuine car failure, the FIA disagreed and he was sent to the back of the grid.\nIn July 2010, Bernie Ecclestone announced that a 10-year deal had been reached with the race organisers, keeping the race on the calendar until at least 2020.\nDue to the COVID-19 pandemic, the FIA announced the 2020 Monaco Grand Prix's postponement, along with the two other races scheduled for May 2020, to help prevent the spread of the virus.\nHowever, later the same day the Automobile Club de Monaco confirmed that the Grand Prix was instead cancelled, making 2020 the first time the Grand Prix was not run since 1954. It is scheduled to return in 2021, on 23 May.\nCircuit.\nThe Circuit de Monaco consists of the city streets of Monte Carlo and La Condamine, which includes the famous harbour. It is unique in having been held on the same circuit every time it has been run over such a long period\u00a0\u2013 only the Italian Grand Prix, which has been held at Autodromo Nazionale Monza during every Formula One regulated year except 1980, has a similarly lengthy and close relationship with a single circuit.\nThe race circuit has many elevation changes, tight corners, and a narrow course that makes it one of the most demanding tracks in Formula One racing. , two drivers have crashed and ended up in the harbour, the most famous being Alberto Ascari in 1955. Despite the fact that the course has had minor changes several times during its history, it is still considered the ultimate test of driving skills in Formula One, and if it were not already an existing Grand Prix, it would not be permitted to be added to the schedule for safety reasons. Even in 1929, 'La Vie Automobile' magazine offered the opinion that \"Any respectable traffic system would have covered the track with \u00abDanger\u00bb sign posts left, right and centre\".\nTriple Formula One champion Nelson Piquet was fond of saying that racing at Monaco was \"like trying to cycle round your living room\", but added that \"a win here was worth two anywhere else\".\nNotably, the course includes a tunnel. The contrast of daylight and gloom when entering/exiting the tunnel presents \"challenges not faced elsewhere\", as the drivers have to \"adjust their vision as they emerge from the tunnel at the fastest point of the track and brake for the chicane in the daylight.\".\nThe fastest-ever qualifying lap was set by Lewis Hamilton in qualifying (Q3) for the 2019 Monaco Grand Prix, at a time of 1m 10.166s.\nViewing areas.\nDuring the Grand Prix weekend, spectators crowd around the Monaco Circuit. There are a number of temporary grandstands built around the circuit, mostly around the harbour area. The rich and famous spectators often arrive on their boats and the yachts through the harbour. Balconies around Monaco become viewing areas for the race as well. Many hotels and residents cash in on the bird's eye views of the race.\nOrganization.\nThe Monaco Grand Prix is organised each year by the \"Automobile Club de Monaco\" which also runs the Monte Carlo Rally and the Junior Monaco Kart Cup.\nThe Monaco Grand Prix differs in several ways from other Grands Prix. The practice session for the race is held on the Thursday preceding the race instead of Friday. This allows the streets to be opened to the public again on Friday. Until the late 1990s the race started at 3:30\u00a0p.m. local time\u00a0\u2013 an hour and a half later than other European Formula One races. In recent years the race has fallen in line with the other Formula One races for the convenience of television viewers. Also, earlier the event was traditionally held on the week of Ascension Day. It is now always held on the last weekend in May. For many years, the numbers of cars admitted to Grands Prix was at the discretion of the race organisers\u00a0\u2013 Monaco had the smallest grids, ostensibly because of its narrow and twisting track. Only 18 cars were permitted to enter the 1975 Monaco Grand Prix, compared to 23 to 26 cars at all other rounds that year.\nThe erecting of the circuit takes six weeks, and the removal after the race takes three weeks. There was no podium as such at the race, until 2017. Instead, a section of the track was closed after the race to act as parc ferm\u00e9, a place where the cars are held for official inspection. The first three drivers in the race left their cars there and walked directly to the royal box where the 'podium' ceremony was held, which was considered a custom for the race. The trophies were handed out before the national anthems for the winning driver and team are played, as opposed to other Grands Prix where the anthems are played first.\nFame.\nThe Monaco Grand Prix is widely considered to be one of the most important and prestigious automobile races in the world alongside the Indianapolis 500 and the 24 Hours of Le Mans. These three races are considered to form a \"Triple Crown\" of the three most famous motor races in the world. As of 2020, Graham Hill is the only driver to have won the Triple Crown, by winning all three races. The practice session for Monaco overlaps with that for the Indianapolis 500, and the races themselves sometimes clash. As the two races take place on opposite sides of the Atlantic Ocean and form part of different championships, it is difficult for one driver to compete effectively in both during his career. Juan Pablo Montoya and Fernando Alonso are the only active drivers to have won two of the three events.\nIn awarding its first gold medal for motorsport to Prince Rainier III, the F\u00e9d\u00e9ration Internationale de l'Automobile (FIA) characterised the Monaco Grand Prix as contributing \"an exceptional location of glamour and prestige\" to motorsport. The Grand Prix has been run under the patronage of three generations of Monaco's royal family: Louis II, Rainier III and Albert II, all of whom have taken a close interest in the race. A large part of the principality's income comes from tourists attracted by the warm climate and the famous casino, but it is also a tax haven and is home to many millionaires, including several Formula One drivers.\nMonaco has produced four native Formula One drivers - Louis Chiron, Andr\u00e9 Testut, Olivier Beretta, and Charles Leclerc - but its tax status has made it home to many drivers over the years, including Gilles Villeneuve and Ayrton Senna. Of the Formula One contenders, several have property in the principality, including Jenson Button and David Coulthard, who was part owner of a hotel there. Because of the small size of the town and the location of the circuit, drivers whose races end early can usually get back to their apartments in minutes. Ayrton Senna famously retired to his apartment after crashing out of the lead of the 1988 race.\nThe Grand Prix attracts big-name celebrities each year who come to experience the glamour and prestige of the event. Big parties are held in the nightclubs on the Grand Prix weekend, and the Port Hercule fills up with party-goers joining in the celebrations.\nOfficial names.\nThe Monaco Grand Prix has gone by several official names, including the following:\nWinners.\nRepeat winners (drivers).\n\"Drivers in bold are competing in the Formula One championship in the current season.\"\nRepeat winners (constructors).\n\"Teams in bold are competing in the Formula One championship in the current season.\"&lt;br&gt;\n\"A pink background indicates an event which was not part of the Formula One World Championship.\"&lt;br&gt;\n\"A yellow background indicates an event which was part of the pre-war European Championship.\"\nRepeat winners (engine manufacturers).\n\"Manufacturers in bold are competing in the Formula One championship in the current season.\"\n\"A pink background indicates an event which was not part of the Formula One World Championship.\"\n\"A yellow background indicates an event which was part of the pre-war European Championship.\"\nBy year.\n\"A pink background indicates an event which was not part of the Formula One World Championship.\"&lt;br&gt;\n\"A yellow background indicates an event which was part of the pre-war European Championship.\""}
{"id": "10947", "revid": "1526960", "url": "https://en.wikipedia.org/wiki?curid=10947", "title": "Fission", "text": "Fission, a splitting of something into two or more parts, may refer to:"}
{"id": "10948", "revid": "40997804", "url": "https://en.wikipedia.org/wiki?curid=10948", "title": "Fusion", "text": "Fusion, or synthesis, is the process of combining two or more distinct entities into a new whole.\nFusion may also refer to:"}
{"id": "10949", "revid": "7130356", "url": "https://en.wikipedia.org/wiki?curid=10949", "title": "Four color theorem", "text": "In mathematics, the four color theorem, or the four color map theorem, states that, given any separation of a plane into contiguous regions, producing a figure called a \"map\", no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. \"Adjacent\" means that two regions share a common boundary curve segment, not merely a corner where three or more regions meet. It was the first major theorem to be proved using a computer. Initially, this proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand. Since then the proof has gained wide acceptance, although some doubters remain.\nThe four color theorem was proved in 1976 by Kenneth Appel and Wolfgang Haken after many false proofs and counterexamples (unlike the five color theorem, proved in the 1800s, which states that five colors are enough to color a map). To dispel any remaining doubts about the Appel\u2013Haken proof, a simpler proof using the same ideas and still relying on computers was published in 1997 by Robertson, Sanders, Seymour, and Thomas. Additionally, in 2005, the theorem was proved by Georges Gonthier with general-purpose theorem-proving software.\nPrecise formulation of the theorem.\nIn graph-theoretic terms, the theorem states that for loopless planar graph formula_1, the chromatic number of its dual graph is formula_2.\nThe intuitive statement of the four color theorem \u2013 \"given any separation of a plane into contiguous regions, the regions can be colored using at most four colors so that no two adjacent regions have the same color\" \u2013 needs to be interpreted appropriately to be correct.\nFirst, regions are adjacent if they share a boundary segment; two regions that share only isolated boundary points are not considered adjacent. Second, bizarre regions, such as those with finite area but infinitely long perimeter, are not allowed; maps with such regions can require more than four colors. (To be safe, we can restrict to regions whose boundaries consist of finitely many straight line segments. It is allowed that a region entirely surround one or more other regions.) Note that the notion of \"contiguous region\" (technically: connected open subset of the plane) is not the same as that of a \"country\" on regular maps, since countries need not be contiguous (e.g., the Cabinda Province as part of Angola, Nakhchivan as part of Azerbaijan, Kaliningrad as part of Russia, and Alaska as part of the United States are not contiguous). If we required the entire territory of a country to receive the same color, then four colors are not always sufficient. For instance, consider a simplified map:\nIn this map, the two regions labeled \"A\" belong to the same country. If we wanted those regions to receive the same color, then five colors would be required, since the two \"A\" regions together are adjacent to four other regions, each of which is adjacent to all the others. Forcing two separate regions to have the same color can be modelled by adding a 'handle' joining them outside the plane.\nSuch construction makes the problem equivalent to coloring a map on a torus (a surface of genus 1), which requires up to 7 colors for an arbitrary map.\nA similar construction also applies if a single color is used for multiple disjoint areas, as for bodies of water on real maps, or there are more countries with disjoint territories. In such cases more colors might be required with a growing genus of a resulting surface. (See the section Generalizations below.)\nA simpler statement of the theorem uses graph theory. The set of regions of a map can be represented more abstractly as an undirected graph that has a vertex for each region and an edge for every pair of regions that share a boundary segment. This graph is planar: it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves without crossings that lead from one region's vertex, across a shared boundary segment, to an adjacent region's vertex. Conversely any planar graph can be formed from a map in this way. In graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be colored with at most four colors so that no two adjacent vertices receive the same color, or for short:\nHistory.\nEarly proof attempts.\nAs far as is known, the conjecture was first proposed on October 23, 1852, when Francis Guthrie, while trying to color the map of counties of England, noticed that only four different colors were needed. At the time, Guthrie's brother, Frederick, was a student of Augustus De Morgan (the former advisor of Francis) at University College London. Francis inquired with Frederick regarding it, who then took it to De Morgan (Francis Guthrie graduated later in 1852, and later became a professor of mathematics in South Africa). According to De Morgan:\n\"A student of mine [Guthrie] asked me to day to give him a reason for a fact which I did not know was a fact\u2014and do not yet. He says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary \"line\" are differently colored\u2014four colors may be wanted but not more\u2014the following is his case in which four colors \"are\" wanted. Query cannot a necessity for five or more be invented\u2026\" \n\"F.G.\", perhaps one of the two Guthries, published the question in \"The Athenaeum\" in 1854, and De Morgan posed the question again in the same magazine in 1860. Another early published reference by in turn credits the conjecture to De Morgan.\nThere were several early failed attempts at proving the theorem. De Morgan believed that it followed from a simple fact about four regions, though he didn't believe that fact could be derived from more elementary facts.\nThis arises in the following way. We never need four colors in a neighborhood unless there be four counties, each of which has boundary lines in common with each of the other three. Such a thing cannot happen with four areas unless one or more of them be inclosed by the rest; and the color used for the inclosed county is thus set free to go on with. Now this principle, that four areas cannot each have common boundary with all the other three without inclosure, is not, we fully believe, capable of demonstration upon anything more evident and more elementary; it must stand as a postulate.\nOne alleged proof was given by Alfred Kempe in 1879, which was widely acclaimed; another was given by Peter Guthrie Tait in 1880. It was not until 1890 that Kempe's proof was shown incorrect by Percy Heawood, and in 1891, Tait's proof was shown incorrect by Julius Petersen\u2014each false proof stood unchallenged for 11 years.\nIn 1890, in addition to exposing the flaw in Kempe's proof, Heawood proved the five color theorem and generalized the four color conjecture to surfaces of arbitrary genus.\nTait, in 1880, showed that the four color theorem is equivalent to the statement that a certain type of graph (called a snark in modern terminology) must be non-planar.\nIn 1943, Hugo Hadwiger formulated the Hadwiger conjecture, a far-reaching generalization of the four-color problem that still remains unsolved.\nProof by computer.\nDuring the 1960s and 1970s, German mathematician Heinrich Heesch developed methods of using computers to search for a proof. Notably he was the first to use discharging for proving the theorem, which turned out to be important in the unavoidability portion of the subsequent Appel\u2013Haken proof. He also expanded on the concept of reducibility and, along with Ken Durre, developed a computer test for it. Unfortunately, at this critical juncture, he was unable to procure the necessary supercomputer time to continue his work.\nOthers took up his methods, including his computer-assisted approach. While other teams of mathematicians were racing to complete proofs, Kenneth Appel and Wolfgang Haken at the University of Illinois announced, on June 21, 1976, that they had proved the theorem. They were assisted in some algorithmic work by John A. Koch.\nIf the four-color conjecture were false, there would be at least one map with the smallest possible number of regions that requires five colors. The proof showed that such a minimal counterexample cannot exist, through the use of two technical concepts:\nUsing mathematical rules and procedures based on properties of reducible configurations, Appel and Haken found an unavoidable set of reducible configurations, thus proving that a minimal counterexample to the four-color conjecture could not exist. Their proof reduced the infinitude of possible maps to 1,834 reducible configurations (later reduced to 1,482) which had to be checked one by one by computer and took over a thousand hours. This reducibility part of the work was independently double checked with different programs and computers. However, the unavoidability part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand with the assistance of Haken's daughter Dorothea Blostein .\nAppel and Haken's announcement was widely reported by the news media around the world, and the math department at the University of Illinois used a postmark stating \"Four colors suffice.\" At the same time the unusual nature of the proof\u2014it was the first major theorem to be proved with extensive computer assistance\u2014and the complexity of the human-verifiable portion aroused considerable controversy .\nIn the early 1980s, rumors spread of a flaw in the Appel\u2013Haken proof. Ulrich Schmidt at RWTH Aachen had examined Appel and Haken's proof for his master's thesis that was published in 1981 . He had checked about 40% of the unavoidability portion and found a significant error in the discharging procedure . In 1986, Appel and Haken were asked by the editor of \"Mathematical Intelligencer\" to write an article addressing the rumors of flaws in their proof. They responded that the rumors were due to a \"misinterpretation of [Schmidt's] results\" and obliged with a detailed article . Their magnum opus, \"Every Planar Map is Four-Colorable\", a book claiming a complete and detailed proof (with a microfiche supplement of over 400 pages), appeared in 1989; it explained and corrected the error discovered by Schmidt as well as several further errors found by others .\nSimplification and verification.\nSince the proving of the theorem, efficient algorithms have been found for 4-coloring maps requiring only O(\"n\"2) time, where \"n\" is the number of vertices. In 1996, Neil Robertson, Daniel P. Sanders, Paul Seymour, and Robin Thomas created a quadratic-time algorithm, improving on a quartic-time algorithm based on Appel and Haken's proof. This new proof is similar to Appel and Haken's but more efficient because it reduces the complexity of the problem and requires checking only 633 reducible configurations. Both the unavoidability and reducibility parts of this new proof must be executed by computer and are impractical to check by hand. In 2001, the same authors announced an alternative proof, by proving the snark conjecture. This proof remains unpublished, however.\nIn 2005, Benjamin Werner and Georges Gonthier formalized a proof of the theorem inside the Coq proof assistant. This removed the need to trust the various computer programs used to verify particular cases; it is only necessary to trust the Coq kernel.\nSummary of proof ideas.\nThe following discussion is a summary based on the introduction to \"Every Planar Map is Four Colorable\" . Although flawed, Kempe's original purported proof of the four color theorem provided some of the basic tools later used to prove it. The explanation here is reworded in terms of the modern graph theory formulation above.\nKempe's argument goes as follows. First, if planar regions separated by the graph are not \"triangulated\", i.e. do not have exactly three edges in their boundaries, we can add edges without introducing new vertices in order to make every region triangular, including the unbounded outer region. If this triangulated graph is colorable using four colors or fewer, so is the original graph since the same coloring is valid if edges are removed. So it suffices to prove the four color theorem for triangulated graphs to prove it for all planar graphs, and without loss of generality we assume the graph is triangulated.\nSuppose \"v\", \"e\", and \"f\" are the number of vertices, edges, and regions (faces). Since each region is triangular and each edge is shared by two regions, we have that 2\"e\" = 3\"f\". This together with Euler's formula, \"v\" \u2212 \"e\" + \"f\" = 2, can be used to show that 6\"v\" \u2212 2\"e\" = 12. Now, the \"degree\" of a vertex is the number of edges abutting it. If \"v\"\"n\" is the number of vertices of degree \"n\" and \"D\" is the maximum degree of any vertex,\nBut since 12 &gt; 0 and 6 \u2212 \"i\" \u2264 0 for all \"i\" \u2265 6, this demonstrates that there is at least one vertex of degree 5 or less.\nIf there is a graph requiring 5 colors, then there is a \"minimal\" such graph, where removing any vertex makes it four-colorable. Call this graph \"G\". Then \"G\" cannot have a vertex of degree 3 or less, because if \"d\"(\"v\") \u2264 3, we can remove \"v\" from \"G\", four-color the smaller graph, then add back \"v\" and extend the four-coloring to it by choosing a color different from its neighbors.\nKempe also showed correctly that \"G\" can have no vertex of degree 4. As before we remove the vertex \"v\" and four-color the remaining vertices. If all four neighbors of \"v\" are different colors, say red, green, blue, and yellow in clockwise order, we look for an alternating path of vertices colored red and blue joining the red and blue neighbors. Such a path is called a Kempe chain. There may be a Kempe chain joining the red and blue neighbors, and there may be a Kempe chain joining the green and yellow neighbors, but not both, since these two paths would necessarily intersect, and the vertex where they intersect cannot be colored. Suppose it is the red and blue neighbors that are not chained together. Explore all vertices attached to the red neighbor by red-blue alternating paths, and then reverse the colors red and blue on all these vertices. The result is still a valid four-coloring, and \"v\" can now be added back and colored red.\nThis leaves only the case where \"G\" has a vertex of degree 5; but Kempe's argument was flawed for this case. Heawood noticed Kempe's mistake and also observed that if one was satisfied with proving only five colors are needed, one could run through the above argument (changing only that the minimal counterexample requires 6 colors) and use Kempe chains in the degree 5 situation to prove the five color theorem.\nIn any case, to deal with this degree 5 vertex case requires a more complicated notion than removing a vertex. Rather the form of the argument is generalized to considering \"configurations\", which are connected subgraphs of \"G\" with the degree of each vertex (in G) specified. For example, the case described in degree 4 vertex situation is the configuration consisting of a single vertex labelled as having degree 4 in \"G\". As above, it suffices to demonstrate that if the configuration is removed and the remaining graph four-colored, then the coloring can be modified in such a way that when the configuration is re-added, the four-coloring can be extended to it as well. A configuration for which this is possible is called a \"reducible configuration\". If at least one of a set of configurations must occur somewhere in G, that set is called \"unavoidable\". The argument above began by giving an unavoidable set of five configurations (a single vertex with degree 1, a single vertex with degree 2, ..., a single vertex with degree 5) and then proceeded to show that the first 4 are reducible; to exhibit an unavoidable set of configurations where every configuration in the set is reducible would prove the theorem.\nBecause \"G\" is triangular, the degree of each vertex in a configuration is known, and all edges internal to the configuration are known, the number of vertices in \"G\" adjacent to a given configuration is fixed, and they are joined in a cycle. These vertices form the \"ring\" of the configuration; a configuration with \"k\" vertices in its ring is a \"k\"-ring configuration, and the configuration together with its ring is called the \"ringed configuration\". As in the simple cases above, one may enumerate all distinct four-colorings of the ring; any coloring that can be extended without modification to a coloring of the configuration is called \"initially good\". For example, the single-vertex configuration above with 3 or less neighbors were initially good. In general, the surrounding graph must be systematically recolored to turn the ring's coloring into a good one, as was done in the case above where there were 4 neighbors; for a general configuration with a larger ring, this requires more complex techniques. Because of the large number of distinct four-colorings of the ring, this is the primary step requiring computer assistance.\nFinally, it remains to identify an unavoidable set of configurations amenable to reduction by this procedure. The primary method used to discover such a set is the method of discharging. The intuitive idea underlying discharging is to consider the planar graph as an electrical network. Initially positive and negative \"electrical charge\" is distributed amongst the vertices so that the total is positive.\nRecall the formula above:\nEach vertex is assigned an initial charge of 6-deg(\"v\"). Then one \"flows\" the charge by systematically redistributing the charge from a vertex to its neighboring vertices according to a set of rules, the \"discharging procedure\". Since charge is preserved, some vertices still have positive charge. The rules restrict the possibilities for configurations of positively charged vertices, so enumerating all such possible configurations gives an unavoidable set.\nAs long as some member of the unavoidable set is not reducible, the discharging procedure is modified to eliminate it (while introducing other configurations). Appel and Haken's final discharging procedure was extremely complex and, together with a description of the resulting unavoidable configuration set, filled a 400-page volume, but the configurations it generated could be checked mechanically to be reducible. Verifying the volume describing the unavoidable configuration set itself was done by peer review over a period of several years.\nA technical detail not discussed here but required to complete the proof is \"immersion reducibility\".\nFalse disproofs.\nThe four color theorem has been notorious for attracting a large number of false proofs and disproofs in its long history. At first, \"The New York Times\" refused as a matter of policy to report on the Appel\u2013Haken proof, fearing that the proof would be shown false like the ones before it . Some alleged proofs, like Kempe's and Tait's mentioned above, stood under public scrutiny for over a decade before they were refuted. But many more, authored by amateurs, were never published at all.\nGenerally, the simplest, though invalid, counterexamples attempt to create one region which touches all other regions. This forces the remaining regions to be colored with only three colors. Because the four color theorem is true, this is always possible; however, because the person drawing the map is focused on the one large region, they fail to notice that the remaining regions can in fact be colored with three colors.\nThis trick can be generalized: there are many maps where if the colors of some regions are selected beforehand, it becomes impossible to color the remaining regions without exceeding four colors. A casual verifier of the counterexample may not think to change the colors of these regions, so that the counterexample will appear as though it is valid.\nPerhaps one effect underlying this common misconception is the fact that the color restriction is not transitive: a region only has to be colored differently from regions it touches directly, not regions touching regions that it touches. If this were the restriction, planar graphs would require arbitrarily large numbers of colors.\nOther false disproofs violate the assumptions of the theorem, such as using a region that consists of multiple disconnected parts, or disallowing regions of the same color from touching at a point.\nThree-coloring.\nWhile every planar map can be colored with four colors, it is NP-complete in complexity to decide whether an arbitrary planar map can be colored with just three colors.\nGeneralizations.\nThe four-color theorem applies not only to finite planar graphs, but also to infinite graphs that can be drawn without crossings in the plane, and even more generally to infinite graphs (possibly with an uncountable number of vertices) for which every finite subgraph is planar. To prove this, one can combine a proof of the theorem for finite planar graphs with the De Bruijn\u2013Erd\u0151s theorem stating that, if every finite subgraph of an infinite graph is \"k\"-colorable, then the whole graph is also \"k\"-colorable . This can also be seen as an immediate consequence of Kurt G\u00f6del's compactness theorem for first-order logic, simply by expressing the colorability of an infinite graph with a set of logical formulae.\nOne can also consider the coloring problem on surfaces other than the plane (Weisstein). The problem on the sphere or cylinder is equivalent to that on the plane. For closed (orientable or non-orientable) surfaces with positive genus, the maximum number \"p\" of colors needed depends on the surface's Euler characteristic \u03c7 according to the formula\nwhere the outermost brackets denote the floor function.\nAlternatively, for an orientable surface the formula can be given in terms of the genus of a surface, \"g\":\nThis formula, the Heawood conjecture, was conjectured by P. J. Heawood in 1890 and proved by Gerhard Ringel and J. W. T. Youngs in 1968. The only exception to the formula is the Klein bottle, which has Euler characteristic 0 (hence the formula gives p = 7) and requires only 6 colors, as shown by P. Franklin in 1934 (Weisstein).\nFor example, the torus has Euler characteristic \u03c7 = 0 (and genus \"g\" = 1) and thus \"p\" = 7, so no more than 7 colors are required to color any map on a torus. This upper bound of 7 is sharp: certain toroidal polyhedra such as the Szilassi polyhedron require seven colors.\nA M\u00f6bius strip requires six colors as do 1-planar graphs (graphs drawn with at most one simple crossing per edge) . If both the vertices and the faces of a planar graph are colored, in such a way that no two adjacent vertices, faces, or vertex-face pair have the same color, then again at most six colors are needed .\nThere is no obvious extension of the coloring result to three-dimensional solid regions. By using a set of \"n\" flexible rods, one can arrange that every rod touches every other rod. The set would then require \"n\" colors, or \"n\"+1 if you consider the empty space that also touches every rod. The number \"n\" can be taken to be any integer, as large as desired. Such examples were known to Fredrick Guthrie in 1880 . Even for axis-parallel cuboids (considered to be adjacent when two cuboids share a two-dimensional boundary area) an unbounded number of colors may be necessary (; ).\nRelation to other areas of mathematics.\nDror Bar-Natan gave a statement concerning Lie algebras and Vassiliev invariants which is equivalent to the four color theorem.\nUse outside of mathematics.\nDespite the motivation from coloring political maps of countries, the theorem is not of particular interest to cartographers. According to an article by the math historian Kenneth May, \"Maps utilizing only four colors are rare, and those that do usually require only three. Books on cartography and the history of mapmaking do not mention the four-color property\" . The theorem also does not guarantee the usual cartographic requirement that non-contiguous regions of the same country (such as the exclave Kaliningrad and the rest of Russia) be colored identically."}
{"id": "10951", "revid": "39011480", "url": "https://en.wikipedia.org/wiki?curid=10951", "title": "Fahrenheit 451", "text": "Fahrenheit 451 is a 1953 dystopian novel by American writer Ray Bradbury. Often regarded as one of his best works, the novel presents a future American society where books are outlawed and \"firemen\" burn any that are found. The book's tagline explains the title as \"'the temperature at which book paper catches fire, and burns\": the autoignition temperature of paper. The lead character, Guy Montag, is a fireman who becomes disillusioned with his role of censoring literature and destroying knowledge, eventually quitting his job and committing himself to the preservation of literary and cultural writings.\nThe novel has been the subject of interpretations focusing on the historical role of book burning in suppressing dissenting ideas for change. In a 1956 radio interview, Bradbury said that he wrote \"Fahrenheit 451\" because of his concerns at the time (during the McCarthy era) about the threat of book burning in the United States. In later years, he described the book as a commentary on how mass media reduces interest in reading literature.\nIn 1954, \"Fahrenheit 451\" won the American Academy of Arts and Letters Award in Literature and the Commonwealth Club of California Gold Medal. It later won the Prometheus \"Hall of Fame\" Award in 1984 and a \"Retro\" Hugo Award, one of a limited number of Best Novel Retro Hugos ever given, in 2004. Bradbury was honored with a Spoken Word Grammy nomination for his 1976 audiobook version.\nAdaptations of the novel include Fran\u00e7ois Truffaut's Fahrenheit 451 (1966 film), Fahrenheit 451 (2018 film) and a 1982 BBC Radio dramatization. Bradbury published a stage play version in 1979 and helped develop a 1984 interactive fiction computer game titled \"Fahrenheit 451\", as well as a collection of his short stories titled \"A Pleasure to Burn\". HBO released a television film based on the novel and written and directed by Ramin Bahrani in 2018.\nPlot summary.\n\"Fahrenheit 451\" is set in an unspecified city (likely in the American Midwest) in the year 2049 (according to Ray Bradbury's Coda), though it is written as if set in a distant future. The earliest editions make clear that it takes place no earlier than the year 1960.\nThe novel is divided into three parts: \"The Hearth and the Salamander,\" \"The Sieve and the Sand,\" and \"Burning Bright.\"\n\"The Hearth and the Salamander\".\nGuy Montag is a fireman employed to burn houses containing outlawed books. He is married but has no children. One fall night while returning from work, he meets his new neighbour, a teenage girl named Clarisse McClellan, whose free-thinking ideals and liberating spirit cause him to question his life and his own perceived happiness. Montag returns home to find that his wife Mildred has overdosed on sleeping pills, and he calls for medical attention. Two uncaring EMTs pump Mildred's stomach, drain her poisoned blood, and fill her with new blood. After the EMTs leave to rescue another overdose victim, Montag goes outside and overhears Clarisse and her family talking about the way life is in this hedonistic, illiterate society. Montag's mind is bombarded with Clarisse's subversive thoughts and the memory of his wife's near-death. Over the next few days, Clarisse faithfully meets Montag each night as he walks home. She tells him about how her simple pleasures and interests make her an outcast among her peers and how she is forced to go to therapy for her behavior and thoughts. Montag looks forward to these meetings, and just as he begins to expect them, Clarisse goes missing. He senses something is wrong.\nIn the following days, while at work with the other firemen ransacking the book-filled house of an old woman and drenching it in kerosene before the inevitable burning, Montag steals a book before any of his coworkers notice. The woman refuses to leave her house and her books, choosing instead to light a match and burn herself alive. Jarred by the woman's suicide, Montag returns home and hides the stolen book under his pillow. Later, Montag wakes Mildred from her sleep and asks her if she has seen or heard anything about Clarisse McClellan. She reveals that Clarisse's family moved away after Clarisse was hit by a speeding car and died four days ago. Dismayed by her failure to mention this earlier, Montag uneasily tries to fall asleep. Outside he suspects the presence of \"The Mechanical Hound\", an eight-legged robotic dog-like creature that resides in the firehouse and aids the firemen in hunting book hoarders.\nMontag awakens ill the next morning. Mildred tries to care for her husband but finds herself more involved in the \"parlor wall\" entertainment in the living room \u2013 large televisions filling the walls. Montag suggests that maybe he should take a break from being a fireman after what happened last night, and Mildred panics over the thought of losing the house and her parlor wall \"family\". Captain Beatty, Montag's fire chief, personally visits Montag to see how he is doing. Sensing his concerns, Beatty recounts the history of how books lost their value and how the firemen were adapted for their current role: over the course of several decades, people began to embrace new media (in this case, film and television), sports, and an ever-quickening pace of life. Books were ruthlessly abridged or degraded to accommodate short attention spans. At the same time, advances in technology resulted in nearly all buildings being made out of fireproof materials, and the traditional role of firemen in preventing fires was no longer necessary. The government instead turned the firemen into officers of society's peace of mind: instead of putting out fires they became responsible for starting them, specifically for the purpose of burning books, which were condemned as sources of confusing and depressing thoughts that only complicated people's lives. After an awkward encounter between Mildred and Montag over the book hidden under Montag's pillow, Beatty becomes suspicious and casually adds a passing threat as he leaves, telling Montag that if a fireman had a book, he would be asked to burn it within the next 24 hours. If he refused, the other firemen would come and burn it for him. The encounter leaves Montag shaken.\nAfter Beatty leaves, Montag reveals to Mildred that, over the last year, he has accumulated a stash of books that he has kept hidden in the air-conditioning duct in their ceiling. In a panic, Mildred grabs a book and rushes to throw it in the kitchen incinerator. Montag subdues her and tells her that the two of them are going to read the books to see if they have value. If they do not, he promises the books will be burned and all will return to normal.\n\"The Sieve and the Sand\".\nMontag and Mildred discuss the stolen books, and Mildred refuses to go along with it, questioning why she or anyone else should care about books. Montag goes on a rant about Mildred's suicide attempt, Clarisse's disappearance and death, the old woman who burned herself, and the imminent threat of war that goes ignored by the masses. He suggests that perhaps the books of the past have messages that can save society from its own destruction. The conversation is interrupted by a call from Mildred's friend, Mrs. Bowles, and they set up a date to watch the \"parlor walls\" that night at Mildred's house.\nMontag concedes that Mildred is a lost cause and he will need help to understand the books. He remembers an old man named Faber, an English professor before books were banned, whom he once met in a park. Montag makes a subway trip to Faber's home along with a rare copy of the Bible, the book he stole at the woman's house. Once there, Montag forces the scared and reluctant Faber into helping him by methodically ripping pages from the Bible. Faber concedes and gives Montag a homemade ear-piece communicator so he can offer constant guidance.\nAt home, Mildred's friends, Mrs. Bowles and Mrs. Phelps, arrive to watch the \"parlor walls\". Not interested in this insipid entertainment, Montag turns off the walls and tries to engage the women in meaningful conversation, only for them to reveal just how indifferent, ignorant, and callous they truly are. Enraged by their idiocy, Montag leaves momentarily and returns with a book of poetry. This confuses the women and alarms Faber, who is listening remotely. Mildred tries to dismiss Montag's actions as a tradition firemen act out once a year: they find an old book and read it as a way to make fun of how silly the past is. Montag proceeds to recite the poem \"Dover Beach\", causing Mrs. Phelps to cry. At the behest of Faber in the ear-piece, Montag burns the book. Mildred's friends leave in disgust, while Mildred locks herself in the bathroom and attempts to kill herself again by overdosing on sleeping pills.\nMontag hides his books in the backyard before returning to the firehouse late at night, where he finds Beatty playing cards with the other firemen. Montag hands Beatty a book to cover for the one he believes Beatty knows he stole the night before, which is unceremoniously tossed into the trash. Beatty tells Montag that he had a dream in which they fought endlessly by quoting books to each other. Thus Beatty reveals that, despite his disillusionment, he was once an enthusiastic reader. A fire alarm sounds, and Beatty picks up the address from the dispatcher system. They drive recklessly in the fire truck to the destination: Montag's house.\n\"Burning Bright\".\nBeatty orders Montag to destroy his house with a flamethrower, rather than the more powerful \"salamander\" that is usually used by the fire team, and tells him that his wife and her friends reported him after what happened the other night. Montag watches as Mildred walks out of the house, too traumatized about losing her parlor wall family to even acknowledge her husband's existence or the situation going on around her, and catches a taxi. Montag obeys the chief, destroying the home piece by piece, but Beatty discovers Montag's earpiece and plans to hunt down Faber. Montag threatens Beatty with the flamethrower and, after Beatty taunts him, Montag burns Beatty alive and knocks his co-workers unconscious. As Montag escapes the scene, the Mechanical Hound attacks him, managing to inject his leg with a tranquilizer. He destroys the Hound with the flamethrower and limps away. Before he escapes, however, he realizes that Beatty had wanted to die a long time ago and had purposely goaded Montag as well as provided him with a weapon.\nMontag runs through the city streets towards Faber's house. On his way, he crosses a wide road as a speeding car attempts to run him over, but he manages to evade the vehicle, and realizes he almost suffered the same fate as Clarisse. Faber urges him to make his way to the countryside and contact the exiled book-lovers who live there. He mentions he will be leaving on an early bus heading to St. Louis and that he and Montag can rendezvous there later. On Faber's television, they watch news reports of another Mechanical Hound being released to track down and kill Montag, with news helicopters following it to create a public spectacle. After wiping his scent from around the house in hopes of thwarting the Hound, Montag leaves Faber's house. He escapes the manhunt by wading into a river and floating downstream. Montag leaves the river in the countryside, where he meets the exiled drifters, led by a man named Granger. Granger shows Montag the ongoing manhunt on a portable battery TV and predicts that \u201cMontag\u201d will be caught within the next few minutes; as predicted, an innocent man is then caught and killed.\nThe drifters are all former intellectuals. They have each memorized books should the day arrive that society comes to an end and is forced to rebuild itself anew, with the survivors learning to embrace the literature of the past. Granger asks Montag what he has to contribute to the group and Montag finds that he had partially memorized the Book of Ecclesiastes, discovering that the group has a special way of unlocking photographic memory. While learning the philosophy of the exiles, Montag and the group watch helplessly as bombers fly overhead and annihilate the city with nuclear weapons: the imminent war has begun and ended in the same night. While Faber would have left on the early bus, everyone else (including Mildred) is immediately killed. Montag and the group are injured and dirtied, but manage to survive the shockwave.\nThe following morning, Granger teaches Montag and the others about the legendary phoenix and its endless cycle of long life, death in flames, and rebirth. He adds that the phoenix must have some relationship to mankind, which constantly repeats its mistakes, but explains that man has something the phoenix does not: mankind can remember its mistakes and try to never repeat them. Granger then muses that a large factory of mirrors should be built so that people can take a long look at themselves and reflect on their lives. When the meal is over, the exiles return to the city to rebuild society.\nTitle.\nThe title page of the book explains the title as follows: \"Fahrenheit 451\u2014The temperature at which book paper catches fire and burns...\". On inquiring about the temperature at which paper would catch fire, Bradbury had been told that was the autoignition temperature of paper. In various studies, scientists have placed the autoignition temperature at a range of temperatures between , depending on the type of paper.\nHistorical context.\nBradbury's lifelong passion for books began at an early age. After graduating from high school, Bradbury's family could not afford for him to attend college so Bradbury began spending time at the Los Angeles Public Library where he essentially educated himself. As a frequent visitor to his local libraries in the 1920s and 1930s, he recalls being disappointed because they did not stock popular science fiction novels, like those of H. G. Wells, because, at the time, they were not deemed literary enough. Between this and learning about the destruction of the Library of Alexandria, a great impression was made on the young man about the vulnerability of books to censure and destruction. Later, as a teenager, Bradbury was horrified by the Nazi book burnings and later by Joseph Stalin's campaign of political repression, the \"Great Purge\", in which writers and poets, among many others, were arrested and often executed.\nShortly after the atomic bombings of Hiroshima and Nagasaki at the conclusion of World War II, the United States focused its concern on the Soviet atomic bomb project and the expansion of communism. The House Un-American Activities Committee (HUAC), formed in 1938 to investigate American citizens and organizations suspected of having communist ties, held hearings in 1947 to investigate alleged communist influence in Hollywood movie-making. These hearings resulted in the blacklisting of the so-called \"Hollywood Ten\", a group of influential screenwriters and directors. This governmental interference in the affairs of artists and creative types greatly angered Bradbury. Bradbury was bitter and concerned about the workings of his government, and a late 1949 nighttime encounter with an overzealous police officer would inspire Bradbury to write \"The Pedestrian\", a short story which would go on to become \"The Fireman\" and then \"Fahrenheit 451\". The rise of Senator Joseph McCarthy's hearings hostile to accused communists, beginning in 1950, deepened Bradbury's contempt for government overreach.\nThe year HUAC began investigating Hollywood is often considered the beginning of the Cold War, as in March 1947, the Truman Doctrine was announced. By about 1950, the Cold War was in full swing, and the American public's fear of nuclear warfare and communist influence was at a feverish level. The stage was set for Bradbury to write the dramatic nuclear holocaust ending of \"Fahrenheit 451\", exemplifying the type of scenario feared by many Americans of the time.\nBradbury's early life witnessed the Golden Age of Radio, while the transition to the Golden Age of Television began right around the time he started to work on the stories that would eventually lead to \"Fahrenheit 451\". Bradbury saw these forms of media as a threat to the reading of books, indeed as a threat to society, as he believed they could act as a distraction from important affairs. This contempt for mass media and technology would express itself through Mildred and her friends and is an important theme in the book.\nWriting and development.\n\"Fahrenheit 451\" developed out of a series of ideas Bradbury had visited in previously written stories. For many years, he tended to single out \"The Pedestrian\" in interviews and lectures as sort of a proto-\"Fahrenheit 451\". In the Preface of his 2006 anthology \"Match to Flame: The Fictional Paths to Fahrenheit 451\" he states that this is an oversimplification. The full genealogy of \"Fahrenheit 451\" given in \"Match to Flame\" is involved. The following covers the most salient aspects.\nBetween 1947 and 1948, Bradbury wrote the short story \"Bright Phoenix\" (not published until the May 1963 issue of \"The Magazine of Fantasy &amp; Science Fiction\") about a librarian who confronts a book-burning \"Chief Censor\" named Jonathan Barnes.\nIn late 1949, Bradbury was stopped and questioned by a police officer while walking late one night. When asked \"What are you doing?\", Bradbury wisecracked, \"Putting one foot in front of another.\" This incident inspired Bradbury to write the 1951 short story \"The Pedestrian\".\nIn \"The Pedestrian\", Leonard Mead is harassed and detained by the city's remotely operated police cruiser (there's only one) for taking nighttime walks, something that has become extremely rare in this future-based setting: everybody else stays inside and watches television (\"viewing screens\"). Alone and without an alibi, Mead is taken to the \"Psychiatric Center for Research on Regressive Tendencies\" for his peculiar habit. \"Fahrenheit 451\" would later echo this theme of an authoritarian society distracted by broadcast media.\nBradbury expanded the book-burning premise of \"Bright Phoenix\" and the totalitarian future of \"The Pedestrian\" into \"The Fireman\", a novella published in the February 1951 issue of \"Galaxy Science Fiction\". \"The Fireman\" was written in the basement of UCLA's Powell Library on a typewriter that he rented for a fee of ten cents per half hour. The first draft was 25,000 words long and was completed in nine days.\nUrged by a publisher at Ballantine Books to double the length of his story to make a novel, Bradbury returned to the same typing room and expanded his work into \"Fahrenheit 451\", again taking just nine days. The fixup was published by Ballantine in 1953.\nSupplementary material.\nBradbury has supplemented the novel with various front and back matter, including a 1979 coda, a 1982 afterword, a 1993 foreword, and several introductions.\nPublication history.\nThe first U.S. printing was a paperback version from October 1953 by The Ballantine Publishing Group. Shortly after the paperback, a hardback version was released that included a special edition of 200 signed and numbered copies bound in asbestos. These were technically collections because the novel was published with two short stories: \"The Playground\" and \"And the Rock Cried Out\", which have been absent in later printings. A few months later, the novel was serialized in the March, April, and May 1954 issues of nascent \"Playboy\" magazine.\nExpurgation.\nStarting in January 1967, \"Fahrenheit 451\" was subject to expurgation by its publisher, Ballantine Books with the release of the \"Bal-Hi Edition\" aimed at high school students. Among the changes made by the publisher were the censorship of the words \"hell\", \"damn\", and \"abortion\"; the modification of seventy-five passages; and the changing of two episodes.\nIn the one case, a drunk man became a \"sick man\" while cleaning fluff out of a human navel became \"cleaning ears\" in the other. For a while both the censored and uncensored versions were available concurrently but by 1973 Ballantine was publishing only the censored version. This continued until 1979 when it came to Bradbury's attention:\nIn 1979, one of Bradbury's friends showed him an expurgated copy. Bradbury demanded that Ballantine Books withdraw that version and replace it with the original, and in 1980 the original version once again became available. In this reinstated work, in the Author's Afterword, Bradbury relates to the reader that it is not uncommon for a publisher to expurgate an author's work, but he asserts that he himself will not tolerate the practice of manuscript \"mutilation\".\nThe \"Bal-Hi\" editions are now referred to by the publisher as the \"Revised Bal-Hi\" editions.\nNon-print publications.\nAn audiobook version read by Bradbury himself was released in 1976 and received a Spoken Word Grammy nomination. Another audiobook was released in 2005 narrated by Christopher Hurt. The e-book version was released in December 2011.\nReception.\nIn 1954, \"Galaxy Science Fiction\" reviewer Groff Conklin placed the novel \"among the great works of the imagination written in English in the last decade or more.\" The \"Chicago Sunday Tribune\"'s August Derleth described the book as \"a savage and shockingly prophetic view of one possible future way of life\", calling it \"compelling\" and praising Bradbury for his \"brilliant imagination\". Over half a century later, Sam Weller wrote, \"upon its publication, \"Fahrenheit 451\" was hailed as a visionary work of social commentary.\" Today, \"Fahrenheit 451\" is still viewed as an important cautionary tale about conformity and the evils of government censorship.\nWhen the novel was first published, there were those who did not find merit in the tale. Anthony Boucher and J. Francis McComas were less enthusiastic, faulting the book for being \"simply padded, occasionally with startlingly ingenious gimmickry,\u00a0... often with coruscating cascades of verbal brilliance [but] too often merely with words.\" Reviewing the book for \"Astounding Science Fiction\", P. Schuyler Miller characterized the title piece as \"one of Bradbury's bitter, almost hysterical diatribes,\" while praising its \"emotional drive and compelling, nagging detail.\" Similarly, \"The New York Times\" was unimpressed with the novel and further accused Bradbury of developing a \"virulent hatred for many aspects of present-day culture, namely, such monstrosities as radio, TV, most movies, amateur and professional sports, automobiles, and other similar aberrations which he feels debase the bright simplicity of the thinking man's existence.\"\n\"Fahrenheit 451\" was number seven on the list of \"Top Check Outs OF ALL TIME\" by the New York Public Library\nCensorship/banning incidents.\nIn the years since its publication, \"Fahrenheit 451\" has occasionally been banned, censored, or redacted in some schools at the behest of parents or teaching staff either unaware of or indifferent to the inherent irony in such censorship. Notable incidents include:\nThemes.\nDiscussions about \"Fahrenheit 451\" often center on its story foremost as a warning against state-based censorship. Indeed, when Bradbury wrote the novel during the McCarthy era, he was concerned about censorship in the United States. During a radio interview in 1956, Bradbury said:I wrote this book at a time when I was worried about the way things were going in this country four years ago. Too many people were afraid of their shadows; there was a threat of book burning. Many of the books were being taken off the shelves at that time. And of course, things have changed a lot in four years. Things are going back in a very healthy direction. But at the time I wanted to do some sort of story where I could comment on what would happen to a country if we let ourselves go too far in this direction, where then all thinking stops, and the dragon swallows his tail, and we sort of vanish into a limbo and we destroy ourselves by this sort of action.\nAs time went by, Bradbury tended to dismiss censorship as a chief motivating factor for writing the story. Instead he usually claimed that the real messages of \"Fahrenheit 451\" were about the dangers of an illiterate society infatuated with mass media and the threat of minority and special interest groups to books. In the late 1950s, Bradbury recounted:In writing the short novel \"Fahrenheit 451\", I thought I was describing a world that might evolve in four or five decades. But only a few weeks ago, in Beverly Hills one night, a husband and wife passed me, walking their dog. I stood staring after them, absolutely stunned. The woman held in one hand a small cigarette-package-sized radio, its antenna quivering. From this sprang tiny copper wires which ended in a dainty cone plugged into her right ear. There she was, oblivious to man and dog, listening to far winds and whispers and soap-opera cries, sleep-walking, helped up and down curbs by a husband who might just as well not have been there. This was \"not\" fiction.\nThis story echoes Mildred's \"Seashell ear-thimbles\" (i.e., a brand of in-ear headphones) that act as an emotional barrier between her and Montag. In a 2007 interview, Bradbury maintained that people misinterpret his book and that \"Fahrenheit 451\" is really a statement on how mass media like television marginalizes the reading of literature. Regarding minorities, he wrote in his 1979 Coda:There is more than one way to burn a book. And the world is full of people running about with lit matches. Every minority, be it Baptist/Unitarian, Irish/Italian/Octogenarian/Zen Buddhist, Zionist/Seventh-day Adventist, Women's Lib/Republican, Mattachine/Four Square Gospel feels it has the will, the right, the duty to douse the kerosene, light the fuse.\u00a0[...] Fire-Captain Beatty, in my novel \"Fahrenheit 451\", described how the books were burned first by minorities, each ripping a page or a paragraph from this book, then that, until the day came when the books were empty and the minds shut and the libraries closed forever.\u00a0[...] Only six weeks ago, I discovered that, over the years, some cubby-hole editors at Ballantine Books, fearful of contaminating the young, had, bit by bit, censored some seventy-five separate sections from the novel. Students, reading the novel, which, after all, deals with censorship and book-burning in the future, wrote to tell me of this exquisite irony. Judy-Lynn del Rey, one of the new Ballantine editors, is having the entire book reset and republished this summer with all the damns and hells back in place.\nBook-burning censorship, Bradbury would argue, was a side-effect of these two primary factors; this is consistent with Captain Beatty's speech to Montag about the history of the firemen. According to Bradbury, it is the people, not the state, who are the culprit in \"Fahrenheit 451\". Nevertheless, the role of censorship, state-based or otherwise, is still perhaps the most frequent theme explored in the work.\nA variety of other themes in the novel besides censorship have been suggested. Two major themes are resistance to conformity and control of individuals via technology and mass media. Bradbury explores how the government is able to use mass media to influence society and suppress individualism through book burning. The characters Beatty and Faber point out that the American population is to blame. Due to their constant desire for a simplistic, positive image, books must be suppressed. Beatty blames the minority groups, who would take offense to published works that displayed them in an unfavorable light. Faber went further to state that the American population simply stopped reading on their own. He notes that the book burnings themselves became a form of entertainment for the general public.\nIn a 1994 interview, Bradbury stated that \"Fahrenheit 451\" was more relevant during this time than in any other, stating that, \"it works even better because we have political correctness now. Political correctness is the real enemy these days. The black groups want to control our thinking and you can't say certain things. The homosexual groups don\u2019t want you to criticize them. It's thought control and freedom of speech control.\"\nPredictions for the future.\nBradbury described himself as \"a \"preventor\" of futures, not a predictor of them.\" He did not believe that book burning was an inevitable part of the future; he wanted to warn against its development. In a later interview, when asked if he believes that teaching \"Fahrenheit 451\" in schools will prevent his totalitarian vision of the future, Bradbury replied in the negative. Rather, he states that education must be at the kindergarten and first-grade level. If students are unable to read then, they will be unable to read \"Fahrenheit 451\".\nOn account of technology, Sam Weller notes that Bradbury \"predicted everything from flat-panel televisions to earbud headphones and twenty-four-hour banking machines.\"\nAdaptations.\nTelevision.\n\"Playhouse 90\" broadcast \"A Sound of Different Drummers\" on CBS in 1957, written by Robert Alan Aurthur. The play combined plot ideas from \"Fahrenheit 451\" and \"Nineteen Eighty-Four\". Bradbury sued and eventually won on appeal.\nFilm.\nA film adaptation written and directed by Fran\u00e7ois Truffaut and starring Oskar Werner and Julie Christie was released in 1966.\nA new film adaptation directed by Ramin Bahrani and starring Michael B. Jordan, Michael Shannon, Sofia Boutella, and Lilly Singh was released in 2018 for HBO.\nTheater.\nIn the late 1970s Bradbury adapted his book into a play. At least part of it was performed at the Colony Theatre in Los Angeles in 1979, but it was not in print until 1986 and the official world premiere was only in November 1988 by the Fort Wayne, Indiana Civic Theatre. The stage adaptation diverges considerably from the book and seems influenced by Truffaut's movie. For example, fire chief Beatty's character is fleshed out and is the wordiest role in the play. As in the movie, Clarisse does not simply disappear but in the finale meets up with Montag as a book character (she as Robert Louis Stevenson, he as Edgar Allan Poe).\nThe UK premiere of Bradbury's stage adaptation was not until 2003 in Nottingham, while it took until 2006 before the Godlight Theatre Company produced and performed its New York City premiere at 59E59 Theaters. After the completion of the New York run, the production then transferred to the Edinburgh Festival where it was a 2006 Edinburgh Festival \"Pick of the Fringe\".\nThe Off-Broadway theatre The American Place Theatre presented a one man show adaptation of \"Fahrenheit 451\" as a part of their 2008\u20132009 Literature to Life season.\n\"Fahrenheit 451\" inspired the Birmingham Repertory Theatre production \"Time Has Fallen Asleep in the Afternoon Sunshine\", which was performed at the Birmingham Central Library in April 2012.\nRadio.\nBBC Radio produced a dramatization by Gregory Evans of the novel in 1982, starring Michael Pennington as Montag. It was broadcast again on February 12, 2012, and April 7 and 8, 2013, on BBC Radio 4 Extra.\nA second BBC adaptation, this one by David Calcutt, was broadcast on BBC Radio 4 in 2003, starring Stephen Tomlin, Christian Rodska, Sunny Ormonde and Tracey Wiles.\nComputer games.\nIn 1984, the novel was adapted into a computer text adventure game of the same name by the software company Trillium.\nComics.\nIn June 2009, a graphic novel edition of the book was published. Entitled \"Ray Bradbury's Fahrenheit 451: The Authorized Adaptation\", the paperback graphic adaptation was illustrated by Tim Hamilton. The introduction in the novel is written by Bradbury.\nCultural references.\nMichael Moore's 2004 documentary \"Fahrenheit 9/11\" refers to Bradbury's novel and the September 11 attacks, emphasized by the film's tagline \"The temperature where freedom burns\". The film takes a critical look at the presidency of George W. Bush, the War on Terror, and its coverage in the news media, and became the highest grossing documentary of all time. Bradbury, a conservative, was upset by what he considered the appropriation of his title, and wanted the film renamed. Moore filmed a subsequent documentary about the election of Donald Trump called \"Fahrenheit 11/9\" in 2018.\nIn \"The Simpsons\"s 2014 episode \"Treehouse of Horror XXV\", Bart Simpson attends the school of hell. One of his school books is named \"Fahrenheit 451,000\".\nIn 2015, the Internet Engineering Steering Group approved the publication of \"An HTTP Status Code to Report Legal Obstacles\", now RFC 7725, which specifies that websites forced to block resources for legal reasons should return a status code of 451 when users request those resources."}
{"id": "10956", "revid": "86247", "url": "https://en.wikipedia.org/wiki?curid=10956", "title": "First Gulf War", "text": ""}
{"id": "10957", "revid": "4770293", "url": "https://en.wikipedia.org/wiki?curid=10957", "title": "Francis Xavier", "text": "Francis Xavier (born Francisco de Jasso y Azpilicueta; Latin: \"Franciscus Xaverius\"; Basque: \"Frantzisko Xabierkoa\"; French: \"Fran\u00e7ois Xavier\", Spanish: \"Francisco Javier\"; Portuguese: \"Francisco Xavier\"; (7 April 15063 December 1552), venerated as Saint Francis Xavier, was a Navarrese Catholic missionary and saint who was the co-founder of the Society of Jesus.\nBorn in Javier (Xavier in Old Spanish and in Navarro-Aragonese, or Xabier (Basque language for \"new house\")), Kingdom of Navarre (in present-day Spain), he was a companion of Ignatius of Loyola and one of the first seven Jesuits who took vows of poverty and chastity at Montmartre, Paris in 1534. He led an extensive mission into Asia, mainly in the Portuguese Empire of the time and was influential in evangelization work, most notably in India. Although some sources claim that the Goa Inquisition was proposed by Francis Xavier, his letter to the king of Portugal, John III, asked for a special minister whose sole office would be to further Christianity in Goa. He also was the first Christian missionary to venture into Japan, Borneo, the Maluku Islands, and other areas. In those areas, struggling to learn the local languages and in the face of opposition, he had less success than he had enjoyed in India. Xavier was about to extend his missionary preaching to China when he died on Shangchuan Island.\nHe was beatified by Pope Paul V on 25 October 1619 and canonized by Pope Gregory XV on 12 March 1622. In 1624, he was made co-patron of Navarre. Known as the \"Apostle of the Indies\" and \"Apostle of Japan\", he is considered to be one of the greatest missionaries since Paul the Apostle. In 1927, Pope Pius XI published the decree \"Apostolicorum in Missionibus\" naming Francis Xavier, along with Th\u00e9r\u00e8se of Lisieux, co-patron of all foreign missions. He is now co-patron saint of Navarre, with Fermin. The Day of Navarre in Navarre, Spain, marks the anniversary of Francis Xavier's death, on 3 December 1552.\nEarly life.\nFrancis Xavier was born in the royal castle of Xavier, in the Kingdom of Navarre, on 7 April 1506 according to a family register. He was the youngest son of Juan de Jasso y Atondo, seneschal of Xavier castle, who belonged to a prosperous farming family and had acquired a doctorate in law at the University of Bologna. Basque and Romance were his two mother tongues. Juan later became a privy counsellor and finance minister to King John III of Navarre (Jean d'Albret). Francis's mother was Do\u00f1a Mar\u00eda de Azpilcueta y Azn\u00e1rez, sole heiress of two noble Navarrese families. He was through her related to the great theologian and philosopher Mart\u00edn de Azpilcueta.\nIn 1512, Ferdinand, King of Aragon and regent of Castile, invaded Navarre, initiating a war that lasted over 18 years. Three years later, Francis's father died when Francis was only nine years old. In 1516, Francis's brothers participated in a failed Navarrese-French attempt to expel the Spanish invaders from the kingdom. The Spanish Governor, Cardinal Cisneros, confiscated the family lands, demolished the outer wall, the gates, and two towers of the family castle, and filled in the moat. In addition, the height of the keep was reduced by half. Only the family residence inside the castle was left. In 1522, one of Francis's brothers participated with 200 Navarrese nobles in dogged but failed resistance against the Castilian Count of Miranda in Amaiur, Baztan, the last Navarrese territorial position south of the Pyrenees.\nIn 1525, Francis went to study in Paris at the Coll\u00e8ge Sainte-Barbe, University of Paris, where he would spend the next eleven years. In the early days he acquired some reputation as an athlete and a high-jumper.\nIn 1529, Francis shared lodgings with his friend Pierre Favre. A new student, Ignatius of Loyola, came to room with them. At 38, Ignatius was much older than Pierre and Francis, who were both 23 at the time. Ignatius convinced Pierre to become a priest, but was unable to convince Francis, who had aspirations of worldly advancement. At first Francis regarded the new lodger as a joke and was sarcastic about his efforts to convert students. When Pierre left their lodgings to visit his family and Ignatius was alone with Francis, he was able to slowly break down Francis's resistance. According to most biographies Ignatius is said to have posed the question: \"What will it profit a man to gain the whole world, and lose his own soul?\" However, according to James Broderick such method is not characteristic of Ignatius and there is no evidence that he employed it at all.\nIn 1530, Francis received the degree of Master of Arts, and afterwards taught Aristotelian philosophy at Beauvais College, University of Paris.\nMissionary work.\nOn 15 August 1534, seven students met in a crypt beneath the Church of Saint Denis (now Saint Pierre de Montmartre), on the hill of Montmartre, overlooking Paris. They were Francis, Ignatius of Loyola, Alfonso Salmeron, Diego La\u00ednez, Nicol\u00e1s Bobadilla from Spain, Peter Faber from Savoy, and Sim\u00e3o Rodrigues from Portugal. They made private vows of poverty, chastity, and obedience to the Pope, and also vowed to go to the Holy Land to convert infidels. Francis began his study of theology in 1534 and was ordained on 24 June 1537.\nIn 1539, after long discussions, Ignatius drew up a formula for a new religious order, the Society of Jesus (the Jesuits). Ignatius's plan for the order was approved by Pope Paul III in 1540.\nIn 1540, King John of Portugal had Pedro Mascarenhas, Portuguese ambassador to the Holy See, request Jesuit missionaries to spread the faith in his new possessions in India, where the king believed that Christian values were eroding among the Portuguese. After successive appeals to the Pope asking for missionaries for the East Indies under the Padroado agreement, John III was encouraged by Diogo de Gouveia, rector of the Coll\u00e8ge Sainte-Barbe, to recruit the newly graduated students that would establish the Society of Jesus.\nIgnatius promptly appointed Nicholas Bobadilla and Sim\u00e3o Rodrigues. At the last moment, however, Bobadilla became seriously ill. With some hesitance and uneasiness, Ignatius asked Francis to go in Bobadilla's place. Thus, Francis Xavier began his life as the first Jesuit missionary almost accidentally.\nLeaving Rome on 15 March 1540, in the Ambassador's train, Francis took with him a breviary, a catechism, and by Croatian humanist Marko Maruli\u0107, a Latin book that had become popular in the Counter-Reformation. According to a 1549 letter of F. Balthasar Gago from Goa, it was the only book that Francis read or studied. Francis reached Lisbon in June 1540 and, four days after his arrival, he and Rodrigues were summoned to a private audience with the King and the Queen.\nFrancis Xavier devoted much of his life to missions in Asia, mainly in four centres: Malacca, Amboina and Ternate, Japan, and off-shore China. His growing information about new places indicated to him that he had to go to what he understood were centres of influence for the whole region. China loomed large from his days in India. Japan was particularly attractive because of its culture. For him, these areas were interconnected; they could not be evangelised separately.\nGoa and India.\nFrancis Xavier left Lisbon on 7 April 1541, his thirty-fifth birthday, along with two other Jesuits and the new viceroy Martim Afonso de Sousa, on board the \"Santiago\". As he departed, Francis was given a brief from the pope appointing him apostolic nuncio to the East. From August until March 1542 he remained in Portuguese Mozambique, and arrived in Goa, then capital of Portuguese India, on 6 May 1542, thirteen months after leaving Lisbon.\nThe Portuguese, following quickly on the great voyages of discovery, had established themselves at Goa thirty years earlier. Francis's primary mission, as ordered by King John III, was to restore Christianity among the Portuguese settlers. According to Teotonio R. DeSouza, recent critical accounts indicate that apart from the posted civil servants, \"the great majority of those who were dispatched as 'discoverers' were the riff-raff of Portuguese society, picked up from Portuguese jails.\" Nor did the soldiers, sailors, or merchants come to do missionary work, and Imperial policy permitted the outflow of disaffected nobility. Many of the arrivals formed liaisons with local women and adopted Indian culture. Missionaries often wrote against the \"scandalous and undisciplined\" behaviour of their fellow Christians.\nThe Christian population had churches, clergy, and a bishop, but there were few preachers and no priests beyond the walls of Goa. The Velliapura family of Velim, Goa, of the St Thomas Christians sect, welcomed the missionaries. Xavier decided that he must begin by instructing the Portuguese themselves, and gave much of his time to the teaching of children. The first five months he spent in preaching and ministering to the sick in the hospitals. After that, he walked through the streets ringing a bell to summon the children and servants to catechism. He was invited to head Saint Paul's College, a pioneer seminary for the education of secular priests, which became the first Jesuit headquarters in Asia.\nXavier soon learned that along the Pearl Fishery Coast, which extends from Cape Comorin on the southern tip of India to the island of Mannar, off Ceylon (Sri Lanka), there was a J\u0101ti of people called Paravas. Many of them had been baptised ten years before, merely to please the Portuguese who had helped them against the Moors, but remained uninstructed in the faith. Accompanied by several native clerics from the seminary at Goa, he set sail for Cape Comorin in October 1542. He taught those who had already been baptised, and preached to those who weren't. His efforts with the high-caste Brahmins remained unavailing.\nHe devoted almost three years to the work of preaching to the people of southern India and Ceylon, converting many. He built nearly 40 churches along the coast, including St. Stephen's Church, Kombuthurai, mentioned in his letters dated 1544.\nDuring this time, he was able to visit the tomb of Thomas the Apostle in Mylapore (now part of Madras/Chennai then in Portuguese India). He set his sights eastward in 1545 and planned a missionary journey to Makassar on the island of Celebes (today's Indonesia).\nAs the first Jesuit in India, Francis had difficulty achieving much success in his missionary trips. His successors, such as de Nobili, Matteo Ricci, and Beschi, attempted to convert the noblemen first as a means to influence more people, while Francis had initially interacted most with the lower classes; (later though, in Japan, Francis changed tack by paying tribute to the Emperor and seeking an audience with him).\nSoutheast Asia.\nIn the spring of 1545 Xavier started for Portuguese Malacca. He laboured there for the last months of that year. About January 1546, Xavier left Malacca for the Maluku Islands, where the Portuguese had some settlements. For a year and a half he preached the Gospel there. He went first to Ambon Island, where he stayed until mid-June. He then visited other Maluku Islands, including Ternate, Baranura, and Morotai. Shortly after Easter 1547, he returned to Ambon Island; a few months later he returned to Malacca.\nJapan.\nIn Malacca in December 1547, Francis Xavier met a Japanese man named Anjir\u014d. Anjir\u014d had heard of Francis in 1545 and had travelled from Kagoshima to Malacca to meet him. Having been charged with murder, Anjir\u014d had fled Japan. He told Francis extensively about his former life, and the customs and culture of his homeland. Anjir\u014d became the first Japanese Christian and adopted the name of 'Paulo de Santa F\u00e9'. He later helped Xavier as a mediator and interpreter for the mission to Japan that now seemed much more possible.\nIn January 1548 Francis returned to Goa to attend to his responsibilities as superior of the mission there. The next 15 months were occupied with various journeys and administrative measures. He left Goa on 15 April 1549, stopped at Malacca, and visited Canton. He was accompanied by Anjiro, two other Japanese men, Father Cosme de Torr\u00e8s and Brother Juan Fern\u00e1ndez. He had taken with him presents for the \"King of Japan\" since he was intending to introduce himself as the Apostolic Nuncio.\nEuropeans had already come to Japan: the Portuguese had landed in 1543 on the island of Tanegashima, where they introduced matchlock firearms to Japan.\nFrom Amboina, he wrote to his companions in Europe: \"I asked a Portuguese merchant, ... who had been for many days in Anjir\u014d's country of Japan, to give me ... some information on that land and its people from what he had seen and heard. ...All the Portuguese merchants coming from Japan tell me that if I go there I shall do great service for God our Lord, more than with the pagans of India, for they are a very reasonable people.\" (To His Companions Residing in Rome, From Cochin, 20 January 1548, no. 18, p.\u00a0178).\nFrancis Xavier reached Japan on 27 July 1549, with Anjiro and three other Jesuits, but he was not permitted to enter any port his ship arrived at until 15 August, when he went ashore at Kagoshima, the principal port of Satsuma Province on the island of Ky\u016bsh\u016b. As a representative of the Portuguese king, he was received in a friendly manner. Shimazu Takahisa (1514\u20131571), \"daimy\u014d\" of Satsuma, gave a friendly reception to Francis on 29 September 1549, but in the following year he forbade the conversion of his subjects to Christianity under penalty of death; Christians in Kagoshima could not be given any catechism in the following years. The Portuguese missionary Pedro de Alc\u00e1\u00e7ova would later write in 1554:\nHe was hosted by Anjir\u014d's family until October 1550. From October to December 1550, he resided in Yamaguchi. Shortly before Christmas, he left for Kyoto but failed to meet with the Emperor. He returned to Yamaguchi in March 1551, where the daimyo of the province gave him permission to preach. However, lacking fluency in the Japanese language, he had to limit himself to reading aloud the translation of a catechism.\nFrancis was the first Jesuit to go to Japan as a missionary. He brought with him paintings of the Madonna and the Madonna and Child. These paintings were used to help teach the Japanese about Christianity. There was a huge language barrier as Japanese was unlike other languages the missionaries had previously encountered. For a long time Francis struggled to learn the language.\nHaving learned that evangelical poverty did not have the appeal in Japan that it had in Europe and in India, he decided to change his approach. Hearing after a time that a Portuguese ship had arrived at a port in the province of Bungo in Kyushu and that the prince there would like to see him, Xavier now set out southward. The Jesuit, in a fine cassock, surplice, and stole, was attended by thirty gentlemen and as many servants, all in their best clothes. Five of them bore on cushions valuable articles, including a portrait of Our Lady and a pair of velvet slippers, these not gifts for the prince, but solemn offerings to Xavier, to impress the onlookers with his eminence. Handsomely dressed, with his companions acting as attendants, he presented himself before Oshindono, the ruler of Nagate, and as a representative of the great kingdom of Portugal, offered him letters and presents: a musical instrument, a watch, and other attractive objects which had been given him by the authorities in India for the emperor.\nFor forty-five years the Jesuits were the only missionaries in Asia, but the Franciscans also began proselytising in Asia as well. Christian missionaries were later forced into exile, along with their assistants. Some were able to stay behind, however Christianity was then kept underground so as to not be persecuted.\nThe Japanese people were not easily converted; many of the people were already Buddhist or Shinto. Francis tried to combat the disposition of some of the Japanese that a God who had created everything, including evil, could not be good. The concept of Hell was also a struggle; the Japanese were bothered by the idea of their ancestors living in Hell. Despite Francis's different religion, he felt that they were good people, much like Europeans, and could be converted.\nXavier was welcomed by the Shingon monks since he used the word \"Dainichi\" for the Christian God; attempting to adapt the concept to local traditions. As Xavier learned more about the religious nuances of the word, he changed to \"Deusu\" from the Latin and Portuguese \"Deus\". The monks later realised that Xavier was preaching a rival religion and grew more aggressive towards his attempts at conversion.\nWith the passage of time, his sojourn in Japan could be considered somewhat fruitful as attested by congregations established in Hirado, Yamaguchi, and Bungo. Xavier worked for more than two years in Japan and saw his successor-Jesuits established. He then decided to return to India. Historians debate the exact path he returned by, but from evidence attributed to the captain of his ship, he may have travelled through Tanegeshima and Minato, and avoided Kagoshima because of the hostility of the daimyo.\nChina.\nDuring his trip from Japan back to India, a tempest forced him to stop on an island near Guangzhou, Guangdong, China, where he met Diogo Pereira, a rich merchant and an old friend from Cochin. Pereira showed him a letter from Portuguese prisoners in Guangzhou, asking for a Portuguese ambassador to speak to the Chinese Emperor on their behalf. Later during the voyage, he stopped at Malacca on 27 December 1551, and was back in Goa by January 1552.\nOn 17 April he set sail with Diogo Pereira on the \"Santa Cruz\" for China. He planned to introduce himself as Apostolic Nuncio and Pereira as ambassador of the King of Portugal. But then he realized that he had forgotten his testimonial letters as an Apostolic Nuncio. Back in Malacca, he was confronted by the captain \u00c1lvaro de Ata\u00edde da Gama who now had total control over the harbour. The captain refused to recognize his title of Nuncio, asked Pereira to resign from his title of ambassador, named a new crew for the ship, and demanded the gifts for the Chinese Emperor be left in Malacca.\nIn late August 1552, the \"Santa Cruz\" reached the Chinese island of Shangchuan, 14\u00a0km away from the southern coast of mainland China, near Taishan, Guangdong, 200\u00a0km south-west of what later became Hong Kong. At this time, he was accompanied only by a Jesuit student, \u00c1lvaro Ferreira, a Chinese man called Ant\u00f3nio, and a Malabar servant called Christopher. Around mid-November he sent a letter saying that a man had agreed to take him to the mainland in exchange for a large sum of money. Having sent back \u00c1lvaro Ferreira, he remained alone with Ant\u00f3nio. He died from a fever at Shangchuan, Taishan, China, on 3 December 1552, while he was waiting for a boat that would take him to mainland China.\nBurials and relics.\nXavier was first buried on a beach at Shangchuan Island, Taishan, Guangdong. His body was taken from the island in February 1553 and temporarily buried in St. Paul's church in Portuguese Malacca on 22 March 1553. An open grave in the church now marks the place of Xavier's burial. Pereira came back from Goa, removed the corpse shortly after 15 April 1553, and moved it to his house. On 11 December 1553, Xavier's body was shipped to Goa.\nThe body is now in the Basilica of Bom Jesus in Goa, where it was placed in a glass container encased in a silver casket on 2 December 1637. This casket, constructed by Goan silversmiths between 1636 and 1637, was an exemplary blend of Italian and Indian aesthetic sensibilities. There are 32 silver plates on all four sides of the casket, depicting different episodes from the life of Xavier:\nThe right forearm, which Xavier used to bless and baptise his converts, was detached by Superior General Claudio Acquaviva in 1614. It has been displayed since in a silver reliquary at the main Jesuit church in Rome, Il Ges\u00f9.\nAnother of Xavier's arm bones was brought to Macau where it was kept in a silver reliquary. The relic was destined for Japan but religious persecution there persuaded the church to keep it in Macau's Cathedral of St. Paul. It was subsequently moved to St. Joseph's and in 1978 to the Chapel of St. Francis Xavier on Coloane Island. More recently the relic was moved to St. Joseph's Church.\nIn 2006, on the 500th anniversary of his birth, the Xavier Tomb Monument and Chapel on the Shangchuan Island, in ruins after years of neglect under communist rule in China, was restored with support from the alumni of Wah Yan College, a Jesuit high school in Hong Kong.\nFrom December 2017 to February 2018, Catholic Christian Outreach (CCO) in cooperation with the Jesuits, and the Archdiocese of Ottawa (Canada) brought Xavier's right forearm to tour throughout Canada. The faithful, especially university students participating with CCO at Rise Up 2017 in Ottawa, venerated the relics. The tour continued to every city where CCO and/or the Jesuits are present in Canada: Quebec City, St. John's, Halifax, St. Francis Xavier University in Antigonish (neither CCO nor the Jesuits are present here), Kingston, Toronto, Winnipeg, Saskatoon, Regina, Calgary, Vancouver, Victoria, and Montreal before returning to Ottawa. The relic was then returned to Rome with a Mass of Thanksgiving celebrated by Archbishop Terrence Prendergast at the Church of the Gesu.\nVeneration.\nBeatification and canonization.\nFrancis Xavier was beatified by Paul V on 25 October 1619, and was canonized by Gregory XV on 12 March 1622, at the same time as Ignatius Loyola. Pius XI proclaimed him the \"Patron of Catholic Missions\". His feast day is 3 December.\nPilgrimage centres.\nGoa.\nSaint Francis Xavier's relics are kept in a silver casket, elevated inside the Bom Jesus Basilica and are exposed (being brought to ground level) generally every ten years, but this is discretionary. The sacred relics went on display starting on 22 November 2014 at the XVII Solemn Exposition. The display closed on 4 January 2015. The previous exposition, the sixteenth, was held from 21 November 2004 to 2 January 2005.\nRelics of Saint Francis Xavier are also found in the Espirito Santo (Holy Spirit) Church, Marg\u00e3o, in Sanv Fransiku Xavierachi Igorz (Church of St. Francis Xavier), Batpal, Canacona, Goa, and at St. Francis Xavier Chapel, Portais, Panjim.\nOther places.\nOther pilgrimage centres include Xavier's birthplace in Navarra, Church of Il Gesu, Rome, Malacca (where he was buried for 2 years, before being brought to Goa), Sancian (place of death), and more.\nXavier is a major venerated saint in both Sonora and the neighbouring U.S. state of Arizona. In Magdalena de Kino in Sonora, Mexico, in the Church of Santa Mar\u00eda Magdalena, there is reclining statue of San Francisco Xavier brought by pioneer Jesuit missionary Padre Eusebio Kino in the early 18th century. The statue is said to be miraculous and is the object of pilgrimage for many in the region. Also Mission San Xavier del Bac is a pilgrimage site. The mission is an active parish church ministering to the people of the San Xavier District, Tohono O'odham Nation, and nearby Tucson, Arizona.\nNovena of grace.\nThe Novena of Grace is a popular devotion to Francis Xavier, typically prayed either on the nine days before 3 December, or on 4 March through 12 March (the anniversary of Pope Gregory XV's canonisation of Xavier in 1622). It began with the Italian Jesuit missionary Marcello Mastrilli. Before he could travel to the Far East, Mastrilli was gravely injured in a freak accident after a festive celebration dedicated to the Immaculate Conception in Naples. Delirious and on the verge of death, Mastrilli saw Xavier, who he later said asked him to choose between travelling or death by holding the respective symbols, to which Mastrilli answered, \"I choose that which God wills.\" Upon regaining his health, Mastrilli made his way via Goa and the Philippines to Satsuma, Japan. The Tokugawa shogunate beheaded the missionary in October 1637, after undergoing three days of tortures involving the volcanic sulphurous fumes from Mt. Unzen, known as the \"Hell mouth\" or \"pit\" that had supposedly caused an earlier missionary to renounce his faith.\nLegacy.\nFrancis Xavier is noteworthy for his missionary work, both as organiser and as pioneer, reputed to have converted more people than anyone else has done since Paul the Apostle. Pope Benedict XVI said of both Ignatius of Loyola and Francis Xavier: \"not only their history which was interwoven for many years from Paris and Rome, but a unique desire \u2013 a unique passion, it could be said \u2013 moved and sustained them through different human events: the passion to give to God-Trinity a glory always greater and to work for the proclamation of the Gospel of Christ to the peoples who had been ignored.\" By consulting with the earlier ancient Christians of St. Thomas in India, Xavier developed Jesuit missionary methods. His success also spurred many Europeans to join the order, as well as become missionaries throughout the world. His personal efforts most affected Christians in India and the East Indies (Indonesia, Malaysia, Timor). India still has numerous Jesuit missions, and many more schools. Xavier also worked to propagate Christianity in China and Japan. However, following the persecutions of Toyotomi Hideyoshi and the subsequent closing of Japan to foreigners, the Christians of Japan were forced to go underground to develop an independent Christian culture. Likewise, while Xavier inspired many missionaries to China, Chinese Christians also were forced underground and developed their own Christian culture.\nA small chapel designed by Achille-Antoine Hermitte was completed in 1869 over Xavier's death place on Shangchuan Island, Canton.\nIt was damaged and restored several times, with the most recent restoration in 2006 to celebrate the 500th anniversary of his birth.\nFrancis Xavier is the patron saint of his native Navarre, which celebrates his feast day on 3 December as a government holiday. In addition to Roman Catholic Masses remembering Xavier on that day (now known as the Day of Navarra), celebrations in the surrounding weeks honour the region's cultural heritage. Furthermore, in the 1940s, devoted Catholics instituted the Javierada, an annual day-long pilgrimage (often on foot) from the capital at Pamplona to Xavier, where the Jesuits have built a basilica and museum and restored his family's castle.\nNamesake.\nAs the foremost saint from Navarre and one of the main Jesuit saints, he is much venerated in Spain and the Hispanic countries where \"Francisco Javier\" or \"Javier\" are common male given names. The alternative spelling \"Xavier\" is also popular in the Basque Country, Portugal, Catalonia, Brazil, France, Belgium, and southern Italy. In India, the spelling \"Xavier\" is almost always used, and the name is quite common among Christians, especially in Goa and the southern states of Tamil Nadu, Kerala, and Karnataka. The names \"Francisco Xavier\", \"Ant\u00f3nio Xavier\", \"Jo\u00e3o Xavier\", \"Caetano Xavier\", \"Domingos Xavier\" et cetera, were very common till quite recently in Goa. \"Fransiskus Xaverius\" is commonly used as a name for Indonesian Catholics, usually abbreviated as FX. In Austria and Bavaria the name is spelled as \"Xaver\" (pronounced (\u02c8k\u0361sa\u02d0f\u0250)) and often used in addition to Francis as \"Franz-Xaver\" (frant\u0361s\u02c8k\u0361sa\u02d0f\u0250). Many Catalan men are named for him, often using the two-name combination Francesc Xavier. In English speaking countries, \"Xavier\" until recently was likely to follow \"Francis\"; in the 2000s, however, \"Xavier\" by itself has become more popular than \"Francis\", and since 2001 is now one of the hundred most common male baby names in the U.S.A. Furthermore, the Sevier family name, possibly most famous in the United States for John Sevier, originated from the name Xavier.\nMany churches all over the world, often founded by Jesuits, have been named in honour of Xavier. The many in the United States include the historic St. Francis Xavier Shrine at Warwick, Maryland (founded 1720), and the Basilica of St. Francis Xavier in Dyersville, Iowa. There are also the American educational teaching order Xaverian Brothers, and the Mission San Xavier del Bac in Tucson, Arizona (founded in 1692, and known for its Spanish Colonial architecture).\nIn art.\nRubens painted \"St Francis Xavier Raising the Dead\" for a Jesuit church in Antwerp, in which he depicted one of St Francis's many miracles. The Charles Bridge in Prague, Czech Republic, features a statue of Francis Xavier.\nIn front of Oita Station of Oita City, in Oita Prefecture, previously known as Bungo Province in Japan, there is one statue of Francis Xavier.\nThe monument Padr\u00e3o dos Descobrimentos in Bel\u00e9m (Lisbon), Portugal, features a Francis Xavier image.\nMissionary.\nShortly before leaving for the East, Xavier issued a famous instruction to Father Gaspar Barazeuz who was leaving to go to Ormuz (a kingdom on an island in the Persian Gulf, formerly attached to the Empire of Persia, now part of Iran), that he should mix with sinners:\nModern scholars place the number of people converted to Christianity by Francis Xavier at around 30,000. And while some of Xavier's methods have been since criticised (he forced converts to take Portuguese names and dress in Western clothes, approved the persecution of the Eastern Church, and used the Goa government as a missionary tool), he has also earned praise. He insisted that missionaries adapt to many of the customs, and most certainly the language, of the culture they wish to evangelise. And unlike later missionaries, Xavier supported an educated native clergy. Though for a time it seemed his work in Japan was subsequently destroyed by persecution, Protestant missionaries three centuries later discovered that approximately 100,000 Christians still practised in the Nagasaki area.\nFrancis Xavier's work initiated permanent change in eastern Indonesia, and he was known as the \"Apostle of the Indies\" where in 1546\u20131547 he worked in the Maluku Islands among the people of Ambon, Ternate, and Morotai (or Moro), and laid the foundations for a permanent mission. After he left the Maluku Islands, others carried on his work and by the 1560s, there were 10,000 Roman Catholics in the area, mostly on Ambon. By the 1590s, there were 50,000 to 60,000.\nRole in the Goa Inquisition.\nThe role of Francis Xavier in the Goa Inquisition is significant. He had written to King Jo\u00e3o III of Portugal in 1546, encouraging him to dispatch the Inquisition to Goa. This he did, after seeing mass immigration of Crypto-Jews and Crypto-Muslims from the Iberian peninsula, and the Inquisition had jurisdiction only over Christians and this would help veer them back to the Faith. Francis Xavier died in 1552 without living to see the commencement of the Goa Inquisition, but his Iberian background meant that he was aware of the Portuguese Inquisition's activities. In an interview to an Indian newspaper, historian Teot\u00f3nio de Souza stated that Francis Xavier and Sim\u00e3o Rodrigues, another founder-member of the Society of Jesus, were together in Lisbon before Francis left for India. Both were asked to assist spiritually the prisoners of the Inquisition and were present at the very first auto-da-f\u00e9 celebrated in Portugal in September 1540, at which 23 were absolved and two were condemned to be burnt, including a French cleric. Hence, he believes that Xavier was aware of the brutal punishment that could be meted out by the Inquisition against relapsed heretics."}
{"id": "10958", "revid": "2510232", "url": "https://en.wikipedia.org/wiki?curid=10958", "title": "Fossil", "text": "A fossil (from Classical Latin: , literally \"obtained by digging\") is any preserved remains, impression, or trace of any once-living thing from a past geological age. Examples include bones, shells, exoskeletons, stone imprints of animals or microbes, objects preserved in amber, hair, petrified wood, oil, coal, and DNA remnants. The totality of fossils is known as the \"fossil record\".\nPaleontology is the study of fossils: their age, method of formation, and evolutionary significance. Specimens are usually considered to be fossils if they are over 10,000\u00a0years old. The oldest fossils are around 3.48\u00a0billion\u00a0years old to 4.1\u00a0billion years old. The observation in the 19th century that certain fossils were associated with certain rock strata led to the recognition of a geological timescale and the relative ages of different fossils. The development of radiometric dating techniques in the early 20th century allowed scientists to quantitatively measure the absolute ages of rocks and the fossils they host.\nThere are many processes that lead to fossilization, including permineralization, casts and molds, authigenic mineralization, replacement and recrystallization, adpression, carbonization, and bioimmuration.\nFossils vary in size from one-micrometre (1\u00a0\u00b5m) bacteria to dinosaurs and trees, many meters long and weighing many tons. A fossil normally preserves only a portion of the deceased organism, usually that portion that was partially mineralized during life, such as the bones and teeth of vertebrates, or the chitinous or calcareous exoskeletons of invertebrates. Fossils may also consist of the marks left behind by the organism while it was alive, such as animal tracks or feces (coprolites). These types of fossil are called trace fossils or \"ichnofossils\", as opposed to \"body fossils\". Some fossils are biochemical and are called \"chemofossils\" or biosignatures.\nFossilization processes.\nThe process of fossilization varies according to tissue type and external conditions.\nPermineralization.\nPermineralization is a process of fossilization that occurs when an organism is buried. The empty spaces within an organism (spaces filled with liquid or gas during life) become filled with mineral-rich groundwater. Minerals precipitate from the groundwater, occupying the empty spaces. This process can occur in very small spaces, such as within the cell wall of a plant cell. Small scale permineralization can produce very detailed fossils. For permineralization to occur, the organism must become covered by sediment soon after death, otherwise the remains destroyed by scavengers or decomposition. The degree to which the remains are decayed when covered determines the later details of the fossil. Some fossils consist only of skeletal remains or teeth; other fossils contain traces of skin, feathers or even soft tissues. This is a form of diagenesis.\nCasts and molds.\nIn some cases, the original remains of the organism completely dissolve or are otherwise destroyed. The remaining organism-shaped hole in the rock is called an \"external mold\". If this hole is later filled with other minerals, it is a \"cast\". An endocast, or \"internal mold\", is formed when sediments or minerals fill the internal cavity of an organism, such as the inside of a bivalve or snail or the hollow of a skull.\nAuthigenic mineralization.\nThis is a special form of cast and mold formation. If the chemistry is right, the organism (or fragment of organism) can act as a nucleus for the precipitation of minerals such as siderite, resulting in a nodule forming around it. If this happens rapidly before significant decay to the organic tissue, very fine three-dimensional morphological detail can be preserved. Nodules from the Carboniferous Mazon Creek fossil beds of Illinois, USA, are among the best documented examples of such mineralization.\nReplacement and recrystallization.\nReplacement occurs when the shell, bone, or other tissue is replaced with another mineral. In some cases mineral replacement of the original shell occurs so gradually and at such fine scales that microstructural features are preserved despite the total loss of original material. A shell is said to be \"recrystallized\" when the original skeletal compounds are still present but in a different crystal form, as from aragonite to calcite.\nAdpression (compression-impression).\nCompression fossils, such as those of fossil ferns, are the result of chemical reduction of the complex organic molecules composing the organism's tissues. In this case the fossil consists of original material, albeit in a geochemically altered state. This chemical change is an expression of diagenesis. Often what remains is a carbonaceous film known as a phytoleim, in which case the fossil is known as a compression. Often, however, the phytoleim is lost and all that remains is an impression of the organism in the rock\u2014an impression fossil. In many cases, however, compressions and impressions occur together. For instance, when the rock is broken open, the phytoleim will often be attached to one part (compression), whereas the counterpart will just be an impression. For this reason, one term covers the two modes of preservation: \"adpression\".\nSoft tissue, cell and molecular preservation.\nBecause of their antiquity, an unexpected exception to the alteration of an organism's tissues by chemical reduction of the complex organic molecules during fossilization has been the discovery of soft tissue in dinosaur fossils, including blood vessels, and the isolation of proteins and evidence for DNA fragments. In 2014, Mary Schweitzer and her colleagues reported the presence of iron particles (goethite-aFeO(OH)) associated with soft tissues recovered from dinosaur fossils. Based on various experiments that studied the interaction of iron in haemoglobin with blood vessel tissue they proposed that solution hypoxia coupled with iron chelation enhances the stability and preservation of soft tissue and provides the basis for an explanation for the unforeseen preservation of fossil soft tissues. However, a slightly older study based on eight taxa ranging in time from the Devonian to the Jurassic found that reasonably well-preserved fibrils that probably represent collagen were preserved in all these fossils and that the quality of preservation depended mostly on the arrangement of the collagen fibers, with tight packing favoring good preservation. There seemed to be no correlation between geological age and quality of preservation, within that timeframe.\nCarbonization and coalification.\nFossils that are carbonized or coalified consist of the organic remains which have been reduced primarily to the chemical element carbon. Carbonized fossils consist of a thin film which forms a silhouette of the original organism, and the original organic remains were typically soft tissues. Coalified fossils consist primarily of coal, and the original organic remains were typically woody in composition.\nBioimmuration.\nBioimmuration occurs when a skeletal organism overgrows or otherwise subsumes another organism, preserving the latter, or an impression of it, within the skeleton. Usually it is a sessile skeletal organism, such as a bryozoan or an oyster, which grows along a substrate, covering other sessile sclerobionts. Sometimes the bioimmured organism is soft-bodied and is then preserved in negative relief as a kind of external mold. There are also cases where an organism settles on top of a living skeletal organism that grows upwards, preserving the settler in its skeleton. Bioimmuration is known in the fossil record from the Ordovician to the Recent.\nTypes.\nIndex.\nIndex fossils (also known as guide fossils, indicator fossils or zone fossils) are fossils used to define and identify geologic periods (or faunal stages). They work on the premise that, although different sediments may look different depending on the conditions under which they were deposited, they may include the remains of the same species of fossil. The shorter the species' time range, the more precisely different sediments can be correlated, and so rapidly evolving species' fossils are particularly valuable. The best index fossils are common, easy to identify at species level and have a broad distribution\u2014otherwise the likelihood of finding and recognizing one in the two sediments is poor.\nTrace.\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and they reflect animal behaviours. Many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\nCoprolites are classified as trace fossils as opposed to body fossils, as they give evidence for the animal's behaviour (in this case, diet) rather than morphology. They were first described by William Buckland in 1829. Prior to this they were known as \"fossil fir cones\" and \"bezoar stones.\" They serve a valuable purpose in paleontology because they provide direct evidence of the predation and diet of extinct organisms. Coprolites may range in size from a few millimetres to over 60 centimetres.\nTransitional.\nA \"transitional fossil\" is any fossilized remains of a life form that exhibits traits common to both an ancestral group and its derived descendant group. This is especially important where the descendant group is sharply differentiated by gross anatomy and mode of living from the ancestral group. Because of the incompleteness of the fossil record, there is usually no way to know exactly how close a transitional fossil is to the point of divergence. These fossils serve as a reminder that taxonomic divisions are human constructs that have been imposed in hindsight on a continuum of variation.\nMicrofossils.\nMicrofossil is a descriptive term applied to fossilized plants and animals whose size is just at or below the level at which the fossil can be analyzed by the naked eye. A commonly applied cutoff point between \"micro\" and \"macro\" fossils is 1\u00a0mm. Microfossils may either be complete (or near-complete) organisms in themselves (such as the marine plankters foraminifera and coccolithophores) or component parts (such as small teeth or spores) of larger animals or plants. Microfossils are of critical importance as a reservoir of paleoclimate information, and are also commonly used by biostratigraphers to assist in the correlation of rock units.\nResin.\nFossil resin (colloquially called amber) is a natural polymer found in many types of strata throughout the world, even the Arctic. The oldest fossil resin dates to the Triassic, though most dates to the Cenozoic. The excretion of the resin by certain plants is thought to be an evolutionary adaptation for protection from insects and to seal wounds. Fossil resin often contains other fossils called inclusions that were captured by the sticky resin. These include bacteria, fungi, other plants, and animals. Animal inclusions are usually small invertebrates, predominantly arthropods such as insects and spiders, and only extremely rarely a vertebrate such as a small lizard. Preservation of inclusions can be exquisite, including small fragments of DNA.\nDerived, or reworked.\nA \"derived\", \"reworked\" or \"remani\u00e9 fossil\" is a fossil found in rock that accumulated significantly later than when the fossilized animal or plant died. Reworked fossils are created by erosion exhuming (freeing) fossils from the rock formation in which they were originally deposited and their redeposition in a younger sedimentary deposit.\nWood.\nFossil wood is wood that is preserved in the fossil record. Wood is usually the part of a plant that is best preserved (and most easily found). Fossil wood may or may not be petrified. The fossil wood may be the only part of the plant that has been preserved: therefore such wood may get a special kind of botanical name. This will usually include \"xylon\" and a term indicating its presumed affinity, such as \"Araucarioxylon\" (wood of \"Araucaria\" or some related genus), \"Palmoxylon\" (wood of an indeterminate palm), or \"Castanoxylon\" (wood of an indeterminate chinkapin).\nSubfossil.\nThe term subfossil can be used to refer to remains, such as bones, nests, or defecations, whose fossilization process is not complete, either because the length of time since the animal involved was living is too short (less than 10,000 years) or because the conditions in which the remains were buried were not optimal for fossilization. Subfossils are often found in caves or other shelters where they can be preserved for thousands of years. The main importance of subfossil vs. fossil remains is that the former contain organic material, which can be used for radiocarbon dating or extraction and sequencing of DNA, protein, or other biomolecules. Additionally, isotope ratios can provide much information about the ecological conditions under which extinct animals lived. Subfossils are useful for studying the evolutionary history of an environment and can be important to studies in paleoclimatology.\nSubfossils are often found in depositionary environments, such as lake sediments, oceanic sediments, and soils. Once deposited, physical and chemical weathering can alter the state of preservation.\nChemical fossils.\nChemical fossils, or chemofossils, are chemicals found in rocks and fossil fuels (petroleum, coal, and natural gas) that provide an organic signature for ancient life. Molecular fossils and isotope ratios represent two types of chemical fossils. The oldest traces of life on Earth are fossils of this type, including carbon isotope anomalies found in zircons that imply the existence of life as early as 4.1\u00a0billion years ago.\nDating.\nEstimating dates.\nPaleontology seeks to map out how life evolved across geologic time. A substantial hurdle is the difficulty of working out fossil ages. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50\u00a0million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to its decay products shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are volcanic ash layers, which may provide termini for the intervening sediments.\nStratigraphy.\nConsequently, palaeontologists rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the \"layer-cake\" that is the sedimentary record. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age is claimed to lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly adjacent. However, fossils of species that survived for a relatively short time can be used to match isolated rocks: this technique is called \"biostratigraphy\". For instance, the conodont \"Eoplacognathus pseudoplanus\" has a short range in the Middle Ordovician period. If rocks of unknown age have traces of \"E. pseudoplanus\", they have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and occupy a short time range to be useful. Misleading results are produced if the index fossils are incorrectly dated. Stratigraphy and biostratigraphy can in general provide only relative dating (\"A\" was before \"B\"), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching rocks of the same age across continents. Family-tree relationships also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X\u00a0million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved earlier.\nIt is also possible to estimate how long ago two living clades diverged, in other words approximately how long ago their last common ancestor must have lived, by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.\nLimitations.\nOrganisms are only rarely preserved as fossils in the best of circumstances, and only a fraction of such fossils have been discovered. This is illustrated by the fact that the number of species known through the fossil record is less than 5% of the number of known living species, suggesting that the number of species known through fossils must be far less than 1% of all the species that have ever lived. Because of the specialized and rare circumstances required for a biological structure to fossilize, only a small percentage of life-forms can be expected to be represented in discoveries, and each discovery represents only a snapshot of the process of evolution. The transition itself can only be illustrated and corroborated by transitional fossils, which will never demonstrate an exact half-way point.\nThe fossil record is strongly biased toward organisms with hard-parts, leaving most groups of soft-bodied organisms with little to no role. It is replete with the mollusks, the vertebrates, the echinoderms, the brachiopods and some groups of arthropods.\nSites.\nLagerst\u00e4tten.\nFossil sites with exceptional preservation\u2014sometimes including preserved soft tissues\u2014are known as Lagerst\u00e4tten\u2014German for \"storage places\". These formations may have resulted from carcass burial in an anoxic environment with minimal bacteria, thus slowing decomposition. Lagerst\u00e4tten span geological time from the Cambrian period to the present. Worldwide, some of the best examples of near-perfect fossilization are the Cambrian Maotianshan shales and Burgess Shale, the Devonian Hunsr\u00fcck Slates, the Jurassic Solnhofen limestone, and the Carboniferous Mazon Creek localities.\nStromatolites.\nStromatolites are layered accretionary structures formed in shallow water by the trapping, binding and cementation of sedimentary grains by biofilms of microorganisms, especially cyanobacteria. Stromatolites provide some of the most ancient fossil records of life on Earth, dating back more than 3.5\u00a0billion years ago.\nStromatolites were much more abundant in Precambrian times. While older, Archean fossil remains are presumed to be colonies of cyanobacteria, younger (that is, Proterozoic) fossils may be primordial forms of the eukaryote chlorophytes (that is, green algae). One genus of stromatolite very common in the geologic record is \"Collenia\". The earliest stromatolite of confirmed microbial origin dates to 2.724\u00a0billion years ago.\nA 2009 discovery provides strong evidence of microbial stromatolites extending as far back as 3.45\u00a0billion years ago.\nStromatolites are a major constituent of the fossil record for life's first 3.5\u00a0billion years, peaking about 1.25\u00a0billion years ago. They subsequently declined in abundance and diversity, which by the start of the Cambrian had fallen to 20% of their peak. The most widely supported explanation is that stromatolite builders fell victims to grazing creatures (the Cambrian substrate revolution), implying that sufficiently complex organisms were common over 1\u00a0billion years ago.\nThe connection between grazer and stromatolite abundance is well documented in the younger Ordovician evolutionary radiation; stromatolite abundance also increased after the end-Ordovician and end-Permian extinctions decimated marine animals, falling back to earlier levels as marine animals recovered. Fluctuations in metazoan population and diversity may not have been the only factor in the reduction in stromatolite abundance. Factors such as the chemistry of the environment may have been responsible for changes.\nWhile prokaryotic cyanobacteria themselves reproduce asexually through cell division, they were instrumental in priming the environment for the evolutionary development of more complex eukaryotic organisms. Cyanobacteria (as well as extremophile Gammaproteobacteria) are thought to be largely responsible for increasing the amount of oxygen in the primeval earth's atmosphere through their continuing photosynthesis. Cyanobacteria use water, carbon dioxide and sunlight to create their food. A layer of mucus often forms over mats of cyanobacterial cells. In modern microbial mats, debris from the surrounding habitat can become trapped within the mucus, which can be cemented by the calcium carbonate to grow thin laminations of limestone. These laminations can accrete over time, resulting in the banded pattern common to stromatolites. The domal morphology of biological stromatolites is the result of the vertical growth necessary for the continued infiltration of sunlight to the organisms for photosynthesis. Layered spherical growth structures termed oncolites are similar to stromatolites and are also known from the fossil record. Thrombolites are poorly laminated or non-laminated clotted structures formed by cyanobacteria common in the fossil record and in modern sediments.\nThe Zebra River Canyon area of the Kubis platform in the deeply dissected Zaris Mountains of southwestern Namibia provides an extremely well exposed example of the thrombolite-stromatolite-metazoan reefs that developed during the Proterozoic period, the stromatolites here being better developed in updip locations under conditions of higher current velocities and greater sediment influx.\nAstrobiology.\nIt has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on the planet Mars. Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.\nOn 24 January 2014, NASA reported that current studies by the \"Curiosity\" and \"Opportunity\" rovers on Mars will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.\nPseudofossils.\n\"Pseudofossils\" are visual patterns in rocks that are produced by geologic processes rather than biologic processes. They can easily be mistaken for real fossils. Some pseudofossils, such as geological dendrite crystals, are formed by naturally occurring fissures in the rock that get filled up by percolating minerals. Other types of pseudofossils are kidney ore (round shapes in iron ore) and moss agates, which look like moss or plant leaves. Concretions, spherical or ovoid-shaped nodules found in some sedimentary strata, were once thought to be dinosaur eggs, and are often mistaken for fossils as well.\nHistory of the study of fossils.\nGathering fossils dates at least to the beginning of recorded history. The fossils themselves are referred to as the fossil record. The fossil record was one of the early sources of data underlying the study of evolution and continues to be relevant to the history of life on Earth. Paleontologists examine the fossil record to understand the process of evolution and the way particular species have evolved.\nAncient civilizations.\nFossils have been visible and common throughout most of natural history, and so documented human interaction with them goes back as far as recorded history, or earlier.\nThere are many examples of paleolithic stone knives in Europe, with fossil echinoderms set precisely at the hand grip, going all the way back to \"Homo heidelbergensis\" and neanderthals. These ancient peoples also drilled holes through the center of those round fossil shells, apparently using them as beads for necklaces.\nThe ancient Egyptians gathered fossils of species that resembled the bones of modern species they worshipped. The god Set was associated with the hippopotamus, therefore fossilized bones of hippo-like species were kept in that deity's temples. Five-rayed fossil sea urchin shells were associated with the deity Sopdu, the Morning Star, equivalent of Venus in Roman mythology.\nFossils appear to have directly contributed to the mythology of many civilizations, including the ancient Greeks. Classical Greek historian Herodotos wrote of an area near Hyperborea where gryphons protected golden treasure. There was indeed gold mining in that approximate region, where beaked \"Protoceratops\" skulls were common as fossils.\nA later Greek scholar, Aristotle, eventually realized that fossil seashells from rocks were similar to those found on the beach, indicating the fossils were once living animals. He had previously explained them in terms of vaporous exhalations, which Persian polymath Avicenna modified into the theory of petrifying fluids (\"succus lapidificatus\"). Recognition of fossil seashells as originating in the sea was built upon in the 14th century by Albert of Saxony, and accepted in some form by most naturalists by the 16th century.\nRoman naturalist Pliny the Elder wrote of \"tongue stones\", which he called glossopetra. These were fossil shark teeth, thought by some classical cultures to look like the tongues of people or snakes. He also wrote about the horns of Ammon, which are fossil ammonites, from whence the group of shelled octopus-cousins ultimately draws its modern name. Pliny also makes one of the earlier known references to toadstones, thought until the 18th century to be a magical cure for poison originating in the heads of toads, but which are fossil teeth from \"Lepidotes\", a Cretaceous ray-finned fish.\nThe Plains tribes of North America are thought to have similarly associated fossils, such as the many intact pterosaur fossils naturally exposed in the region, with their own mythology of the thunderbird.\nThere is no such direct mythological connection known from prehistoric Africa, but there is considerable evidence of tribes there excavating and moving fossils to ceremonial sites, apparently treating them with some reverence.\nIn Japan, fossil shark teeth were associated with the mythical tengu, thought to be the razor-sharp claws of the creature, documented some time after the 8th century AD.\nIn medieval China, the fossil bones of ancient mammals including \"Homo erectus\" were often mistaken for \"dragon bones\" and used as medicine and aphrodisiacs. In addition, some of these fossil bones are collected as \"art\" by scholars, who left scripts on various artifacts, indicating the time they were added to a collection. One good example is the famous scholar Huang Tingjian of the South Song Dynasty during the 11th century, who kept a specific seashell fossil with his own poem engraved on it. In the West fossilized sea creatures on mountainsides were seen as proof of the biblical deluge.\nIn 1027, the Persian Avicenna explained fossils' stoniness in \"The Book of Healing\":\nFrom the 13th century to the present day, scholars pointed out that the fossil skulls of Deinotherium giganteum, found in Crete and Greece, might have been interpreted as being the skulls of the Cyclopes of Greek mythology, and are possibly the origin of that Greek myth. Their skulls appear to have a single eye-hole in the front, just like their modern elephant cousins, though in fact it's actually the opening for their trunk.\nIn Norse mythology, echinoderm shells (the round five-part button left over from a sea urchin) were associated with the god Thor, not only being incorporated in thunderstones, representations of Thor's hammer and subsequent hammer-shaped crosses as Christianity was adopted, but also kept in houses to garner Thor's protection.\nThese grew into the shepherd's crowns of English folklore, used for decoration and as good luck charms, placed by the doorway of homes and churches. In Suffolk, a different species was used as a good-luck charm by bakers, who referred to them as fairy loaves, associating them with the similarly shaped loaves of bread they baked.\nEarly modern explanations.\nMore scientific views of fossils emerged during the Renaissance. Leonardo da Vinci concurred with Aristotle's view that fossils were the remains of ancient life. For example, da Vinci noticed discrepancies with the biblical flood narrative as an explanation for fossil origins:\nIn 1666, Nicholas Steno examined a shark, and made the association of its teeth with the \"tongue stones\" of ancient Greco-Roman mythology, concluding that those were not in fact the tongues of venomous snakes, but the teeth of some long-extinct species of shark.\nRobert Hooke (1635-1703) included micrographs of fossils in his \"Micrographia\" and was among the first to observe fossil forams. His observations on fossils, which he stated to be the petrified remains of creatures some of which no longer existed, were published posthumously in 1705.\nWilliam Smith (1769\u20131839), an English canal engineer, observed that rocks of different ages (based on the law of superposition) preserved different assemblages of fossils, and that these assemblages succeeded one another in a regular and determinable order. He observed that rocks from distant locations could be correlated based on the fossils they contained. He termed this the principle of \"faunal succession\". This principle became one of Darwin's chief pieces of evidence that biological evolution was real.\nGeorges Cuvier came to believe that most if not all the animal fossils he examined were remains of extinct species. This led Cuvier to become an active proponent of the geological school of thought called catastrophism. Near the end of his 1796 paper on living and fossil elephants he said:\nInterest in fossils, and geology more generally, expanded during the early nineteenth century. In Britain, Mary Anning's discoveries of fossils, including the first complete ichthyosaur and a complete plesiosaurus skeleton, sparked both public and scholarly interest.\nLinnaeus and Darwin.\nEarly naturalists well understood the similarities and differences of living species leading Linnaeus to develop a hierarchical classification system still in use today. Darwin and his contemporaries first linked the hierarchical structure of the tree of life with the then very sparse fossil record. Darwin eloquently described a process of descent with modification, or evolution, whereby organisms either adapt to natural and changing environmental pressures, or they perish.\nWhen Darwin wrote \"On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life\", the oldest animal fossils were those from the Cambrian Period, now known to be about 540\u00a0million years old. He worried about the absence of older fossils because of the implications on the validity of his theories, but he expressed hope that such fossils would be found, noting that: \"only a small portion of the world is known with accuracy.\" Darwin also pondered the sudden appearance of many groups (i.e. phyla) in the oldest known Cambrian fossiliferous strata.\nAfter Darwin.\nSince Darwin's time, the fossil record has been extended to between 2.3 and 3.5\u00a0billion years. Most of these Precambrian fossils are microscopic bacteria or microfossils. However, macroscopic fossils are now known from the late Proterozoic. The Ediacara biota (also called Vendian biota) dating from 575\u00a0million years ago collectively constitutes a richly diverse assembly of early multicellular eukaryotes.\nThe fossil record and faunal succession form the basis of the science of biostratigraphy or determining the age of rocks based on embedded fossils. For the first 150 years of geology, biostratigraphy and superposition were the only means for determining the relative age of rocks. The geologic time scale was developed based on the relative ages of rock strata as determined by the early paleontologists and stratigraphers.\nSince the early years of the twentieth century, absolute dating methods, such as radiometric dating (including potassium/argon, argon/argon, uranium series, and, for very recent fossils, radiocarbon dating) have been used to verify the relative ages obtained by fossils and to provide absolute ages for many fossils. Radiometric dating has shown that the earliest known stromatolites are over 3.4\u00a0billion years old.\nModern era.\nPaleontology has joined with evolutionary biology to share the interdisciplinary task of outlining the tree of life, which inevitably leads backwards in time to Precambrian microscopic life when cell structure and functions evolved. Earth's deep time in the Proterozoic and deeper still in the Archean is only \"recounted by microscopic fossils and subtle chemical signals.\" Molecular biologists, using phylogenetics, can compare protein amino acid or nucleotide sequence homology (i.e., similarity) to evaluate taxonomy and evolutionary distances among organisms, with limited statistical confidence. The study of fossils, on the other hand, can more specifically pinpoint when and in what organism a mutation first appeared. Phylogenetics and paleontology work together in the clarification of science's still dim view of the appearance of life and its evolution.\nNiles Eldredge's study of the \"Phacops\" trilobite genus supported the hypothesis that modifications to the arrangement of the trilobite's eye lenses proceeded by fits and starts over millions of years during the Devonian. Eldredge's interpretation of the \"Phacops\" fossil record was that the aftermaths of the lens changes, but not the rapidly occurring evolutionary process, were fossilized. This and other data led Stephen Jay Gould and Niles Eldredge to publish their seminal paper on punctuated equilibrium in 1971.\nSynchrotron X-ray tomographic analysis of early Cambrian bilaterian embryonic microfossils yielded new insights of metazoan evolution at its earliest stages. The tomography technique provides previously unattainable three-dimensional resolution at the limits of fossilization. Fossils of two enigmatic bilaterians, the worm-like \"Markuelia\" and a putative, primitive protostome, \"Pseudooides\", provide a peek at germ layer embryonic development. These 543-million-year-old embryos support the emergence of some aspects of arthropod development earlier than previously thought in the late Proterozoic. The preserved embryos from China and Siberia underwent rapid diagenetic phosphatization resulting in exquisite preservation, including cell structures. This research is a notable example of how knowledge encoded by the fossil record continues to contribute otherwise unattainable information on the emergence and development of life on Earth. For example, the research suggests \"Markuelia\" has closest affinity to priapulid worms, and is adjacent to the evolutionary branching of Priapulida, Nematoda and Arthropoda.\nTrading and collecting.\n\"Fossil trading\" is the practice of buying and selling fossils. This is many times done illegally with artifacts stolen from research sites, costing many important scientific specimens each year. The problem is quite pronounced in China, where many specimens have been stolen.\n\"Fossil collecting\" (sometimes, in a non-scientific sense, fossil hunting) is the collection of fossils for scientific study, hobby, or profit. Fossil collecting, as practiced by amateurs, is the predecessor of modern paleontology and many still collect fossils and study fossils as amateurs. Professionals and amateurs alike collect fossils for their scientific value.\nFossils as medicine.\nThese is some medicinal and preventive use for some fossils. Largely the use of fossils as medicine is a matter of placebo effect. However, the consumption of certain fossils has been proven to help against stomach acidity and mineral depletion. The use of fossils to address health issues is rooted in traditional medicine and include the use of fossils as talismans. The specific fossil to use to alleviate or cure an illness is often based on its resemblance of the fossils and the symptoms or affected organ. The use dinosaur bones as \"dragon bones\" has persisted in Traditional Chinese medicine into modern times, with Mid Cretaceous dinosaur bones being used for the purpose in Ruyang County during the early 21st century."}
{"id": "10960", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=10960", "title": "Family Educational Rights and Privacy Act", "text": "The Family Educational Rights and Privacy Act of 1974 (FERPA or the Buckley Amendment) is a United States federal law that governs the access to educational information and records by public entities such as potential employers, publicly funded educational institutions, and foreign governments.\nOverview.\nFERPA gives parents access to their child's education records, an opportunity to seek to have the records amended, and some control over the disclosure of information from the records. With several exceptions, schools must have a student's consent prior to the disclosure of education records \"after that student is 18 years old\". The law applies only to educational agencies and institutions that receive \nfunds under a program administered by the U.S. Department of Education.\nOther regulations under this act, effective starting January 3, 2012, allow for greater disclosures of personal and directory student identifying information and regulate student IDs and e-mail addresses. For example, schools may provide external companies a student's personally identifiable information without the student's consent. Conversely, tying student directory information to other information may result in a violation, as the combination creates an education record.\nExamples of situations affected by FERPA include school employees divulging information to anyone other than the student about the student's grades or behavior, and school work posted on a bulletin board with a grade. Generally, schools must have written permission from the parent or eligible student in order to release any information from a student's education record.\nThis privacy policy also governs how state agencies transmit testing data to federal agencies, such as the Education Data Exchange Network.\nThis U.S. federal law also gave students 18 years of age or older, or students of any age if enrolled in any post-secondary educational institution, the right of privacy regarding grades, enrollment, and even billing information unless the school has specific permission from the student to share that specific type of information.\nFERPA also permits a school to disclose personally identifiable information from education records of an \"eligible student\" (a student age 18 or older or enrolled in a postsecondary institution at any age) to his or her parents if the student is a \"dependent student\" as that term is defined in Section 152 of the Internal Revenue Code. Generally, if either parent has claimed the student as a dependent on the parent's most recent income tax statement, the school may non-consensually disclose the student's education records to both parents.\nThe law allowed students who apply to an educational institution such as graduate school permission to view recommendations submitted by others as part of the application. However, on standard application forms, students are given the option to waive this right.\nFERPA specifically excludes employees of an educational institution if they are not students.\nThe act is also referred to as the \"Buckley Amendment\", for one of its proponents, Senator James L. Buckley of New York.\nFERPA is now a guide to communicating higher education issues to privacy issues that include sexual assault and campus safety.\u00a0 It provides a framework on addressing needs of certain population in higher education.\nAccess to public records.\nThe citing of FERPA to conceal public records that are not \"educational\" in nature has been widely criticized, including by the act's primary Senate sponsor. For example, in the \"Owasso Independent School District v. Falvo\" case, an important part of the debate was determining the relationship between peer-grading and \"education records\" as defined in FERPA. The plaintiffs argued \"that allowing students to score each other's tests[...]as the teachers explain the correct answers to the entire class[...] embarrassed [...] children\", but lost in a summary judgment by the district court. In the Court of Appeals, it was ruled that students placing grades on the work of other students made such work into an \"education record.\" Thus, peer-grading was determined as a violation of FERPA privacy policies because students had access to other students' academic performance without full consent. However, on appeal to the Supreme Court, it was unanimously ruled that peer-grading was not a violation of FERPA. This is because a grade written on a student's work does not become an \"education record\" until the teacher writes the final grade into a grade book.\nStudent medical records.\nLegal experts have debated the issue of whether student medical records (for example records of therapy sessions with a therapist at an on-campus counseling center) might be released to the school administration under certain triggering events, such as when a student sued his college or university.\nUsually, student medical treatment records will remain under the protection of FERPA, not the Health Insurance Portability and Accountability Act (HIPAA). This is due to the \"FERPA Exception\" written within HIPAA."}
{"id": "10961", "revid": "28550269", "url": "https://en.wikipedia.org/wiki?curid=10961", "title": "FERPA", "text": ""}
{"id": "10963", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=10963", "title": "Forgetting", "text": "Forgetting or disremembering is the apparent loss or modification of information already encoded and stored in an individual's short or long-term memory. It is a spontaneous or gradual process in which old memories are unable to be recalled from memory storage. Problems with remembering, learning and retaining new information are a few of the most common complaints of older adults. \nStudies show that retention improves with increased rehearsal. This improvement occurs because rehearsal helps to transfer information into long-term memory.\nForgetting curves (amount remembered as a function of time since an event was first experienced) have been extensively analyzed. The most recent evidence suggests that a power function provides the closest mathematical fit to the forgetting function.\nOverview.\nFailing to retrieve an event does not mean that this specific event has been forever forgotten. Research has shown that there are a few health behaviors that to some extent can prevent forgetting from happening so often. One of the simplest ways to keep the brain healthy and prevent forgetting is to stay active and exercise. Staying active is important because overall it keeps the body healthy. When the body is healthy the brain is healthy and less inflamed as well. Older adults who were more active were found to have had less episodes of forgetting compared to those older adults who were less active. A healthy diet can also contribute to a healthier brain and aging process which in turn results in less frequent forgetting.\nHistory.\nOne of the first to study the mechanisms of forgetting was the German psychologist Hermann Ebbinghaus (1885). Using himself as the sole subject in his experiment, he memorized lists of three letter nonsense syllable words\u2014two consonants and one vowel in the middle. He then measured his own capacity to relearn a given list of words after a variety of given time period. He found that forgetting occurs in a systematic manner, beginning rapidly and then leveling off. Although his methods were primitive, his basic premises have held true today and have been reaffirmed by more methodologically sound methods. The Ebbinghaus \"forgetting curve\" is the name of his results which he plotted out and made 2 conclusions. The first being that much of what we forget is lost soon after it is originally learned. The second being that the amount of forgetting eventually levels off.\nAround the same time Ebbinghaus developed the forgetting curve, psychologist Sigmund Freud theorized that people intentionally forgot things in order to push bad thoughts and feelings deep into their unconscious, a process he called \"repression\".\nThere is debate as to whether (or how often) memory repression really occurs and mainstream psychology holds that true memory repression occurs only very rarely.\nOne process model for memory was proposed by Richard Atkinson and Richard Shiffrin in the 1960s as a way to explain the operation of memory. This modal model of memory, also known as the Atkinson-Shiffrin model of memory, suggests there are three types of memory: sensory memory, short-term memory, and long-term memory. Each type of memory is separate in its capacity and duration. In the modal model, how quickly information is forgotten is related to the type of memory where that information is stored. Information in the first stage, sensory memory, is forgotten after only a few seconds. In the second stage, short-term memory, information is forgotten after about 20. While information in long-term memory can be remembered for years or even decades, it may be forgotten when the retrieval processes for that information fail.\nConcerning unwanted memories, modern terminology divides motivated forgetting into unconscious repression (which is disputed) and conscious thought suppression.\nMeasurements.\nForgetting can be measured in different ways all of which are based on recall:\nRecall.\nFor this type of measurement, a participant has to identify material that was previously learned. The participant is asked to remember a list of material. Later on they are shown the same list of material with additional information and they are asked to identify the material that was on the original list. The more they recognize, the less information is forgotten.\nFree recall and variants.\nFree recall is a basic paradigm used to study human memory. In a free recall task, a subject is presented a list of to-be-remembered items, one at a time. For example, an experimenter might read a list of 20 words aloud, presenting a new word to the subject every 4 seconds. At the end of the presentation of the list, the subject is asked to recall the items (e.g., by writing down as many items from the list as possible). It is called a free recall task because the subject is free to recall the items in any order that he or she desires.\nPrompted (cued) recall.\nPrompted recall is a slight variation of free recall that consists of presenting hints or prompts to increase the likelihood that the behavior will be produced. Usually these prompts are stimuli that were not there during the training period. Thus in order to measure the degree of forgetting, one can see how many prompts the subject misses or the number of prompts required to produce the behavior.\nRelearning method.\nThis method measures forgetting by the amount of training required to reach the previous level of performance. German psychologist Hermann Ebbinghaus (1885) used this method on himself. He memorized lists of nonsensical syllables until he could repeat the list two times without error. After a certain interval, he relearned the list and saw how long it would take him to do this task. If it took fewer times, then there had been less forgetting. His experiment was one of the first to study forgetting.\nTheories.\nThe four main theories of forgetting apparent in the study of psychology are as follows:\nCue-dependent forgetting.\nCue-dependent forgetting (also, context-dependent forgetting) or retrieval failure, is the failure to recall a memory due to missing stimuli or cues that were present at the time the memory was encoded. Encoding is the first step in creating and remembering a memory. How well something has been encoded in the memory can be measured by completing specific tests of retrieval. Examples of these tests would be explicit ones like cued recall or implicit tests like word fragment completion. Cue-dependent forgetting is one of five cognitive psychology theories of forgetting. This theory states that a memory is sometimes temporarily forgotten purely because it cannot be retrieved, but the proper cue can bring it to mind. A good metaphor for this is searching for a book in a library without the reference number, title, author or even subject. The information still exists, but without these cues retrieval is unlikely. Furthermore, a good retrieval cue must be consistent with the original encoding of the information. If the sound of the word is emphasized during the encoding process, the cue that should be used should also put emphasis on the phonetic quality of the word. Information is available however, just not readily available without these cues. Depending on the age of a person, retrieval cues and skills may not work as well. This is usually common in older adults but that is not always the case. When information is encoded into the memory and retrieved with a technique called spaced retrieval, this helps older adults retrieve the events stored in the memory better. There is also evidence from different studies that show age related changes in memory. These specific studies have shown that episodic memory performance does in fact decline with age and have made known that older adults produce vivid rates of forgetting when two items are combined and not encoded.\nOrganic causes.\nForgetting that occurs through physiological damage or dilapidation to the brain are referred to as organic causes of forgetting. These theories encompass the loss of information already retained in long-term memory or the inability to encode new information again. Examples include Alzheimer's, amnesia, dementia, consolidation theory and the gradual slowing down of the central nervous system due to aging.\nInterference theories.\nInterference theory refers to the idea that when the learning of something new causes forgetting of older material on the basis of competition between the two. This essentially states that memory's information may become confused or combined with other information during encoding, resulting in the distortion or disruption of memories. In nature, the interfering items are said to originate from an overstimulating environment. Interference theory exists in three branches: Proactive, Retroactive and Output. Retroactive and Proactive inhibition each referring in contrast to the other. Retroactive interference is when new information (memories) interferes with older information. On the other hand, proactive interference is when old information interferes with the retrieval of new information. This is sometimes thought to occur especially when memories are similar. Output Interference occurs when the initial act of recalling specific information interferes with the retrieval of the original information. This theory shows a contradiction: an extremely intelligent individual is expected to forget more hastily than one who has a slow mentality. For this reason, an intelligent individual has stored up more memory in his mind which will cause interferences and impair their ability to recall specific information. Based on current research, testing interference has only been carried out by recalling from a list of words rather than using situation from daily lives, thus it's hard to generalize the findings for this theory.\nTrace decay theory.\nDecay theory states that when something new is learned, a neurochemical, physical \"memory trace\" is formed in the brain and over time this trace tends to disintegrate, unless it is occasionally used. Decay theory states the reason we eventually forget something or an event is because the memory of it fades with time. If we do not attempt to look back at an event, the greater the interval time between the time when the event from happening and the time when we try to remember, the memory will start to fade. Time is the greatest impact in remembering an event.\nTrace decay theory explains memories that are stored in both short-term and long-term memory system, and assumes that the memories leave a trace in the brain. According to this theory, short-term memory (STM) can only retain information for a limited amount of time, around 15 to 30 seconds unless it is rehearsed. If it is not rehearsed, the information will start to gradually fade away and decay. Donald Hebb proposed that incoming information causes a series of neurons to create a neurological memory trace in the brain which would result in change in the morphological and/or chemical changes in the brain and would fade with time. Repeated firing causes a structural change in the synapses. Rehearsal of repeated firing maintains the memory in STM until a structural change is made. Therefore, forgetting happens as a result of automatic decay of the memory trace in brain. This theory states that the events between learning and recall have no effects on recall; the important factor that affects is the duration that the information has been retained. Hence, as longer time passes more of traces are subject to decay and as a result the information is forgotten.\nOne major problem about this theory is that in real-life situation, the time between encoding a piece of information and recalling it, is going to be filled with all different kinds of events that might happen to the individual. Therefore, it is difficult to conclude that forgetting is a result of only the time duration. It is also important to consider the effectiveness of this theory. Although it seems very plausible, it is about impossible to test. It is difficult to create a situation where there is a blank period of time between presenting the material and recalling it later.\nThis theory is supposedly contradicted by the fact that one is able to ride a bike even after not having done so for decades. \"Flashbulb memories\" are another piece of seemingly contradicting evidence. It is believed that certain memories \"trace decay\" while others don't. Sleep is believed to play a key role in halting trace decay, although the exact mechanism of this is unknown.\nImpairments and lack of forgetting.\nForgetting can have very different causes than simply removal of stored content. Forgetting can mean access problems, availability problems, or can have other reasons such as amnesia caused by an accident.\nAn inability to forget can cause distress, as with posttraumatic stress disorder and hyperthymesia (in which people have an extremely detailed autobiographical memory).\nSocial forgetting.\nPsychologists have called attention to \"social aspects of forgetting\". Though often loosely defined, social amnesia is generally considered to be the opposite of collective memory. \"Social amnesia\" was first discussed by Russell Jacoby, yet his use of the term was restricted to a narrow approach, which was limited to what he perceived to be a relative neglect of psychoanalytical theory in psychology. The cultural historian Peter Burke suggested that \"it may be worth investigating the social organization of forgetting, the rules of exclusion, suppression or repression, and the question of who wants whom to forget what\". In an in-depth historical study spanning two centuries, Guy Beiner proposed the term \"social forgetting\", which he distinguished from crude notions of \"collective amnesia\" and \"total oblivion\", arguing that \"social forgetting is to be found in the interface of public silence and more private remembrance\"."}
{"id": "10964", "revid": "7523687", "url": "https://en.wikipedia.org/wiki?curid=10964", "title": "Radical (Chemistry II)", "text": ""}
{"id": "10965", "revid": "41196973", "url": "https://en.wikipedia.org/wiki?curid=10965", "title": "Fay Wray", "text": "Vina Fay Wray (September 15, 1907 \u2013 August 8, 2004) was a Canadian-born American actress best remembered for starring as Ann Darrow in the 1933 film \"King Kong\". Through an acting career that spanned nearly six decades, Wray attained international recognition as an actress in horror films. She has been dubbed one of the early \"scream queens\".\nAfter appearing in minor film roles, Wray gained media attention after being selected as one of the \"WAMPAS Baby Stars\" in 1926. This led to her being contracted to Paramount Pictures as a teenager, where she made more than a dozen feature films. After leaving Paramount, she signed deals with various film companies, being cast in her first horror film roles, in addition to many other types of roles, including in \"The Bowery\" (1933) and \"Viva Villa\" (1934), both of which starred Wallace Beery. For RKO Radio Pictures, Inc., she starred in the film with which she is most identified, \"King Kong\" (1933). After the success of \"King Kong\", Wray made numerous appearances in both film and television; she retired in 1980.\nEarly life.\nWray was born on a ranch near Cardston in the province of Alberta, Canada to parents who were members of The Church of Jesus Christ of Latter-day Saints, Elvina Marguerite Jones, who was from Salt Lake City, Utah, and Joseph Heber Wray, who was from Kingston upon Hull, England. She was one of six children and was a granddaughter of LDS pioneer Daniel Webster Jones. Her ancestors came from England, Scotland, Ireland and Wales. Wray was never baptized a member of The Church of Jesus Christ of Latter-day Saints.\nHer family returned to the United States a few years after she was born; they moved to Salt Lake City in 1912 and moved to Lark, Utah, in 1914. In 1919, the Wray family returned to Salt Lake City, and then relocated to Hollywood, where Fay attended Hollywood High School.\nEarly acting career.\nIn 1923, Wray appeared in her first film at the age of 16, when she landed a role in a short historical film sponsored by a local newspaper. In the 1920s, Wray landed a major role in the silent film \"The Coast Patrol\" (1925), as well as uncredited bit parts at the Hal Roach Studios.\nIn 1926, the Western Association of Motion Picture Advertisers selected Wray as one of the \"WAMPAS Baby Stars\", a group of women whom they believed to be on the threshold of movie stardom. She was at the time under contract to Universal Studios, mostly co-starring in low-budget Westerns opposite Buck Jones.\nThe following year, Wray was signed to a contract with Paramount Pictures. In 1926, director Erich von Stroheim cast her as the main female lead in his film \"The Wedding March\", released by Paramount two years later. While the film was noted for its high budget and production values, it was a financial failure. It also gave Wray her first lead role. Wray stayed with Paramount to make more than a dozen films and made the transition from silent films to \"talkies\".\nHorror films and \"King Kong\".\nAfter leaving Paramount, Wray signed with various film companies. Under these deals, Wray was cast in a variety of horror films, including \"Doctor X\" (1932) and \"Mystery of the Wax Museum\" (1933). However, her best known films were produced under her deal with RKO Radio Pictures. Her first film with RKO was \"The Most Dangerous Game\" (1932), co-starring Joel McCrea. The production was filmed at night on the same jungle sets that were being used for \"King Kong\" during the day, and with Wray and Robert Armstrong starring in both movies.\n\"The Most Dangerous Game\" was followed by the release of Wray's most memorable film, \"King Kong\". According to Wray, Jean Harlow had been RKO's original choice, but because MGM put Harlow under exclusive contract during the pre-production phase of the film, she became unavailable. Wray was approached by director Merian C. Cooper to play the blonde captive of King Kong; the role of Ann Darrow was one she would most be associated with and she was paid $10,000 ($ in dollars) to play her. The film was a commercial success and Wray was reportedly proud that the film saved RKO from bankruptcy.\nLater career.\nShe continued to star in various films, including \"The Richest Girl in the World\", a second film with Joel McCrea, but by the early 1940s, her appearances became less frequent. She retired from acting in 1942 after her second marriage but due to financial exigencies she soon resumed her acting career, and over the next three decades, Wray appeared in several films and she also frequently appeared on television.\nWray was cast as Catherine Morrison in the 1953\u201354 sitcom \"The Pride of the Family.\" Paul Hartman played her husband, Albie Morrison. Natalie Wood and Robert Hyatt played their children, Ann and Junior Morrison, respectively. Wray appeared with fellow WAMPAS Baby Star Joan Crawford in \"Queen Bee\", released in 1955.\nWray appeared in three episodes of \"Perry Mason\": \"The Case of the Prodigal Parent\" (1958); \"The Case of the Watery Witness\" (1959), as murder victim Lorna Thomas; and \"The Case of the Fatal Fetish\" (1965), as voodoo practitioner Mignon Germaine. In 1959, Wray was cast as Tula Marsh in the episode \"The Second Happiest Day\" of \"Playhouse 90\". Other roles around this time were in the episodes \"Dip in the Pool\" (1958) and \"The Morning After\" of CBS's \"Alfred Hitchcock Presents\". In 1960, she appeared as Clara in an episode of \"77 Sunset Strip\", \"Who Killed Cock Robin?\" Another 1960 role was that of Mrs. Staunton, with Gigi Perreau as her daughter, in the episode \"Flight from Terror\" of \"The Islanders\".\nWray appeared in a 1961 episode of \"The Real McCoys\" titled \"Theatre in the Barn\". In 1963, she played Mrs. Brubaker in the episode \"You're So Smart, Why Can't You Be Good?\" of \"The Eleventh Hour\". She ended her acting career in the 1980 made-for-television film \"Gideon's Trumpet\".\nIn 1988, she published her autobiography \"On the Other Hand\". In her later years, Wray continued to make public appearances. In 1991, she was crowned Queen of the Beaux Arts Ball, presiding with King Herbert Huncke.\nShe was approached by James Cameron to play the part of Rose Dawson Calvert for his blockbuster \"Titanic\" (1997) with Kate Winslet to play her younger self, but she turned down the role, which ended up being played by Gloria Stuart. She was a special guest at the 70th Academy Awards, where the show's host Billy Crystal introduced her as the \"Beauty who charmed the Beast.\" She was the only 1920s Hollywood actress in attendance that evening with fellow 1930s actress Gloria Stuart nominated for an award. On October 3, 1998, she appeared at the Pine Bluff Film Festival, which showed \"The Wedding March\" with live orchestral accompaniment.\nIn January 2003, the 95-year-old Wray appeared at the 2003 Palm Beach International Film Festival to celebrate the Rick McKay documentary film \"\", where she was honored with a \"Legend in Film\" award. In her later years, she visited the Empire State Building frequently; in 1991, she was a guest of honor at the building's 60th anniversary, and in May 2004, she made one of her later public appearances. Her final public appearance was at an after-party at Sardi's restaurant in New York City, following the premiere of the documentary film \"Broadway: The Golden Age, by the Legends Who Were There\". \nPersonal life.\nWray married three times \u2013 to writers John Monk Saunders and Robert Riskin and the neurosurgeon Sanford Rothenberg (January 28, 1919 \u2013 January 4, 1991). She had three children: Susan Saunders, Victoria Riskin, and Robert Riskin Jr.\nShe became a naturalized citizen of the United States in 1933.\nDeath.\nIn 2004, Wray was approached by director Peter Jackson to appear in a small cameo for the 2005 remake of \"King Kong\". She met with Naomi Watts, who was to play the role of Ann Darrow. She politely declined the cameo, and claimed that the original \"Kong\" was the true \"King.\" Before the filming of the remake commenced, Wray died in her sleep of natural causes on August 8, 2004 in her apartment in Manhattan, five weeks before her 97th birthday. Wray is interred at the Hollywood Forever Cemetery in Hollywood, California.\nTwo days after her death, the lights of the Empire State Building were lowered for 15 minutes in her memory.\nHonors.\nIn 1989, Wray was awarded the Women in Film Crystal Award. Wray was honored with a Legend in Film award at the 2003 Palm Beach International Film Festival. For her contribution to the motion picture industry, Wray was honored with a star on the Hollywood Walk of Fame at 6349 Hollywood Blvd. She received a star posthumously on Canada's Walk of Fame in Toronto on June 5, 2005. A small park near Lee's Creek on Main Street in Cardston, Alberta, her birthplace, was named Fay Wray Park in her honour. The small sign at the edge of the park on Main Street has a silhouette of King Kong on it, remembering her role in the film \"King Kong\". A large oil portrait of Wray by Alberta artist Neil Boyle is on display in the Empress Theatre in Fort Macleod, Alberta. In May 2006, Wray became one of the first four entertainers to be honored by Canada Post by being featured on a postage stamp."}
{"id": "10967", "revid": "21956377", "url": "https://en.wikipedia.org/wiki?curid=10967", "title": "Forgetting curve", "text": "The forgetting curve hypothesizes the decline of memory retention in time. This curve shows how information is lost over time when there is no attempt to retain it. A related concept is the strength of memory that refers to the durability that memory traces in the brain. The stronger the memory, the longer period of time that a person is able to recall it. A typical graph of the forgetting curve purports to show that humans tend to halve their memory of newly learned knowledge in a matter of days or weeks unless they consciously review the learned material.\nThe forgetting curve supports one of the seven kinds of memory failures: transience, which is the process of forgetting that occurs with the passage of time.\nHistory.\nFrom 1880 to 1885, Hermann Ebbinghaus ran a limited, incomplete study on himself and published his hypothesis in 1885 as \"\" (later translated into English as \"Memory: A Contribution to Experimental Psychology\"). Ebbinghaus studied the memorisation of nonsense syllables, such as \"WID\" and \"ZOF\" (CVCs or Consonant\u2013Vowel\u2013Consonant) by repeatedly testing himself after various time periods and recording the results. He plotted these results on a graph creating what is now known as the \"forgetting curve\". Ebbinghaus investigated the rate of forgetting, but not the effect of spaced repetition on the increase in retrievability of memories.\nEbbinghaus's publication also included an equation to approximate his forgetting curve:\nformula_1\nHere, formula_2 represents 'Savings' expressed as a percentage, and formula_3 represents time in minutes. Savings is defined as the relative amount of time saved on the second learning trial as a result of having had the first. A savings of 100% would indicate that all items were still known from the first trial. A 75% savings would mean that relearning missed items required 25% as long as the original learning session (to learn all items). 'Savings' is thus, analogous to retention rate.\nIn 2015, an attempt to replicate the forgetting curve with one study subject has shown the experimental results similar to Ebbinghaus' original data.\nEbbinghaus' experiment contributed a lot to experimental psychology. He was the first to carry out a series of well-designed experiments on the subject of forgetting, and he was one of the first to choose artificial stimuli in the research of experimental psychology. Since his introduction of nonsense syllables, a large number of experiments in experimental psychology has been based on highly controlled artificial stimuli.\nIncreasing rate of learning.\nHermann Ebbinghaus hypothesized that the speed of forgetting depends on a number of factors such as the difficulty of the learned material (e.g. how meaningful it is), its representation and other physiological factors such as stress and sleep. He further hypothesized that the basal forgetting rate differs little between individuals. He concluded that the difference in performance can be explained by mnemonic representation skills.\nHe went on to hypothesize that basic training in mnemonic techniques can help overcome those differences in part. He asserted that the best methods for increasing the strength of memory are:\nHis premise was that each repetition in learning increases the optimum interval before the next repetition is needed (for near-perfect retention, initial repetitions may need to be made within days, but later they can be made after years). He discovered that information is easier to recall when it's built upon things you already know, and the forgetting curve was flattened by every repetition. It appeared that by applying frequent training in learning, the information was solidified by repeated recalling.\nLater research also suggested that, other than the two factors Ebbinghaus proposed, higher original learning would also produce slower forgetting. The more information was originally learned, the slower the forgetting rate would be.\nSpending time each day to remember information will greatly decrease the effects of the forgetting curve. Some learning consultants claim reviewing material in the first 24 hours after learning information is the optimum time to re-read notes and reduce the amount of knowledge forgotten. Evidence suggests waiting 10\u201320% of the time towards when the information will be needed is the optimum time for a single review.\nHowever, some memories remain free from the detrimental effects of interference and do not necessarily follow the typical forgetting curve as various noise and outside factors influence what information would be remembered. There is debate among supporters of the hypothesis about the shape of the curve for events and facts that are more significant to the subject. Some supporters, for example, suggest that memories of shocking events such as the Kennedy Assassination or 9/11 are vividly imprinted in memory (flashbulb memory). Others have compared contemporaneous written recollections with recollections recorded years later, and found considerable variations as the subject's memory incorporates after-acquired information. There is considerable research in this area as it relates to eyewitness identification testimony, and eyewitness accounts are found demonstrably unreliable.\nEquations.\nMany equations have since been proposed to approximate forgetting, perhaps the simplest being an exponential curve described by the equation\nformula_4\nwhere formula_5 is retrievability (a measure of how easy it is to retrieve a piece of information from memory), formula_6 is stability of memory (determines how fast formula_5 falls over time in the absence of training, testing or other recall), and formula_3 is time.\nSimple equations such as this one were not found to provide a good fit to the available data."}
{"id": "10969", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=10969", "title": "Field-programmable gate array", "text": "A field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturinghence the term \"field-programmable\". The FPGA configuration is generally specified using a hardware description language (HDL), similar to that used for an application-specific integrated circuit (ASIC). Circuit diagrams were previously used to specify the configuration, but this is increasingly rare due to the advent of electronic design automation tools.\nFPGAs contain an array of programmable logic blocks, and a hierarchy of \"reconfigurable interconnects\" allowing blocks to be \"wired together\", like many logic gates that can be inter-wired in different configurations. Logic blocks can be configured to perform complex combinational functions, or merely simple logic gates like AND and XOR. In most FPGAs, logic blocks also include memory elements, which may be simple flip-flops or more complete blocks of memory. Many FPGAs can be reprogrammed to implement different logic functions, allowing flexible reconfigurable computing as performed in computer software.\nFPGAs have a remarkable role in embedded system development due to their capability to start system software (SW) development simultaneously with hardware (HW), \nenable system performance simulations at a very early phase of the development, and allow various system partitioning (SW and HW) trials and iterations before final freezing of the system architecture.\nTechnical design.\nContemporary field-programmable gate arrays (FPGAs) have large resources of logic gates and RAM blocks to implement complex digital computations. As FPGA designs employ very fast I/O rates and bidirectional data buses, it becomes a challenge to verify correct timing of valid data within setup time and hold time.\nFloor planning enables resource allocation within FPGAs to meet these time constraints. FPGAs can be used to implement any logical function that an ASIC can perform. The ability to update the functionality after shipping, partial re-configuration of a portion of the design and the low non-recurring engineering costs relative to an ASIC design (notwithstanding the generally higher unit cost), offer advantages for many applications.\nSome FPGAs have analog features in addition to digital functions. The most common analog feature is a programmable slew rate on each output pin, allowing the engineer to set low rates on lightly loaded pins that would otherwise ring or couple unacceptably, and to set higher rates on heavily loaded pins on high-speed channels that would otherwise run too slowly. Also common are quartz-crystal oscillators, on-chip resistance-capacitance oscillators, and phase-locked loops with embedded voltage-controlled oscillators used for clock generation and management as well as for high-speed serializer-deserializer (SERDES) transmit clocks and receiver clock recovery. Fairly common are differential comparators on input pins designed to be connected to differential signaling channels. A few \"mixed signal FPGAs\" have integrated peripheral analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) with analog signal conditioning blocks allowing them to operate as a system-on-a-chip (SoC). Such devices blur the line between an FPGA, which carries digital ones and zeros on its internal programmable interconnect fabric, and field-programmable analog array (FPAA), which carries analog values on its internal programmable interconnect fabric.\nHistory.\nThe FPGA industry sprouted from programmable read-only memory (PROM) and programmable logic devices (PLDs). PROMs and PLDs both had the option of being programmed in batches in a factory or in the field (field-programmable). However, programmable logic was hard-wired between logic gates.\nAltera was founded in 1983 and delivered the industry's first reprogrammable logic device in 1984 \u2013 the EP300 \u2013 which featured a quartz window in the package that allowed users to shine an ultra-violet lamp on the die to erase the EPROM cells that held the device configuration. In December 2015, Intel acquired Altera.\nXilinx co-founders Ross Freeman and Bernard Vonderschmitt invented the first commercially viable field-programmable gate array in 1985\u00a0\u2013 the XC2064. The XC2064 had programmable gates and programmable interconnects between gates, the beginnings of a new technology and market. The XC2064 had 64 configurable logic blocks (CLBs), with two three-input lookup tables (LUTs). More than 20 years later, Freeman was entered into the National Inventors Hall of Fame for his invention.\nIn 1987, the Naval Surface Warfare Center funded an experiment proposed by Steve Casselman to develop a computer that would implement 600,000 reprogrammable gates. Casselman was successful and a patent related to the system was issued in 1992.\nAltera and Xilinx continued unchallenged and quickly grew from 1985 to the mid-1990s when competitors sprouted up, eroding a significant portion of their market share. By 1993, Actel (now Microsemi) was serving about 18 percent of the market. By 2013, Altera (31 percent), Actel (10 percent) and Xilinx (36 percent) together represented approximately 77 percent of the FPGA market.\nThe 1990s were a period of rapid growth for FPGAs, both in circuit sophistication and the volume of production. In the early 1990s, FPGAs were primarily used in telecommunications and networking. By the end of the decade, FPGAs found their way into consumer, automotive, and industrial applications.\nCompanies like Microsoft have started to use FPGAs to accelerate high-performance, computationally intensive systems (like the data centers that operate their Bing search engine), due to the performance per watt advantage FPGAs deliver. Microsoft began using FPGAs to accelerate Bing in 2014, and in 2018 began deploying FPGAs across other data center workloads for their Azure cloud computing platform.\nIntegration.\nIn 2012 the coarse-grained architectural approach was taken a step further by combining the logic blocks and interconnects of traditional FPGAs with embedded microprocessors and related peripherals to form a complete \"system on a programmable chip\". This work mirrors the architecture created by Ron Perloff and Hanan Potash of Burroughs Advanced Systems Group in 1982 which combined a reconfigurable CPU architecture on a single chip called the SB24.\nExamples of such hybrid technologies can be found in the Xilinx Zynq-7000 all Programmable SoC, which includes a 1.0\u00a0GHz dual-core ARM Cortex-A9 MPCore processor embedded within the FPGA's logic fabric or in the Altera Arria V FPGA, which includes an 800\u00a0MHz dual-core ARM Cortex-A9 MPCore. The Atmel FPSLIC is another such device, which uses an AVR processor in combination with Atmel's programmable logic architecture. The Microsemi SmartFusion devices incorporate an ARM Cortex-M3 hard processor core (with up to 512\u00a0kB of flash and 64\u00a0kB of RAM) and analog peripherals such as a multi-channel analog-to-digital converters and digital-to-analog converters to their flash memory-based FPGA fabric.\nSoft core.\nAn alternate approach to using hard-macro processors is to make use of soft processor IP cores that are implemented within the FPGA logic. Nios II, MicroBlaze and Mico32 are examples of popular softcore processors. Many modern FPGAs are programmed at \"run time\", which has led to the idea of reconfigurable computing or reconfigurable systems\u00a0\u2013 CPUs that reconfigure themselves to suit the task at hand. Additionally, new, non-FPGA architectures are beginning to emerge. Software-configurable microprocessors such as the Stretch S5000 adopt a hybrid approach by providing an array of processor cores and FPGA-like programmable cores on the same chip.\nTimelines.\nDesign starts.\nA \"design start\" is a new custom design for implementation on an FPGA.\nComparisons.\nTo ASICs.\nHistorically, FPGAs have been slower, less energy efficient and generally achieved less functionality than their fixed ASIC counterparts. An older study showed that designs implemented on FPGAs need on average 40 times as much area, draw 12 times as much dynamic power, and run at one third the speed of corresponding ASIC implementations.\nMore recently, FPGAs such as the Xilinx Virtex-7 or the Altera Stratix 5 have come to rival corresponding ASIC and ASSP (\"Application-specific standard part\", such as a standalone USB interface chip) solutions by providing significantly reduced power usage, increased speed, lower materials cost, minimal implementation real-estate, and increased possibilities for re-configuration 'on-the-fly'. A design that included 6 to 10 ASICs can now be achieved using only one FPGA.\nAdvantages of FPGAs include the ability to re-program when already deployed (i.e. \"in the field\") to fix bugs, and often include shorter time to market and lower non-recurring engineering costs. Vendors can also take a middle road via FPGA prototyping: developing their prototype hardware on FPGAs, but manufacture their final version as an ASIC so that it can no longer be modified after the design has been committed. This is often also the case with new processor designs.\nTrends.\nSome FPGAs have the capability of partial re-configuration that lets one portion of the device be re-programmed while other portions continue running.\nComplex Programmable Logic Devices (CPLD).\nThe primary differences between complex programmable logic devices (CPLDs) and FPGAs are architectural. A CPLD has a comparatively restrictive structure consisting of one or more programmable sum-of-products logic arrays feeding a relatively small number of clocked registers. As a result, CPLDs are less flexible, but have the advantage of more predictable timing delays and FPGA architectures, on the other hand, are dominated by interconnect. This makes them far more flexible (in terms of the range of designs that are practical for implementation on them) but also far more complex to design for, or at least requiring more complex electronic design automation (EDA) software.\nIn practice, the distinction between FPGAs and CPLDs is often one of size as FPGAs are usually much larger in terms of resources than CPLDs. Typically only FPGAs contain more complex embedded functions such as adders, multipliers, memory, and serializer/deserializers. Another common distinction is that CPLDs contain embedded flash memory to store their configuration while FPGAs usually require external non-volatile memory (but not always).\nWhen a design requires simple instant-on (logic is already configured at power-up) CPLDs are generally preferred. For most other applications FPGAs are generally preferred. Sometimes both CPLDs and FPGAs are used in a single system design. In those designs, CPLDs generally perform glue logic functions, and are responsible for \u201cbooting\u201d the FPGA as well as controlling reset and boot sequence of the complete circuit board. Therefore, depending on the application it may be judicious to use both FPGAs and CPLDs in a single design.\nSecurity considerations.\nFPGAs have both advantages and disadvantages as compared to ASICs or secure microprocessors, concerning hardware security. FPGAs' flexibility makes malicious modifications during fabrication a lower risk. Previously, for many FPGAs, the design bitstream was exposed while the FPGA loads it from external memory (typically on every power-on). All major FPGA vendors now offer a spectrum of security solutions to designers such as bitstream encryption and authentication. For example, Altera and Xilinx offer AES encryption (up to 256-bit) for bitstreams stored in an external flash memory.\nFPGAs that store their configuration internally in nonvolatile flash memory, such as Microsemi's ProAsic 3 or Lattice's XP2 programmable devices, do not expose the bitstream and do not need encryption. In addition, flash memory for a lookup table provides single event upset protection for space applications. Customers wanting a higher guarantee of tamper resistance can use write-once, antifuse FPGAs from vendors such as Microsemi.\nWith its Stratix 10 FPGAs and SoCs, Altera introduced a Secure Device Manager and physically uncloneable functions to provide high levels of protection against physical attacks.\nIn 2012 researchers Sergei Skorobogatov and Christopher Woods demonstrated that FPGAs can be vulnerable to hostile intent. They discovered a critical backdoor vulnerability had been manufactured in silicon as part of the Actel/Microsemi ProAsic 3 making it vulnerable on many levels such as reprogramming crypto and access keys, accessing unencrypted bitstream, modifying low-level silicon features, and extracting configuration data.\nApplications.\nAn FPGA can be used to solve any problem which is computable. This is trivially proven by the fact that FPGAs can be used to implement a soft microprocessor, such as the Xilinx MicroBlaze or Altera Nios II. Their advantage lies in that they are significantly faster for some applications because of their parallel nature and optimality in terms of the number of gates used for certain processes.\nFPGAs originally began as competitors to CPLDs to implement glue logic for printed circuit boards. As their size, capabilities, and speed increased, FPGAs took over additional functions to the point where some are now marketed as full systems on chips (SoCs). Particularly with the introduction of dedicated multipliers into FPGA architectures in the late 1990s, applications which had traditionally been the sole reserve of digital signal processor hardware (DSPs) began to incorporate FPGAs instead.\nAnother trend in the use of FPGAs is hardware acceleration, where one can use the FPGA to accelerate certain parts of an algorithm and share part of the computation between the FPGA and a generic processor. The search engine Bing is noted for adopting FPGA acceleration for its search algorithm in 2014. , FPGAs are seeing increased use as AI accelerators including Microsoft's so-termed \"Project Catapult\" and for accelerating artificial neural networks for machine learning applications.\nTraditionally, FPGAs have been reserved for specific vertical applications where the volume of production is small. For these low-volume applications, the premium that companies pay in hardware cost per unit for a programmable chip is more affordable than the development resources spent on creating an ASIC. , new cost and performance dynamics have broadened the range of viable applications.\nThe company Gigabyte created an i-RAM card which used a Xilinx FPGA although a custom made chip would be cheaper if made in large quantities. The FPGA was chosen to bring it quickly to market and the initially run was only to be 1000 units making an FPGA the best choice. This device allows people to use computer ram as a hard drive. \nArchitecture.\nLogic blocks.\nThe most common FPGA architecture consists of an array of logic blocks, I/O pads, and routing channels. Generally, all the routing channels have the same width (number of wires). Multiple I/O pads may fit into the height of one row or the width of one column in the array.\nAn application circuit must be mapped into an FPGA with adequate resources. While the number of CLBs/LABs and I/Os required is easily determined from the design, the number of routing tracks needed may vary considerably even among designs with the same amount of logic.\nFor example, a crossbar switch requires much more routing than a systolic array with the same gate count. Since unused routing tracks increase the cost (and decrease the performance) of the part without providing any benefit, FPGA manufacturers try to provide just enough tracks so that most designs that will fit in terms of lookup tables (LUTs) and I/Os can be routed. This is determined by estimates such as those derived from Rent's rule or by experiments with existing designs. , network-on-chip architectures for routing and interconnection are being developed.\nIn general, a logic block consists of a few logical cells (called ALM, LE, slice etc.). A typical cell consists of a 4-input LUT, a full adder (FA) and a D-type flip-flop, as shown above. The LUTs are in this figure split into two 3-input LUTs. In \"normal mode\" those are combined into a 4-input LUT through the left multiplexer (mux). In \"arithmetic\" mode, their outputs are fed to the adder. The selection of mode is programmed into the middle MUX. The output can be either synchronous or asynchronous, depending on the programming of the mux to the right, in the figure example. In practice, entire or parts of the adder are stored as functions into the LUTs in order to save space.\nHard blocks.\nModern FPGA families expand upon the above capabilities to include higher level functionality fixed in silicon. Having these common functions embedded in the circuit reduces the area required and gives those functions increased speed compared to building them from logical primitives. Examples of these include multipliers, generic DSP blocks, embedded processors, high speed I/O logic and embedded memories.\nHigher-end FPGAs can contain high speed multi-gigabit transceivers and \"hard IP cores\" such as processor cores, Ethernet medium access control units, PCI/PCI Express controllers, and external memory controllers. These cores exist alongside the programmable fabric, but they are built out of transistors instead of LUTs so they have ASIC-level performance and power consumption without consuming a significant amount of fabric resources, leaving more of the fabric free for the application-specific logic. The multi-gigabit transceivers also contain high performance analog input and output circuitry along with high-speed serializers and deserializers, components which cannot be built out of LUTs. Higher-level physical layer (PHY) functionality such as line coding may or may not be implemented alongside the serializers and deserializers in hard logic, depending on the FPGA.\nClocking.\nMost of the circuitry built inside of an FPGA is synchronous circuitry that requires a clock signal. FPGAs contain dedicated global and regional routing networks for clock and reset so they can be delivered with minimal skew. Also, FPGAs generally contain analog phase-locked loop and/or delay-locked loop components to synthesize new clock frequencies as well as attenuate jitter. Complex designs can use multiple clocks with different frequency and phase relationships, each forming separate clock domains. These clock signals can be generated locally by an oscillator or they can be recovered from a high speed serial data stream. Care must be taken when building clock domain crossing circuitry to avoid metastability. FPGAs generally contain block RAMs that are capable of working as dual port RAMs with different clocks, aiding in the construction of building FIFOs and dual port buffers that connect differing clock domains.\n3D architectures.\nTo shrink the size and power consumption of FPGAs, vendors such as Tabula and Xilinx have introduced 3D or stacked architectures. Following the introduction of its 28\u00a0nm 7-series FPGAs, Xilinx said that several of the highest-density parts in those FPGA product lines will be constructed using multiple dies in one package, employing technology developed for 3D construction and stacked-die assemblies.\nXilinx's approach stacks several (three or four) active FPGA dies side by side on a silicon interposer\u00a0\u2013 a single piece of silicon that carries passive interconnect. The multi-die construction also allows different parts of the FPGA to be created with different process technologies, as the process requirements are different between the FPGA fabric itself and the very high speed 28 Gbit/s serial transceivers. An FPGA built in this way is called a \"heterogeneous FPGA\".\nAltera's heterogeneous approach involves using a single monolithic FPGA die and connecting other die/technologies to the FPGA using Intel's embedded multi-die interconnect bridge (EMIB) technology.\nDesign and programming.\nTo define the behavior of the FPGA, the user provides a design in a hardware description language (HDL) or as a schematic design. The HDL form is more suited to work with large structures because it's possible to specify high-level functional behavior rather than drawing every piece by hand. However, schematic entry can allow for easier visualization of a design and its component modules.\nUsing an electronic design automation tool, a technology-mapped netlist is generated. The netlist can then be fit to the actual FPGA architecture using a process called place-and-route, usually performed by the FPGA company's proprietary place-and-route software. The user will validate the map, place and route results via timing analysis, simulation, and other verification and validation methodologies. Once the design and validation process is complete, the binary file generated, typically using the FPGA vendor's proprietary software, is used to (re-)configure the FPGA. This file is transferred to the FPGA/CPLD via a serial interface (JTAG) or to an external memory device like an EEPROM.\nThe most common HDLs are VHDL and Verilog as well as extensions such as SystemVerilog. However, in an attempt to reduce the complexity of designing in HDLs, which have been compared to the equivalent of assembly languages, there are moves to raise the abstraction level through the introduction of alternative languages. National Instruments' LabVIEW graphical programming language (sometimes referred to as \"G\") has an FPGA add-in module available to target and program FPGA hardware. Verilog was created to simplify the process making HDL more robust and flexible. Verilog is currently the most popular. Verilog creates a level of abstraction to hide away the details of its implementation. Verilog has a C-like syntax, unlike VHDL. \nTo simplify the design of complex systems in FPGAs, there exist libraries of predefined complex functions and circuits that have been tested and optimized to speed up the design process. These predefined circuits are commonly called \"intellectual property (IP) cores\", and are available from FPGA vendors and third-party IP suppliers. They are rarely free, and typically released under proprietary licenses. Other predefined circuits are available from developer communities such as OpenCores (typically released under free and open source licenses such as the GPL, BSD or similar license), and other sources. Such designs are known as \"open-source hardware.\"\nIn a typical design flow, an FPGA application developer will simulate the design at multiple stages throughout the design process. Initially the RTL description in VHDL or Verilog is simulated by creating test benches to simulate the system and observe results. Then, after the synthesis engine has mapped the design to a netlist, the netlist is translated to a gate-level description where simulation is repeated to confirm the synthesis proceeded without errors. Finally the design is laid out in the FPGA at which point propagation delays can be added and the simulation run again with these values back-annotated onto the netlist.\nMore recently, OpenCL (Open Computing Language) is being used by programmers to take advantage of the performance and power efficiencies that FPGAs provide. OpenCL allows programmers to develop code in the C programming language and target FPGA functions as OpenCL kernels using OpenCL constructs. For further information, see high-level synthesis and C to HDL.\nMajor manufacturers.\nIn 2016, long-time industry rivals Xilinx (now AMD) and Altera (now an Intel subsidiary) were the FPGA market leaders. At that time, they controlled nearly 90 percent of the market.\nBoth Xilinx and Altera provide proprietary electronic design automation software for Windows and Linux (ISE/Vivado and Quartus) which enables engineers to design, analyze, simulate, and synthesize (compile) their designs.\nOther manufacturers include:\nIn March 2010, Tabula announced their FPGA technology that uses time-multiplexed logic and interconnect that claims potential cost savings for high-density applications. On March 24, 2015, Tabula officially shut down.\nOn June 1, 2015, Intel announced it would acquire Altera for approximately $16.7 billion and completed the acquisition on December 30, 2015.\nOn October 27, 2020, AMD announced it would acquire Xilinx."}
{"id": "10970", "revid": "668752", "url": "https://en.wikipedia.org/wiki?curid=10970", "title": "Forgetting rate", "text": ""}
{"id": "10971", "revid": "41199609", "url": "https://en.wikipedia.org/wiki?curid=10971", "title": "Free-running sleep", "text": "Free-running sleep is a sleep pattern that is not adjusted (entrained) to the 24-hour cycle in nature nor to any artificial cycle.\nIt occurs as the sleep disorder non-24-hour sleep-wake disorder or artificially as part of experiments used in the study of circadian and other rhythms in biology. Study subjects are shielded from all time cues, often by a constant light protocol, by a constant dark protocol or by the use of light/dark conditions to which the organism cannot entrain such as the ultrashort protocol of one hour dark and two hours light. Also, limited amounts of food may be made available at short intervals so as to avoid entrainment to mealtimes. Subjects are thus forced to live by their internal circadian \"clocks\".\nBackground.\nThe individual's or animal's circadian phase can be known only by the monitoring of some kind of output of the circadian system, the internal \"body clock\". The researcher can precisely determine, for example, the daily cycles of gene-activity, body temperature, blood pressure, hormone secretion and/or sleep and activity/alertness. Alertness in humans can be determined by many kinds of verbal and non-verbal tests; activity in animals by observation, for example of wheel-running in rodents.\nWhen animals or people \"free-run\", experiments can be done to see what sort of signals, known as zeitgebers, are effective in entrainment. Also, much work has been done to see how long or short a circadian cycle can be entrained to various organisms. For example, some animals can be entrained to a 22-hour day, but they can not be entrained to a 20-hour day. In recent studies funded by the U.S. space industry, it has been shown that most humans can be entrained to a 23.5-hour day and to a 24.65-hour day.\nThe effect of unintended time cues is called \"masking\" and can totally confound experimental results. Examples of masking are morning rush traffic audible to the subjects, or researchers or maintenance staff visiting subjects on a regular schedule.\nIn humans.\nNon-24-hour sleep\u2013wake disorder, also referred to as \"free-running disorder\" (FRD) or \"Non-24\", is one of the circadian rhythm sleep disorders in humans. It affects more than half of people who are totally blind and a smaller number of sighted individuals.\nAmong blind people, the cause is the inability to register, and therefore to entrain to, light cues. The many blind people who do entrain to the 24-hour light/dark cycle have eyes with functioning retinas including operative non-visual light-sensitive cells, ipRGCs. These ganglion cells, which contain melanopsin, convey their signals to the \"circadian clock\" via the retinohypothalamic tract (branching off from the optic nerve), linking the retina to the pineal gland.\nAmong sighted individuals, Non-24 usually first appears in the teens or early twenties. As with delayed sleep phase disorder (DSPS or DSPD), in the absence of neurological damage due to trauma or stroke, cases almost never appear after the age of 30. Non-24 affects more sighted males than sighted females. A quarter of sighted individuals with Non-24 also have an associated psychiatric condition, and a quarter of them have previously shown symptoms of DSPS."}
{"id": "10972", "revid": "5718152", "url": "https://en.wikipedia.org/wiki?curid=10972", "title": "Fenrir", "text": " \n Fenrir (Old Norse: \"fen-dweller\") or Fenris\u00falfr (Old Norse: \"Fenrir's wolf\", often translated \"Fenris-wolf\"), also referred to as Hr\u00f3\u00f0vitnir (\"fame-wolf\") and V\u00e1nagandr (\"monster of the [River] V\u00e1n\"), or Vanargand, is a monstrous wolf in Norse mythology. Fenrir, together with Hel and the World Serpent, is a child of Loki and giantess Angrbo\u00f0a. He is attested in the \"Poetic Edda\", compiled in the 13th century from earlier traditional sources, and the \"Prose Edda\" and \"Heimskringla\", written in the 13th century by Snorri Sturluson. In both the \"Poetic Edda\" and \"Prose Edda\", Fenrir is the father of the wolves Sk\u00f6ll and Hati Hr\u00f3\u00f0vitnisson, is a son of Loki and is foretold to kill the god Odin during the events of Ragnar\u00f6k, but will in turn be killed by Odin's son V\u00ed\u00f0arr.\nIn the \"Prose Edda\", additional information is given about Fenrir, including that, due to the gods' knowledge of prophecies foretelling great trouble from Fenrir and his rapid growth, the gods bound him and as a result Fenrir bit off the right hand of the god T\u00fdr. Depictions of Fenrir have been identified on various objects and scholarly theories have been proposed regarding Fenrir's relation to other canine beings in Norse mythology. Fenrir has been the subject of artistic depictions and he appears in literature.\nAttestations.\n\"Poetic Edda\".\nFenrir is mentioned in three stanzas of the poem \"V\u00f6lusp\u00e1\" and in two stanzas of the poem \"Vaf\u00fer\u00fa\u00f0nism\u00e1l\". In stanza 40 of the poem \"V\u00f6lusp\u00e1\", a v\u00f6lva divulges to Odin that, in the east, an old woman sat in the forest J\u00e1rnvi\u00f0r \"and bred there the broods of Fenrir. There will come from them all one of that number to be a moon-snatcher in troll's skin.\" Further into the poem the v\u00f6lva foretells that Odin will be consumed by Fenrir at Ragnar\u00f6k:\nIn the stanza that follows the v\u00f6lva describes that Odin's \"tall child of Triumph's Sire\" (Odin's son V\u00ed\u00f0arr) will then come to \"strike at the beast of slaughter\" and with his hands he will drive a sword into the heart of \"Hve\u00f0rungr's son,\" avenging the death of his father.\nIn the first of two stanzas mentioning Fenrir in \"Vaf\u00fer\u00fa\u00f0nism\u00e1l\" Odin poses a question to the wise j\u00f6tunn Vaf\u00fer\u00fa\u00f0nir:\nIn the stanza that follows Vaf\u00fer\u00fa\u00f0nir responds that S\u00f3l (here referred to as \"\u00c1lfr\u00f6\u00f0ull\") will bear a daughter before Fenrir attacks her, and that this daughter shall continue the paths of her deceased mother through the heavens.\n\"Prose Edda\".\nIn the \"Prose Edda\", Fenrir is mentioned in three books: \"Gylfaginning\", \"Sk\u00e1ldskaparm\u00e1l\" and \"H\u00e1ttatal\".\n\"Gylfaginning\" chapters 13 and 25.\nIn chapter 13 of the \"Prose Edda\" book \"Gylfaginning\", Fenrir is first mentioned in a stanza quoted from \"V\u00f6lusp\u00e1\". Fenrir is first mentioned in prose in chapter 25, where the enthroned figure of High tells Gangleri (described as King Gylfi in disguise) about the god T\u00fdr. High says that one example of T\u00fdr's bravery is that when the \u00c6sir were luring Fenrir (referred to here as \"Fenris\u00falfr\") to place the fetter Gleipnir on the wolf, T\u00fdr placed his hand within the wolf's mouth as a pledge. This was done at Fenrir's own request because he did not trust that the \u00c6sir would let him go. As a result, when the \u00c6sir refused to release him, he bit off T\u00fdr's hand at a location \"now called the wolf-joint\" (the wrist), causing T\u00fdr to be one-handed and \"not considered to be a promoter of settlements between people.\"\n\"Gylfaginning\" chapter 34.\nIn chapter 34, High describes Loki, and says that Loki had three children with a woman named Angrbo\u00f0a located in the land of J\u00f6tunheimr; Fenris\u00falfr, the serpent J\u00f6rmungandr, and the female being Hel. High continues that, once the gods found that these three children were being brought up in the land of J\u00f6tunheimr, and when the gods \"traced prophecies that from these siblings great mischief and disaster would arise for them\" the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.\nHigh says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw J\u00f6rmungandr into \"that deep sea that lies round all lands\", and then threw Hel into Niflheim, and bestowed upon her authority over nine worlds. However, the \u00c6sir brought up the wolf \"at home\", and only T\u00fdr had the courage to approach Fenrir, and give Fenrir food. The gods noticed that Fenrir was growing rapidly every day, and since all prophecies foretold that Fenrir was destined to cause them harm, the gods formed a plan. The gods prepared three fetters: The first, greatly strong, was called Leyding. They brought Leyding to Fenrir and suggested that the wolf try his strength with it. Fenrir judged that it was not beyond his strength, and so let the gods do what they wanted with it. At Fenrir's first kick the bind snapped, and Fenrir loosened himself from Leyding. The gods made a second fetter, twice as strong, and named it Dromi. The gods asked Fenrir to try the new fetter, and that should he break this feat of engineering, Fenrir would achieve great fame for his strength. Fenrir considered that the fetter was very strong, yet also that his strength had grown since he broke Leyding, yet that he would have to take some risks if he were to become famous. Fenrir allowed them to place the fetter.\nWhen the \u00c6sir exclaimed that they were ready, Fenrir shook himself, knocked the fetter to the ground, strained hard, and kicking with his feet, snapped the fetter \u2013 breaking it into pieces that flew far into the distance. High says that, as a result, to \"loose from Leyding\" or to \"strike out of Dromi\" have become sayings for when something is achieved with great effort. The \u00c6sir started to fear that they would not be able to bind Fenrir, and so Odin sent Freyr's messenger Sk\u00edrnir down into the land of Svart\u00e1lfaheimr to \"some dwarfs\" and had them make a fetter called Gleipnir. The dwarves constructed Gleipnir from six mythical ingredients. After an exchange between Gangleri and High, High continues that the fetter was smooth and soft as a silken ribbon, yet strong and firm. The messenger brought the ribbon to the \u00c6sir, and they thanked him heartily for completing the task.\nThe \u00c6sir went out on to the lake Amsvartnir sent for Fenrir to accompany them, and continued to the island Lyngvi (Old Norse \"a place overgrown with heather\"). The gods showed Fenrir the silken fetter Gleipnir, told him to tear it, stated that it was much stronger than it appeared, passed it among themselves, used their hands to pull it, and yet it did not tear. However, they said that Fenrir would be able to tear it, to which Fenrir replied:\n\"It looks to me that with this ribbon as though I will gain no fame from it if I do tear apart such a slender band, but if it is made with art and trickery, then even if it does look thin, this band is not going on my legs.\"\nThe \u00c6sir said Fenrir would quickly tear apart a thin silken strip, noting that Fenrir earlier broke great iron binds, and added that if Fenrir wasn't able to break slender Gleipnir then Fenrir is nothing for the gods to fear, and as a result would be freed. Fenrir responded:\n\"If you bind me so that I am unable to release myself, then you will be standing by in such a way that I should have to wait a long time before I got any help from you. I am reluctant to have this band put on me. But rather than that you question my courage, let someone put his hand in my mouth as a pledge that this is done in good faith.\"\nWith this statement, all of the \u00c6sir look to one another, finding themselves in a dilemma. Everyone refused to place their hand in Fenrir's mouth until T\u00fdr put out his right hand and placed it into the wolf's jaws. When Fenrir kicked, Gleipnir caught tightly, and the more Fenrir struggled, the stronger the band grew. At this, everyone laughed, except T\u00fdr, who there lost his right hand. When the gods knew that Fenrir was fully bound, they took a cord called Gelgja (Old Norse \"fetter\") hanging from Gleipnir, inserted the cord through a large stone slab called Gj\u00f6ll (Old Norse \"scream\"), and the gods fastened the stone slab deep into the ground. After, the gods took a great rock called Thviti (Old Norse \"hitter, batterer\"), and thrust it even further into the ground as an anchoring peg. Fenrir reacted violently; he opened his jaws very wide, and tried to bite the gods. \nThen the gods thrust a sword into his mouth. Its hilt touched the lower jaw and its point the upper one; by means of it the jaws of the wolf were spread apart and the wolf gagged.\nFenrir \"howled horribly,\" saliva ran from his mouth, and this saliva formed the river V\u00e1n (Old Norse \"hope\"). There Fenrir will lie until Ragnar\u00f6k. Gangleri comments that Loki created a \"pretty terrible family\" though important, and asks why the \u00c6sir did not just kill Fenrir there since they expected great malice from him. High replies that \"so greatly did the gods respect their holy places and places of sanctuary that they did not want to defile them with the wolf's blood even though the prophecies say that he will be the death of Odin.\"\n\"Gylfaginning\" chapters 38 and 51.\nIn chapter 38, High says that there are many men in Valhalla, and many more who will arrive, yet they will \"seem too few when the wolf comes.\" In chapter 51, High foretells that as part of the events of Ragnar\u00f6k, after Fenrir's son Sk\u00f6ll has swallowed the sun and his other son Hati Hr\u00f3\u00f0vitnisson has swallowed the moon, the stars will disappear from the sky. The earth will shake violently, trees will be uprooted, mountains will fall, and all binds will snap \u2013 Fenris\u00falfr will be free. Fenris\u00falfr will go forth with his mouth opened wide, his upper jaw touching the sky and his lower jaw the earth, and flames will burn from his eyes and nostrils. Later, Fenris\u00falfr will arrive at the field V\u00edgr\u00ed\u00f0r with his sibling J\u00f6rmungandr. With the forces assembled there, an immense battle will take place. During this, Odin will ride to fight Fenris\u00falfr. During the battle, Fenris\u00falfr will eventually swallow Odin, killing him, and Odin's son V\u00ed\u00f0arr will move forward and kick one foot into the lower jaw of the wolf. This foot will bear a legendary shoe \"for which the material has been collected throughout all time.\" With one hand, V\u00ed\u00f0arr will take hold of the wolf's upper jaw and tear apart his mouth, killing Fenris\u00falfr. High follows this prose description by citing various quotes from \"V\u00f6lusp\u00e1\" in support, some of which mention Fenrir.\n\"Sk\u00e1ldskaparm\u00e1l\" and \"H\u00e1ttatal\".\nIn the Epilogue section of the \"Prose Edda\" book \"Sk\u00e1ldskaparm\u00e1l\", a euhemerized monologue equates Fenris\u00falfr to Pyrrhus, attempting to rationalize that \"it killed Odin, and Pyrrhus could be said to be a wolf according to their religion, for he paid no respect to places of sanctuary when he killed the king in the temple in front of Thor's altar.\" In chapter 2, \"wolf's enemy\" is cited as a kenning for Odin as used by the 10th century skald Egill Skallagr\u00edmsson. In chapter 9, \"feeder of the wolf\" is given as a kenning for T\u00fdr and, in chapter 11, \"slayer of Fenris\u00falfr\" is presented as a kenning for V\u00ed\u00f0arr. In chapter 50, a section of \"Ragnarsdr\u00e1pa\" by the 9th century skald Bragi Boddason is quoted that refers to Hel, the being, as \"the monstrous wolf's sister.\" In chapter 75, names for wargs and wolves are listed, including both \"Hr\u00f3\u00f0vitnir\" and \"Fenrir.\"\n\"Fenrir\" appears twice in verse as a common noun for a \"wolf\" or \"warg\" in chapter 58 of \"Sk\u00e1ldskaparm\u00e1l\", and in chapter 56 of the book \"H\u00e1ttatal\". Additionally, the name \"Fenrir\" can be found among a list of j\u00f6tnar in chapter 75 of \"Sk\u00e1ldskaparm\u00e1l\".\n\"Heimskringla\".\nAt the end of the \"Heimskringla\" saga \"H\u00e1konar saga g\u00f3\u00f0a\", the poem \"H\u00e1konarm\u00e1l\" by the 10th century skald Eyvindr sk\u00e1ldaspillir is presented. The poem is about the fall of King Haakon I of Norway; although he is Christian, he is taken by two valkyries to Valhalla, and is there received as one of the Einherjar. Towards the end of the poem, a stanza relates sooner will the bonds of Fenrir snap than as good a king as Haakon shall stand in his place:\nArchaeological record.\nThorwald's Cross.\n, a partially surviving runestone erected at Kirk Andreas on the Isle of Man, depicts a bearded human holding a spear downward at a wolf, his right foot in its mouth, while a large bird sits at his shoulder. Rundata dates it to 940, while Pluskowski dates it to the 11th century. This depiction has been interpreted as Odin, with a raven or eagle at his shoulder, being consumed by Fenrir at Ragnar\u00f6k. On the reverse of the stone is another image parallel to it that has been described as Christ triumphing over Satan. These combined elements have led to the cross as being described as \"syncretic art\"; a mixture of pagan and Christian beliefs.\nGosforth Cross.\nThe mid-11th century Gosforth Cross, located in Cumbria, England, has been described as depicting a combination of scenes from the Christian Judgement Day and the pagan Ragnar\u00f6k. The cross features various figures depicted in Borre style, including a man with a spear facing a monstrous head, one of whose feet is thrust into the beast's forked tongue and on its lower jaw, while a hand is placed against its upper jaw, a scene interpreted as V\u00ed\u00f0arr fighting Fenrir. This depiction has been theorized as a metaphor for Christ's defeat of Satan.\nLedberg stone.\nThe 11th century Ledberg stone in Sweden, similarly to Thorwald's Cross, features a figure with his foot at the mouth of a four-legged beast, and this may also be a depiction of Odin being devoured by Fenrir at Ragnar\u00f6k. Below the beast and the man is a depiction of a legless, helmeted man, with his arms in a prostrate position. The Younger Futhark inscription on the stone bears a commonly seen memorial dedication, but is followed by an encoded runic sequence that has been described as \"mysterious,\" and \"an interesting magic formula which is known from all over the ancient Norse world.\"\nOther.\nIf the images on the Tullstorp Runestone are correctly identified as depicting Ragnar\u00f6k, then Fenrir is shown above the ship Naglfar.\nMeyer Schapiro theorizes a connection between the \"Hell Mouth\" that appears in medieval Christian iconography and Fenrir. According to Schapiro, \"the Anglo-Saxon taste for the Hell Mouth was perhaps influenced by the northern pagan myth of the Crack of Doom and the battle with the wolf, who devoured Odin.\"\nScholars propose that a variety of objects from the archaeological record depict T\u00fdr. For example, a Migration Period gold bracteate from Trollh\u00e4ttan, Sweden, features a person receiving a bite on the hand from a beast, which may depict T\u00fdr and Fenrir. A Viking Age hogback in Sockburn, County Durham, North East England may depict T\u00fdr and Fenrir.\nTheories.\nIn reference to Fenrir's presentation in the \"Prose Edda\", Andy Orchard theorizes that \"the hound (or wolf)\" Garmr, Sk\u00f6ll, and Hati Hr\u00f3\u00f0vitnisson were originally simply all Fenrir, stating that \"Snorri, characteristically, is careful to make distinctions, naming the wolves who devour the sun and moon as Sk\u00f6ll and Hati Hr\u00f3\u00f0vitnisson respectively, and describing an encounter between Garm and T\u00fdr (who, one would have thought, might like to get his hand on Fenrir) at Ragnar\u00f6k.\"\nJohn Lindow says that it is unclear why the gods decide to raise Fenrir as opposed to his siblings Hel and J\u00f6rmungandr in \"Gylfaginning\" chapter 35, theorizing that it may be \"because Odin had a connection with wolves? Because Loki was Odin's blood brother?\" Referring to the same chapter, Lindow comments that neither of the phrases that Fenrir's binding result in have left any other traces. Lindow compares Fenrir's role to his father Loki and Fenrir's sibling J\u00f6rmungandr, in that they all spend time with the gods, are bound or cast out by them, return \"at the end of the current mythic order to destroy them, only to be destroyed himself as a younger generation of gods, one of them his slayer, survives into the new world order.\" He also points to Fenrir's binding as part of a recurring theme of the bound monster, where an enemy of the gods is bound, but destined to break free at Ragnarok.\nIndo-European parallels have been proposed between myths of Fenrir and the Persian demon Ahriman. The Yashts refer to a story where Taxma Urupi rode Angra Mainyu as a horse for thirty years. An elaboration of this allusion is found only in a late Parsi commentary. The ruler Taxmoruw (Taxma Urupi) managed to lasso Ahriman (Angra Mainyu) and keep him tied up while taking him for a ride three times a day. After thirty years, Ahriman outwitted and swallowed Taxmoruw. In a sexual encounter with Ahriman, Jamshid, Taxmoruw's brother, inserted his hand into Ahriman's anus and pulled out his brother's corpse. His hand withered from contact with the diabolic innards. The suggested parallels with Fenrir myths are the binding of an evil being by a ruler figure and the subsequent swallowing of the ruler figure by the evil being (Odin and Fenrir), trickery involving the thrusting of a hand into a monster's orifice and the affliction of the inserted limb (T\u00fdr and Fenrir).\nEthologist Valerius Geist wrote that Fenrir's maiming and ultimate killing of Odin, who had previously nurtured him, was likely based on true experiences of wolf-behaviour, seeing as wolves are genetically encoded to rise up in the pack hierarchy and have, on occasion, been recorded to rebel against, and kill, their parents. Geist states that \"apparently, even the ancients knew that wolves may turn on their parents and siblings and kill them.\"\nModern influence.\nFenrir has been depicted in the artwork \"Odin and Fenris\" (1909) and \"The Binding of Fenris\" (around 1900) by Dorothy Hardy, \"Odin und Fenriswolf\" and \"Fesselung des Fenriswolfe\" (1901) by Emil Doepler, and is the subject of the metal sculpture \"Fenrir\" by Arne Vinje Gunnerud located on the island of Ask\u00f8y, Norway.\nFenrir appears in modern literature in the poem \"Om Fenrisulven og Tyr\" (1819) by Adam Gottlob Oehlenschl\u00e4ger (collected in \"Nordens Guder\"), the novel \"Der Fenriswolf\" by K. H. Strobl, and \"Til kamp mod d\u00f8dbideriet\" (1974) by E. K. Reich and E. Larsen.\nFenrir also appears in at least three Young Adult fiction books. First, he inspired the werewolf Fenrir Greyback in the \"Harry Potter\" series by J.K. Rowling. He also appears in the form of Fenris Wolf in \"Magnus Chase and the Gods of Asgard\", by Rick Riordan, as the main adversary in the first book of the series. His influence is also seen in Sarah J. Maas' \"Throne of Glass\" series in the character Fenrys, who can transform into a large wolf.\nFenris Ulf (also known as Maugrim) is a wolf and the Captain of the White Witch's Secret Police in C. S. Lewis's novel \"The Lion, the Witch and the Wardrobe\". The character is named \"Fenris Ulf\" in American editions of the book until the 1990s, as well as in the 1979 animated adaptation.\nFenrir as a minion of Hela appears in the 2017 Marvel Studios film \"\".\nFenrir was also the influence for Carcharoth, an evil wolf serving Morgoth in J. R. R. Tolkien's fantasy world of Middle-earth.\nIn 2015, W Motors unveiled their second vehicle, titled the Fenyr SuperSport. The vehicle's name is derived from the name \"Fenrir\", appropriately capturing the power and speed of the car in the name inspired by the mighty wolf."}
{"id": "10974", "revid": "16761480", "url": "https://en.wikipedia.org/wiki?curid=10974", "title": "Final Fantasy", "text": " is a Japanese anthology science fantasy media franchise created by Hironobu Sakaguchi, and developed and owned by Square Enix (formerly Square). The franchise centers on a series of fantasy and science fantasy role-playing video games. The first game in the series was released in 1987, with 14 other main-numbered entries being released since then. The franchise has since branched into other video game genres such as tactical role-playing, action role-playing, massively multiplayer online role-playing, racing, third-person shooter, fighting, and rhythm, as well as branching into other media, including CGI films, anime, manga, and novels.\n\"Final Fantasy\" installments are generally stand-alone stories or role playing games, each with different settings, plots and main characters, but the franchise is linked by several recurring elements, including game mechanics and recurring character names. Each plot centers on a particular group of heroes who are battling a great evil, but also explores the characters' internal struggles and relationships. Character names are frequently derived from the history, languages, pop culture, and mythologies of cultures worldwide. The mechanics of each game involve similar battle systems and maps.\nThe \"Final Fantasy\" video game series has been both critically and commercially successful, selling more than software units worldwide, making it one of the best-selling video game franchises of all time. The series is well known for its innovation, visuals, and music, such as the inclusion of full-motion videos (FMVs), photorealistic character models, and music by Nobuo Uematsu. It has popularized many features now common in role-playing games, also popularizing the genre as a whole in markets outside Japan.\nMedia.\nGames.\nThe first installment of the series was released in Japan on December 18, 1987. Subsequent games are numbered and given a story unrelated to previous games, so the numbers refer to volumes rather than to sequels. Many \"Final Fantasy\" games have been localized for markets in North America, Europe, and Australia on numerous video game consoles, personal computers (PC), and mobile phones. Future installments will appear on seventh and eighth generation consoles. As of November 2016, the series includes the main installments from \"Final Fantasy\" to \"Final Fantasy XV\", as well as direct sequels and spin-offs, both released and confirmed as being in development. Most of the older games have been remade or re-released on multiple platforms.\nMain series.\nThree \"Final Fantasy\" installments were released on the Nintendo Entertainment System (NES). \"Final Fantasy\" was released in Japan in 1987 and in North America in 1990. It introduced many concepts to the console RPG genre, and has since been remade on several platforms. \"Final Fantasy\u00a0II\", released in 1988 in Japan, has been bundled with \"Final Fantasy\" in several re-releases. The last of the NES installments, \"Final Fantasy\u00a0III\", was released in Japan in 1990; however, it was not released elsewhere until a Nintendo DS remake in 2006.\nThe Super Nintendo Entertainment System (SNES) also featured three installments of the main series, all of which have been re-released on several platforms. \"Final Fantasy\u00a0IV\" was released in 1991; in North America, it was released as \"Final Fantasy\u00a0II\". It introduced the \"Active Time Battle\" system. \"Final Fantasy\u00a0V\", released in 1992 in Japan, was the first game in the series to spawn a sequel: a short anime series, \"\". \"Final Fantasy\u00a0VI\" was released in Japan in 1994, titled \"Final Fantasy\u00a0III\" in North America.\nThe PlayStation console saw the release of three main \"Final Fantasy\" games. \"Final Fantasy\u00a0VII\" (1997) moved away from the two-dimensional (2D) graphics used in the first six games to three-dimensional (3D) computer graphics; the game features polygonal characters on pre-rendered backgrounds. It also introduced a more modern setting, a style that was carried over to the next game. It was also the second in the series to be released in Europe, with the first being \"Final Fantasy Mystic Quest\". \"Final Fantasy VIII\" was published in 1999, and was the first to consistently use realistically proportioned characters and feature a vocal piece as its theme music. \"Final Fantasy\u00a0IX\", released in 2000, returned to the series' roots by revisiting a more traditional \"Final Fantasy\" setting rather than the more modern worlds of \"VII\" and \"VIII\".\nThree main installments, as well as one online game, were published for the PlayStation 2 (PS2). \"Final Fantasy\u00a0X\" (2001) introduced full 3D areas and voice acting to the series, and was the first to spawn a sub-sequel (\"Final Fantasy X-2\", published in 2003). The first massively multiplayer online role-playing game (MMORPG) in the series, \"Final Fantasy\u00a0XI\", was released on the PS2 and PC in 2002, and later on the Xbox 360. It introduced real-time battles instead of random encounters. \"Final Fantasy XII\", published in 2006, also includes real-time battles in large, interconnected playfields. The game is also the first in the main series to utilize a world used in a previous game, namely the land of Ivalice, which had previously featured in \"Final Fantasy Tactics\" and \"Vagrant Story\".\nIn 2009, \"Final Fantasy XIII\" was released in Japan, and in North America and Europe the following year, for PlayStation 3 and Xbox 360. It is the flagship installment of the \"Fabula Nova Crystallis Final Fantasy\" series and became the first mainline game to spawn two sub-sequels (\"XIII-2\" and '). It was also the first game released in Chinese and high definition along with being released on two consoles at once. \"Final Fantasy XIV\", a MMORPG, was released worldwide on Microsoft Windows in 2010, but it received heavy criticism when it was launched, prompting Square Enix to rerelease the game as ', this time to the PlayStation 3 as well, in 2013. \"Final Fantasy XV\" is an action role-playing game that was released for PlayStation 4 and Xbox One in 2016. Originally a \"XIII\" spin-off titled \"Versus XIII\", \"XV\" uses the mythos of the \"Fabula Nova Crystallis\" series, although in many other respects the game stands on its own and has since been distanced from the series by its developers. The next mainline entry, \"Final Fantasy XVI\", was announced in September 2020 for the PlayStation 5.\nRemakes, sequels and spin-offs.\n\"Final Fantasy\" has spawned numerous spin-offs and metaseries. Several are, in fact, not \"Final Fantasy\" games, but were rebranded for North American release. Examples include the \"SaGa\" series, rebranded \"The Final Fantasy Legend\", and its two sequels, \"Final Fantasy Legend II\" and \"Final Fantasy Legend III\". \"Final Fantasy Mystic Quest\" was specifically developed for a United States audience, and \"Final Fantasy Tactics\" is a tactical RPG that features many references and themes found in the series. The spin-off \"Chocobo\" series, \"Crystal Chronicles\" series, and \"Kingdom Hearts\" series also include multiple \"Final Fantasy\" elements. In 2003, the \"Final Fantasy\" series' first sub-sequel, \"Final Fantasy X-2\", was released. \"Final Fantasy XIII\" was originally intended to stand on its own, but the team wanted to explore the world, characters and mythos more, resulting in the development and release of two sequels in 2011 and respectively, creating the series' first official trilogy. \"Dissidia Final Fantasy\" was released in 2009, a fighting game that features heroes and villains from the first ten games of the main series. It was followed by a prequel in 2011. Other spin-offs have taken the form of subseries\u2014\"Compilation of Final Fantasy\u00a0VII\", \"Ivalice Alliance\", and \"Fabula Nova Crystallis Final Fantasy\". Enhanced 3D remakes of \"Final Fantasy III\" and \"Final Fantasy IV\" would be released in 2006 and 2007 respectively. \"Final Fantasy VII Remake\" was released on the PlayStation 4 in 2020.\nOther media.\nFilm and television.\nSquare Enix has expanded the \"Final Fantasy\" series into various media. Multiple anime and computer-generated imagery (CGI) films have been produced that are based either on individual \"Final Fantasy\" games or on the series as a whole. The first was an original video animation (OVA), ', a sequel to \"Final Fantasy\u00a0V\". The story was set in the same world as the game, although 200 years in the future. It was released as four 30-minute episodes, first in Japan in 1994 and later in the United States by Urban Vision in 1998. In 2001, Square Pictures released its first feature film, '. The film is set on a future Earth invaded by alien life forms. \"The Spirits Within\" was the first animated feature to seriously attempt to portray photorealistic CGI humans, but was considered a box office bomb and garnered mixed reviews.\nA 25-episode anime television series, \",\" was released in 2001 based on the common elements of the \"Final Fantasy\" series. It was broadcast in Japan by TV Tokyo and released in North America by ADV Films.\nIn 2005, ', a feature length direct-to-DVD CGI film, and ', a non-canon OVA, were released as part of the \"Compilation of Final Fantasy\u00a0VII\". \"Advent Children\" was animated by Visual Works, which helped the company create CG sequences for the games. The film, unlike \"The Spirits Within\", became a commercial success. \"Last Order,\" on the other hand, was released in Japan in a special DVD bundle package with \"Advent Children\". \"Last Order\" sold out quickly and was positively received by Western critics, though fan reaction was mixed over changes to established story scenes.\nTwo animated tie-ins for \"Final Fantasy XV\" were announced at the Uncovered Final Fantasy XV fan and press event, forming part of a larger multimedia project dubbed the \"Final Fantasy XV\" Universe. ' is a series of five 10-to-20-minute-long episodes developed by A-1 Pictures and Square Enix detailing the backstories of the main cast. ', a CGI movie set for release prior to the game in Summer 2016, is set during the game's opening and follows new and secondary characters.\nOn February 26, 2019 Square Enix released a short anime, produced by Satelight Inc, called \"Final Fantasy XV: Episode Ardyn \u2013 Prologue\" on their YouTube channel which acts as the background story for the final piece of DLC for \"Final Fantasy XV\" giving insight into Ardyn's past.\nSquare Enix also released \"\", an 8-episode Japanese soap opera. It features a mix of live-action scenes and \"Final Fantasy XIV\" gameplay footage. It premiered in Japan on April 16, 2017 and became available worldwide via Netflix in September of the same year.\nIt was announced in June 2019 that Sony Pictures Television is working on a first ever live-action adaptation of the series with Hivemind and Square Enix. Jason F. Brown, Sean Daniel and Dinesh Shamdasani for Hivemind will be the producers while Ben Lustig and Jake Thornton will write for the series and will serve as executive producers.\nOther media.\nSeveral video games have either been adapted into or have had spin-offs in the form of manga and novels. The first was the novelization of \"Final Fantasy\u00a0II\" in 1989, and was followed by a manga adaptation of \"Final Fantasy\u00a0III\" in 1992. The past decade has seen an increase in the number of non-video game adaptations and spin-offs. \"Final Fantasy: The Spirits Within\" has been adapted into a novel, the spin-off game \"Final Fantasy Crystal Chronicles\" has been adapted into a manga, and \"Final Fantasy\u00a0XI\" has had a novel and manga set in its continuity. Seven novellas based on the \"Final Fantasy\u00a0VII\" universe have also been released. The \"Final Fantasy: Unlimited\" story was partially continued in novels and a manga after the anime series ended. The \"Final Fantasy X\" and \"Final Fantasy XIII\" series have also had novellas and audio dramas released. Two games, \"Final Fantasy Tactics Advance\" and \"Final Fantasy: Unlimited\", have been adapted into radio dramas.\nA trading card game named the \"Final Fantasy trading card game\" is produced by Square Enix and Hobby Japan, first released Japan in 2012 with an English version in 2016. The game has been compared to \"\", and a tournament circuit for the game also takes place.\nCommon elements.\nAlthough most \"Final Fantasy\" installments are independent, many gameplay elements recur throughout the series. Most games contain elements of fantasy and science fiction and feature recycled names often inspired from various cultures' history, languages and mythology, including Asian, European, and Middle-Eastern. Examples include weapon names like Excalibur and Masamune\u2014derived from Arthurian legend and the Japanese swordsmith Masamune respectively\u2014as well as the spell names Holy, Meteor, and Ultima. Beginning with \"Final Fantasy\u00a0IV\", the main series adopted its current logo style that features the same typeface and an emblem designed by Japanese artist Yoshitaka Amano. The emblem relates to a game's plot and typically portrays a character or object in the story. Subsequent remakes of the first three games have replaced the previous logos with ones similar to the rest of the series.\nPlot and themes.\nThe central conflict in many \"Final Fantasy\" games focuses on a group of characters battling an evil, and sometimes ancient, antagonist that dominates the game's world. Stories frequently involve a sovereign state in rebellion, with the protagonists taking part in the rebellion. The heroes are often destined to defeat the evil, and occasionally gather as a direct result of the antagonist's malicious actions. Another staple of the series is the existence of two villains; the main villain is not always who it appears to be, as the primary antagonist may actually be subservient to another character or entity. The main antagonist introduced at the beginning of the game is not always the final enemy, and the characters must continue their quest beyond what appears to be the final fight.\nStories in the series frequently emphasize the internal struggles, passions, and tragedies of the characters, and the main plot often recedes into the background as the focus shifts to their personal lives. Games also explore relationships between characters, ranging from love to rivalry. Other recurring situations that drive the plot include amnesia, a hero corrupted by an evil force, mistaken identity, and self-sacrifice. Magical orbs and crystals are recurring in-game items that are frequently connected to the themes of the games' plots. Crystals often play a central role in the creation of the world, and a majority of the \"Final Fantasy\" games link crystals and orbs to the planet's life force. As such, control over these crystals drives the main conflict. The classical elements are also a recurring theme in the series related to the heroes, villains, and items. Other common plot and setting themes include the Gaia hypothesis, an apocalypse, and conflicts between advanced technology and nature.\nCharacters.\nThe series features a number of recurring character archetypes. Most famously, every game since \"Final Fantasy\u00a0II\", including subsequent remakes of the original \"Final Fantasy\", features a character named Cid. Cid's appearance, personality, goals, and role in the game (non-playable ally, party member, villain) vary dramatically. However, two characteristics many versions of Cid have in common are 1) being a scientist or engineer, and 2) being tied in some way to an airship the party eventually acquires. Every Cid has at least one of these two traits.\nBiggs and Wedge, inspired by two \"Star Wars\" characters of the same name, appear in numerous games as minor characters, sometimes as comic relief. The later games in the series feature several males with effeminate characteristics. Recurring creatures include Chocobos, Moogles, and Cactuars. Chocobos are large, often flightless birds that appear in several installments as a means of long-distance travel for characters. Moogles are white, stout creatures resembling teddy bears with wings and a single antenna. They serve different capacities in games including mail delivery, weaponsmiths, party members, and saving the game. Cactuars are anthropomorphic cacti with \"haniwa\"-like faces presented in a running or dashing pose. They usually appear as recurring enemy units, and also as summoned allies or friendly non-player characters in certain titles. Chocobo and Moogle appearances are often accompanied by specific musical themes that have been arranged differently for separate games.\nGameplay.\nIn \"Final Fantasy\" games, players command a party of characters as they progress through the game's story by exploring the game world and defeating opponents. Enemies are typically encountered randomly through exploring, a trend which changed in \"Final Fantasy\u00a0XI\" and \"Final Fantasy\u00a0XII\". The player issues combat orders\u2014like \"Fight\", \"Magic\", and \"Item\"\u2014to individual characters via a menu-driven interface while engaging in battles. Throughout the series, the games have used different battle systems. Prior to \"Final Fantasy\u00a0XI\", battles were turn-based with the protagonists and antagonists on different sides of the battlefield. \"Final Fantasy\u00a0IV\" introduced the \"Active Time Battle\" (ATB) system that augmented the turn-based nature with a perpetual time-keeping system. Designed by Hiroyuki Ito, it injected urgency and excitement into combat by requiring the player to act before an enemy attacks, and was used until \"Final Fantasy\u00a0X\", which implemented the \"Conditional Turn-Based\" (CTB) system. This new system returned to the previous turn-based system, but added nuances to offer players more challenge. \"Final Fantasy\u00a0XI\" adopted a real-time battle system where characters continuously act depending on the issued command. \"Final Fantasy\u00a0XII\" continued this gameplay with the \"Active Dimension Battle\" system. \"Final Fantasy XIII\"s combat system, designed by the same man who worked on \"X\", was meant to have an action-oriented feel, emulating the cinematic battles in \"Final Fantasy VII: Advent Children\". The latest installment to the franchise, Final Fantasy XV, introduces a new \"Open Combat\" system. Unlike previous battle systems in the franchise, the \"Open Combat\" system (OCS) allows players to take on a fully active battle scenario, allowing for free range attacks and movement, giving a much more fluid feel of combat. This system also incorporates a \"Tactical\" Option during battle, which pauses active battle to allow use of items.\nLike most RPGs, the \"Final Fantasy\" installments use an experience level system for character advancement, in which experience points are accumulated by killing enemies. Character classes, specific jobs that enable unique abilities for characters, are another recurring theme. Introduced in the first game, character classes have been used differently in each game. Some restrict a character to a single job to integrate it into the story, while other games feature dynamic job systems that allow the player to choose from multiple classes and switch throughout the game. Though used heavily in many games, such systems have become less prevalent in favor of characters that are more versatile; characters still match an archetype, but are able to learn skills outside their class.\nMagic is another common RPG element in the series. The method by which characters gain magic varies between installments, but is generally divided into classes organized by color: \"White magic\", which focuses on spells that assist teammates; \"Black magic\", which focuses on harming enemies; \"Red magic\", which is a combination of white and black magic, \"Blue magic\", which mimics enemy attacks; and \"Green magic\" which focuses on applying status effects to either allies or enemies. Other types of magic frequently appear such as \"Time magic\", focusing on the themes of time, space, and gravity; and \"Summoning magic\", which evokes legendary creatures to aid in battle and is a feature that has persisted since \"Final Fantasy\u00a0III\". Summoned creatures are often referred to by names like \"Espers\" or \"Eidolons\" and have been inspired by mythologies from Arabic, Hindu, Norse, and Greek cultures.\nDifferent means of transportation have appeared through the series. The most common is the airship for long range travel, accompanied by chocobos for travelling short distances, but others include sea and land vessels. Following \"Final Fantasy\u00a0VII\", more modern and futuristic vehicle designs have been included.\nDevelopment and history.\nOrigin.\nIn the mid-1980s, Square entered the Japanese video game industry with simple RPGs, racing games, and platformers for Nintendo's Famicom Disk System. In 1987, Square designer Hironobu Sakaguchi chose to create a new fantasy role-playing game for the cartridge-based NES, and drew inspiration from popular fantasy games: Enix's \"Dragon Quest\", Nintendo's \"The Legend of Zelda\", and Origin Systems's \"Ultima\" series. Though often attributed to the company allegedly facing bankruptcy, Sakaguchi explained that the game was his personal last-ditch effort in the game industry and that its title, \"Final Fantasy\", stemmed from his feelings at the time; had the game not sold well, he would have quit the business and gone back to university. Despite his explanation, publications have also attributed the name to the company's hopes that the project would solve its financial troubles. In 2015, Sakaguchi explained the name's origin: the team wanted a title that would abbreviate to \"FF\", which would sound good in Japanese. The name was originally going to be \"Fighting Fantasy\", but due to concerns over trademark conflicts with the roleplaying gamebook series of the same name, they needed to settle for something else. As the word \"Final\" was a famous word in Japan, Sakaguchi settled on that. According to Sakaguchi, any title that created the \"FF\" abbreviation would have done.\nThe game indeed reversed Square's lagging fortunes, and it became the company's flagship franchise. Following the success, Square immediately developed a second installment. Because Sakaguchi assumed \"Final Fantasy\" would be a stand-alone game, its story was not designed to be expanded by a sequel. The developers instead chose to carry over only thematic similarities from its predecessor, while some of the gameplay elements, such as the character advancement system, were overhauled. This approach has continued throughout the series; each major \"Final Fantasy\" game features a new setting, a new cast of characters, and an upgraded battle system. Video game writer John Harris attributed the concept of reworking the game system of each installment to Nihon Falcom's \"Dragon Slayer\" series, with which Square was previously involved as a publisher. The company regularly released new games in the main series. However, the time between the releases of \"Final Fantasy XI\" (2002), \"Final Fantasy XII\" (2006), and \"Final Fantasy XIII\" (2009) were much longer than previous games. Following \"Final Fantasy XIV\", Square Enix stated that it intended to release \"Final Fantasy\" games either annually or biennially. This switch was to mimic the development cycles of Western games in the \"Call of Duty\", \"Assassin's Creed\" and \"Battlefield\" series, as well as maintain fan-interest.\nDesign.\nFor the original \"Final Fantasy\", Sakaguchi required a larger production team than Square's previous games. He began crafting the game's story while experimenting with gameplay ideas. Once the gameplay system and game world size were established, Sakaguchi integrated his story ideas into the available resources. A different approach has been taken for subsequent games; the story is completed first and the game built around it. Designers have never been restricted by consistency, though most feel each game should have a minimum number of common elements. The development teams strive to create completely new worlds for each game, and avoid making new games too similar to previous ones. Game locations are conceptualized early in development and design details like building parts are fleshed out as a base for entire structures.\nThe first five games were directed by Sakaguchi, who also provided the original concepts. He drew inspiration for game elements from anime films by Hayao Miyazaki; series staples like the airships and chocobos are inspired by elements in \"Castle in the Sky\" and \"Nausica\u00e4 of the Valley of the Wind\", respectively. Sakaguchi served as a producer for subsequent games until he left Square in 2001. Yoshinori Kitase took over directing the games until \"Final Fantasy\u00a0VIII\", and has been followed by a new director for each new game. Hiroyuki Ito designed several gameplay systems, including \"Final Fantasy\u00a0V\"s \"Job System\", \"Final Fantasy\u00a0VIII\"s \"Junction System\" and the Active Time Battle concept, which was used from \"Final Fantasy\u00a0IV\" until \"Final Fantasy\u00a0IX\". In designing the Active Time Battle system, Ito drew inspiration from Formula One racing; he thought it would be interesting if character types had different speeds after watching race cars pass each other. Ito also co-directed \"Final Fantasy\u00a0VI\" with Kitase. Kenji Terada was the scenario writer for the first three games; Kitase took over as scenario writer for \"Final Fantasy\u00a0V\" through \"Final Fantasy\u00a0VII\". Kazushige Nojima became the series' primary scenario writer from \"Final Fantasy\u00a0VII\" until his resignation in October 2003; he has since formed his own company, Stellavista. Nojima partially or completely wrote the stories for \"Final Fantasy\u00a0VII\", \"Final Fantasy\u00a0VIII\", \"Final Fantasy\u00a0X\", and \"Final Fantasy X-2\". He also worked as the scenario writer for the spin-off series, \"Kingdom Hearts\". Daisuke Watanabe co-wrote the scenarios for \"Final Fantasy\u00a0X\" and \"XII\", and was the main writer for the \"XIII\" games.\nArtistic design, including character and monster creations, was handled by Japanese artist Yoshitaka Amano from \"Final Fantasy\" through \"Final Fantasy\u00a0VI\". Amano also handled title logo designs for all of the main series and the image illustrations from \"Final Fantasy\u00a0VII\" onward. Tetsuya Nomura was chosen to replace Amano because Nomura's designs were more adaptable to 3D graphics. He worked with the series from \"Final Fantasy\u00a0VII\" through \"Final Fantasy\u00a0X\"; for \"Final Fantasy\u00a0IX\", however, character designs were handled by Shuk\u014d Murase, Toshiyuki Itahana, and Shin Nagasawa. Nomura is also the character designer of the \"Kingdom Hearts\" series, \"Compilation of Final Fantasy\u00a0VII\", and \"Fabula Nova Crystallis: Final Fantasy\". Other designers include Nobuyoshi Mihara and Akihiko Yoshida. Mihara was the character designer for \"Final Fantasy\u00a0XI\", and Yoshida served as character designer for \"Final Fantasy Tactics\", the Square-produced \"Vagrant Story\", and \"Final Fantasy\u00a0XII\".\nGraphics and technology.\nBecause of graphical limitations, the first games on the NES feature small sprite representations of the leading party members on the main world screen. Battle screens use more detailed, full versions of characters in a side-view perspective. This practice was used until \"Final Fantasy\u00a0VI\", which uses detailed versions for both screens. The NES sprites are 26\u00a0pixels high and use a color palette of 4\u00a0colors. 6\u00a0frames of animation are used to depict different character statuses like \"healthy\" and \"fatigued\". The SNES installments use updated graphics and effects, as well as higher quality audio than in previous games, but are otherwise similar to their predecessors in basic design. The SNES sprites are 2\u00a0pixels shorter, but have larger palettes and feature more animation frames: 11\u00a0colors and 40\u00a0frames respectively. The upgrade allowed designers to have characters be more detailed in appearance and express more emotions. The first game includes non-player characters (NPCs) the player could interact with, but they are mostly static in-game objects. Beginning with the second game, Square used predetermined pathways for NPCs to create more dynamic scenes that include comedy and drama.\nIn 1995, Square showed an interactive SGI technical demonstration of \"Final Fantasy VI\" for the then next generation of consoles. The demonstration used Silicon Graphics's prototype Nintendo 64 workstations to create 3D graphics. Fans believed the demo was of a new \"Final Fantasy\" game for the Nintendo 64 console; however, 1997 saw the release of \"Final Fantasy\u00a0VII\" for the Sony PlayStation. The switch was due to a dispute with Nintendo over its use of faster but more expensive cartridges, as opposed to the slower and cheaper, but much higher capacity Compact Discs used on rival systems. \"Final Fantasy\u00a0VII\" introduced 3D graphics with fully pre-rendered backgrounds. It was because of this switch to 3D that a CD-ROM format was chosen over a cartridge format. The switch also led to increased production costs and a greater subdivision of the creative staff for \"Final Fantasy\u00a0VII\" and subsequent 3D games in the series.\nStarting with \"Final Fantasy\u00a0VIII\", the series adopted a more photo-realistic look. Like \"Final Fantasy\u00a0VII\", full motion video (FMV) sequences would have video playing in the background, with the polygonal characters composited on top. \"Final Fantasy\u00a0IX\" returned to the more stylized design of earlier games in the series, although it still maintained, and in many cases slightly upgraded, most of the graphical techniques used in the previous two games. \"Final Fantasy\u00a0X\" was released on the PlayStation 2, and used the more powerful hardware to render graphics in real-time instead of using pre-rendered material to obtain a more dynamic look; the game features full 3D environments, rather than have 3D character models move about pre-rendered backgrounds. It is also the first \"Final Fantasy\" game to introduce voice acting, occurring throughout the majority of the game, even with many minor characters. This aspect added a whole new dimension of depth to the character's reactions, emotions, and development.\nTaking a temporary divergence, \"Final Fantasy\u00a0XI\" used the PlayStation 2's online capabilities as an MMORPG. Initially released for the PlayStation 2 with a PC port arriving six months later, \"Final Fantasy\u00a0XI\" was also released on the Xbox 360 nearly four years after its original release in Japan. This was the first \"Final Fantasy\" game to use a free rotating camera. \"Final Fantasy\u00a0XII\" was released in 2006 for the PlayStation 2 and uses only half as many polygons as \"Final Fantasy\u00a0X\", in exchange for more advanced textures and lighting. It also retains the freely rotating camera from \"Final Fantasy\u00a0XI\". \"Final Fantasy\u00a0XIII\" and \"Final Fantasy\u00a0XIV\" both make use of Crystal Tools, a middleware engine developed by Square Enix. \nMusic.\nThe \"Final Fantasy\" games feature a variety of music, and frequently reuse themes. Most of the games open with a piece called \"Prelude\", which has evolved from a simple, 2-voice arpeggio in the early games to a complex, melodic arrangement in recent installments. Victories in combat are often accompanied by a victory fanfare, a theme that has become one of the most recognized pieces of music in the series. The basic theme that accompanies Chocobo appearances has been rearranged in a different musical style for each installment. A piece called \"Prologue\" (and sometimes \"Final Fantasy\"), originally featured in the first game, is often played during the ending credits. Although leitmotifs are common in the more character-driven installments, theme music is typically reserved for main characters and recurring plot elements.\nNobuo Uematsu was the primary composer of the \"Final Fantasy\" series until his resignation from Square Enix in November 2004. Other notable composers who have worked on main entries in the series include Masashi Hamauzu, Hitoshi Sakimoto, and Yoko Shimomura. Uematsu was allowed to create much of the music with little direction from the production staff. Sakaguchi, however, would request pieces to fit specific game scenes including battles and exploring different areas of the game world. Once a game's major scenarios were completed, Uematsu would begin writing the music based on the story, characters, and accompanying artwork. He started with a game's main theme, and developed other pieces to match its style. In creating character themes, Uematsu read the game's scenario to determine the characters' personality. He would also ask the scenario writer for more details to scenes he was unsure about. Technical limitations were prevalent in earlier games; Sakaguchi would sometimes instruct Uematsu to only use specific notes. It was not until \"Final Fantasy\u00a0IV\" on the SNES that Uematsu was able to add more subtlety to the music.\nReception.\nOverall, the \"Final Fantasy\" series has been critically acclaimed and commercially successful, though each installment has seen different levels of success. The series has seen a steady increase in total sales; it sold over 10\u00a0million software units worldwide by early 1996, 45\u00a0million by August 2003, 63\u00a0million by December 2005, and 85\u00a0million by July 2008. In June 2011, Square Enix announced that the series had sold over units, and by March 2014, it had sold over 110\u00a0million units. Its high sales numbers have ranked it as one of the best-selling video game franchises in the industry; in January 2007, the series was listed as number three, and later in July as number four. As of 2019, the series had sold over 149 million units worldwide. As of November 2020, the series has sold over 159million units worldwide.\nSeveral games within the series have become best-selling games. At the end of 2007, the seventh, eighth, and ninth best-selling RPGs were \"Final Fantasy\u00a0VII\", \"Final Fantasy\u00a0VIII\", and \"Final Fantasy\u00a0X\" respectively. \"Final Fantasy\u00a0VII\" has shipped over 12.8\u00a0million copies worldwide, earning it the position of the best-selling \"Final Fantasy\" game. Within two days of \"Final Fantasy\u00a0VIII\"s North American release on September 9, 1999, it became the top-selling video game in the United States, a position it held for more than three weeks. \"Final Fantasy\u00a0X\" sold over 1.4\u00a0million Japanese units in pre-orders alone, which set a record for the fastest-selling console RPG. The MMORPG, \"Final Fantasy\u00a0XI\", reached over 200,000 active daily players in March 2006 and had reached over half a million subscribers by July 2007. \"Final Fantasy\u00a0XII\" sold more than 1.7\u00a0million copies in its first week in Japan. By November 6, 2006\u2014one week after its release\u2014\"Final Fantasy\u00a0XII\" had shipped approximately 1.5\u00a0million copies in North America. \"Final Fantasy XIII\" became the fastest-selling game in the franchise, and sold one million units on its first day of sale in Japan. \"Final Fantasy XIV: A Realm Reborn\", in comparison to its predecessor, was a runaway success, originally suffering from servers being overcrowded, and eventually gaining over one million unique subscribers within two months of its launch.\nThe series has received critical acclaim for the quality of its visuals and soundtracks. In 1996, \"Next Generation\" ranked the series collectively as the 17th best game of all time, speaking very highly of its graphics, music and stories. In 1999, \"Next Generation\" listed the \"Final Fantasy\" series as number 16 on their \"Top 50 Games of All Time\", commenting that, \"By pairing state-of-the-art technology with memorable, sometimes shamelessly melodramatic storylines, the series has successfully outlasted its competitors [...] and improved with each new installation.\" It was awarded a star on the Walk of Game in 2006, making it the first franchise to win a star on the event (other winners were individual games, not franchises). WalkOfGame.com commented that the series has sought perfection as well as having been a risk taker in innovation. In 2006, GameFAQs held a contest for the best video game series ever, with \"Final Fantasy\" finishing as the runner-up to \"The Legend of Zelda\". In a 2008 public poll held by The Game Group plc, \"Final Fantasy\" was voted the best game series, with five games appearing in their \"Greatest Games of All Time\" list.\nMany \"Final Fantasy\" games have been included in various lists of top games. Several games have been listed on multiple IGN \"Top Games\" lists. Twelve games were listed on \"Famitsu\"'s 2006 \"Top 100 Favorite Games of All Time\", four of which were in the top ten, with \"Final Fantasy\u00a0X\" and \"Final Fantasy\u00a0VII\" coming first and second, respectively. The series holds seven Guinness World Records in the \"Guinness World Records Gamer's Edition 2008\", which include the \"Most Games in an RPG Series\" (13 main games, seven enhanced games, and 32 spin-off games), the \"Longest Development Period\" (the production of \"Final Fantasy\u00a0XII\" took five years), and the \"Fastest-Selling Console RPG in a Single Day\" (\"Final Fantasy\u00a0X\"). The 2009 edition listed two games from the series among the top 50 consoles games: \"Final Fantasy\u00a0XII\" at number 8 and \"Final Fantasy\u00a0VII\" at number 20. In 2018, \"Final Fantasy VII\" was inducted as a member of the World Video Game Hall of Fame.\nHowever, the series has garnered some criticism. IGN has commented that the menu system used by the games is a major detractor for many and is a \"significant reason why they haven't touched the series.\" The site has also heavily criticized the use of random encounters in the series' battle systems. IGN further stated the various attempts to bring the series into film and animation have either been unsuccessful, unremarkable, or did not live up to the standards of the games. In 2007, \"Edge\" criticized the series for a number of related games that include the phrase \"Final Fantasy\" in their titles, which are considered inferior to previous games. It also commented that with the departure of Hironobu Sakaguchi, the series might be in danger of growing stale.\nSeveral individual \"Final Fantasy\" games have garnered extra attention; some for their positive reception and others for their negative reception. \"Final Fantasy VII\" topped \"GamePro's\" \"26 Best RPGs of All Time\" list, as well as GameFAQs \"Best Game Ever\" audience polls in 2004 and 2005. Despite the success of \"Final Fantasy\u00a0VII\", it is sometimes criticized as being overrated. In 2003, GameSpy listed it as the seventh most overrated game of all time, while IGN presented views from both sides. \"\" shipped 392,000\u00a0units in its first week of release, but received review scores that were much lower than that of other \"Final Fantasy\" games. A delayed, negative review after the Japanese release of \"Dirge of Cerberus\" from Japanese gaming magazine \"Famitsu\" hinted at a controversy between the magazine and Square Enix. Though \"Final Fantasy: The Spirits Within\" was praised for its visuals, the plot was criticized and the film was considered a box office bomb. \"Final Fantasy Crystal Chronicles\" for the GameCube received overall positive review scores, but reviews stated that the use of Game Boy Advances as controllers was a big detractor. The predominantly negative reception of the original version of \"Final Fantasy XIV\" caused then-president Yoichi Wada to issue an official apology during a Tokyo press conference, stating that the brand had been \"greatly damaged\" by the game's reception.\nRankings and aggregators.\nVarious video game publications have created rankings of the mainline \"Final Fantasy\" games. In the table below, the lower the number given, the better the game is in the view of the respective publication. By way of comparison, the rating provided by the review aggregator \"Metacritic\" is also given; in this row, higher numbers indicate better reviews.\nLegacy.\nFinal Fantasy has been very influential in the history of video game mechanics. \"Final Fantasy IV\" is considered a milestone for the genre, introducing a dramatic storyline with a strong emphasis on character development and personal relationships. \"Final Fantasy\u00a0VII\" is credited as having the largest industry impact of the series, and with allowing console role-playing games to gain mass-market appeal. \"Final Fantasy\u00a0VII\" is considered to be one of the most important and influential video games of all time.\nThe series affected Square's business on several levels. The commercial failure of \"Final Fantasy: The Spirits Within\" resulted in hesitation and delays from Enix during merger discussions with Square. Square's decision to produce games exclusively for the Sony PlayStation\u2014a move followed by Enix's decision with the \"Dragon Quest\" series\u2014severed their relationship with Nintendo. \"Final Fantasy\" games were absent from Nintendo consoles, specifically the Nintendo 64, for seven years. Critics attribute the switch of strong third-party games like the \"Final Fantasy\" and \"Dragon Quest\" games to Sony's PlayStation, and away from the Nintendo 64, as one of the reasons behind PlayStation being the more successful of the two consoles. The release of the Nintendo GameCube, which used optical disc media, in 2001 caught the attention of Square. To produce games for the system, Square created the shell company The Game Designers Studio and released \"Final Fantasy Crystal Chronicles\", which spawned its own metaseries within the main franchise. \"Final Fantasy\u00a0XI\"s lack of an online method of subscription cancellation prompted the creation of legislation in Illinois that requires internet gaming services to provide such a method to the state's residents.\nThe series' popularity has resulted in its appearance and reference in numerous facets of popular culture like anime, TV series, and webcomics. Music from the series has permeated into different areas of culture. \"Final Fantasy\u00a0IV\"s \"Theme of Love\" was integrated into the curriculum of Japanese school children and has been performed live by orchestras and metal bands. In 2003, Uematsu co-founded The Black Mages, an instrumental rock group independent of Square that has released albums of arranged \"Final Fantasy\" tunes. Bronze medalists Alison Bartosik and Anna Kozlova performed their synchronized swimming routine at the 2004 Summer Olympics to music from \"Final Fantasy\u00a0VIII\". Many of the soundtracks have also been released for sale. Numerous companion books, which normally provide in-depth game information, have been published. In Japan, they are published by Square and are called \"Ultimania\" books.\nThe series has inspired numerous game developers. \"Fable\" creator Peter Molyneux considers \"Final Fantasy VII\" to be the RPG that \"defined the genre\" for him. BioWare founder Greg Zeschuk cited \"Final Fantasy VII\" as \"the first really emotionally engaging game\" he played and said it had \"a big impact\" on BioWare's work. \"The Witcher 3\" senior environmental artist Jonas Mattsson cited \"Final Fantasy\" as \"a huge influence\" and said it was \"the first RPG\" he played through. \"Mass Effect\" art director Derek Watts cited \"Final Fantasy: The Spirits Within\" as a major influence on the visual design and art direction of the series. BioWare senior product manager David Silverman cited \"Final Fantasy XII\"s gambit system as an influence on the gameplay of \"\". Ubisoft Toronto creative director Maxime Beland cited the original \"Final Fantasy\" as a major influence on him. Media Molecule's Constantin Jupp credited \"Final Fantasy VII\" with getting him into game design. Tim Schafer also cited \"Final Fantasy VII\" as one of his favourite games of all time."}
{"id": "10975", "revid": "30212854", "url": "https://en.wikipedia.org/wiki?curid=10975", "title": "Fatty acid", "text": " In chemistry, particularly in biochemistry, a fatty acid is a carboxylic acid with a long aliphatic chain, which is either saturated or unsaturated. Most naturally occurring fatty acids have an unbranched chain of an even number of carbon atoms, from 4 to 28. Fatty acids are usually not found in organisms in their standalone form, but instead exist as three main classes of esters: triglycerides, phospholipids, and cholesteryl esters. In any of these forms, fatty acids are both important dietary sources of fuel for animals and they are important structural components for cells.\nHistory.\nThe concept of fatty acid (\"acide gras\") was introduced in 1813 by Michel Eug\u00e8ne Chevreul, though he initially used some variant terms: \"graisse acide\" and \"acide huileux\" (\"acid fat\" and \"oily acid\").\nTypes of fatty acids.\nFatty acids are classified in many ways: by length, by saturation vs unsaturation, by even vs odd carbon content, and by linear vs branched.\nSaturated fatty acids.\nSaturated fatty acids have no C=C double bonds. They have the same formula CH3(CH2)nCOOH, with variations in \"n\". An important saturated fatty acid is stearic acid (n = 16), which when neutralized with lye is the most common form of soap.\nUnsaturated fatty acids.\nUnsaturated fatty acids have one or more C=C double bonds. The C=C double bonds can give either \"cis\" or \"trans\" isomers.\nIn most naturally occurring unsaturated fatty acids, each double bond has three (n-3), six (n-6), or nine (n-9) carbon atoms after it, and all double bonds have a cis configuration. Most fatty acids in the \"trans\" configuration (trans fats) are not found in nature and are the result of human processing (e.g., hydrogenation). Some trans fatty acids also occur naturally in the milk and meat of ruminants (such as cattle and sheep). They are produced, by fermentation, in the rumen of these animals. They are also found in dairy products from milk of ruminants, and may be also found in breast milk of women who obtained them from their diet.\nThe geometric differences between the various types of unsaturated fatty acids, as well as between saturated and unsaturated fatty acids, play an important role in biological processes, and in the construction of biological structures (such as cell membranes).\nEven- vs odd-chained fatty acids.\nMost fatty acids are even-chained, e.g. stearic (C18) and oleic (C18), meaning they are composed of an even number of carbon atoms. Some fatty acids have odd numbers of carbon atoms; they are referred to as odd-chained fatty acids (OCFA). The most common OCFA are the saturated C15 and C17 derivatives, pentadecanoic acid and heptadecanoic acid respectively, which are found in dairy products. On a molecular level, OCFAs are biosynthesized and metabolized slightly differently from the even-chained relatives.\nNomenclature.\nCarbon atom numbering.\nMost naturally occurring fatty acids have an unbranched chain of carbon atoms, with a carboxyl group (\u2013COOH) at one end, and a methyl group (\u2013CH3) at the other end.\nThe position of the carbon atoms in the backbone of a fatty acid are usually indicated by counting from 1 at the \u2212COOH end. Carbon number \"x\" is often abbreviated or C-\"x\" (or sometimes C\"x\"), with \"x\"=1, 2, 3, etc. This is the numbering scheme recommended by the IUPAC.\nAnother convention uses letters of the Greek alphabet in sequence, starting with the first carbon \"after\" the carboxyl. Thus carbon \u03b1 (alpha) is C-2, carbon \u03b2 (beta) is C-3, and so forth. \nAlthough fatty acids can be of diverse lengths, in this second convention the last carbon in the chain is always labelled as \u03c9 (omega), which is the last letter in the Greek alphabet. A third numbering convention counts the carbons from that end, using the labels \"\u03c9\", \"\u03c9\u22121\", \"\u03c9\u22122\". Alternatively, the label \"\u03c9\u2212\"x\"\" is written \"n\u2212\"x\"\", where the \"n\" is meant to represent the number of carbons in the chain.\nIn either numbering scheme, the position of a double bond in a fatty acid chain is always specified by giving the label of the carbon closest to the carboxyl end. Thus, in an 18 carbon fatty acid, a double bond between C-12 (or \u03c9\u22126) and C-13 (or \u03c9\u22125) is said to be \"at\" position C-12 or \u03c9\u22126. The IUPAC naming of the acid, such as \"octadec-12-enoic acid\" (or the more pronounceable variant \"12-octadecanoic acid\") is always based on the \"C\" numbering.\nThe notation \u0394\"x\",\"y\"... is traditionally used to specify a fatty acid with double bonds at positions \"x\",\"y\"... (The capital Greek letter \"\u0394\" (delta) corresponds to Roman \"D\", for Double bond). Thus, for example, the 20-carbon arachidonic acid is \u03945,8,11,14, meaning that it has double bonds between carbons 5 and 6, 8 and 9, 11 and 12, and 14 and 15.\nIn the context of human diet and fat metabolism, unsaturated fatty acids are often classified by the position of the double bond closest to the \u03c9 carbon (only), even in the case of multiple double bonds such as the essential fatty acids. Thus linoleic acid (18 carbons, \u03949,12), \u03b3-linolenic acid (18-carbon, \u03946,9,12), and arachidonic acid (20-carbon, \u03945,8,11,14) are all classified as \"\u03c9\u22126\" fatty acids; meaning that their formula ends with \u2013CH=CH\u2013\u2013\u2013\u2013\u2013.\nFatty acids with an odd number of carbon atoms are called odd-chain fatty acids, whereas the rest are even-chain fatty acids. The difference is relevant to gluconeogenesis.\nNaming of fatty acids.\nThe following table describes the most common systems of naming fatty acids.\nFree fatty acids.\nWhen circulating in the plasma (plasma fatty acids), not in their ester, fatty acids are known as non-esterified fatty acids (NEFAs) or free fatty acids (FFAs). FFAs are always bound to a transport protein, such as albumin.\nProduction.\nIndustrial.\nFatty acids are usually produced industrially by the hydrolysis of triglycerides, with the removal of glycerol (see oleochemicals). Phospholipids represent another source. Some fatty acids are produced synthetically by hydrocarboxylation of alkenes.\nHyper-oxygenated fatty acids.\nHyper-oxygenated fatty acids are produced by a specific industrial processes for topical skin creams. The process is based on the introduction or saturation of peroxides into fatty acid esters via the presence of ultraviolet light and gaseous oxygen bubbling under controlled temperatures. Specifically linolenic acids have been shown to play an important role in maintaining the moisture barrier function of the skin (preventing water loss and skin dehydration). A study in Spain reported in the Journal of Wound Care in March 2005 compared a commercial product with a greasy placebo and that specific product was more effective and also cost-effective. A range of such OTC medical products is now widely available. However, topically applied olive oil was not found to be inferior in a \"randomised triple-blind controlled non-inferiority\" trial conducted in Spain during 2015. Commercial products are likely to be less messy to handle and more washable than either olive oil or petroleum jelly, both of which, if applied topically may stain clothing and bedding.\nBy animals.\nIn animals, fatty acids are formed from carbohydrates predominantly in the liver, adipose tissue, and the mammary glands during lactation.\nCarbohydrates are converted into pyruvate by glycolysis as the first important step in the conversion of carbohydrates into fatty acids. Pyruvate is then decarboxylated to form acetyl-CoA in the mitochondrion. However, this acetyl CoA needs to be transported into cytosol where the synthesis of fatty acids occurs. This cannot occur directly. To obtain cytosolic acetyl-CoA, citrate (produced by the condensation of acetyl-CoA with oxaloacetate) is removed from the citric acid cycle and carried across the inner mitochondrial membrane into the cytosol. There it is cleaved by ATP citrate lyase into acetyl-CoA and oxaloacetate. The oxaloacetate is returned to the mitochondrion as malate. The cytosolic acetyl-CoA is carboxylated by acetyl CoA carboxylase into malonyl-CoA, the first committed step in the synthesis of fatty acids.\nMalonyl-CoA is then involved in a repeating series of reactions that lengthens the growing fatty acid chain by two carbons at a time. Almost all natural fatty acids, therefore, have even numbers of carbon atoms. When synthesis is complete the free fatty acids are nearly always combined with glycerol (three fatty acids to one glycerol molecule) to form triglycerides, the main storage form of fatty acids, and thus of energy in animals. However, fatty acids are also important components of the phospholipids that form the phospholipid bilayers out of which all the membranes of the cell are constructed (the cell wall, and the membranes that enclose all the organelles within the cells, such as the nucleus, the mitochondria, endoplasmic reticulum, and the Golgi apparatus).\nThe \"uncombined fatty acids\" or \"free fatty acids\" found in the circulation of animals come from the breakdown (or lipolysis) of stored triglycerides. Because they are insoluble in water, these fatty acids are transported bound to plasma albumin. The levels of \"free fatty acids\" in the blood are limited by the availability of albumin binding sites. They can be taken up from the blood by all cells that have mitochondria (with the exception of the cells of the central nervous system). Fatty acids can only be broken down in mitochondria, by means of beta-oxidation followed by further combustion in the citric acid cycle to CO2 and water. Cells in the central nervous system, although they possess mitochondria, cannot take free fatty acids up from the blood, as the blood-brain barrier is impervious to most free fatty acids, excluding short-chain fatty acids and medium-chain fatty acids. These cells have to manufacture their own fatty acids from carbohydrates, as described above, in order to produce and maintain the phospholipids of their cell membranes, and those of their organelles.\nVariation between animal species.\nStudies on the cell membranes of mammals and reptiles discovered that mammalian cell membranes are composed of a higher proportion of polyunsaturated fatty acids (DHA, omega-3 fatty acid) than reptiles. Studies on bird fatty acid composition have noted similar proportions to mammals but with 1/3rd less omega-3 fatty acids as compared to omega-6 for a given body size. This fatty acid composition results in a more fluid cell membrane but also one that is permeable to various ions ( &amp; ), resulting in cell membranes that are more costly to maintain. This maintenance cost has been argued to be one of the key causes for the high metabolic rates and concomitant warm-bloodedness of mammals and birds. However polyunsaturation of cell membranes may also occur in response to chronic cold temperatures as well. In fish increasingly cold environments lead to increasingly high cell membrane content of both monounsaturated and polyunsaturated fatty acids, to maintain greater membrane fluidity (and functionality) at the lower temperatures.\nFatty acids in dietary fats.\nThe following table gives the fatty acid, vitamin E and cholesterol composition of some common dietary fats.\nReactions of fatty acids.\nFatty acids exhibit reactions like other carboxylic acids, i.e. they undergo esterification and acid-base reactions.\nAcidity.\nFatty acids do not show a great variation in their acidities, as indicated by their respective p\"K\"a. Nonanoic acid, for example, has a p\"K\"a of 4.96, being only slightly weaker than acetic acid (4.76). As the chain length increases, the solubility of the fatty acids in water decreases, so that the longer-chain fatty acids have minimal effect on the pH of an aqueous solution. Even those fatty acids that are insoluble in water will dissolve in warm ethanol, and can be titrated with sodium hydroxide solution using phenolphthalein as an indicator. This analysis is used to determine the free fatty acid content of fats; i.e., the proportion of the triglycerides that have been hydrolyzed.\nNeutralization of fatty acids, i.e. saponification, is a widely practiced route to metallic soaps.\nHydrogenation and hardening.\nHydrogenation of unsaturated fatty acids is widely practiced. Typical conditions involve 2.0\u20133.0 MPa of H2 pressure, 150\u00a0\u00b0C, and nickel supported on silica as a catalyst. This treatment affords saturated fatty acids. The extent of hydrogenation is indicated by the iodine number. Hydrogenated fatty acids are less prone toward rancidification. Since the saturated fatty acids are higher melting than the unsaturated precursors, the process is called hardening. Related technology is used to convert vegetable oils into margarine. The hydrogenation of triglycerides (vs fatty acids) is advantageous because the carboxylic acids degrade the nickel catalysts, affording nickel soaps. During partial hydrogenation, unsaturated fatty acids can be isomerized from \"cis\" to \"trans\" configuration.\nMore forcing hydrogenation, i.e. using higher pressures of H2 and higher temperatures, converts fatty acids into fatty alcohols. Fatty alcohols are, however, more easily produced from fatty acid esters.\nIn the Varrentrapp reaction certain unsaturated fatty acids are cleaved in molten alkali, a reaction which was, at one point of time, relevant to structure elucidation.\nAuto-oxidation and rancidity.\nUnsaturated fatty acids undergo a chemical change known as auto-oxidation. The process requires oxygen (air) and is accelerated by the presence of trace metals. Vegetable oils resist this process to a small degree because they contain antioxidants, such as tocopherol. Fats and oils often are treated with chelating agents such as citric acid to remove the metal catalysts.\nOzonolysis.\nUnsaturated fatty acids are susceptible to degradation by ozone. This reaction is practiced in the production of azelaic acid ((CH2)7(CO2H)2) from oleic acid.\nCirculation.\nDigestion and intake.\nShort- and medium-chain fatty acids are absorbed directly into the blood via intestine capillaries and travel through the portal vein just as other absorbed nutrients do. However, long-chain fatty acids are not directly released into the intestinal capillaries. Instead they are absorbed into the fatty walls of the intestine villi and reassemble again into triglycerides. The triglycerides are coated with cholesterol and protein (protein coat) into a compound called a chylomicron.\nFrom within the cell, the chylomicron is released into a lymphatic capillary called a lacteal, which merges into larger lymphatic vessels. It is transported via the lymphatic system and the thoracic duct up to a location near the heart (where the arteries and veins are larger). The thoracic duct empties the chylomicrons into the bloodstream via the left subclavian vein. At this point the chylomicrons can transport the triglycerides to tissues where they are stored or metabolized for energy.\nMetabolism.\nWhen metabolized, fatty acids yield large quantities of ATP. Many cell types can use either glucose or fatty acids for this purpose. Fatty acids (provided either by ingestion or by drawing on triglycerides stored in fatty tissues) are distributed to cells to serve as a fuel for muscular contraction and general metabolism. They are broken down to CO2 and water by the intra-cellular mitochondria, releasing large amounts of energy, captured in the form of ATP through beta oxidation and the citric acid cycle.\nEssential fatty acids.\nFatty acids that are required for good health but cannot be made in sufficient quantity from other substrates, and therefore must be obtained from food, are called essential fatty acids. There are two series of essential fatty acids: one has a double bond three carbon atoms away from the methyl end; the other has a double bond six carbon atoms away from the methyl end. Humans lack the ability to introduce double bonds in fatty acids beyond carbons 9 and 10, as counted from the carboxylic acid side. Two essential fatty acids are linoleic acid (LA) and alpha-linolenic acid (ALA). These fatty acids are widely distributed in plant oils. The human body has a limited ability to convert ALA into the longer-chain omega-3 fatty acids \u2014 eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), which can also be obtained from fish. Omega-3 and omega-6 fatty acids are biosynthetic precursors to endocannabinoids with antinociceptive, anxiolytic, and neurogenic properties.\nDistribution.\nBlood fatty acids adopt distinct forms in different stages in the blood circulation. They are taken in through the intestine in chylomicrons, but also exist in very low density lipoproteins (VLDL) and low density lipoproteins (LDL) after processing in the liver. In addition, when released from adipocytes, fatty acids exist in the blood as free fatty acids.\nIt is proposed that the blend of fatty acids exuded by mammalian skin, together with lactic acid and pyruvic acid, is distinctive and enables animals with a keen sense of smell to differentiate individuals.\nAnalysis.\nThe chemical analysis of fatty acids in lipids typically begins with an interesterification step that breaks down their original esters (triglycerides, waxes, phospholipids etc) and converts them to methyl esters, which are then separated by gas chromatography. or analyzed by gas chromatography and mid-infrared spectroscopy. \nSeparation of unsaturated isomers is possible by silver ion (argentation) thin-layer chromatography. Other separation techniques include high-performance liquid chromatography (with short columns packed with silica gel with bonded phenylsulfonic acid groups whose hydrogen atoms have been exchanged for silver ions). The role of silver lies in its ability to form complexes with unsaturated compounds.\nIndustrial uses.\nFatty acids are mainly used in the production of soap, both for cosmetic purposes and, in the case of metallic soaps, as lubricants. Fatty acids are also converted, via their methyl esters, to fatty alcohols and fatty amines, which are precursors to surfactants, detergents, and lubricants. Other applications include their use as emulsifiers, texturizing agents, wetting agents, anti-foam agents, or stabilizing agents.\nEsters of fatty acids with simpler alcohols (such as methyl-, ethyl-, n-propyl-, isopropyl- and butyl esters) are used as emollients in cosmetics and other personal care products and as synthetic lubricants. Esters of fatty acids with more complex alcohols, such as sorbitol, ethylene glycol, diethylene glycol, and polyethylene glycol are consumed in food, or used for personal care and water treatment, or used as synthetic lubricants or fluids for metal working."}
{"id": "10977", "revid": "27529051", "url": "https://en.wikipedia.org/wiki?curid=10977", "title": "Fearless (1993 film)", "text": "Fearless is a 1993 American drama film directed by Peter Weir and starring Jeff Bridges, Isabella Rossellini, Rosie Perez and John Turturro. It was written by Rafael Yglesias, adapted from his novel of the same name.\nRosie Perez was nominated for an Academy Award for Best Supporting Actress for her role as Carla Rodrigo. The film was also entered into the 44th Berlin International Film Festival. Jeff Bridges' role as Max Klein is widely regarded as one of the best performances of his career. The film's soundtrack features part of the first movement of Henryk G\u00f3recki's Symphony No. 3, subtitled \"Symphony of Sorrowful Songs\". The film's screenwriter was inspired to write the script after he was in a car accident. Yglesias began writing the story after reading about United Airlines Flight 232, that crashed in Sioux City, Iowa, in 1989.\nPlot.\nMax Klein survives an airline crash. The plane plummets, but strangely Max is calm. His calm enables him to dispel fear in the flight cabin. He sits next to Byron Hummel, a young boy flying alone. Flight attendants move through the cabin, telling another passenger, Carla Rodrigo, traveling with an infant, to hold the infant in her lap as the plane plummets out of control, while telling other passengers to buckle into their seats. Max was telling his business partner, Jeff Gordon, of his fear of flying as they took off.\nIn the aftermath of the crash, most passengers died. Among the few survivors, most are terribly injured. Max is unhurt. The crash site is chaotic, filled with first responders and other emergency personnel. Focusing on the survivors, a team of investigators from the FAA and the airline company conduct interviews. Max is repelled by all the chaos. He is disgusted by the investigators wanting to interview him.\nMax rents a car and starts driving home. Along the way he meets an old girlfriend from high school, Alison. They last met 20 years ago. At the restaurant Alison notices Max eating a strawberry. Max is allergic to strawberries. Max grins. He finishes the strawberry without an allergic reaction. The next morning, he is accosted by FBI investigators. They question his choice to not contact family to tell them he is fine. The airline representative offers him train tickets. Max asks for airline tickets. He wants to fly home, having no fear of air travel. The airline books him on the flight. They seat him next to Dr. Bill Perlman, the airline's psychiatrist.\nDr. Perlman annoyingly tags behind Max back to his home, prodding him for information about the crash. Max is forced to snap back at the psychiatrist rudely, to be rid of him. Laura Klein, Max's wife, notices the strange behavior. Max seems different, changed somehow. Max's late business partner's wife, Nan Gordon, asks about Jeff's last moments. Max says Jeff died in the crash.\nThe media call Max \"The Good Samaritan\" in news reports. The boy Max sat next to, Byron, publicly thanks him in television interviews, for the way he comforted passengers while the plane fell out of control during the crash. Max is a hero.\nMax avoids the press and becomes distant from Laura and his son Jonah. His persona is radically changed. He is preoccupied with his new perspective on life following his near death experience. He begins drawing abstract pictures of the crash. As he survived without injury, he thinks himself invulnerable to death. Because of his confidence, Dr. Perlman encourages Max to meet with another survivor, Carla Rodrigo, whose infant was held in her lap while the plane fell. Carla struggles with survivor's guilt, and is traumatized for not holding onto him tightly enough, although she was following the flight attendant's instructions (this was before child safety seats were required for infant passengers). Max and Carla develop a close friendship. He helps her to get past the trauma, to free herself from guilt, deliberately crashing his car to show that it was physically impossible for any person to hold onto anything due to the forces of the crash.\nAttorney Steven Brillstein encourages Max to exaggerate testimony, to maximize the settlement offer from the airline. Max reluctantly agrees when he is confronted with Nan's financial predicament as a widow. Cognitive dissonance spurs Max to a panic attack. He runs out of the office, to the roof of the building. He climbs onto the roof's edge. As Max stands on the ledge, looking down at the streets below, his panic subsides. He rejoices in fearlessness. Laura finds Max on the ledge. He is spinning around on the ledge with his overcoat billowing across his face.\nBrillstein arrives at the Klein home to celebrate the airline's settlement offer. He brings a fruit basket. Max eats one of the strawberries. This time he experiences an allergic reaction. Max is resuscitated by Laura and survives. He recovers his emotional connection to his family, to the world and to the reality of yet another chance at life.\nAesthetic elements.\nA book containing the painting \"The Ascent into the Empyrean\" by Hieronymus Bosch is shown, and it is said that the dying go into the light of heaven \"naked and alone\". Near the finale as Max lies on the ground, he relives moving from the fuselage of the aircraft and for a moment moves towards the tunnel of light that appears to be modeled on the painting.\nReception.\nThe film was positively received by critics as review aggregator Rotten Tomatoes evaluates that 85% of critics have given the film a positive review, based on 40 reviews, with an average score of 7.8/10. Its consensus states \"This underrated gem from director Peter Weir features an outstanding performance from Jeff Bridges as a man dealing with profound grief.\"\nHome media.\nWith video and audio quality superseding previous home video releases, \"Fearless\" was released on Blu-ray Disc by the Warner Archive Collection in November 2013."}
{"id": "10979", "revid": "25048049", "url": "https://en.wikipedia.org/wiki?curid=10979", "title": "Franklin D. Roosevelt", "text": "Franklin Delano Roosevelt (, ; January 30, 1882April 12, 1945), often referred to by his initials FDR, was an American politician who served as the 32nd president of the United States from 1933 until his death in 1945. A member of the Democratic Party, he won a record four presidential elections and became a central figure in world events during the first half of the 20th century. Roosevelt directed the federal government during most of the Great Depression, implementing his New Deal domestic agenda in response to the worst economic crisis in U.S. history. As a dominant leader of his party, he built the New Deal Coalition, which defined modern liberalism in the United States throughout the middle third of the 20th century. His third and fourth terms were dominated by World War II, which ended shortly after he died in office.\nRoosevelt was born in Hyde Park, New York, to the Roosevelt family made well known by the reputation of Theodore Roosevelt, the 26th president of the United States, as well as by the reputation of prominent businessman William Henry Aspinwall. He graduated from the Groton School and Harvard College, and attended Columbia Law School but left after passing the bar exam to practice law in New York City. In 1905, he married his fifth cousin once removed, Eleanor Roosevelt. They had six children, of whom five survived into adulthood. He won election to the New York State Senate in 1910, and then served as Assistant Secretary of the Navy under President Woodrow Wilson during World War I. Roosevelt was James M. Cox's running mate on the Democratic Party's 1920 national ticket, but Cox was defeated by Republican Warren G. Harding. In 1921, Roosevelt contracted a paralytic illness, believed at the time to be polio, and his legs became permanently paralyzed. While attempting to recover from his condition, Roosevelt founded a polio rehabilitation center in Warm Springs, Georgia. In spite of being unable to walk unaided, Roosevelt returned to public office by winning election as Governor of New York in 1928. He served as governor from 1929 to 1933, promoting programs to combat the economic crisis besetting the United States.\nIn the 1932 presidential election, Roosevelt defeated Republican incumbent Herbert Hoover in a landslide. Roosevelt took office in the midst of the Great Depression, the worst economic crisis in U.S. history. During the first 100 days of the 73rd United States Congress, Roosevelt spearheaded unprecedented federal legislation and issued a profusion of executive orders that instituted the New Deal\u00a0\u2014 a variety of programs designed to produce relief, recovery, and reform. He created numerous programs to provide relief to the unemployed and farmers while seeking economic recovery with the National Recovery Administration and other programs. He also instituted major regulatory reforms related to finance, communications, and labor, and presided over the end of Prohibition. He used radio to speak directly to the American people, giving 30 \"fireside chat\" radio addresses during his presidency and becoming the first American president to be televised. With the economy having improved rapidly from 1933 to 1936, Roosevelt won a landslide reelection in 1936. After the 1936 election, Roosevelt sought passage of the Judiciary Reorganization Bill of 1937 (the \"court packing plan\"), which would have expanded the size of the Supreme Court of the United States. The bipartisan Conservative Coalition that formed in 1937 prevented passage of the bill and blocked the implementation of further New Deal programs and reforms. The economy then relapsed into a deep recession in 1937 and 1938. Major surviving programs and legislation implemented under Roosevelt include the Securities and Exchange Commission, the National Labor Relations Act, the Federal Deposit Insurance Corporation, Social Security, and the Fair Labor Standards Act of 1938.\nThe United States reelected FDR in 1940 for his third term, making him the only U.S. president to serve for more than two terms. With World War II looming after 1938, the U.S. remained officially neutral, but Roosevelt gave strong diplomatic and financial support to China, the United Kingdom and eventually the Soviet Union. Following the Japanese attack on Pearl Harbor on December 7, 1941, an event he called \"a date which will live in infamy\", Roosevelt obtained a congressional declaration of war on Japan, and, a few days later, on Germany and Italy. Assisted by his top aide Harry Hopkins and with very strong national support, he worked closely with British Prime Minister Winston Churchill, Soviet leader Joseph Stalin and Chinese Generalissimo Chiang Kai-shek in leading the Allied Powers against the Axis Powers. Roosevelt supervised the mobilization of the U.S. economy to support the war effort, and implemented a Europe first strategy, making the defeat of Germany a priority over that of Japan. He also initiated the development of the world's first atomic bomb, and worked with other Allied leaders to lay the groundwork for the United Nations and other post-war institutions. Roosevelt won reelection in 1944, but with his physical health declining during the war years, he died in April 1945, less than three months into his fourth term. The Axis Powers surrendered to the Allies in the months following Roosevelt's death, during the presidency of his successor, Harry S. Truman. Roosevelt is usually rated by scholars among the nation's greatest presidents, with George Washington and Abraham Lincoln, but has also been subject to substantial criticism.\nEarly life and marriage.\nChildhood.\nFranklin Delano Roosevelt was born on January 30, 1882, in the Hudson Valley town of Hyde Park, New York, to businessman James Roosevelt I and his second wife, Sara Ann Delano. Roosevelt's parents, who were sixth cousins, both came from wealthy old New York families, the Roosevelts, the Aspinwalls and the Delanos, respectively. Roosevelt's patrilineal ancestor migrated to New Amsterdam in the 17th century, and the Roosevelts flourished as merchants and landowners. The Delano family progenitor, Philip Delano, traveled to the New World on the \"Fortune\" in 1621, and the Delanos prospered as merchants and shipbuilders in Massachusetts. Franklin had a half-brother, James \"Rosy\" Roosevelt, from his father's previous marriage.\nRoosevelt grew up in a wealthy family. His father James graduated from Harvard Law School in 1851, but chose not to practice law after receiving an inheritance from his grandfather, James Roosevelt. Roosevelt's father was a prominent Bourbon Democrat who once took Franklin to meet President Grover Cleveland in the White House. The president said to him: \"My little man, I am making a strange wish for you. It is that you may never be President of the United States.\" His mother Sara was the dominant influence in Franklin's early years. She once declared, \"My son Franklin is a Delano, not a Roosevelt at all.\" James, who was 54 when Franklin was born, was considered by some as a remote father, though biographer James MacGregor Burns indicates James interacted with his son more than was typical at the time.\nRoosevelt learned to ride, shoot, row, and to play polo and lawn tennis. He took up golf in his teen years, becoming a skilled long hitter. He learned to sail early, and when he was 16, his father gave him a sailboat.\nEducation and early career.\nFrequent trips to Europe\u00a0\u2014 he made his first excursion at the age of two and went with his parents every year from the ages of seven to fifteen\u00a0\u2014 helped Roosevelt become conversant in German and French. Except for attending public school in Germany at age nine, Roosevelt was home-schooled by tutors until age 14. He then attended Groton School, an Episcopal boarding school in Groton, Massachusetts, joining the third form. Its headmaster, Endicott Peabody, preached the duty of Christians to help the less fortunate and urged his students to enter public service. Peabody remained a strong influence throughout Roosevelt's life, officiating at his wedding and visiting him as president.\nLike most of his Groton classmates, Roosevelt went to Harvard College. Roosevelt was an average student academically, and he later declared, \"I took economics courses in college for four years, and everything I was taught was wrong.\" He was a member of the Alpha Delta Phi fraternity and the Fly Club, and served as a school cheerleader. Roosevelt was relatively undistinguished as a student or athlete, but he became editor-in-chief of \"The Harvard Crimson\" daily newspaper, a position that required great ambition, energy, and the ability to manage others.\nRoosevelt's father died in 1900, causing great distress for him. The following year, Roosevelt's fifth cousin Theodore Roosevelt became President of the United States. Theodore's vigorous leadership style and reforming zeal made him Franklin's role model and hero. Roosevelt graduated from Harvard in 1903 with an A.B. in history. He entered Columbia Law School in 1904, but dropped out in 1907 after passing the New York bar exam. In 1908, he took a job with the prestigious law firm of Carter Ledyard &amp; Milburn, working in the firm's admiralty law division.\nMarriage, family, and affairs.\nIn mid-1902, Franklin began courting his future wife Eleanor Roosevelt, with whom he had been acquainted as a child. Eleanor and Franklin were fifth cousins, once removed, and Eleanor was a niece of Theodore Roosevelt. They began corresponding with each other in 1902, and in October 1903, Franklin proposed marriage to Eleanor.\nOn March 17, 1905, Roosevelt married Eleanor, despite the fierce resistance of his mother. While she did not dislike Eleanor, Sara Roosevelt was very possessive of her son, believing he was too young for marriage. She attempted to break the engagement several times. Eleanor's uncle, President Theodore Roosevelt, stood in at the wedding for Eleanor's deceased father, Elliott. The young couple moved into Springwood, his family's estate at Hyde Park. The home was owned by Sara Roosevelt until her death in 1941 and was very much her home as well. In addition, Franklin and Sara Roosevelt did the planning and furnishing of a townhouse Sara had built for the young couple in New York City; Sara had a twin house built alongside for herself. Eleanor never felt at home in the houses at Hyde Park or New York, but she loved the family's vacation home on Campobello Island, which Sara gave to the couple.\nBiographer James MacGregor Burns said that young Roosevelt was self-assured and at ease in the upper-class. In contrast, Eleanor at the time was shy and disliked social life, and at first, stayed at home to raise their several children. As his father had, Franklin left the raising of the children to his wife, while Eleanor in turn largely relied on hired caregivers to raise the children. Referring to her early experience as a mother, she later stated that she knew \"absolutely nothing about handling or feeding a baby.\" Although Eleanor had an aversion to sexual intercourse and considered it \"an ordeal to be endured\", she and Franklin had six children. Anna, James, and Elliott were born in 1906, 1907, and 1910, respectively. The couple's second son, Franklin, died in infancy in 1909. Another son, also named Franklin, was born in 1914, and the youngest child, John, was born in 1916.\nRoosevelt had several extra-marital affairs, including one with Eleanor's social secretary Lucy Mercer, which began soon after she was hired in early 1914. In September 1918, Eleanor found letters revealing the affair in Roosevelt's luggage. Franklin contemplated divorcing Eleanor, but Sara objected strongly and Lucy would not agree to marry a divorced man with five children. Franklin and Eleanor remained married, and Roosevelt promised never to see Lucy again. Eleanor never truly forgave him, and their marriage from that point on was more of a political partnership. Eleanor soon thereafter established a separate home in Hyde Park at Val-Kill, and increasingly devoted herself to various social and political causes independently of her husband. The emotional break in their marriage was so severe that when Roosevelt asked Eleanor in 1942\u00a0\u2014 in light of his failing health\u00a0\u2014 to come back home and live with him again, she refused. He was not always aware of when she visited the White House and for some time she could not easily reach him on the telephone without his secretary's help; Roosevelt, in turn, did not visit Eleanor's New York City apartment until late 1944.\nFranklin broke his promise to Eleanor to refrain from having affairs. He and Lucy maintained a formal correspondence, and began seeing each other again in 1941, or perhaps earlier. Lucy was with Roosevelt on the day he died in 1945. Despite this, Roosevelt's affair was not widely known until the 1960s. Roosevelt's son Elliott claimed that his father had a 20-year affair with his private secretary, Marguerite \"Missy\" LeHand. Another son, James, stated that \"there is a real possibility that a romantic relationship existed\" between his father and Crown Princess M\u00e4rtha of Norway, who resided in the White House during part of World War II. Aides began to refer to her at the time as \"the president's girlfriend\", and gossip linking the two romantically appeared in the newspapers.\nEarly political career (1910\u20131920).\nNew York state senator (1910\u20131913).\nRoosevelt held little passion for the practice of law and confided to friends that he planned to eventually enter politics. Despite his admiration for his cousin Theodore, Franklin inherited his father's affiliation with the Democratic Party. Prior to the 1910 elections, the local Democratic Party recruited Roosevelt to run for a seat in the New York State Assembly. Roosevelt was an attractive recruit for the party because Theodore was still one of the country's most prominent politicians, and a Democratic Roosevelt was good publicity; the candidate could also pay for his own campaign. Roosevelt's campaign for the state assembly ended after the Democratic incumbent, Lewis Stuyvesant Chanler, chose to seek re-election. Rather than putting his political hopes on hold, Roosevelt ran for a seat in the state senate. The senate district, located in Dutchess County, Columbia County, and Putnam County, was strongly Republican. Roosevelt feared that open opposition from Theodore could effectively end his campaign, but Theodore privately encouraged his cousin's candidacy despite their differences in partisan affiliation. Acting as his own campaign manager, Roosevelt traveled throughout the senate district via automobile at a time when many could not afford cars. Due to his aggressive and effective campaign, the Roosevelt name's influence in the Hudson Valley, and the Democratic landslide in the 1910 United States elections, Roosevelt won, surprising almost everyone.\nThough legislative sessions rarely lasted more than ten weeks, Roosevelt treated his new position as a full-time career. Taking his seat on January 1, 1911, Roosevelt immediately became the leader of a group of \"Insurgents\" who opposed the bossism of the Tammany Hall machine that dominated the state Democratic Party. In the 1911 U.S. Senate election, which was determined in a joint session of the New York state legislature, Roosevelt and nineteen other Democrats caused a prolonged deadlock by opposing a series of Tammany-backed candidates. Finally, Tammany threw its backing behind James A. O'Gorman, a highly regarded judge whom Roosevelt found acceptable, and O'Gorman won the election in late March. Roosevelt soon became a popular figure among New York Democrats, though he had not yet become an eloquent speaker. News articles and cartoons began depicting \"the second coming of a Roosevelt\" that sent \"cold shivers down the spine of Tammany\".\nRoosevelt, again in opposition to Tammany Hall, supported New Jersey Governor Woodrow Wilson's successful bid for the 1912 Democratic nomination, earning an informal designation as an original Wilson man. The election became a three-way contest, as Theodore Roosevelt left the Republican Party to launch a third party campaign against Wilson and sitting Republican President William Howard Taft. Franklin's decision to back Wilson over Theodore Roosevelt in the general election alienated some members of his family, although Theodore himself was not offended. Wilson's victory over the divided Republican Party made him the first Democrat to win a presidential election since 1892. Overcoming a bout with typhoid fever, and with extensive assistance from journalist Louis McHenry Howe, Roosevelt was re-elected in the 1912 elections. After the election, he served for a short time as chairman of the Agriculture Committee, and his success with farm and labor bills was a precursor to his New Deal policies twenty years later. By this time he had become more consistently progressive, in support of labor and social welfare programs for women and children; cousin Theodore was of some influence on these issues.\nAssistant Secretary of the Navy (1913\u20131919).\nRoosevelt's support of Wilson led to his appointment in March 1913 as Assistant Secretary of the Navy, the second-ranking official in the Navy Department after Secretary Josephus Daniels. Roosevelt had a lifelong affection for the Navy\u00a0\u2014 he had already collected almost 10,000 naval books and claimed to have read all but one\u00a0\u2014 and was more ardent than Daniels in supporting a large and efficient naval force. With Wilson's support, Daniels and Roosevelt instituted a merit-based promotion system and made other reforms to extend civilian control over the autonomous departments of the Navy. Roosevelt oversaw the Navy's civilian employees and earned the respect of union leaders for his fairness in resolving disputes. Not a single strike occurred during his seven-plus years in the office, during which Roosevelt gained experience in labor issues, government management during wartime, naval issues, and logistics, all valuable areas for future office.\nIn 1914, Roosevelt made an ill-conceived decision to run for the seat of retiring Republican Senator Elihu Root of New York. Though Roosevelt won the backing of Treasury Secretary William Gibbs McAdoo and Governor Martin H. Glynn, he faced a formidable opponent in the Tammany-backed James W. Gerard. He also lacked Wilson's backing, as Wilson needed Tammany's forces to help marshal his legislation and secure his 1916 re-election. Roosevelt was soundly defeated in the Democratic primary by Gerard, who in turn lost the general election to Republican James Wolcott Wadsworth Jr. Roosevelt learned a valuable lesson, that federal patronage alone, without White House support, could not defeat a strong local organization. After the election, Roosevelt and the boss of the Tammany Hall machine, Charles Francis Murphy, sought an accommodation with one another and became political allies.\nFollowing his defeat in the Senate primary, Roosevelt refocused on the Navy Department. World War I broke out in July 1914, with the Central Powers of Germany, Austria-Hungary, and the Ottoman Empire seeking to defeat the Allied Powers of Britain, France, and Russia. Though he remained publicly supportive of Wilson, Roosevelt sympathized with the Preparedness Movement, whose leaders strongly favored the Allied Powers and called for a military build-up. The Wilson administration initiated an expansion of the Navy after the sinking of the RMS Lusitania by a German submarine, and Roosevelt helped establish the United States Navy Reserve and the Council of National Defense. In April 1917, after Germany declared it would engage in unrestricted submarine warfare and attacked several U.S. ships, Wilson asked Congress for a declaration of war. Congress approved the declaration of war on Germany on April 6.\nRoosevelt requested that he be allowed to serve as a naval officer, but Wilson insisted that he continue to serve as Assistant Secretary of the Navy. For the next year, Roosevelt remained in Washington to coordinate the mobilization, supply, and deployment of naval vessels and personnel. In the first six months after the U.S. entered the war, the Navy expanded fourfold. In the summer of 1918, Roosevelt traveled to Europe to inspect naval installations and meet with French and British officials. In September, he returned to the United States on board the USS \"Leviathan\", a large troop carrier. On the 11-day voyage, the pandemic influenza virus struck and killed many on board. Roosevelt became very ill with influenza and a complicating pneumonia, but he recovered by the time the ship landed in New York. After Germany signed an armistice in November 1918, surrendering and ending the fighting, Daniels and Roosevelt supervised the demobilization of the Navy. Against the advice of older officers such as Admiral William Bensonwho claimed he could not \"conceive of any use the fleet will ever have for aviation\"Roosevelt personally ordered the preservation of the Navy's . With the Wilson administration coming to an end, Roosevelt began planning for his next run for office. Roosevelt and his associates approached Herbert Hoover about running for the 1920 Democratic presidential nomination, with Roosevelt as his running mate.\nCampaign for vice president (1920).\nRoosevelt's plan to convince Hoover to run for the Democratic nomination fell through after Hoover publicly declared himself to be a Republican, but Roosevelt nonetheless decided to seek the 1920 vice presidential nomination. After Governor James M. Cox of Ohio won the party's presidential nomination at the 1920 Democratic National Convention, he chose Roosevelt as his running mate, and the party formally nominated Roosevelt by acclamation. Although his nomination surprised most people, Roosevelt balanced the ticket as a moderate, a Wilsonian, and a prohibitionist with a famous name. Roosevelt had just turned 38, four years younger than Theodore had been when he received the same nomination from his party. Roosevelt resigned as Assistant Secretary of the Navy after the Democratic convention and campaigned across the nation for the Cox\u2013Roosevelt ticket.\nDuring the campaign, Cox and Roosevelt defended the Wilson administration and the League of Nations, both of which were unpopular in 1920. Roosevelt personally supported U.S. membership in the League of Nations, but, unlike Wilson, he favored compromising with Senator Henry Cabot Lodge and other \"Reservationists.\" The Cox\u2013Roosevelt ticket was defeated by Republicans Warren G. Harding and Calvin Coolidge in the presidential election by a wide margin, and the Republican ticket carried every state outside of the South. Roosevelt accepted the loss without issue and later reflected that the relationships and good will that he built in the 1920 campaign proved to be a major asset in his 1932 campaign. The 1920 election also saw the first public participation of Eleanor Roosevelt who, with the support of Louis Howe, established herself as a valuable political ally.\nParalytic illness and political comeback (1921\u20131928).\nAfter the election, Roosevelt returned to New York City, where he practiced law and served as a vice president of the Fidelity and Deposit Company. He also sought to build support for a political comeback in the 1922 elections, but his career was derailed by illness. While the Roosevelts were vacationing at Campobello Island in August 1921, he fell ill. His main symptoms were fever; symmetric, ascending paralysis; facial paralysis; bowel and bladder dysfunction; numbness and hyperesthesia; and a descending pattern of recovery. Roosevelt was left permanently paralyzed from the waist down. He was diagnosed with poliomyelitis at the time, but his symptoms are now believed to be more consistent with Guillain\u2013Barr\u00e9 syndrome \u2013 an autoimmune neuropathy which Roosevelt's doctors failed to consider as a diagnostic possibility.\nThough his mother favored his retirement from public life, Roosevelt, his wife, and Roosevelt's close friend and adviser, Louis Howe, were all determined that he continue his political career. He convinced many people that he was improving, which he believed to be essential prior to running for public office again. He laboriously taught himself to walk short distances while wearing iron braces on his hips and legs by swiveling his torso, supporting himself with a cane. He was careful never to be seen using his wheelchair in public, and great care was taken to prevent any portrayal in the press that would highlight his disability. However, his disability was well known before and during his presidency and became a major part of his image. He usually appeared in public standing upright, supported on one side by an aide or one of his sons.\nBeginning in 1925, Roosevelt spent most of his time in the Southern United States, at first on his houseboat, the \"Larooco\". Intrigued by the potential benefits of hydrotherapy, he established a rehabilitation center at Warm Springs, Georgia, in 1926. To create the rehabilitation center, he assembled a staff of physical therapists and used most of his inheritance to purchase the Merriweather Inn. In 1938, he founded the National Foundation for Infantile Paralysis, leading to the development of polio vaccines.\nRoosevelt maintained contacts with the Democratic Party during the 1920s, and he remained active in New York politics while also establishing contacts in the South, particularly in Georgia. He issued an open letter endorsing Al Smith's successful campaign in New York's 1922 gubernatorial election, which both aided Smith and showed Roosevelt's continuing relevance as a political figure. Roosevelt and Smith came from different backgrounds and never fully trusted one another, but Roosevelt supported Smith's progressive policies, while Smith was happy to have the backing of the prominent and well-respected Roosevelt.\nRoosevelt gave presidential nominating speeches for Smith at the 1924 and 1928 Democratic National Conventions; the speech at the 1924 convention marked a return to public life following his illness and convalescence. That year, the Democrats were badly divided between an urban wing, led by Smith, and a conservative, rural wing, led by William Gibbs McAdoo, on the 101st ballot, the nomination went to John W. Davis, a compromise candidate who suffered a landslide defeat in the 1924 presidential election. Like many others throughout the United States, Roosevelt did not abstain from alcohol during the Prohibition era, but publicly he sought to find a compromise on Prohibition acceptable to both wings of the party.\nIn 1925, Smith appointed Roosevelt to the Taconic State Park Commission, and his fellow commissioners chose him as chairman. In this role, he came into conflict with Robert Moses, a Smith prot\u00e9g\u00e9, who was the primary force behind the Long Island State Park Commission and the New York State Council of Parks. Roosevelt accused Moses of using the name recognition of prominent individuals including Roosevelt to win political support for state parks, but then diverting funds to the ones Moses favored on Long Island, while Moses worked to block the appointment of Howe to a salaried position as the Taconic commission's secretary. Roosevelt served on the commission until the end of 1928, and his contentious relationship with Moses continued as their careers progressed.\nGovernor of New York (1929\u20131932).\nAs the Democratic Party presidential nominee in the 1928 election, Smith, in turn, asked Roosevelt to run for governor in the state election. Roosevelt initially resisted the entreaties of Smith and others within the party, as he was reluctant to leave Warm Springs and feared a Republican landslide in 1928. He agreed to run when party leaders convinced him that only he could defeat the Republican gubernatorial nominee, New York Attorney General Albert Ottinger. Roosevelt won the party's gubernatorial nomination by acclamation, and he once again turned to Howe to lead his campaign. Roosevelt was also joined on the campaign trail by Samuel Rosenman, Frances Perkins, and James Farley, all of whom would become important political associates. While Smith lost the presidency in a landslide, and was defeated in his home state, Roosevelt was elected governor by a one-percent margin. Roosevelt's election as governor of the most populous state immediately made him a contender in the next presidential election.\nUpon taking office in January 1929, Roosevelt proposed the construction of a series of hydroelectric power plants and sought to address the ongoing farm crisis of the 1920s. Relations between Roosevelt and Smith suffered after Roosevelt chose not to retain key Smith appointees like Moses. Roosevelt and his wife Eleanor established a political understanding that would last for the duration of his political career; she would dutifully serve as the governor's wife but would also be free to pursue her own agenda and interests. He also began holding \"fireside chats\", in which he directly addressed his constituents via radio, often using these chats to pressure the New York State Legislature to advance his agenda.\nIn October 1929, the Wall Street Crash occurred, and the country began sliding into the Great Depression. While President Hoover and many state governors believed that the economic crisis would subside, Roosevelt saw the seriousness of the situation and established a state employment commission. He also became the first governor to publicly endorse the idea of unemployment insurance.\nWhen Roosevelt began his run for a second term in May 1930, he reiterated his doctrine from the campaign two years before: \"that progressive government by its very terms must be a living and growing thing, that the battle for it is never-ending and that if we let up for one single moment or one single year, not merely do we stand still but we fall back in the march of civilization.\" He ran on a platform that called for aid to farmers, full employment, unemployment insurance, and old-age pensions. His Republican opponent could not overcome the public's criticism of the Republican Party during the economic downturn, and Roosevelt was elected to a second term by a 14% margin.\nWith the Hoover administration resisting proposals to directly address the economic crisis, Roosevelt proposed an economic relief package and the establishment of the Temporary Emergency Relief Administration to distribute those funds. Led first by Jesse I. Straus and then by Harry Hopkins, the agency assisted well over one-third of New York's population between 1932 and 1938. Roosevelt also began an investigation into corruption in New York City among the judiciary, the police force, and organized crime, prompting the creation of the Seabury Commission. Many public officials were removed from office as a result.\n1932 presidential election.\nAs the 1932 presidential election approached, Roosevelt increasingly turned his attention to national politics. He established a campaign team led by Howe and Farley and a \"brain trust\" of policy advisers. With the economy ailing, many Democrats hoped that the 1932 elections would result in the election of the first Democratic president since Woodrow Wilson.\nRoosevelt's re-election as governor had established him as the front-runner for the 1932 Democratic presidential nomination. Roosevelt rallied the progressive supporters of the Wilson administration while also appealing to many conservatives, establishing himself as the leading candidate in the South and West. The chief opposition to Roosevelt's candidacy came from Northeastern conservatives such as Al Smith, the 1928 Democratic presidential nominee. Smith hoped to deny Roosevelt the two-thirds support necessary to win the party's presidential nomination at the 1932 Democratic National Convention in Chicago, and then emerge as the nominee after multiple rounds of balloting.\nRoosevelt entered the convention with a delegate lead due to his success in the 1932 Democratic primaries, but most delegates entered the convention unbound to any particular candidate. On the first presidential ballot of the convention, Roosevelt received the votes of more than half but less than two-thirds of the delegates, with Smith finishing in a distant second place. Speaker of the House John Nance Garner, who controlled the votes of Texas and California, threw his support behind Roosevelt after the third ballot, and Roosevelt clinched the nomination on the fourth ballot. With little input from Roosevelt, Garner won the vice-presidential nomination. Roosevelt flew in from New York after learning that he had won the nomination, becoming the first major-party presidential nominee to accept the nomination in person.\nIn his acceptance speech, Roosevelt declared, \"I pledge you, I pledge myself to a new deal for the American people... This is more than a political campaign. It is a call to arms.\" Roosevelt promised securities regulation, tariff reduction, farm relief, government-funded public works, and other government actions to address the Great Depression. Reflecting changing public opinion, the Democratic platform included a call for the repeal of Prohibition; Roosevelt himself had not taken a public stand on the issue prior to the convention but promised to uphold the party platform.\nAfter the convention, Roosevelt won endorsements from several progressive Republicans, including George W. Norris, Hiram Johnson, and Robert La Follette Jr. He also reconciled with the party's conservative wing, and even Al Smith was persuaded to support the Democratic ticket. Hoover's handling of the Bonus Army further damaged the incumbent's popularity, as newspapers across the country criticized the use of force to disperse assembled veterans.\nRoosevelt won 57% of the popular vote and carried all but six states. Historians and political scientists consider the 1932\u201336 elections to be a political realignment. Roosevelt's victory was enabled by the creation of the New Deal coalition, small farmers, the Southern whites, Catholics, big city political machines, labor unions, northern African Americans (southern ones were still disfranchised), Jews, intellectuals, and political liberals. The creation of the New Deal coalition transformed American politics and started what political scientists call the \"New Deal Party System\" or the Fifth Party System. Between the Civil War and 1929, Democrats had rarely controlled both houses of Congress and had won just four of seventeen presidential elections; from 1932 to 1979, Democrats won eight of twelve presidential elections and generally controlled both houses of Congress.\nPresidency (1933\u20131945).\nRoosevelt was elected in November 1932 but, like his predecessors, did not take office until the following March. After the election, President Hoover sought to convince Roosevelt to renounce much of his campaign platform and to endorse the Hoover administration's policies. Roosevelt refused Hoover's request to develop a joint program to stop the downward economic spiral, claiming that it would tie his hands and that Hoover had all the power to act if necessary. The economy spiraled downward until the banking system began a complete nationwide shutdown as Hoover's term ended. Roosevelt used the transition period to select the personnel for his incoming administration, and he chose Howe as his chief of staff, Farley as Postmaster General, and Frances Perkins as Secretary of Labor. William H. Woodin, a Republican industrialist close to Roosevelt, was the choice for Secretary of the Treasury, while Roosevelt chose Senator Cordell Hull of Tennessee as Secretary of State. Harold L. Ickes and Henry A. Wallace, two progressive Republicans, were selected for the roles of Secretary of the Interior and Secretary of Agriculture, respectively. In February 1933, Roosevelt escaped an assassination attempt by Giuseppe Zangara, who expressed a \"hate for all rulers.\" Attempting to shoot Roosevelt, Zangara instead mortally wounded Chicago Mayor Anton Cermak, who was sitting alongside Roosevelt.\nRoosevelt appointed powerful men to top positions but made all the major decisions, regardless of delays, inefficiency or resentment. Analyzing the president's administrative style, historian James MacGregor Burns concludes:\nFirst and second terms (1933\u20131941).\nWhen Roosevelt was inaugurated on March 4, 1933, the U.S. was at the nadir of the worst depression in its history. A quarter of the workforce was unemployed. Farmers were in deep trouble as prices had fallen by 60%. Industrial production had fallen by more than half since 1929. Two million people were homeless. By the evening of March 4, 32 of the 48 states\u00a0\u2013 as well as the District of Columbia\u00a0\u2013 had closed their banks.\nHistorians categorized Roosevelt's program as \"relief, recovery, and reform.\" Relief was urgently needed by tens of millions of unemployed. Recovery meant boosting the economy back to normal. Reform meant long-term fixes of what was wrong, especially with the financial and banking systems. Through Roosevelt's series of radio talks, known as fireside chats, he presented his proposals directly to the American public. Energized by his personal victory over his paralytic illness, Roosevelt relied on his persistent optimism and activism to renew the national spirit.\nFirst New Deal (1933\u20131934).\nOn his second day in office, Roosevelt declared a four-day national \"bank holiday\" and called for a special session of Congress to start March 9, on which date Congress passed the Emergency Banking Act. The act, which was based on a plan developed by the Hoover administration and Wall Street bankers, gave the president the power to determine the opening and closing of banks and authorized the Federal Reserve Banks to issue banknotes. The ensuing \"First 100 Days\" of the 73rd United States Congress saw an unprecedented amount of legislation and set a benchmark against which future presidents would be compared. When the banks reopened on Monday, March 15, stock prices rose by 15 percent and bank deposits exceeded withdrawals, thus ending the bank panic. On March 22, Roosevelt signed the Cullen\u2013Harrison Act, which effectively ended federal Prohibition.\nRoosevelt presided over the establishment of several agencies and measures designed to provide relief for the unemployed and others in need. The Federal Emergency Relief Administration (FERA), under the leadership of Harry Hopkins, was designed to distribute relief to state governments. The Public Works Administration (PWA), under the leadership of Secretary of the Interior Harold Ickes, was created to oversee the construction of large-scale public works such as dams, bridges, and schools. The most popular of all New Deal agencies\u00a0\u2013 and Roosevelt's favorite\u00a0\u2013 was the Civilian Conservation Corps (CCC), which hired 250,000 unemployed young men to work on local rural projects. Roosevelt also expanded a Hoover agency, the Reconstruction Finance Corporation, making it a major source of financing for railroads and industry. Congress gave the Federal Trade Commission broad new regulatory powers and provided mortgage relief to millions of farmers and homeowners. Roosevelt also made agricultural relief a high priority and set up the Agricultural Adjustment Administration (AAA). The AAA tried to force higher prices for commodities by paying farmers to leave land uncultivated and to cut herds.\nReform of the economy was the goal of the National Industrial Recovery Act (NIRA) of 1933. It sought to end cutthroat competition by forcing industries to establish rules of operation for all firms within specific industries, such as minimum prices, agreements not to compete, and production restrictions. Industry leaders negotiated the rules which were approved by NIRA officials. Industry needed to raise wages as a condition for approval. Provisions encouraged unions and suspended antitrust laws. NIRA was found to be unconstitutional by unanimous decision of the Supreme Court in May 1935; Roosevelt strongly protested the decision. Roosevelt reformed the financial regulatory structure of the nation with the Glass\u2013Steagall Act, creating the Federal Deposit Insurance Corporation (FDIC) to underwrite savings deposits. The act also sought to curb speculation by limiting affiliations between commercial banks and securities firms. In 1934, the Securities and Exchange Commission was created to regulate the trading of securities, while the Federal Communications Commission was established to regulate telecommunications.\nRecovery was pursued through federal spending. The NIRA included $3.3\u00a0billion (equivalent to $\u00a0billion in ) of spending through the Public Works Administration. Roosevelt worked with Senator Norris to create the largest government-owned industrial enterprise in American history\u00a0\u2014 the Tennessee Valley Authority (TVA)\u00a0\u2014 which built dams and power stations, controlled floods, and modernized agriculture and home conditions in the poverty-stricken Tennessee Valley. Executive Order 6102 declared that all privately held gold of American citizens was to be sold to the U.S. Treasury and the price raised from $20 to $35 per ounce. The goal was to counter the deflation which was paralyzing the economy.\nRoosevelt tried to keep his campaign promise by cutting the federal budget\u00a0\u2014 including a reduction in military spending from $752\u00a0million in 1932 to $531\u00a0million in 1934 and a 40% cut in spending on veterans benefits\u00a0\u2014 by removing 500,000 veterans and widows from the pension rolls and reducing benefits for the remainder, as well as cutting the salaries of federal employees and reducing spending on research and education. But the veterans were well organized and strongly protested, and most benefits were restored or increased by 1934. Veterans groups such as the American Legion and the Veterans of Foreign Wars won their campaign to transform their benefits from payments due in 1945 to immediate cash when Congress overrode the President's veto and passed the Bonus Act in January 1936. It pumped sums equal to 2% of the GDP into the consumer economy and had a major stimulus effect.\nSecond New Deal (1935\u20131936).\nRoosevelt expected that his party would lose several races in the 1934 Congressional elections, as the president's party had done in most previous midterm elections, but the Democrats picked up seats in both houses of Congress. Empowered by the public's apparent vote of confidence in his administration, the first item on Roosevelt's agenda in the 74th Congress was the creation of a social insurance program. The Social Security Act established Social Security and promised economic security for the elderly, the poor and the sick. Roosevelt insisted that it should be funded by payroll taxes rather than from the general fund, saying, \"We put those payroll contributions there so as to give the contributors a legal, moral, and political right to collect their pensions and unemployment benefits. With those taxes in there, no damn politician can ever scrap my social security program.\" Compared with the social security systems in western European countries, the Social Security Act of 1935 was rather conservative. But for the first time, the federal government took responsibility for the economic security of the aged, the temporarily unemployed, dependent children, and the handicapped. Against Roosevelt's original intention for universal coverage, the act only applied to roughly sixty percent of the labor force, as farmers, domestic workers, and other groups were excluded.\nRoosevelt consolidated the various relief organizations, though some, like the PWA, continued to exist. After winning Congressional authorization for further funding of relief efforts, Roosevelt established the Works Progress Administration (WPA). Under the leadership of Harry Hopkins, the WPA employed over three million people in its first year of existence. The WPA undertook numerous construction projects and provided funding to the National Youth Administration and arts organizations.\nSenator Robert Wagner wrote the National Labor Relations Act, which guaranteed workers the right to collective bargaining through unions of their own choice. The act also established the National Labor Relations Board (NLRB) to facilitate wage agreements and to suppress the repeated labor disturbances. The Wagner Act did not compel employers to reach an agreement with their employees, but it opened possibilities for American labor. The result was a tremendous growth of membership in the labor unions, especially in the mass-production sector. When the Flint sit-down strike threatened the production of General Motors, Roosevelt broke with the precedent set by many former presidents and refused to intervene; the strike ultimately led to the unionization of both General Motors and its rivals in the American automobile industry.\nWhile the First New Deal of 1933 had broad support from most sectors, the Second New Deal challenged the business community. Conservative Democrats, led by Al Smith, fought back with the American Liberty League, savagely attacking Roosevelt and equating him with Karl Marx and Vladimir Lenin. But Smith overplayed his hand, and his boisterous rhetoric let Roosevelt isolate his opponents and identify them with the wealthy vested interests that opposed the New Deal, strengthening Roosevelt for the 1936 landslide. By contrast, labor unions, energized by the Wagner Act, signed up millions of new members and became a major backer of Roosevelt's reelections in 1936, 1940 and 1944.\nBiographer James M. Burns suggests that Roosevelt's policy decisions were guided more by pragmatism than ideology and that he \"was like the general of a guerrilla army whose columns, fighting blindly in the mountains through dense ravines and thickets, suddenly converge, half by plan and half by coincidence, and debouch into the plain below.\" Roosevelt argued that such apparently haphazard methodology was necessary. \"The country needs and, unless I mistake its temper, the country demands bold, persistent experimentation,\" he wrote. \"It is common sense to take a method and try it; if it fails, admit it frankly and try another. But above all, try something.\"\nLandslide re-election, 1936.\nThough eight million workers remained unemployed in 1936, economic conditions had improved since 1932 and Roosevelt was widely popular. An attempt by Louisiana Senator Huey Long and other individuals to organize a left-wing alternative to the Democratic Party collapsed after Long's assassination in 1935. Roosevelt won re-nomination with little opposition at the 1936 Democratic National Convention, while his allies overcame Southern resistance to permanently abolish the long-established rule that had required Democratic presidential candidates to win the votes of two-thirds of the delegates rather than a simple majority. The Republicans nominated Kansas Governor Alf Landon, a well-respected but bland candidate whose chances were damaged by the public re-emergence of the still-unpopular Herbert Hoover. While Roosevelt campaigned on his New Deal programs and continued to attack Hoover, Landon sought to win voters who approved of the goals of the New Deal but disagreed with its implementation.\nIn the election against Landon and a third-party candidate, Roosevelt won 60.8% of the vote and carried every state except Maine and Vermont. The Democratic ticket won the highest proportion of the popular vote. Democrats also expanded their majorities in Congress, winning control of over three-quarters of the seats in each house. The election also saw the consolidation of the New Deal coalition; while the Democrats lost some of their traditional allies in big business, they were replaced by groups such as organized labor and African Americans, the latter of whom voted Democratic for the first time since the Civil War. Roosevelt lost high income voters, especially businessmen and professionals, but made major gains among the poor and minorities. He won 86 percent of the Jewish vote, 81 percent of Catholics, 80 percent of union members, 76 percent of Southerners, 76 percent of blacks in northern cities, and 75 percent of people on relief. Roosevelt carried 102 of the country's 106 cities with a population of 100,000 or more.\nSupreme Court fight and second term legislation.\nThe Supreme Court became Roosevelt's primary domestic focus during his second term after the court overturned many of his programs, including NIRA. The more conservative members of the court upheld the principles of the Lochner era, which saw numerous economic regulations struck down on the basis of freedom of contract. Roosevelt proposed the Judicial Procedures Reform Bill of 1937, which would have allowed him to appoint an additional Justice for each incumbent Justice over the age of 70; in 1937, there were six Supreme Court Justices over the age of 70. The size of the Court had been set at nine since the passage of the Judiciary Act of 1869, and Congress had altered the number of Justices six other times throughout U.S. history. Roosevelt's \"court packing\" plan ran into intense political opposition from his own party, led by Vice President Garner, since it upset the separation of powers. A bipartisan coalition of liberals and conservatives of both parties opposed the bill, and Chief Justice Charles Evans Hughes broke with precedent by publicly advocating defeat of the bill. Any chance of passing the bill ended with the death of Senate Majority Leader Joseph Taylor Robinson in July 1937.\nStarting with the 1937 case of \"West Coast Hotel Co. v. Parrish\", the court began to take a more favorable view of economic regulations. That same year, Roosevelt appointed a Supreme Court Justice for the first time, and by 1941, seven of the nine Justices had been appointed by Roosevelt. After \"Parish\", the Court shifted its focus from judicial review of economic regulations to the protection of civil liberties. Four of Roosevelt's Supreme Court appointees, Felix Frankfurter, Robert H. Jackson,\nHugo Black, and William O. Douglas, would be particularly influential in re-shaping the jurisprudence of the Court.\nWith Roosevelt's influence on the wane following the failure of the Judicial Procedures Reform Bill of 1937, conservative Democrats joined with Republicans to block the implementation of further New Deal programs. Roosevelt did manage to pass some legislation, including the Housing Act of 1937, a second Agricultural Adjustment Act, and the Fair Labor Standards Act (FLSA) of 1938, which was the last major piece of New Deal legislation. The FLSA outlawed child labor, established a federal minimum wage, and required overtime pay for certain employees who work in excess of forty-hours per week. He also won passage of the Reorganization Act of 1939 and subsequently created the Executive Office of the President, making it \"the nerve center of the federal administrative system.\" When the economy began to deteriorate again in late 1937, Roosevelt asked Congress for $5\u00a0billion (equivalent to $\u00a0billion in ) in relief and public works funding. This managed to eventually create as many as 3.3\u00a0million WPA jobs by 1938. Projects accomplished under the WPA ranged from new federal courthouses and post offices to facilities and infrastructure for national parks, bridges and other infrastructure across the country, and architectural surveys and archaeological excavations\u00a0\u2014 investments to construct facilities and preserve important resources. Beyond this, however, Roosevelt recommended to a special congressional session only a permanent national farm act, administrative reorganization, and regional planning measures, all of which were leftovers from a regular session. According to Burns, this attempt illustrated Roosevelt's inability to decide on a basic economic program.\nDetermined to overcome the opposition of conservative Democrats in Congress, Roosevelt became involved in the 1938 Democratic primaries, actively campaigning for challengers who were more supportive of New Deal reform. Roosevelt failed badly, managing to defeat only one target, a conservative Democrat from New York City. In the November 1938 elections, Democrats lost six Senate seats and 71 House seats, with losses concentrated among pro-New Deal Democrats. When Congress reconvened in 1939, Republicans under Senator Robert Taft formed a Conservative coalition with Southern Democrats, virtually ending Roosevelt's ability to enact his domestic proposals. Despite their opposition to Roosevelt's domestic policies, many of these conservative Congressmen would provide crucial support for Roosevelt's foreign policy before and during World War II.\nConservation and the environment.\nRoosevelt had a lifelong interest in the environment and conservation starting with his youthful interest in forestry on his family estate. Although Roosevelt was never an outdoorsman or sportsman on Theodore Roosevelt's scale, his growth of the national systems were comparable. Roosevelt was active in expanding, funding, and promoting the National Park and National Forest systems. Under Roosevelt, their popularity soared, from three million visitors a year at the start of the decade to 15.5\u00a0million in 1939. The Civilian Conservation Corps enrolled 3.4\u00a0million young men and built of trails, planted two billion trees, and upgraded of dirt roads. Every state had its own state parks, and Roosevelt made sure that WPA and CCC projects were set up to upgrade them as well as the national systems.\nGNP and unemployment rates.\nGovernment spending increased from 8.0% of gross national product (GNP) under Hoover in 1932 to 10.2% of the GNP in 1936. The national debt as a percentage of the GNP had more than doubled under Hoover from 16% to 40% of the GNP in early 1933. It held steady at close to 40% as late as fall 1941, then grew rapidly during the war. The GNP was 34% higher in 1936 than in 1932 and 58% higher in 1940 on the eve of war. That is, the economy grew 58% from 1932 to 1940 in eight years of peacetime, and then grew 56% from 1940 to 1945 in five years of wartime. Unemployment fell dramatically during Roosevelt's first term. It increased in 1938 (\"a depression within a depression\") but continually declined after 1938. Total employment during Roosevelt's term expanded by 18.31\u00a0million jobs, with an average annual increase in jobs during his administration of 5.3%.\nForeign policy (1933\u20131941).\nThe main foreign policy initiative of Roosevelt's first term was the Good Neighbor Policy, which was a re-evaluation of U.S. policy toward Latin America. The United States had frequently intervened in Latin America following the promulgation of the Monroe Doctrine in 1823, and the United States had occupied several Latin American nations in the Banana Wars that had occurred following the Spanish\u2013American War of 1898. After Roosevelt took office, he withdrew U.S. forces from Haiti and reached new treaties with Cuba and Panama, ending their status as U.S. protectorates. In December 1933, Roosevelt signed the Montevideo Convention on the Rights and Duties of States, renouncing the right to intervene unilaterally in the affairs of Latin American countries. Roosevelt also normalized relations with the Soviet Union, which the United States had refused to recognize since the 1920s. He hoped to renegotiate the Russian debt from World War I and open trade relations, but no progress was made on either issue, and \"both nations were soon disillusioned by the accord.\"\nThe rejection of the Treaty of Versailles in 1919\u20131920 marked the dominance of isolationism in American foreign policy. Despite Roosevelt's Wilsonian background, he and Secretary of State Cordell Hull acted with great care not to provoke isolationist sentiment. The isolationist movement was bolstered in the early to mid-1930s by Senator Gerald Nye and others who succeeded in their effort to stop the \"merchants of death\" in the U.S. from selling arms abroad. This effort took the form of the Neutrality Acts; the president asked for, but was refused, a provision to give him the discretion to allow the sale of arms to victims of aggression. Focused on domestic policy, Roosevelt largely acquiesced to Congress's non-interventionist policies in the early-to-mid 1930s. In the interim, Fascist Italy under Benito Mussolini proceeded to overcome Ethiopia, and the Italians joined Nazi Germany under Adolf Hitler in supporting General Francisco Franco and the Nationalist cause in the Spanish Civil War. As that conflict drew to a close in early 1939, Roosevelt expressed regret in not aiding the Spanish Republicans. When Japan invaded China in 1937, isolationism limited Roosevelt's ability to aid China, despite atrocities like the Nanking Massacre and the USS Panay incident.\nGermany annexed Austria in 1938, and soon turned its attention to its eastern neighbors. Roosevelt made it clear that, in the event of German aggression against Czechoslovakia, the U.S. would remain neutral. After completion of the Munich Agreement and the execution of Kristallnacht, American public opinion turned against Germany, and Roosevelt began preparing for a possible war with Germany. Relying on an interventionist political coalition of Southern Democrats and business-oriented Republicans, Roosevelt oversaw the expansion of U.S. airpower and war production capacity.\nWhen World War II began in September 1939 with Germany's invasion of Poland and Britain and France's subsequent declaration of war upon Germany, Roosevelt sought ways to assist Britain and France militarily. Isolationist leaders like Charles Lindbergh and Senator William Borah successfully mobilized opposition to Roosevelt's proposed repeal of the Neutrality Act, but Roosevelt won Congressional approval of the sale of arms on a cash-and-carry basis. He also began a regular secret correspondence with Britain's First Lord of the Admiralty, Winston Churchill, in September 1939\u00a0\u2014 the first of 1,700 letters and telegrams between them. Roosevelt forged a close personal relationship with Churchill, who became Prime Minister of the United Kingdom in May 1940.\nThe Fall of France in June 1940 shocked the American public, and isolationist sentiment declined. In July 1940, Roosevelt appointed two interventionist Republican leaders, Henry L. Stimson and Frank Knox, as Secretaries of War and the Navy, respectively. Both parties gave support to his plans for a rapid build-up of the American military, but the isolationists warned that Roosevelt would get the nation into an unnecessary war with Germany. In July 1940, a group of Congressmen introduced a bill that would authorize the nation's first peacetime draft, and with the support of the Roosevelt administration the Selective Training and Service Act of 1940 passed in September. The size of the army would increase from 189,000 men at the end of 1939 to 1.4\u00a0million men in mid-1941. In September 1940, Roosevelt openly defied the Neutrality Acts by reaching the Destroyers for Bases Agreement, which, in exchange for military base rights in the British Caribbean Islands, gave 50 WWI American destroyers to Britain.\nElection of 1940: Breaking with tradition.\nIn the months prior to the July 1940 Democratic National Convention, there was much speculation as to whether Roosevelt would run for an unprecedented third term. The two-term tradition, although not yet enshrined in the Constitution, had been established by George Washington when he refused to run for a third term in the 1796 presidential election. Roosevelt refused to give a definitive statement as to his willingness to be a candidate again, and he even indicated to some ambitious Democrats, such as James Farley, that he would not run for a third term and that they could seek the Democratic nomination. However, as Germany swept through Western Europe and menaced Britain in mid-1940, Roosevelt decided that only he had the necessary experience and skills to see the nation safely through the Nazi threat. He was aided by the party's political bosses, who feared that no Democrat except Roosevelt could defeat Wendell Willkie, the popular Republican nominee.\nAt the July 1940 Democratic Convention in Chicago, Roosevelt easily swept aside challenges from Farley and Vice President Garner, who had turned against Roosevelt in his second term because of his liberal economic and social policies. To replace Garner on the ticket, Roosevelt turned to Secretary of Agriculture Henry Wallace of Iowa, a former Republican who strongly supported the New Deal and was popular in farm states. The choice was strenuously opposed by many of the party's conservatives, who felt Wallace was too radical and \"eccentric\" in his private life to be an effective running mate. But Roosevelt insisted that without Wallace on the ticket he would decline re-nomination, and Wallace won the vice-presidential nomination, defeating Speaker of the House William B. Bankhead and other candidates.\nA late August poll taken by Gallup found the race to be essentially tied, but Roosevelt's popularity surged in September following the announcement of the Destroyers for Bases Agreement. Willkie supported much of the New Deal as well as rearmament and aid to Britain, but warned that Roosevelt would drag the country into another European war. Responding to Willkie's attacks, Roosevelt promised to keep the country out of the war. Roosevelt won the 1940 election with 55% of the popular vote, 38 of the 48 states, and almost 85% of the electoral vote.\nThird and fourth terms (1941\u20131945).\nThe world war dominated FDR's attention, with far more time devoted to world affairs than ever before. Domestic politics and relations with Congress were largely shaped by his efforts to achieve total mobilization of the nation's economic, financial, and institutional resources for the war effort. Even relationships with Latin America and Canada were structured by wartime demands. Roosevelt maintained close personal control of all major diplomatic and military decisions, working closely with his generals and admirals, the war and Navy departments, the British, and even with the Soviet Union. His key advisors on diplomacy were Harry Hopkins (who was based in the White House), Sumner Welles (based in the State Department), and Henry Morgenthau Jr. at Treasury. In military affairs, FDR worked most closely with Secretary Henry L. Stimson at the War Department, Army Chief of Staff George Marshall, and Admiral William D. Leahy.\nLead-up to the war.\nBy late 1940, re-armament was in high gear, partly to expand and re-equip the Army and Navy and partly to become the \"Arsenal of Democracy\" for Britain and other countries. With his Four Freedoms speech in January 1941, Roosevelt laid out the case for an Allied battle for basic rights throughout the world. Assisted by Willkie, Roosevelt won Congressional approval of the Lend-Lease program, which directed massive military and economic aid to Britain, and China. In sharp contrast to the loans of World War I, there would be no repayment after the war. As Roosevelt took a firmer stance against Japan, Germany, and Italy, American isolationists such as Charles Lindbergh and the America First Committee vehemently attacked Roosevelt as an irresponsible warmonger. When Germany invaded the Soviet Union in June 1941, Roosevelt agreed to extend Lend-Lease to the Soviets. Thus, Roosevelt had committed the U.S. to the Allied side with a policy of \"all aid short of war.\" By July 1941, Roosevelt authorized the creation of the Office of the Coordinator of Inter-American Affairs (OCIAA) to counter perceived propaganda efforts in Latin America by Germany and Italy.\nIn August 1941, Roosevelt and Churchill conducted a highly secret bilateral meeting in which they drafted the Atlantic Charter, conceptually outlining global wartime and postwar goals. This would be the first of several wartime conferences; Churchill and Roosevelt would meet ten more times in person. Though Churchill pressed for an American declaration of war against Germany, Roosevelt believed that Congress would reject any attempt to bring the United States into the war. In September, a German submarine fired on the U.S. destroyer \"Greer,\" and Roosevelt declared that the U.S. Navy would assume an escort role for Allied convoys in the Atlantic as far east as Great Britain and would fire upon German ships or submarines (U-boats) of the Kriegsmarine if they entered the U.S. Navy zone. This \"shoot on sight\" policy effectively declared naval war on Germany and was favored by Americans by a margin of 2-to-1.\nPearl Harbor and declarations of war.\nAfter the German invasion of Poland, the primary concern of both Roosevelt and his top military staff was on the war in Europe, but Japan also presented foreign policy challenges. Relations with Japan had continually deteriorated since its invasion of Manchuria in 1931, and they had further worsened with Roosevelt's support of China. With the war in Europe occupying the attention of the major colonial powers, Japanese leaders eyed vulnerable colonies such as the Dutch East Indies, French Indochina, and British Malaya. After Roosevelt announced a $100\u00a0million loan (equivalent to $\u00a0billion in ) to China in reaction to Japan's occupation of northern French Indochina, Japan signed the Tripartite Pact with Germany and Italy. The pact bound each country to defend the others against attack, and Germany, Japan, and Italy became known as the Axis powers. Overcoming those who favored invading the Soviet Union, the Japanese Army high command successfully advocated for the conquest of Southeast Asia to ensure continued access to raw materials. In July 1941, after Japan occupied the remainder of French Indochina, Roosevelt cut off the sale of oil to Japan, depriving Japan of more than 95 percent of its oil supply. He also placed the Philippine military under American command and reinstated General Douglas MacArthur into active duty to command U.S. forces in the Philippines.\nThe Japanese were incensed by the embargo and Japanese leaders became determined to attack the United States unless it lifted the embargo. The Roosevelt administration was unwilling to reverse policy, and Secretary of State Hull blocked a potential summit between Roosevelt and Prime Minister Fumimaro Konoe. After diplomatic efforts to end the embargo failed, the Privy Council of Japan authorized a strike against the United States. The Japanese believed that the destruction of the United States Asiatic Fleet (stationed in the Philippines) and the United States Pacific Fleet (stationed at Pearl Harbor in Hawaii) was vital to the conquest of Southeast Asia. On the morning of December 7, 1941, the Japanese struck the U.S. naval base at Pearl Harbor with a surprise attack, knocking out the main American battleship fleet and killing 2,403 American servicemen and civilians. At the same time, separate Japanese task forces attacked Thailand, British Hong Kong, the Philippines, and other targets. Roosevelt called for war in his \"Infamy Speech\" to Congress, in which he said: \"Yesterday, December 7, 1941\u00a0\u2014 a date which will live in infamy\u00a0\u2014 the United States of America was suddenly and deliberately attacked by naval and air forces of the Empire of Japan.\" In a nearly unanimous vote, Congress declared war on Japan. After the Japanese attack at Pearl Harbor, antiwar sentiment in the United States largely evaporated overnight. On December 11, 1941, Hitler and Mussolini declared war on the United States, which responded in kind.\nA majority of scholars have rejected the conspiracy theories that Roosevelt, or any other high government officials, knew in advance about the Japanese attack on Pearl Harbor. The Japanese had kept their secrets closely guarded. Senior American officials were aware that war was imminent, but they did not expect an attack on Pearl Harbor. Roosevelt had expected that the Japanese would attack either the Dutch East Indies or Thailand.\nWar plans.\nIn late December 1941 Churchill and Roosevelt met at the Arcadia Conference, which established a joint strategy between the U.S. and Britain.\nBoth agreed on a Europe first strategy that prioritized the defeat of Germany before Japan. The U.S. and Britain established the Combined Chiefs of Staff to coordinate military policy and the Combined Munitions Assignments Board to coordinate the allocation of supplies. An agreement was also reached to establish a centralized command in the Pacific theater called ABDA, named for the American, British, Dutch, and Australian forces in the theater. On January 1, 1942, the United States, Britain, China, the Soviet Union, and twenty-two other countries (the Allied Powers) issued the Declaration by United Nations, in which each nation pledged to defeat the Axis powers.\nIn 1942, Roosevelt formed a new body, the Joint Chiefs of Staff, which made the final decisions on American military strategy. Admiral Ernest J. King as Chief of Naval Operations commanded the Navy and Marines, while General George C. Marshall led the Army and was in nominal control of the Air Force, which in practice was commanded by General Hap Arnold. The Joint Chiefs were chaired by Admiral William D. Leahy, the most senior officer in the military. Roosevelt avoided micromanaging the war and let his top military officers make most decisions. Roosevelt's civilian appointees handled the draft and procurement of men and equipment, but no civilians \u2013 not even the secretaries of War or Navy \u2013 had a voice in strategy. Roosevelt avoided the State Department and conducted high-level diplomacy through his aides, especially Harry Hopkins, whose influence was bolstered by his control of the Lend Lease funds.\nNuclear program.\nIn August 1939, Leo Szilard and Albert Einstein sent the Einstein\u2013Szil\u00e1rd letter to Roosevelt, warning of the possibility of a German project to develop nuclear weapons. Szilard realized that the recently discovered process of nuclear fission could be used to create a nuclear chain reaction that could be used as a weapon of mass destruction. Roosevelt feared the consequences of allowing Germany to have sole possession of the technology and authorized preliminary research into nuclear weapons. After the attack on Pearl Harbor, the Roosevelt administration secured the funds needed to continue research and selected General Leslie Groves to oversee the Manhattan Project, which was charged with developing the first nuclear weapons. Roosevelt and Churchill agreed to jointly pursue the project, and Roosevelt helped ensure that American scientists cooperated with their British counterparts.\nWartime conferences.\nRoosevelt coined the term \"Four Policemen\" to refer to the \"Big Four\" Allied powers of World War II, the United States, the United Kingdom, the Soviet Union, and China. The \"Big Three\" of Roosevelt, Churchill, and Soviet leader Joseph Stalin, together with Chinese Generalissimo Chiang Kai-shek, cooperated informally on a plan in which American and British troops concentrated in the West; Soviet troops fought on the Eastern front; and Chinese, British and American troops fought in Asia and the Pacific. The United States also continued to send aid via the Lend-Lease program to the Soviet Union and other countries. The Allies formulated strategy in a series of high-profile conferences as well as by contact through diplomatic and military channels. Beginning in May 1942, the Soviets urged an Anglo-American invasion of German-occupied France in order to divert troops from the Eastern front. Concerned that their forces were not yet ready for an invasion of France, Churchill and Roosevelt decided to delay such an invasion until at least 1943 and instead focus on a landing in North Africa, known as Operation Torch.\nIn November 1943, Roosevelt, Churchill, and Stalin met to discuss strategy and post-war plans at the Tehran Conference, where Roosevelt met Stalin for the first time. At the conference, Britain and the United States committed to opening a second front against Germany in 1944, while Stalin committed to entering the war against Japan at an unspecified date. Subsequent conferences at Bretton Woods and Dumbarton Oaks established the framework for the post-war international monetary system and the United Nations, an intergovernmental organization similar to Wilson's failed League of Nations.\nRoosevelt, Churchill, and Stalin met for a second time at the February 1945 Yalta Conference in Crimea. With the end of the war in Europe approaching, Roosevelt's primary focus was on convincing Stalin to enter the war against Japan; the Joint Chiefs had estimated that an American invasion of Japan would cause as many as one million American casualties. In return for the Soviet Union's entrance into the war against Japan, the Soviet Union was promised control of Asian territories such as Sakhalin Island. The three leaders agreed to hold a conference in 1945 to establish the United Nations, and they also agreed on the structure of the United Nations Security Council, which would be charged with ensuring international peace and security. Roosevelt did not push for the immediate evacuation of Soviet soldiers from Poland, but he won the issuance of the Declaration on Liberated Europe, which promised free elections in countries that had been occupied by Germany. Germany itself would not be dismembered, but would be jointly occupied by the United States, France, Britain, and the Soviet Union. Against Soviet pressure, Roosevelt and Churchill refused to consent to imposing huge reparations and deindustrialization on Germany after the war. Roosevelt's role in the Yalta Conference has been controversial; critics charge that he naively trusted the Soviet Union to allow free elections in Eastern Europe, while supporters argue that there was little more that Roosevelt could have done for the Eastern European countries given the Soviet occupation and the need for cooperation with the Soviet Union during and after the war.\nCourse of the war.\nThe Allies invaded French North Africa in November 1942, securing the surrender of Vichy French forces within days of landing. At the January 1943 Casablanca Conference, the Allies agreed to defeat Axis forces in North Africa and then launch an invasion of Sicily, with an attack on France to take place in 1944. At the conference, Roosevelt also announced that he would only accept the unconditional surrender of Germany, Japan, and Italy. In February 1943, the Soviet Union won a major victory at the Battle of Stalingrad, and in May 1943, the Allies secured the surrender of over 250,000 German and Italian soldiers in North Africa, ending the North African Campaign. The Allies launched an invasion of Sicily in July 1943, capturing the island by the end of the following month. In September 1943, the Allies secured an armistice from Italian Prime Minister Pietro Badoglio, but Germany quickly restored Mussolini to power. The Allied invasion of mainland Italy commenced in September 1943, but the Italian Campaign continued until 1945 as German and Italian troops resisted the Allied advance.\nTo command the invasion of France, Roosevelt chose General Dwight D. Eisenhower, who had successfully commanded a multinational coalition in North Africa and Sicily. Eisenhower chose to launch Operation Overlord on June 6, 1944. Supported by 12,000 aircraft and the largest naval force ever assembled, the Allies successfully established a beachhead in Normandy and then advanced further into France. Though reluctant to back an unelected government, Roosevelt recognized Charles de Gaulle's Provisional Government of the French Republic as the de facto government of France in July 1944. After most of France had been liberated from German occupation, Roosevelt granted formal recognition to de Gaulle's government in October 1944. Over the following months, the Allies liberated more territory from Nazi occupation and began the invasion of Germany. By April 1945, Nazi resistance was crumbling in the face of advances by both the Western Allies and the Soviet Union.\nIn the opening weeks of the war, Japan conquered the Philippines and the British and Dutch colonies in Southeast Asia. The Japanese advance reached its maximum extent by June 1942, when the U.S. Navy scored a decisive victory at the Battle of Midway. American and Australian forces then began a slow and costly strategy called island hopping or leapfrogging through the Pacific Islands, with the objective of gaining bases from which strategic airpower could be brought to bear on Japan and from which Japan could ultimately be invaded. In contrast to Hitler, Roosevelt took no direct part in the tactical naval operations, though he approved strategic decisions. Roosevelt gave way in part to insistent demands from the public and Congress that more effort be devoted against Japan, but he always insisted on Germany first. The strength of the Japanese navy was decimated in the Battle of Leyte Gulf, and by April 1945 the Allies had re-captured much of their lost territory in the Pacific.\nHome front.\nThe home front was subject to dynamic social changes throughout the war, though domestic issues were no longer Roosevelt's most urgent policy concern. The military buildup spurred economic growth. Unemployment fell in half from 7.7\u00a0million in spring 1940 to 3.4\u00a0million in fall 1941 and fell in half again to 1.5\u00a0million in fall 1942, out of a labor force of 54\u00a0million. There was a growing labor shortage, accelerating the second wave of the Great Migration of African Americans, farmers and rural populations to manufacturing centers. African Americans from the South went to California and other West Coast states for new jobs in the defense industry. To pay for increased government spending, in 1941 Roosevelt proposed that Congress enact an income tax rate of 99.5% on all income over $100,000; when the proposal failed, he issued an executive order imposing an income tax of 100% on income over $25,000, which Congress rescinded. The Revenue Act of 1942 instituted top tax rates as high as 94% (after accounting for the excess profits tax), greatly increased the tax base, and instituted the first federal withholding tax. In 1944, Roosevelt requested that Congress enact legislation which would tax all \"unreasonable\" profits, both corporate and individual, and thereby support his declared need for over $10\u00a0billion in revenue for the war and other government measures. Congress overrode Roosevelt's veto to pass a smaller revenue bill raising $2\u00a0billion.\nIn 1942, with the United States now in the conflict, war production increased dramatically, but fell short of the goals established by the president, due in part to manpower shortages. The effort was also hindered by numerous strikes, especially among union workers in the coal mining and railroad industries, which lasted well into 1944. Nonetheless, between 1941 and 1945, the United States produced 2.4\u00a0million trucks, 300,000 military aircraft, 88,400 tanks, and 40\u00a0billion rounds of ammunition. The production capacity of the United States dwarfed that of other countries; for example, in 1944, the United States produced more military aircraft than the combined production of Germany, Japan, Britain, and the Soviet Union. The White House became the ultimate site for labor mediation, conciliation or arbitration. One particular battle royale occurred between Vice President Wallace, who headed the Board of Economic Warfare, and Jesse H. Jones, in charge of the Reconstruction Finance Corporation; both agencies assumed responsibility for acquisition of rubber supplies and came to loggerheads over funding. Roosevelt resolved the dispute by dissolving both agencies. In 1943, Roosevelt established the Office of War Mobilization to oversee the home front; the agency was led by James F. Byrnes, who came to be known as the \"assistant president\" due to his influence.\nRoosevelt's 1944 State of the Union Address advocated that Americans should think of basic economic rights as a Second Bill of Rights. He stated that all Americans should have the right to \"adequate medical care\", \"a good education\", \"a decent home\", and a \"useful and remunerative job\". In the most ambitious domestic proposal of his third term, Roosevelt proposed the G.I. Bill, which would create a massive benefits program for returning soldiers. Benefits included post-secondary education, medical care, unemployment insurance, job counseling, and low-cost loans for homes and businesses. The G.I. Bill passed unanimously in both houses of Congress and was signed into law in June 1944. Of the fifteen million Americans who served in World War II, more than half benefitted from the educational opportunities provided for in the G.I. Bill.\nDeclining health.\nRoosevelt, a chain-smoker throughout his entire adult life, had been in declining physical health since at least 1940. In March 1944, shortly after his 62nd birthday, he underwent testing at Bethesda Hospital and was found to have high blood pressure, atherosclerosis, coronary artery disease causing angina pectoris, and congestive heart failure.\nHospital physicians and two outside specialists ordered Roosevelt to rest. His personal physician, Admiral Ross McIntire, created a daily schedule that banned business guests for lunch and incorporated two hours of rest each day. During the 1944 re-election campaign, McIntire denied several times that Roosevelt's health was poor; on October 12, for example, he announced that \"The President's health is perfectly OK. There are absolutely no organic difficulties at all.\" Roosevelt realized that his declining health could eventually make it impossible for him to continue as president, and in 1945 he told a confidant that he might resign from the presidency following the end of the war.\nElection of 1944.\nWhile some Democrats had opposed Roosevelt's nomination in 1940, the president faced little difficulty in securing his re-nomination at the 1944 Democratic National Convention. Roosevelt made it clear before the convention that he was seeking another term, and on the lone presidential ballot of the convention, Roosevelt won the vast majority of delegates, although a minority of Southern Democrats voted for Harry F. Byrd. Party leaders prevailed upon Roosevelt to drop Vice President Wallace from the ticket, believing him to be an electoral liability and a poor potential successor in case of Roosevelt's death. Roosevelt preferred Byrnes as Wallace's replacement but was convinced to support Senator Harry S. Truman of Missouri, who had earned renown for his investigation of war production inefficiency and was acceptable to the various factions of the party. On the second vice presidential ballot of the convention, Truman defeated Wallace to win the nomination.\nThe Republicans nominated Thomas E. Dewey, the governor of New York, who had a reputation as a liberal in his party. The opposition accused Roosevelt and his administration of domestic corruption, bureaucratic inefficiency, tolerance of Communism, and military blunders. Labor unions, which had grown rapidly in the war, fully supported Roosevelt. Roosevelt and Truman won the 1944 election by a comfortable margin, defeating Dewey and his running mate John W. Bricker with 53.4% of the popular vote and 432 out of the 531 electoral votes. The president campaigned in favor of a strong United Nations, so his victory symbolized support for the nation's future participation in the international community.\nFinal months, death and aftermath (1945).\nWhen Roosevelt returned to the United States from the Yalta Conference, many were shocked to see how old, thin and frail he looked. He spoke while seated in the well of the House, an unprecedented concession to his physical incapacity. During March 1945, he sent strongly worded messages to Stalin accusing him of breaking his Yalta commitments over Poland, Germany, prisoners of war and other issues. When Stalin accused the western Allies of plotting behind his back a separate peace with Hitler, Roosevelt replied: \"I cannot avoid a feeling of bitter resentment towards your informers, whoever they are, for such vile misrepresentations of my actions or those of my trusted subordinates.\"\nOn March 29, 1945, Roosevelt went to the Little White House at Warm Springs, Georgia, to rest before his anticipated appearance at the founding conference of the United Nations.\nIn the afternoon of April 12, 1945, in Warm Springs, Georgia, while sitting for a portrait, Roosevelt said \"I have a terrific headache.\" He then slumped forward in his chair, unconscious, and was carried into his bedroom. The president's attending cardiologist, Dr. Howard Bruenn, diagnosed the medical emergency as a massive intracerebral hemorrhage. At 3:35\u00a0p.m. that day, Roosevelt died at the age of 63.\nOn the morning of April 13, Roosevelt's body was placed in a flag-draped coffin and loaded onto the presidential train for the trip back to Washington. Along the route, thousands flocked to the tracks to pay their respects. After a White House funeral on April 14, Roosevelt was transported by train from Washington, D.C., to his place of birth at Hyde Park. As was his wish, Roosevelt was buried on April 15 in the Rose Garden of his Springwood estate.\nRoosevelt's declining physical health had been kept secret from the general public. His death was met with shock and grief across the U.S. and around the world. After Germany surrendered the following month, newly sworn-in President Truman dedicated Victory in Europe Day and its celebrations to Roosevelt's memory, and kept the flags across the U.S. at half-staff for the remainder of the 30-day mourning period, saying that his only wish was \"that Franklin D. Roosevelt had lived to witness this day\". World War II finally ended with the signed surrender of Japan in September following the atomic bombings of Hiroshima and Nagasaki and the very late Soviet entry into the war against the Japanese. Truman would preside over the demobilization of the war effort and the establishment of the United Nations and other postwar institutions envisioned during Roosevelt's presidency.\nCivil rights, internment, and the Holocaust.\nRoosevelt was viewed as a hero by many African Americans, Catholics, and Jews, and he was highly successful in attracting large majorities of these voters into his New Deal coalition. He won strong support from Chinese Americans and Filipino Americans, but not Japanese Americans, as he presided over their internment in concentration camps during the war. African Americans and Native Americans fared well in two New Deal relief programs, the Civilian Conservation Corps and the Indian Reorganization Act, respectively. Sitkoff reports that the WPA \"provided an economic floor for the whole black community in the 1930s, rivaling both agriculture and domestic service as the chief source\" of income.\nRoosevelt did not join NAACP leaders in pushing for federal anti-lynching legislation, as he believed that such legislation was unlikely to pass and that his support for it would alienate Southern congressmen. He did, however, appoint a \"Black Cabinet\" of African American advisers to advise on race relations and African American issues, and he publicly denounced lynching as \"murder.\" First Lady Eleanor Roosevelt vocally supported efforts designed to aid the African American community, including the Fair Labor Standards Act, which helped boost wages for nonwhite workers in the South. In 1941, Roosevelt established the Fair Employment Practices Committee (FEPC) to implement Executive Order 8802, which prohibited racial and religious discrimination in employment among defense contractors. The FEPC was the first national program directed against employment discrimination, and it played a major role in opening up new employment opportunities to non-white workers. During World War II, the proportion of African American men employed in manufacturing positions rose significantly. In response to Roosevelt's policies, African Americans increasingly defected from the Republican Party during the 1930s and 1940s, becoming an important Democratic voting bloc in several Northern states.\nThe attack on Pearl Harbor raised concerns in the public regarding the possibility of sabotage by Japanese Americans. This suspicion was fed by long-standing racism against Japanese immigrants, as well as the findings of the Roberts Commission, which concluded that the attack on Pearl Harbor had been assisted by Japanese spies. On February 19, 1942, President Roosevelt signed Executive Order 9066, which relocated hundreds of thousands of the Japanese-American citizens and immigrants. They were forced to liquidate their properties and businesses and interned in hastily built camps in interior, harsh locations. Distracted by other issues, Roosevelt had delegated the decision for internment to Secretary of War Stimson, who in turn relied on the judgment of Assistant Secretary of War John J. McCloy. The Supreme Court upheld the constitutionality of the executive order in the 1944 case of \"Korematsu v. United States\". Many German and Italian citizens were also arrested or placed into internment camps.\nAfter Kristallnacht in 1938, Roosevelt helped expedite Jewish immigration from Germany and Austria, and allowed German citizens already in the United States to stay indefinitely. However, he was prevented from accepting further Jewish immigrants, practically refugees, by the restrictive Immigration Act of 1924, and antisemitism among voters. Hitler chose to implement the \"Final Solution\"\u00a0\u2014 the extermination of the European Jewish population\u00a0\u2014 by January 1942, and American officials learned of the scale of the Nazi extermination campaign in the following months. Against the objections of the State Department, Roosevelt convinced the other Allied leaders to jointly issue the Joint Declaration by Members of the United Nations, which condemned the ongoing Holocaust and warned to try its perpetrators as war criminals. In January 1944, Roosevelt established the War Refugee Board to aid Jews and other victims of Axis atrocities. Aside from these actions, Roosevelt believed that the best way to help the persecuted populations of Europe was to end the war as quickly as possible. Top military leaders and War Department leaders rejected any campaign to bomb the extermination camps or the rail lines leading to the camps, fearing it would be a diversion from the war effort. According to biographer Jean Edward Smith, there is no evidence that anyone ever proposed such a campaign to Roosevelt.\nLegacy.\nHistorical reputation.\nRoosevelt is widely considered to be one of the most important figures in the history of the United States, as well as one of the most influential figures of the 20th century. Historians and political scientists consistently rank Roosevelt, George Washington, and Abraham Lincoln as the three greatest presidents. Reflecting on Roosevelt's presidency, \"which brought the United States through the Great Depression and World War II to a prosperous future\", said FDR biographer Jean Edward Smith in 2007, \"He lifted himself from a wheelchair to lift the nation from its knees.\"\nThe rapid expansion of government programs that occurred during Roosevelt's term redefined the role of the government in the United States, and Roosevelt's advocacy of government social programs was instrumental in redefining liberalism for coming generations. Roosevelt firmly established the United States' leadership role on the world stage, with his role in shaping and financing World War II. His isolationist critics faded away, and even the Republicans joined in his overall policies. He also created a new understanding of the presidency, permanently increasing the power of the president at the expense of Congress.\nHis Second Bill of Rights became, according to historian Joshua Zeitz, \"the basis of the Democratic Party's aspirations for the better part of four decades.\" After his death, his widow, Eleanor, continued to be a forceful presence in U.S. and world politics, serving as delegate to the conference which established the United Nations and championing civil rights and liberalism generally. Some junior New Dealers played leading roles in the presidencies of Truman, John Kennedy, and Lyndon Johnson. Kennedy came from a Roosevelt-hating family. Historian William Leuchtenburg says that before 1960, \"Kennedy showed a conspicuous lack of inclination to identify himself as a New Deal liberal.\" He adds, as president, \"Kennedy never wholly embraced the Roosevelt tradition and at times he deliberately severed himself from it.\" By contrast, young Lyndon Johnson had been an enthusiastic New Dealer and a favorite of Roosevelt. Johnson modelled his presidency on FDR and relied heavily on New Deal lawyer Abe Fortas, as well as James H. Rowe, Anna M. Rosenberg, Thomas Gardiner Corcoran, and Benjamin V. Cohen.\nDuring his presidency, and continuing to a lesser extent afterwards, there has been much criticism of Roosevelt, some of it intense. Critics have questioned not only his policies, positions, and the consolidation of power that occurred due to his responses to the crises of the Depression and World War II but also his breaking with tradition by running for a third term as president. Long after his death, new lines of attack criticized Roosevelt's policies regarding helping the Jews of Europe, incarcerating the Japanese on the West Coast, and opposing anti-lynching legislation.\nMemorials.\nRoosevelt's home in Hyde Park is now a National Historic Site and home to his Presidential library. Washington D.C., hosts two memorials to the former president. The largest, the Roosevelt Memorial, is located next to the Jefferson Memorial on the Tidal Basin. A more modest memorial, a block of marble in front of the National Archives building suggested by Roosevelt himself, was erected in 1965. Roosevelt's leadership in the March of Dimes is one reason he is commemorated on the American dime. Roosevelt has also appeared on several U.S. Postage stamps."}
{"id": "10980", "revid": "34209889", "url": "https://en.wikipedia.org/wiki?curid=10980", "title": "Four Freedoms", "text": "The Four Freedoms were goals articulated by United States President Franklin D. Roosevelt on Monday, January 6, 1941. In an address known as the (technically the 1941 State of the Union address), he proposed four fundamental freedoms that people \"everywhere in the world\" ought to enjoy:\nRoosevelt delivered his speech 11 months before the surprise Japanese attack on U.S. forces in Pearl Harbor, Hawaii that caused the United States to declare war on Japan, December 8, 1941. The State of the Union speech before Congress was largely about the national security of the United States and the threat to other democracies from world war that was being waged across the continents in the eastern hemisphere. In the speech, he made a break with the long-held tradition of United States non-interventionism. He outlined the U.S. role in helping allies already engaged in warfare.\nIn that context, he summarized the values of democracy behind the bipartisan consensus on international involvement that existed at the time. A famous quote from the speech prefaces those values: \"As men do not live by bread alone, they do not fight by armaments alone.\" In the second half of the speech, he lists the benefits of democracy, which include economic opportunity, employment, social security, and the promise of \"adequate health care\". The first two freedoms, of speech and religion, are protected by the First Amendment in the United States Constitution. His inclusion of the latter two freedoms went beyond the traditional Constitutional values protected by the U.S. Bill of Rights. Roosevelt endorsed a broader human right to economic security and anticipated what would become known decades later as the \"human security\" paradigm in social science and economic development. He also included the \"freedom from fear\" against national aggression and took it to the new United Nations he was setting up.\nHistorical context.\nIn the 1930s many Americans, arguing that the involvement in World War I had been a mistake, were adamantly against continued intervention in European affairs. With the Neutrality Acts established after 1935, U.S. law banned the sale of armaments to countries that were at war and placed restrictions on travel with belligerent vessels.\nWhen World War II began in September 1939, the neutrality laws were still in effect and ensured that no substantial support could be given to Britain and France. With the revision of the Neutrality Act in 1939, Roosevelt adopted a \"methods-short-of-war policy\" whereby supplies and armaments could be given to European Allies, provided no declaration of war could be made and no troops committed. By December 1940, Europe was largely at the mercy of Adolf Hitler and Germany's Nazi regime. With Germany's defeat of France in June 1940, Britain and its overseas Empire stood alone against the military alliance of Germany, Italy, and Japan. Winston Churchill, as Prime Minister of Britain, called for Roosevelt and the United States to supply them with armaments in order to continue with the war effort.\nThe 1939 New York World's Fair had celebrated Four Freedoms \u2013 religion, speech, press, and assembly \u2013 and commissioned Leo Friedlander to create sculptures representing them. Mayor of New York City Fiorello La Guardia described the resulting statues as the \"heart of the fair\". Later Roosevelt would declare his own \"Four Essential Freedoms\" and call on Walter Russell to create a \"Four Freedoms Monument\" that was eventually dedicated at Madison Square Garden in New York City.\nThey also appeared on the reverse of the AM-lira, the Allied Military Currency note issue that was issued in Italy during WWII, by the Americans, that was in effect occupation currency, guaranteed by the American dollar.\nDeclarations.\nThe Four Freedoms Speech was given on January 6, 1941. Roosevelt's hope was to provide a rationale for why the United States should abandon the isolationist policies that emerged from World War I. In the address, Roosevelt critiqued Isolationism, saying: \"No realistic American can expect from a dictator's peace international generosity, or return of true independence, or world disarmament, or freedom of expression, or freedom of religion\u2013or even good business. Such a peace would bring no security for us or for our neighbors. \"Those, who would give up essential liberty to purchase a little temporary safety, deserve neither liberty nor safety.\"\nThe speech coincided with the introduction of the Lend-Lease Act, which promoted Roosevelt's plan to become the \"arsenal of democracy\" and support the Allies (mainly the British) with much-needed supplies. Furthermore, the speech established what would become the ideological basis for America's involvement in World War II, all framed in terms of individual rights and liberties that are the hallmark of American politics.\nThe speech delivered by President Roosevelt incorporated the following text, known as the \"Four Freedoms\":\nThe declaration of the Four Freedoms as a justification for war would resonate through the remainder of the war, and for decades longer as a frame of remembrance. The Freedoms became the staple of America's war aims and the center of all attempts to rally public support for the war. With the creation of the Office of War Information (1942), as well as the famous paintings by Norman Rockwell, the Freedoms were advertised as values central to American life and examples of American exceptionalism.\nOpposition.\nThe Four Freedoms Speech was popular, and the goals were influential in postwar politics. However, in 1941 the speech received heavy criticism from anti-war elements. Critics argued that the Four Freedoms were simply a charter for Roosevelt's New Deal, social reforms that had already created sharp divisions within Congress. Conservatives who opposed social programs and increased government intervention argued against Roosevelt's attempt to justify and depict the war as necessary for the defense of lofty goals.\nWhile the Freedoms did become a forceful aspect of American thought on the war, they were never the exclusive justification for the war. Polls and surveys conducted by the United States Office of War Information (OWI) revealed that \"self-defense\", and vengeance for the attack on Pearl Harbor were still the most prevalent reasons for war.\nLimitations.\nIn a 1942 radio address, President Roosevelt declared the Four Freedoms embodied \"rights of men of every creed and every race, wherever they live.\"\nOn February 19, 1942, President Roosevelt authorized Japanese American internment and internment of Italian Americans with Executive Order 9066, which allowed local military commanders to designate \"military areas\" as \"exclusion zones,\" from which \"any or all persons may be excluded.\" This power was used to declare that all people of Japanese ancestry were excluded from the entire Pacific coast, including all of California and much of Oregon, Washington, and Arizona, except for those in internment camps. By 1946, the United States had incarcerated 120,000 individuals of Japanese descent, of whom about 80,000 had been born in the United States.\nUnited Nations.\nThe concept of the Four Freedoms became part of the personal mission undertaken by First Lady Eleanor Roosevelt regarding her inspiration behind the United Nations Declaration of Human Rights, General Assembly Resolution 217A. Indeed, these Four Freedoms were explicitly incorporated into the preamble to the Universal Declaration of Human Rights which reads, \"\"Whereas\" disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy the freedom of speech and belief and freedom from fear and want has been proclaimed the highest aspiration of the common people...\"\nDisarmament.\nFDR called for \"a world-wide reduction of armaments\" as a goal for \"the future days, which we seek to make secure\" but one that was \"attainable in our own time and generation.\" More immediately, though, he called for a massive build-up of U.S. arms production:\nFranklin D. Roosevelt Four Freedoms Park.\nThe Franklin D. Roosevelt Four Freedoms Park is a park designed by the architect Louis Kahn for the south point of Roosevelt Island. The Park celebrates the famous speech, and text from the speech is inscribed on a granite wall in the final design of the Park.\nAwards.\nThe Roosevelt Institute honors outstanding individuals who have demonstrated a lifelong commitment to these ideals. The Four Freedoms Award medals are awarded at ceremonies at Hyde Park, New York and Middelburg, Netherlands during alternate years. The awards were first presented in 1982 on the centenary of President Roosevelt's birth as well as the bicentenary of diplomatic relations between the United States and the Netherlands.\nAmong the laureates have been:\nNorman Rockwell's paintings.\nRoosevelt's speech inspired a set of four paintings by Norman Rockwell.\nPaintings.\nThe members of the set, known collectively as \"The Four Freedoms\", were published in four consecutive issues of \"The Saturday Evening Post\". The four paintings subsequently were displayed around the US by the United States Department of the Treasury.\nEssays.\nEach painting was published with a matching essay on that particular \"Freedom\":\nPostage stamps.\nRockwell's \"Four Freedoms\" paintings were reproduced as postage stamps by the United States Post Office in 1943, in 1946, and in 1994, the centenary of Rockwell's birth."}
